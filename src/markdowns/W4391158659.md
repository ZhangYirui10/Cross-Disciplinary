# Hallucination is Inevitable: An Innate Limitation of Large Language Models

Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

Skip to main content

We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate

# Hallucination is Inevitable: An Innate Limitation of Large Language Models

# Abstract

Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

&gt;
cs
&gt;
arXiv:2401.11817

Help | Advanced Search

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text

Search

# Hallucination is Inevitable: An Innate Limitation of Large Language Models

# Computer Science > Computation and Language

arXiv:2401.11817 (cs)

[Submitted on 22 Jan 2024 (v1), last revised 13 Feb 2025 (this version, v2)]

# Title: Hallucination is Inevitable: An Innate Limitation of Large Language Models

Authors:Ziwei Xu, Sanjay Jain, Mohan Kankanhalli

View a PDF of the paper titled Hallucination is Inevitable: An Innate Limitation of Large Language Models, by Ziwei Xu and 2 other authors

View PDF
HTML (experimental)

Abstract: Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.

|Subjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|
|---|---|
|Cite as:|arXiv:2401.11817 [cs.CL]|

# Submission history

From: Ziwei Xu [view email]
<br/>
[v1]
Mon, 22 Jan 2024 10:26:14 UTC (291 KB)<br/>
[v2]
Thu, 13 Feb 2025 08:11:25 UTC (307 KB)<br/>

Full-text links:
# Access Paper:

- View a PDF of the paper titled Hallucination is Inevitable: An Innate Limitation of Large Language Models, by Ziwei Xu and 2 other authors

View PDF
- HTML (experimental)
- TeX Source
- Other Formats

view license

|&nbsp;|(or  arXiv:2401.11817v2 [cs.CL] for this version)|
|---|---|
|&nbsp;|https://doi.org/10.48550/arXiv.2401.11817  <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"> </path> </svg> Focus to learn more   arXiv-issued DOI via DataCite|

cs.CL

&lt;&nbsp;prev

&nbsp; | &nbsp;

next&nbsp;&gt;
<br/>

new
|
recent
|
2024-01

Change to browse by:

cs<br class="is-hidden-mobile">
cs.AI<br class="is-hidden-mobile">
cs.LG<br class="is-hidden-mobile">

# References &amp; Citations

- NASA ADS
- Google Scholar
- Semantic Scholar

a
export BibTeX citation
Loading...

# BibTeX formatted citation

&times;

loading...

Data provided by:

# Bookmark

Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer (What is the Explorer?)

<input
id="connectedpapers-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/connectedpapers.js?20210617"
aria-labelledby="label-for-connected-papers">

Connected Papers Toggle

Connected Papers
(What is Connected Papers?)

<input
id="litmaps-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
aria-labelledby="label-for-litmaps">

Litmaps Toggle

Litmaps
(What is Litmaps?)

<input
id="scite-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
aria-labelledby="label-for-scite">

scite.ai Toggle

scite Smart Citations
(What are Smart Citations?)

Code, Data, Media

# Code, Data and Media Associated with this Article

<input
id="alphaxiv-toggle"

alphaXiv Toggle

alphaXiv (What is alphaXiv?)

Links to Code Toggle

CatalyzeX Code Finder for Papers (What is CatalyzeX?)

DagsHub Toggle

DagsHub (What is DagsHub?)

GotitPub Toggle

Gotit.pub (What is Gotit.pub?)

<input
id="huggingface-toggle"
data-script-url="/static/browse/0.3.4/js/huggingface.js"
type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">

Huggingface Toggle

Hugging Face (<a
href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?
)

<input
id="paperwithcode-toggle"
data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">

Links to Code Toggle

Papers with Code (<a
href="https://paperswithcode.com/" target="_blank">What is Papers with
Code?)

<input
id="sciencecast-toggle"
data-script-url="/static/browse/0.3.4/js/sciencecast.js"
type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">

ScienceCast Toggle

ScienceCast (<a
href="https://sciencecast.org/welcome" target="_blank">What is
ScienceCast?)

# Demos

Replicate Toggle

Replicate (What is Replicate?)

Spaces Toggle

Hugging Face Spaces (What is Spaces?)

Spaces Toggle

TXYZ (What is TXYZ?)

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower
(What are Influence Flowers?)

Core recommender toggle

CORE Recommender
(What is CORE?)

Author
Venue
Institution
Topic

<svg id="flower-graph-author"></svg>

<svg id="flower-graph-venue"></svg>

<svg id="flower-graph-inst"></svg>

About arXivLabs

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811">

</svg>

Which authors of this paper are endorsers? |
Disable MathJax (What is MathJax?)

mathjaxToggle();

About
Help

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon  filter-black" role="presentation">
# contact arXiv
<desc>Click here to contact arXiv</desc>

</svg>
Contact

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon  filter-black" role="presentation">
# subscribe to arXiv mailings
<desc>Click here to subscribe</desc>

</svg>
Subscribe

Copyright
Privacy Policy

arXiv Operational Status
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation">

</svg>

Get status notifications via

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">

</svg>email
or

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation">

</svg>slack

