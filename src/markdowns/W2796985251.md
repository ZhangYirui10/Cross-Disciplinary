# Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing

Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.

Skip to main content

# Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing

Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task.

In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background.

We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.

We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate

&gt;
cs
&gt;
arXiv:1804.03287

Help | Advanced Search

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text

Search

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white">
# open search

</svg>

GO

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu">
# open navigation menu

</svg>

# quick links

- Login
- Help Pages
- About

# Computer Science > Computer Vision and Pattern Recognition

arXiv:1804.03287 (cs)

[Submitted on 10 Apr 2018 (v1), last revised 6 Jul 2018 (this version, v3)]

Title: Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing

Authors:
Jian Zhao,
Jianshu Li,
Yu Cheng,
Li Zhou,
Terence Sim,
Shuicheng Yan,
Jiashi Feng

View a PDF of the paper titled Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing, by Jian Zhao and 6 other authors

View PDF

Abstract: Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human

parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.

|Comments:|The first three authors are with equal contributions|
|---|---|
|Subjects:|Computer Vision and Pattern Recognition (cs.CV)|
|Cite as:|arXiv:1804.03287 [cs.CV]|
|&nbsp;|(or arXiv:1804.03287v3 [cs.CV] for this version)|
|&nbsp;|https://doi.org/10.48550/arXiv.1804.03287  <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"> </path> </svg> Focus to learn more   arXiv-issued DOI via DataCite|
|Related DOI:||

# Submission history

From: Jian Zhao [view email]
<br/>
[v1]
Tue, 10 Apr 2018 00:41:26 UTC (13,622 KB)<br/>
[v2]
Mon, 4 Jun 2018 02:04:05 UTC (14,048 KB)<br/>
[v3]
Fri, 6 Jul 2018 05:49:40 UTC (14,048 KB)<br/>

Full-text links:
# Access Paper:

- View a PDF of the paper titled Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing, by Jian Zhao and 6 other authors

View PDF
- TeX Source
- Other Formats

view license

Current browse context: cs.CV

&lt;&nbsp;prev

new
|
recent
|
2018-04

Change to browse by:

cs<br class="is-hidden-mobile">

# References &amp; Citations

- NASA ADS
- Google Scholar
- Semantic Scholar

# DBLP - CS Bibliography

listing |
bibtex

Jian Zhao<br/>
Jianshu Li<br/>
Yu Cheng<br/>
Li Zhou<br/>
Terence Sim
&hellip;

a
export BibTeX

# Bookmark

Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer (What is the Explorer?)

<input
id="connectedpapers-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
aria-labelledby="label-for-connected-papers">

Connected Papers Toggle

Connected Papers
(What is Connected Papers?)

<input
id="litmaps-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
aria-labelledby="label-for-litmaps">

Litmaps Toggle

Litmaps
(What is Litmaps?)

<input
id="scite-toggle"
type="checkbox"
class="lab-toggle"
data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
aria-labelledby="label-for-scite">

scite.ai Toggle

scite Smart Citations
(What are Smart Citations?)

Code, Data, Media

# Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv (What is alphaXiv?)

Links to Code Toggle

CatalyzeX Code Finder for Papers (What is CatalyzeX?)

DagsHub Toggle

DagsHub (What is DagsHub?)

Got It Published Toggle

Got It Published (What is Got It Published?)

GotitPub Toggle

Gotit.pub (What is GotitPub?)

Huggingface Toggle

Hugging Face (What is Huggingface?)

Links to Code Toggle

Papers with Code (What is Papers with Code?)

ScienceCast Toggle

ScienceCast

ScienceCast

(What is ScienceCast?)

Demos

# Demos

Replicate Toggle

Replicate (What is Replicate?)

Spaces Toggle

Hugging Face Spaces (What is Spaces?)

TXYZ Toggle

TXYZ (What is TXYZ?)

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower (What are Influence Flowers?)

Core recommender toggle

CORE Recommender (What is CORE?)

Author
Venue
Institution
Topic

<svg id="flower-graph-author"></svg>

<svg id="flower-graph-venue"></svg>

<svg id="flower-graph-inst"></svg>

<svg id="flower-graph-topic"></svg>

About arXivLabs

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811">

</svg>

Which authors of this paper are endorsers? |
Disable MathJax (What is MathJax?)

mathjaxToggle();

About
Help

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">
# contact arXiv
<desc>Click here to contact arXiv</desc>

</svg>
Contact

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">
# subscribe to arXiv mailings
<desc>Click here to subscribe</desc>

</svg>
Subscribe

Copyright
Privacy Policy

Web Accessibility Assistance

arXiv Operational Status
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation">

</svg>

Get status notifications via

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">

</svg>email or

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation">

</svg>slack

</footer>
  </div>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>
</body>
</html>

