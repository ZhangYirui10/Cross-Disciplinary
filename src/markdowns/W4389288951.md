# The Evolution of AI Governance

Simon Chesterman1, Yuting Gao2, Jungpil Hahn2, and Sticher Valerie2

1AI Singapore

2Affiliation not available

December 7, 2023

# Abstract

Many companies and a growing number of governments now have guides, frameworks, or principles claiming to govern their use of AI. Seven years ago, virtually none did. This article presents original research on documents produced by 193 countries and the top 100 companies by market capitalization. A key shift occurred in 2016 when the Cambridge Analytica scandal showed the potential harms of misused AI. The widespread use of large language models such as ChatGPT beginning in late 2022 is further increasing calls for governance of the AI space. Analyzing the evolving practice of releasing such documents and the language that they use offers important insights into how norms around AI are spreading and changing – and where they might go next.

# The Evolution of AI Governance

# Working paper version November 2023

# Simon Chesterman,* Yuting Gao,† Jungpil Hahn,‡ Valerie Sticher§

# ABSTRACT

Many companies and a growing number of governments now have guides, frameworks, or principles claiming to govern their use of AI. Seven years ago, virtually none did. This article presents original research on documents produced by 193 countries and the top 100 companies by market capitalization. A key shift occurred in 2016 when the Cambridge Analytica scandal showed the potential harms of misused AI. The widespread use of large language models such as ChatGPT beginning in late 2022 is further increasing calls for governance of the AI space. Analyzing the evolving practice of releasing such documents and the language that they use offers important insights into how norms around AI are spreading and changing — and where they might go next.

# Keywords

K.4.1 Public Policy Issues, K.4.1.c Ethics, K.4.1.d Human safety, K.4.1.g Regulation

Governance of new technologies often vacillates between irrelevance and excess. Regulating too little or too late makes governance an afterthought; overregulation runs the risk of constraining innovation or driving it elsewhere. It is striking that the development of artificial intelligence (AI) for most of its history saw few serious discussions of governance at all. Indeed, as recently as 2011 there were serious calls for limited immunity to be granted to developers, lest the possibility of lawsuits deter investment. It was reported that Korea had considered using them as the basis for a proposed Robot Ethics Charter. That was one of many attempts to codify norms governing “robots” since the turn of the century, accelerating in the wake of the First International Symposium on Roboethics in Sanremo, Italy, in 2004. The European Robotics Research Network produced its ‘Roboethics Roadmap’ in 2006, while the first multidisciplinary set of principles for robotics was adopted at a ‘Robotics Retreat’ held by two British Research Councils in 2010.

# Authors

*Simon Chesterman is a Professor in the Faculty of Law at the National University of Singapore and Senior Director of AI Governance at AI Singapore, 117602, Republic of Singapore. His research interests include regulation and governance of artificial intelligence. Chesterman received his D.Phil. in law from the University of Oxford. Contact him at chesterman@nus.edu.sg

† Yuting Gao is an Assistant Professor in the Department of Information & Operations Management at ESCP Business School, Madrid, 28035, Spain. Her research interests include human-computer interaction, human-AI interaction, and the economic and social impacts of the Internet. Gao received her Ph.D. in information systems and analytics from the National University of Singapore. Contact her at yg@escp.eu

‡ Jungpil Hahn is a Professor at the School of Computing of National University of Singapore and Deputy Director of AI Governance at AI Singapore, 117602, Republic of Singapore. His research interests include open innovation, organizational learning and knowledge management, and human-computer interaction. He received his Ph.D. in information and decision sciences from the University of Minnesota. Contact him at jungpil@nus.edu.sg

§ Valerie Sticher is a visiting scholar at the School of Advanced International Studies, Johns Hopkins University, Washington, DC 20001, United States. Her current research, funded by the Swiss National Science Foundation, examines the impacts of artificial intelligence on strategic decision-making in armed conflicts. She received her Ph.D. in governance and global affairs at the University of Leiden. Contact her at sticherv@gmail.com

or a company had released documents containing AI governance principles. Much has happened since. The years after 2016 saw a proliferation of guides, frameworks, and principles focused specifically on AI. Some were the product of conferences or industry associations, notably the Partnership on AI’s Tenets (2016), the Future of Life Institute’s Asilomar AI Principles (2017), the Beijing Academy of Artificial Intelligence’s Beijing AI Principles (2019), and the Institute of Electrical and Electronics Engineers (IEEE)’s Ethically Aligned Design (2019). Others were drafted by individual companies, including Microsoft’s Responsible AI Principles, IBM’s Principles for Trust and Transparency, and Google’s AI Principles — all published in the first half of 2018. Two factors driving this proliferation appear to have been the machine-learning (ML) revolution of the 2010s and the Cambridge Analytica scandal of 2016. The latter, in particular, showed that potential harms of misused AI went far beyond biased decisions or errant autonomous vehicles to include impacting elections in the most powerful country on the planet.¹⁶

Our data show that the introduction of AI governance principles is a relatively recent phenomenon: in 2016, no company and only one country — the United States — had released official documents explicitly governing AI. But the number quickly increased in the years that followed. Generally, Western Europe and other countries were early adopters in defining relevant AI governance principles, with the rest of the world following suit.7,10,¹²

Analysis of when, how, and why to govern AI systems has also become more sophisticated, drawing on regulatory theory as well as more nuanced understanding of what machine learning models can (and cannot) do.¹ Key questions include the scope of application (such as how “AI” is defined), the precise harms to be avoided, and how any governance regime should balance the opportunities afforded by AI against the risks.2,¹¹

Today, an ever-expanding number of companies and countries have adopted documents setting out principles to govern the development and use of AI. Early movers may have been technology companies in Western countries, but this has quickly become a global phenomenon. As the practice of adopting such documents spreads — by sector and by country — overlapping language has converged on certain key principles with remarkable consistency. At the same time, there are important divergences in what those principles are understood to mean.18,¹⁹ This article analyzes the evolution of these AI governance principles, focusing on the spread of similar terms around the world and the differing ways in which those terms are understood — in theory as well as in practice. Although this field is rapidly developing, an analysis of the first half-dozen years of serious AI governance debates provides valuable insights into the development, spread, and future directions of AI norms.

# 1 The rise of AI governance

To gather representative data, we examined records of 193 member states of the United Nations and the world’s 100 largest companies by market capitalization.⁹ We first conducted a systematic search on Google** to determine whether a government or a company had released documents containing AI governance principles. The following Google search terms were used: (AI OR artificial intelligence) AND (principles OR guidelines OR recommendations) AND ("[country/company name]") respectively. This procedure was developed iteratively after several rounds of pilot searches.

|100%|Western Europe and others (28)|
|---|---|
|90%|Eastern Europe (23)|
|80%|Asia-Pacific (55)|
|70%|Latin America & Caribbean (33)|
|60%|Africa (54)|
|50%| |
|40%| |
|30%| |
|20%| |
|10%| |
|0%|2016 2017 2018 2019 2020 2021|

Figure 1: Percentage of countries in a region that have released at least one AI Governance Principles document, total number of countries in each region in parentheses.

|100%|Technology (21)|
|---|---|
|90%|Financials (17)|
|80%|Health Care (19)|
|70%| |
|60%| |
|50%|Others (43)|
|40%| |
|30%| |
|20%| |
|10%| |
|0%|2016 2017 2018 2019 2020 2021|

Figure 2: Percentage of companies in a sector that have released at least one AI Governance Principles document, total number of companies in each sector in parentheses.

specifically, it may refer to one or more of the following three aspects of the design, development, and/or deployment of AI systems:

Despite this clear trend, many countries and companies still lack official documents outlining AI governance principles. By the start of 2022, less than one in three governments (60 out of 193 states), and only one in five of the largest 100 companies (21 out of 100 companies) had released documents governing their use of AI systems — a proportion likely to be considerably lower in smaller companies. Even in the technology sector, less than half of the largest technology companies (10 out of 21) had released an AI governance document by 2022, with the ratio much lower in sectors like finance (23.53%= 4/17) or health care (15.79%=3/19), where AI plays an increasingly important role.

# Accountability

The accountability principle is concerned with the regulation of AI systems through legal governance, risk management, and compliance mechanisms. This principle seeks to minimize the risks associated with AI and provide appropriate remedies for losses.

# Transparency/explainability

The transparency/explainability principle is generally concerned with facilitating human understanding of AI systems, including their algorithms and decisions. More specifically, it may refer to one or more of the following three aspects of the design, development, and/or deployment of AI systems:

- Transparency: ensuring that AI systems are designed, developed and deployed in a manner that facilitates human oversight through openness to external scrutiny.
- Explainability: ensuring that AI systems are designed, developed, and deployed in a manner that facilitates interpretability by laypersons. This allows affected individuals to understand the general workings of an AI system and the decision-making process behind specific outcomes.
- Openness: encouraging the open sharing of data, as well as open-source research and collaboration in the design and development of AI systems.

# 2 Convergence, but in name only?

The documents examined vary greatly in formality and format. Some are fully-fledged AI policies; others are simple lists of AI principles on a company’s website. Yet remarkable consistency emerged in the terms that they use for these principles. To quantify this trend, we coded all collected documents that were available in English. We used existing literature and the content of the coded documents to iteratively develop a taxonomy for the principles, coalescing around fairness, accountability, transparency/explainability, ethics/human-centricity, safety/security, and privacy. Box 1 offers a description of each and their potential content.

# Box 1: AI governance principles

# Fairness

The fairness principle is generally concerned with bias arising from AI systems and the inclusiveness of AI systems. More specifically, it may refer to one or more of the following three aspects of the design, development, and/or deployment of AI systems:

- Non-discrimination: ensuring that the design, development, and deployment of AI systems do not engender unfair biases or exacerbate existing ones.
- Inclusiveness in impact: ensuring that the benefits of AI development and deployment benefit society at large rather than a narrow subset of individuals.
- Inclusiveness in design: involving individuals from diverse backgrounds in AI design and development.

# Ethics/human-centricity

The ethics/human-centricity principle is concerned with the alignment of AI systems with human values. More specifically, it may refer to one or more of the following three aspects of the design, development, and/or deployment of AI systems:

- Benefiting humans: ensuring that AI systems are designed, developed, and deployed for the good of humankind. This encompasses calls for human-aligned, non-maleficent AI.
- Human control of technology: ensuring that humans remain in control and have effective oversight of AI systems.
- Human rights compliance: ensuring that AI design, development, and deployment comply with international human rights frameworks and norms.

# Safety/security

The safety/security principle is concerned with the reliable operation of AI systems and their vulnerability to external threats. It may refer to either or both of the following two aspects:

- Safety: ensuring that AI systems operate reliably and do not harm living beings. This dimension emphasizes

the importance of guarding against possible misuse of very similar for safety/security, fairness, transparency/explainability, and ethics/human-centricity. Concerns for privacy appear to develop equally among countries and companies (Figure 5).

# Privacy

Privacy is concerned with ensuring that data is appropriately handled in the design, development, and deployment of AI systems. This principle emphasizes user control in relation to data and compliance with existing privacy frameworks and norms.

# Other AI governance principles

Other AI governance principles also appear in various government and company documents, in particular:

- Sustainability: considering the long-term impact of AI development and deployment on humankind and the environment. This includes the impact of AI systems on future generations and any extinction risk that may be posed by AI.
- Professional responsibility of developers: highlighting ethical obligations of individuals involved in the design, development, and deployment of AI systems, with an emphasis on the role of developers in building safe and beneficial AI.
- Technical competence: ensuring the technical competence of individuals involved in the design, development, and deployment of AI systems.

Our data reveal convergence over time at the level of principle. From the first releases to 2021, documents became more similar in terms of the principles they included. By 2021, more than 50% of documents included all six key principles that we coded (Figure 3). While principles like privacy and ethics/human-centricity were less often included in the earlier years, they became more common from 2018 onward, as official documents on AI governance spread from the West to the rest of the world, and from tech companies to other sectors.

# Convergence in form

Convergence in form may not, however, mean convergence in substance. What do different governments and companies actually mean when they refer to a specific principle? As we elaborate in box 1, several principles can refer to diverse aspects of the design, development, and deployment of AI. For example, what we capture under the broad umbrella term “fairness” may refer to non-discrimination, inclusiveness in impact, or inclusiveness in design. Our analysis nonetheless shows that government and company actors are all more likely to mean non-discrimination when including “fairness” as a principle (Figure 6).

# Comparison between governments and companies

A more detailed comparison between governments and companies shows that company documents tend to be more comprehensive than government documents, although the gap is closing. Figure 4 illustrates this for the principles of accountability: the patterns are

|100%|90%|80%|70%|60%|50%|40%|30%|20%|10%|0%|
|---|---|---|---|---|---|---|---|---|---|---|
| |2016|2017|2018|2019|2020|2021| | | | |

Figure 3: Percentage of AI Governance Principles documents that refer to a specific principle (countries and companies together).

|Accountability|Transparency/explainability|Fairness|Ethics|Privacy|Others|Safety/security| | | | |
|---|---|---|---|---|---|---|---|---|---|---|
|100%|90%|80%|70%|60%|50%|40%|30%|20%|10%|0%|
|2016|2017|2018|2019|2020|2021| | | | | |

Figure 4: Percentage of documents that refer to accountability.

# Non-discrimination

# Benefitting humans

|100%|100%|
|---|---|
|80%|80%|
|60%|60%|
|40%|40%|
|20%|20%|
|0%|0%|

Inclusiveness design

Inclusiveness impact

Country

Company

Figure 6: Diverse aspects of fairness (government versus company documents).

In other areas, there is divergence between governments and companies. In the area of transparency/explainability, for example, many government documents highlight “openness” as an aspect of transparency and explainability, whereas companies are less likely to include it within their principles (Figure 7). Similarly, governments are more likely than companies to link compliance with human rights obligations under the auspices of ethics/human-centricity (Figure 8).

# Survival of the fittest

As AI systems play ever greater roles across society and the economy — affecting the way we live, work, and play — countries and companies will see greater demand for accountability and appropriate levels of transparency in how those systems are deployed. Those demands are certain to grow as generative AI models play a greater role in the wider culture, as seen in the popularity of image and text generators such as Stable Diffusion and ChatGPT — and the controversies surrounding their early (some would say premature) deployment.

AI governance policies have emerged as a means of showing citizens and consumers that these concerns are acknowledged. For some governments and companies, they have also offered useful guardrails to limit certain potentially harmful deployments of AI or raise the costs for new entrants into the market. All of this is separate, however, from the impact that such guides, frameworks, and principles have in practice. Despite the fact that many such principles began in Western countries or technology companies, their spread around the world and across sectors has seen countervailing trends: convergence around similar language, but with diverse interpretations of what that language means.

As the first efforts to move from governance by principle to regulation by law begin, most prominently in the European Union with its draft AI Act, governments will need to decide whether, when, and how to operationalize these norms. At the same time, some companies may find that there is a market value to embracing “responsible AI”, above and beyond compliance or minimization of legal exposure. Against this, however, the rush to deploy large-language models in 2023 — with accounts of down-

# Transparency

100%80%60%40%20%0%
Openness

Explainability

Country

Company

Figure 7: Diverse aspects of transparency/explainability (government versus company documents).

As the first efforts to move from governance by principle to regulation by law begin, most prominently in the European Union with its draft AI Act, governments will need to decide whether, when, and how to operationalize these norms. At the same time, some companies may find that there is a market value to embracing “responsible AI”, above and beyond compliance or minimization of legal exposure. Against this, however, the rush to deploy large-language models in 2023 — with accounts of down-

sizing or marginalization of ethics and responsible AI teams —

# 10. Editorial, Cambridge Analytica controversy must spur researchers to update data ethics.

Nature 555 (2018), 559–560.

The flaw in many of these debates has long been the assumption that an updated version of Asimov’s laws would “solve” the problem of AI risk.¹⁷ The problem is that Asimov was a much better author than legislator; indeed, if his laws had worked, his literary career would have been short. In fact, even in the first story where he introduced the laws, they did not work.

The challenge, now, is having reached broad agreement on principles that should guide AI design, development, and deployment in theory, it is necessary to apply this to specific use cases to show what those principles actually mean in practice. An important part will be institutionalizing governance and regulatory frameworks to ensure that the words increasingly and consistently used by governments and corporate actors lead to action.2,¹¹

# Acknowledgements

Many thanks to Rishika Madan, Khin Yadanar Oo, Suzuki Tomoe, Yu Shi Jie, and Yonggang Li for invaluable research assistance in gathering and analyzing the data.

# References

1. Alfonsi, C. Taming tech giants requires fixing the revolving door. Kennedy School Review 19 (2019), 166-170.
2. Almeida, V., Schertel Mendes, L., Doneda, D. On the development of AI governance frameworks. IEEE Internet Computing 27:1 (2023), 70-74.
3. Auld, G., Casovan, A., Clarke, A. & Faveri, B. Governing AI through ethical standards: Learning from the experiences of other private governance initiatives. Journal of European Public Policy 29:11 (2022), 1822-1844.
4. Balkin, J.M., The three laws of robotics in the age of big data. (2017) 78 Ohio State Law Journal 78 (2017), 1217-1241.
5. Bostrom, N. Superintelligence: Paths, dangers, strategies, Oxford (2014).
6. Calo, M.R. Open robotics. Maryland Law Review. 70 (2011) 571-613 (2011).
7. Chesterman, S. We, the robots? Regulating artificial intelligence and the limits of the law, Cambridge (2021).
8. Collingridge, D. The social control of technology, Frances Pinter (1980).
9. Companies Market Cap, Largest Companies by Market Cap https://companiesmarketcap.com (data as of May 2022).
10. Gasser, U. & Almeida, V.A.F. A layered model for AI governance. IEEE Internet Computing 21:6 (2017), 58-62.
11. Hu, M. Cambridge Analytica’s black box. Big Data & Society 7(2) (2020).
12. Jobin, A., Ienca, M. & Vayena, E. The global landscape of AI ethics guidelines. Nature Machine Intelligence 1 (2019), 389-399.
13. de Laat, P. B. Companies committed to responsible AI: From principles towards implementation and regulation?. Philosophy & technology, 34 (2021), 1135-1193.
14. Munn, L. The uselessness of AI ethics. AI Ethics (2022). https://doi.org/10.1007/s43681-022-00209-w
15. Papyshev, G., & Yarime, M. The state’s role in governing artificial intelligence: development, control, and promotion through national strategies. Policy Design and Practice (2023), 1-24.
16. Pasquale, F. New laws of robotics, Harvard (2020).
17. Srikumar, M., Finlay, R., Abuhamad, G. et al. Advancing ethics review practices in AI research. Nature Machine Intelligence 4 (2022), 1061–1064.
18. Theodorou, A., Dignum, V. Towards ethical and socio-legal governance in AI. Nature Machine Intelligence 2 (2020), 10–12.

