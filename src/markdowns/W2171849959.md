# UC Santa Barbara

# UC Santa Barbara Previously Published Works

# Title

Discriminant subspace analysis: A Fukunaga-Koontz approach

# Permalink

https://escholarship.org/uc/item/9k52756v

# Journal

IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(10)

# ISSN

0162-8828

# Authors

Zhang, Sheng
Sim, Terence

# Publication Date

2007-10-01

# Peer reviewed

eScholarship.org

Powered by the California Digital Library

University of California

# Discriminant Subspace Analysis: A Fukunaga-Koontz Approach

# Sheng Zhang, Member, IEEE, and Terence Sim, Member, IEEE

Abstract—The Fisher Linear Discriminant (FLD) is commonly used in pattern recognition. It finds a linear subspace that maximally separates class patterns according to the Fisher Criterion. Several methods of computing the FLD have been proposed in the literature, most of which require the calculation of the so-called scatter matrices. In this paper, we bring a fresh perspective to FLD via the Fukunaga-Koontz Transform (FKT). We do this by decomposing the whole data space into four subspaces with different discriminabilities, as measured by eigenvalue ratios. By connecting the eigenvalue ratio with the generalized eigenvalue, we show where the Fisher Criterion is maximally satisfied. We prove the relationship between FLD and FKT analytically and propose a unified framework to understanding some existing work. Furthermore, we extend our theory to the Multiple Discriminant Analysis (MDA). This is done by transforming the data into intraclass and extraclass spaces, followed by maximizing the Bhattacharyya distance. Based on our FKT analysis, we identify the discriminant subspaces of MDA/FKT and propose an efficient algorithm, which works even when the scatter matrices are singular or too large to be formed. Our method is general and may be applied to different pattern recognition problems. We validate our method by experimenting on synthetic and real data.

Index Terms—Discriminant subspace analysis, Fukunaga-Koontz transform, pattern classification.

# 1 INTRODUCTION

IN recent years, discriminant subspace analysis has been extensively studied in computer vision and recognition. It has been widely used for feature extraction and dimensionality reduction in face recognition [2], [3], [15] and text classification [4]. One popular method is the Fisher Linear Discriminant (FLD), also known as the Linear Discriminant Analysis (LDA) [5], [7]. It tries to find an optimal subspace such that the separability of two classes is maximized. This is achieved by minimizing the within-class distance and maximizing the between-class distance simultaneously. To be more specific, in terms of the between-class scatter matrix Sb and the within-class scatter matrix Sw, the Fisher Criterion can be written as

JF( ) = trace( TSw ) 1( >Sb );

where  is a linear transformation matrix. By maximizing the criterion JF, FLD finds the subspaces in which the classes are most linearly separable. The solution that maximizes JF is a set of the first eigenvectors f ig that must satisfy

Sb = Sw;

This is called the generalized eigenvalue problem. The discriminant subspace is spanned by the generalized eigenvectors. The discriminability of each eigenvector is measured by the corresponding generalized eigenvalue, for example,

In our previous work, we proposed a better solution by applying the Fukunaga-Koontz Transform (FKT) to the LDA problem. Based on the eigenvalue ratio of FKT, we decomposed the whole data space into four subspaces. This revealed the relationship between LDA, FKT, and GSVD and allowed us to correctly maximize JF even when Sw is singular.

In this paper, we extend our previous work in two ways: First, we present a unified framework for understanding other LDA-based methods. This provides valuable insights on how to choose the discriminant subspaces of the LDA problem. Second, we propose a new approach to multiple discriminant analysis.

. S. Zhang is with the Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA 93106-9560. E-mail: zhangs@ece.ucsb.edu.

. T. Sim is with the School of Computing, National University of Singapore, 3 Science Drive 2, Singapore 117543. E-mail: tsim@comp.nus.edu.sg.

Manuscript received 13 Feb. 2006; revised 28 Aug. 2006; accepted 13 Dec. 2006; published online 18 Jan. 2007. Recommended for acceptance by M. Figueiredo. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-0150-0206. Digital Object Identifier no. 10.1109/TPAMI.2007.1089.

0162-8828/07/$25.00 © 2007 IEEE Published by the IEEE Computer Society

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

discriminant analysis (MDA). This is done by casting the multiclass problem into a two-class one and by maximizing the Bhattacharyya distance (which is the error bound of the Bayes Classifier [5]) rather than the Fisher Criterion. Then, the discriminant subspace is obtained algebraically via FKT. This means that our method can find the global optimum directly (no iteration required), which is not the case in [6]. For completeness, in this paper, we include details of our previous work [22] as well.

To summarize, our work has three main contributions:

1. We present a unifying framework to understand different methods, namely, LDA, FKT, and GSVD. To be more specific, we show that, for the LDA problem, GSVD is equivalent to FKT and the generalized eigenvalue of LDA is equal to both the eigenvalue ratio of FKT and the square of the generalized singular value of GSVD.
2. We prove that our approach is useful for general pattern recognition. Our theoretical analyses demonstrate how to choose the best subspaces for maximum discriminability and unify other subspace methods such as Fisherface, PCA+NULL space, LDA/QR, and LDA/GSVD.
3. We further propose a new criterion to handle MDA, derived from the Bhattacharyya distance. Because the Bhattacharyya distance upper bounds the Bayes error [5], this new criterion is theoretically superior to the Fisher Criterion, which is not related to the Bayes error in general.

The rest of this paper is organized as follows: Section 2 reviews related work, that is, PCA, LDA, Fisherface, PCA+NULL Space, LDA/QR, and LDA/GSVD. We discuss FKT in Section 3, where discriminant subspace analysis based on FKT is also presented. In Section 4, we show how to unify some LDA-based methods based on our theory. Moreover, we demonstrate how to handle the multiclass problem by FKT in Section 5. We apply our theory to the classification problem on synthetic and real data in Section 6 and conclude our paper in Section 7.

# 2 RELATED WORK

Notation. Let A = {a1, ..., aN}, ai ∈ ℝD denote a data set of given D-dimensional vectors. Each data point belongs to exactly one of C object classes {L1, ..., LC}. The number of vectors in class Li is denoted by Ni; thus, N = ΣNi. Observe that, for high-dimensional data, for example, face images, generally, C << N << D. The between-class scatter matrix Sb, the within-class scatter matrix Sw, and the total scatter matrix St are defined as follows:

Sb = C Σ Ni (mi - m)(mi - m)T = HbHTSw = C Σ (aj - mi)(aj - mi)T = HwHTSt = N (ai - m)(ai - m)T = HtHTSt = Sb + Sw

# 2.1 Principal Component Analysis (PCA)

Principal Component Analysis (PCA) [13] is one of the well-known subspace methods for dimensionality reduction. It is the optimal method for statistical pattern representation in terms of the mean square error. PCA can be readily computed by applying the eigendecomposition on the total scatter matrix, that is, St = UDUT. By keeping the eigenvectors (principal components) corresponding to the largest eigenvalues, we can compute the PCA projection matrix. To solve the appearance-based face recognition problem, Turk and Pentland [18] proposed “Eigenface” by using PCA. Note that PCA is optimal for pattern representation, not necessarily for classification [5]. LDA [5], however, is another well-known subspace method designed for pattern classification.

# 2.2 Linear Discriminant Analysis (LDA)

Given the data matrix A, which can be divided into C classes, we try to find a linear transformation matrix W ∈ ℝD×d, where d < D. This maps a high-dimensional data to a low-dimensional space. From the perspective of pattern classification, LDA aims to find the optimal transformation W such that the projected data are well separated.

Regarding pattern classification, usually, there are two types of criteria that are used to measure the separability of classes [7]. One is a family of criteria that gives the upper bound on the Bayes error, for example, Bhattacharyya distance. The other is based on a family of functions of scatter matrices. As shown in (1), the Fisher Criterion belongs to the latter one. Moreover, the solution of the criterion is the generalized eigenvector and eigenvalue of the scatter matrices (see (2)). However, if Sw is nonsingular, it can be solved by the generalized eigendecomposition: Sw-1Sb = λ. Otherwise, Sw is singular and we circumvent this by methods such as Fisherface [2], PCA+NULL Space [10], LDA/QR [20], and LDA/GSVD [9].

# 2.3 Fisherface

To handle face recognition under different lightings, Belhumeur et al. [2] proposed “Fisherface,” which is an application of LDA. In the Fisherface method, PCA is performed first so as to make Sw nonsingular, followed by LDA. This means that Fisherface = LDA + PCA. However, there exist at least two problems: 1) During PCA, it is not clear how many dimensions should be kept so that Sw is nonsingular and 2) to avoid the singularity of Sw, some...

1734 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

directions/eigenvectors (corresponding to the small non-zero eigenvalues) are thrown away in the PCA step, which may contain discriminant information [21].

# 2.4 PCA + NULL Space

Considering that the null space of Sw contains discriminant information, Huang et al. [10] first remove the null space of St. This is the intersection of null space of Sb and Sw and has been proven to be useless for discrimination [10]. It can be done by applying PCA first, followed by computing the principal components of Sb within the null space of Sw. More precisely, it is realized in three steps:

1. Step 1. Remove the null space of St: Eigendecompose St, St = UDU>, and U is the set of eigenvectors corresponding to the nonzero eigenvalues. Let S0 = UTSwU and S0 = UTSb.

Step 2. Compute the null space of Sw: Eigendecompose S0 and let Q? be the set of eigenvectors corresponding to the zero eigenvalues. Let S00 = QTS0Q?.
2. Step 3. Remove the null space of Sb if it exists: Eigendecompose S00 and keep the set of eigenvectors corresponding to the nonzero eigenvalues.

The key difference between PCA+NULL Space and Fisherface is in the first step: PCA+NULL Space removes the eigenvectors with zero eigenvalues, whereas Fisherface removes eigenvectors corresponding to zero and nonzero eigenvalues.

# 2.5 LDA/QR

In [20], Ye and Li proposed a two-stage LDA method, namely, LDA/QR. It not only overcomes the singularity problems of LDA but also achieves computational efficiency. This is done by applying QR decomposition on Hb first, followed by LDA. To be more specific, it is realized in two steps:

1. Step 1. Apply QR decomposition on Hb: Hb = QR, where Q ∈ ℝD×rb has orthogonal columns that span the space of Hb and R ∈ ℝrb×C is an upper triangular matrix. Then, define Sb = eQ SbQ and Sw = Q SwQ.
2. Step 2. Apply LDA on Sb and Sw: Keep the set of eigenvectors corresponding to the smallest eigenvalues of Sb Sw.

Note that, to reduce computational load, QR decomposition is employed here, whereas, in the Fisherface and PCA+NULL space methods, the subspace is obtained by using eigendecomposition.

# 2.6 LDA/GSVD

The GSVD was originally defined by Van Loan [14] and then Page and Saunders [16] extended it to handle any two matrices with the same number of columns. We will briefly review the mechanism of GSVD, using LDA as an example. Howland and Park [9] extended the applicability of LDA to the case when Sw is singular. This is done by using simultaneous diagonalization of the scatter matrices via the GSVD [8]. First, to reduce computational load, Hb and Hw are used instead of Sb and Sw. Then, based on GSVD, there exist orthogonal matrices Y ∈ ℝC and Z ∈ ℝN and a nonsingular matrix X ∈ ℝd×d such that...

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

0.8

L0.6 2 3 4

0.4

0.2

W Dimension

Fig. 1. The whole data space is decomposed into four subspaces via FKT. In U?, the null space of St, there is no discriminant information. In U, b + w = 1. Note that we represent all possible subspaces, but, in real cases, some of these subspaces may not be available.

the dominant eigenvector of e T S1 and vice versa. The dominant eigenvectors therefore form a subspace in which the two classes are separable. Classification can then be done by, say, picking the nearest neighbor (NN) in this subspace.

Recently, it was proven that, under certain conditions, FKT is the best linear approximation to a quadratic classifier [11]. Interested readers may refer to [7] and [11] for more details.

# 3.2 LDA/FKT

Generally speaking, for the LDA problem, there are more than two classes. To handle the multiclass problem, we replace the autocorrelation matrices S1 and S2 with the scatter matrices Sb and Sw. Since Sb, Sw, and St are p.s.d. and symmetric and St = Sb + Sw, we can apply FKT on Sb, Sw, and St, which is called LDA/FKT hereafter in this paper. The whole data space is decomposed into U and U? (Fig. 1). On one hand, U? is the set of eigenvectors corresponding to the zero eigenvalues of St. This has been proven to be the intersection of the null spaces of Sb and Sw and contains no discriminant information [10]. On the other hand, U is the set of eigenvectors corresponding to the nonzero eigenvalues of St. It contains discriminant information.

Based on FKT, e > e > Sb = P SbP and Sw = P SwP share the same eigenspace and the sum of two eigenvalues corresponding to the same eigenvector is equal to 1.

# Lemma 1.

For the LDA problem, GSVD is equivalent to FKT, with X = [UD 1/2V; U?, b = > b, and w = > w, where X, b, and w are from GSVD (10), (11), and U, D, V, U?, and are matrices from FKT (16), (17), (18).

Sb = V bV ; (16)

Sw = V wV ; (17)

I = b + w; (18)

Now, based on the above lemma, we can investigate the relationship between the eigenvalue ratio of FKT and the generalized eigenvalue of the Fisher Criterion JF.

# Theorem 1.

If is the solution of (2) (the generalized eigenvalue of Sb and Sw) and b and w are the eigenvalues after applying FKT on Sb and Sw, then = b, where b + w = 1.

Proof. w

Based on GSVD, it is easy to verify that

Sb = HbHb = X b0 0 X : (19)

# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

# TABLE 1

Comparison between Different Methods: N Is The Number of Training Samples, D Is The Dimension, and C Is The Number of Classes

|Method|Time complexity|Space complexity|Discriminant subspaces|Remarks|
|---|---|---|---|---|
|PCA|O(DN2)|O(DN)|N/A|Optimal for pattern representation|
|Fisherface|O(DN2)|O(DN)|2 and 3|Sub-optimal|
|PCA+NULL|O(DN2)|O(DN)|1|for the Fisher|
|LDAIQR|O(DNC)|O(DC)|1 and 2|Criterion|
|LDAIGSVD|O((N + C)PD)|O(DN)|1, 2, 3 and 4|Optimal for the|
|LDAFKT|O(DN2)|O(DN)|1, 2 and 3|Fisher Criterion|
|MDAFKT|O(DN2)|O(DN)|1, 2 and 3|Optimal for Bhattacharyya distance|

For discriminant subspaces, please refer to Fig. 1.

According to Lemma 1,  b ¼  > b, thus, equal to the generalized eigenvalue , the measure of discriminability. According to Fig. 1, Subspace 1, with the infinite eigenvalue ratio b , is the most discriminant subspace, followed by Subspace 2 and Subspace 3. However, Subspace 4 contains no discriminant information and can be safely thrown away. Therefore, the eigenvalue ratio  ᵇ or the generalized singular value  2   w suggests how to choose the most discriminant subspaces.

Since Sb  ¼  Sw , X >  b  0  X 1  ¼  X >  w      0  X 1 :

Letting v ¼ X 1 , and multiplying X> on both sides, we obtain the following:

b   0  v ¼     w   0  v:

If we add   0b  0 on both sides of the above equation, then ð1 þ  Þ   b   0  v ¼    I  0  v:

This means that ð1 þ  Þ  ¼  , which can be rewritten as  ¼ ð1     Þ ¼         because   þ  ¼ 1. Now, we can observe that   ¼  .

Corollary 1. If   is the generalized eigenvalue of Sb and Sw, and                     are the solutions of (10) and (11), and  = is the generalized singular value of the matrix pair ðH  ;H              Þ, then  ¼  ² , where  2 þ  2 ¼ 1.

Proof. In Lemma 1, we have proven that   ¼  > and  ¼  > , that is,    ¼  2 and    ¼  2  b         b  b . According to Theorem 1, we observe that   ¼  b . Therefore, it is easy to see that   ¼  ² . Note that     w is the generalized singular value of ðH  ²  b;HwÞ by GSVD and                          is the generalized eigenvalue of ðS ;S Þ.

The corollary   b  w suggests how to evaluate discriminant subspaces of LDA/GSVD. Actually, Howland and Park in [9] applied the corollary implicitly, but, in this paper, we explicitly connect the generalized singular value with the generalized eigenvalue, the measure of discriminability.

Based on our analysis, the eigenvalue ratio      b and the square of the generalized singular value  2       w are both.

# Fig. 2. Algorithm 1: Apply QR decomposition to compute LDA/FKT.

ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH
       and perform   eigendecomposition   on  the                   rt   rt
              e 1e
       matrix St                   Sb. Since rt < N, C   D, the most inten-
       sive step is Line 2, which takes OðDN2Þ                      time to

       compute   the QR  decomposition. Thus,  the                     time
       complexity is OðDN2Þ.
   2.                   Space complexity. Lines 2 and 4 involve matrices Ht
                     and Hb. Because of the size of the matrix, Ht requires
       OðDN Þ space in memory, and Hb                       requires OðDCÞ.
       Lines 3, 5, and 6 only involve R 2 IRrᵗ N              , Z 2 IRrᵗ C,
       and  e   e     r  r
            St, Sb 2 IR t                  t, which are all small matrices.
       Therefore, the space complexity is OðDNÞ.
 4   COMPARISON

 Although  Fisherface,  PCA+NULL,     LDA/GSVD,                         and
 LDA/QR were all                          proposed independently and appear
 to be different algorithms, in this section, we explain how
 FKT provides insights into these methods.
 4.1  Fisherface: Subspaces 2 and 3
 In the Fisherface method, PCA is performed first so as to
 make Sw  nonsingular. This is done by throwing away
 Subspaces 1 and 4 (Fig. 1). As a result, Subspace 1, the most
 discriminant subspace in terms of the Fisher Criterion, is
 discarded. Therefore, Fisherface operates only in Subspaces 2
 and 3 and is suboptimal.

 4.2  PCA + NULL Space: Subspace 1
 Considering the discriminant information contained in the
 null space of Sw, PCA+NULL Space first removes the null
 space of St, which  is Subspace  4 (Fig. 1). Now,                     only
 Subspaces 1, 2, and 3 are left. Second, within Subspace 1, the
 principal components  of Sb are  computed.  Thus,                     only
 Subspace 1, the most discriminant feature space, is used.
                    Other null space methods have also been reported in the
 literature, such as Direct LDA [21] and NULL Space [3]. The
 criterion used in these methods is a modified version of the
 Fisher Criterion, namely,
         ¼ arg maxk >S  k      s:t:k >S  k ¼ 0:                        ð25Þ
       opt              b              w
 Equation (25) shows that                        is the set of eigenvectors
 associated with  the zero  opt
 maximum eigenvalues of S   eigenvalues of  Sw  and                     the
                                           b. Based on the eigenvalue ratio
 from Fig. 1, this is Subspace 1. Thus, the PCA+NULL, Direct
 LDA,  and  NULL    space methods   all operate only                     in
 Subspace 1. However, as we will show in our experiments
 in Section 6.1, using Subspace 1 alone is sometimes not
 sufficient for good discrimination because Subspaces 2 and
 3 may be necessary. In the worst case, when Subspace 1
 does not exist,¹ these null space methods will fail.
 4.3  LDA/QR: Subspaces 1 and 2
 To circumvent the nonsingularity requirement of Sw and
 reduce the computation, a two-stage strategy is used in LDA/
 QR [20]. The eigenspace (corresponding to nonzero eigenva-
 lues) of Sb is computed by applying QR on Hb. In fact, this is
 Subspace 1S Subspace 2 (Fig. 1) because the eigenvalues ofSb
 associated with Subspaces 3 and 4 are all zero, which are

 thrown away by the QR decomposition. Then, the eigenvec-
 tors corresponding to the smallest eigenvalues of  e 1e
                                                    Sb                   Sw

   1. Subspace 1 will not exist if S  is full rank and invertible. This can
                           w
 happen if there are enough training samples.
                                 1737
are computed, equivalently, computing the eigenvectors
corresponding to the largest  . Note that e 1e
e e          b  e  Sb                         Sw, rather than
S 1S        w   1
w  b, is eigendecomposed because Sw     may still be singular
( w is zero within Subspace 1). Therefore, as Fig. 1 illustrated,
Subspaces 1 and 2 are preserved by LDA/QR. This means that
LDA/QR operates in Subspaces 1 and 2.
4.4 LDA/GSVD: Subspaces 1, 2, 3, and 4
Both LDA/GSVD and LDA/FKT simultaneously diagona-
lize two matrices, but, so far, nobody has investigated the

relationship between these two methods. In this paper, one
of our contributions is the proof that LDA/GSVD and
LDA/FKT are equivalent (see Appendix A for the proof).
More specifically, from the perspective of FKT, the Y and Z
in LDA/GSVD (see (10) and (11)) are just arbitrary rotation
matrices. The discriminant subspace of LDA/GSVD X          is
equal to        ½UD 1=2V;U?, where U? is Subspace 4, and U is
the union of Subspaces 1, 2, and 3. This means X contains
Subspaces 1, 2, 3, and 4 (see Fig. 1). Therefore, the subspaces
obtained by LDA/GSVD are exactly            those obtained by
LDA/FKT. However, LDA/GSVD is computationally ex-
pensive (Table 1). In Fig. 2, we presented an       efficient
algorithm to compute LDA/FKT. This is achieved by using
QR decomposition on St to obtain Subspaces 1, 2, and 3. We
do not have to compute for Subspace 4, since it contains no
discriminant information.

5  MULTIPLE DISCRIMINANT ANALYSIS (MDA)
5.1 MDA/FKT
From the perspective of the Bayes Classifier, LDA is optimal
only for two Gaussian distributions with equal covariance
matrices [5], [7], and the Fisher Criterion has been extended

to handle multiple Gaussian distributions or classes with
unequal covariance matrices. This suggests that, for multi-
ple Gaussian distributions or classes with unequal covar-
iance matrices, LDA-based methods are not optimal with
respect to the Bayes Classifier. The worst case occurs when
all classes have the same mean. In this case, Sb ¼ 0 and all
LDA-based methods will fail. Subspaces 1 and 2 do not
exist and we are left with only Subspaces 3 and 4, which are
less discriminative. To handle these problems, we cast the
multiclass problem into a binary pattern       classification
problem  by introducing   ¼ ai   aj and defining          the
intraclass space  I ¼ fðai   ajÞ j LðaiÞ ¼ LðajÞg, as well as
the extraclass space  E ¼ fðai   ajÞ j LðaiÞ 6 ¼LðajÞg, where
LðaiÞ is the class label of ai. This idea has been used by
other researchers, for example, Moghaddam in [15]. The
statistics of  I and  E are defined as follows:

   mI ¼ mE ¼ 0; 1 X                                      ð26Þ
    I ¼ HIH> ¼  ðai   ajÞðai   ajÞ>;                     ð27Þ
            I  NI LðaiÞ¼LðajÞ
         ¼ H H> ¼ 1 X  ða   a Þða   a Þ>:                ð28Þ
     E      E E  i j   i   j
           P  NE LðaiÞ6¼LðajÞ
Here, N ¼  1  n ðn   1Þis the number of samples in        and
      P
N  ¼  I    2 n ni  i           I
 E   L ¼L   i j           is the number of samples in  E. For
     i     j
example, if every class has the same number of training
samples, ni ¼ n for i ¼ 1; . . .; C, then NI ¼ 1 Nðn   1Þ and
N  ¼ 1 NðN   nÞ. Note that, usually, rankð 2
 E  2                                          EÞ andrankð IÞ

# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

are both greater than C1, where C is the number of classes. Compared with other related work on MDA, our MDA/HI and HE are the precursor matrices of I and E given by FKT has some unique features. First, the discriminant subspace obtained by MDA/FKT is optimal in terms of Bhattacharyya distance. Moghaddam [15] computed the top eigenvectors of E and I individually as the projection subspaces, which may not be discriminant. Second, our method finds the globally optimal subspace by analytically maximizing the Bhattacharyya distance, which is the error bound of the Bayes Classifier. Another method recently proposed by De la Torre Frade and Kanade [6] maximizes the Kullback-Leibler divergence, which does not relate to the Bayes Classifier. Their method finds a local optimum by using an iterative method. Finally, MDA/FKT can provide more than C1 discriminant eigenvectors because usually the rank of E and I is greater than C1, the upper bound of rank(Sb). By comparison, LDA-based methods can only provide C1 discriminative eigenvectors because C1 is the upper bound of rank(Sb).

# 5.2 Algorithm for MDA/FKT

Based on the new criterion (32), our analyses on FKT show that Subspaces 1 and 3 are the most discriminant subspaces (Fig. 1). However, we cannot directly work on I ∈ ℝD and E ∈ ℝD, which may be singular or too large to be formed. An alternative is to use the precursor matrices HI ∈ ℝNI and HE ∈ ℝNE. However, it is not efficient to use HE as well because 2NE is too large. As shown above, I / N, but NE / N, where N is the total number of samples. Although N << D, N2 could be close to D or even greater. For example, in our face recognition experiments, when C = 67, D = 5,600, and n = 2 (two training samples per class), then N = Cn = 134, NI = 67, and NE = 8,844. E is 5,600 × 5,600 and HE is 5,600 × 8,844 in size.

Can we find an efficient way to obtain the Subspaces 1 and 3 of MDA/FKT without HE or E? Yes. Based on the relationship between St, I, and E, we devise a method that works with HI ∈ ℝNI and Ht ∈ ℝN only. Let us start with a lemma (see Appendix B for the proof):

Lemma 2. If St is the total scatter matrix defined in (5) and I and E are the covariance matrices of the intraclass and extraclass defined in (27) and (28), then 2NS = NI + NE, where N is the total number of samples, NI is the number of intraclass samples, and NE is the number of extraclass samples.

Let us define 0 = NI I and 0 = NE E, then St = 0 + 0 I2N E2N. To efficiently compute the generalized eigenvalues and eigenvectors of (I, E), we need the following theorem:

Theorem 2. If (λ, v) is the dominant generalized eigenvalue and eigenvector of the matrix pair (I, E) and (λ0, v0) is the dominant generalized eigenvalue and eigenvector of matrix pair (0, 0), then v = v0 and λ = NI 0.

Proof. The generalized eigenvalue equation of matrix pair (λ, λ) is v = λ v. Since λ = 2N0 and λ = 2N0, we have:

2N0 v = 2N0 v0.

Thus, the eigenvector of Subspace 2 corresponding to equal eigenvalues (I = E = 0.5) is the least discriminative. To distinguish from LDA/FKT, we will call this technique MDA/FKT. The key difference between them is that MDA/FKT is optimal in terms of the Bhattacharyya distance, whereas LDA/FKT is optimal in terms of the Fisher Criterion.

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

# 1739

Input: The data matrix A.

**Statistics of Our Data Sets**
|Type|Dataset|Size|Dimension|# of classes|
|---|---|---|---|---|
|Synthetic|Toy 1|300|3|3|
| |Toy 2|125|3|2|
| |MFEAT|2000|649|10|
|Real data|PIE|1608|5600|67|
| |Banca|6240|2805|52|

MDA/FKT is comparable to most of LDA-based methods. MDA/FKT, however, is optimal in terms of Bhattacharyya distance, the error bound of the Bayes Classifier, which is not the case for other methods.

# XPERIMENTS

Up until now, we have shown that FKT can be used to unify other LDA-based methods. Moreover, we proposed a new approach for MDA. In this section, we evaluate the performance of LDA/FKT and MDA/FKT by using synthetic and real data. The synthetic data has two sets, whereas the real data consists of three sets for digit recognition and face recognition. Table 2 shows the statistics of the data sets in our experiments.

Furthermore, it is obvious that the proof of Theorem 2 is valid for the other generalized eigenvalues and eigenvectors as well, not just the dominant one. There is a one-to-one mapping between the corresponding eigenvalues and eigenvectors of the two pairs of matrices.

Therefore, to compute any generalized eigenvalue and eigenvector of the matrix pair (I, E), we can work on the matrix pair (0, 0). Since St = 0 + 0 and, based on our FKT analysis, each generalized eigenvalue is equal to ₀ᴵ, the eigenvalue ratio of FKT. This can be realized by using smaller matrices Ht ∈ ℝD×N and HI ∈ ℝD×Nᴵ, where N = D and NI = D.

# 6.1 Toy Problems

To evaluate the performance of MDA/FKT, we begin with two toy examples:

- Toy 1: Three Gaussian classes: same mean, different covariance matrices. The three classes share the same zero mean in 3D space and each class has 100 points. They have different covariance matrices:
- - C1 = [1, 1, 0] [1, 1, 0] + 0.1[0, 1, 1] [0, 1, 1];
- C2 = [0, 1, 1] [0, 1, 1] + 0.1[1, 0, 1] [1, 0, 1];
- C3 = [1, 0, 1] [1, 0, 1] + 0.1[1, 1, 0] [1, 1, 0];

Toy 2: Two classes: Gaussian mixture. We also have two classes in 3D space. One class contains 50 points and the other contains 75. The first class is generated from a single Gaussian with zero mean and 0.5I covariance. The second class is a Gaussian mixture which consists of three components with different means: [1, 4, 0], [2, 3, 2], and [2, 3, 2]. Each component has a different covariance.

Table 1 compares the time/space complexity of the methods mentioned in this paper. Observe that our methods perform better.

# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

# 6.2 Digit Recognition

We perform digit recognition to compare MDA/FKT with LDA-based methods on the MFEAT [12] data set. This consists of handwritten digits (“0”-“9”) (10 classes) with 649-dimensional features. These features comprise six feature sets: Fourier coefficients, profile correlations, Karhunen-Love coefficients, pixel averages in 2x3 windows, Zernike moments, and morphological features. For each class, we have 200 patterns; 30 of them are chosen randomly as training samples and the rest for testing. To evaluate the stability of each method, we repeat the sampling 10 times so that we can compute the mean and standard deviation of the recognition accuracy.

As shown in Fig. 6a, the accuracy of LDA/FKT and MDA/FKT is about 95 percent with small standard deviations, which means an accurate and stable performance. For MDA/FKT, we can investigate the relationship between performance and the projected dimension. Fig. 6b shows a plot of accuracy versus projected dimensions. The accuracy reaches the maximum when the projected dimension is around 8, after which it remains flat even if we increase the projected dimension. This suggests that MDA/FKT reaches its best performance around eight dimensions. Another observation is that the projected dimension should not be limited by the number of classes. For example, here, we have C = 10 classes, but Fig. 6b illustrates that seven or eight dimensions can give almost the same accuracy as C - 1 = 9 projected dimensions.

# 6.3 Face Recognition

We also perform experiments on real data on two face data sets:

1. PIE face data set [17]. We choose 67 subjects and each subject has 24 frontal face images taken under room lighting. All of these face images are aligned based on eye coordinates and cropped to 70x80. Fig. 7a shows a sample of PIE face images used in our experiments. The major challenge in this data set is to do face recognition under different illuminations.
2. Banca face data set [1]. This contains 52 subjects and each subject has 120 face images, which are normalized to 51x55 in size. By using a Webcam and an expensive camera, these subjects were recorded in three different scenarios over a period of three months. Each face image contains illumination, expression, and pose variations because the subjects are required to talk during the recording (Fig. 7b).

Fig. 7 shows a sample of PIE and Banca face images used in our experiments.

For face recognition, usually, we have an undersampled problem, which is also the reason for the singularity of Sw. To evaluate the performance under such a situation, we randomly choose N training samples from each subject, N = 2, ..., 12, and the remaining images are used for testing. For each set of N training samples, we employ cross validation so that we can compute the mean and standard deviation for classification accuracies. We show the mean and standard deviation (in parenthesis) of the recognition rate from 10 runs (see Table 3). Note that the largest number in each row is highlighted in bold.

3. The PIE data set contains 68 subjects altogether. We omitted one because s/he has too few frontal face images.

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

Fig. 5. (a) Original 3D data. (b) Two-dimensional projection by MDA/FKT. (c) One-dimensional projection by LDA/FKT. The projection of MDA/FKT is more separable than that of LDA/FKT because the former can provide a larger discriminant subspace.

As shown in Table 3, we observe that the more training samples, the better the recognition accuracy. To be more specific, on both data sets, for each method, increasing the number of training samples increases the mean recognition rate and decreases the standard deviation. Note that, when the training set is small, LDA/FKT significantly outperforms the other methods. For example, for the PIE data set, with two training samples, LDA/FKT achieves about 98 percent accuracy compared with the next highest of 90 percent from PCA+NULL. Moreover, with four training samples, LDA/FKT achieves about 5 percent higher in accuracy than the next highest. This shows that LDA/FKT can handle small-sample-size problems very well. With more training samples, that is, 6-12, LDA/FKT is not the best but falls behind the highest by no more than 1.8 percent. One possible reason is that LDA/FKT is optimal as a linear classifier, whereas, for the Banca data set, the face images under expression and pose variations are nonlinearly distributed.

To compare PCA, MDA/FKT, and various LDA-based methods with different training samples, we also visualize the average classification accuracies in Fig. 8.

Fig. 6. The accuracy rate (mean and standard deviation) of digit recognition by using: (a) PCA, MDA, and LDA-based methods. (b) MDA/FKT: accuracy versus projected dimension.

Each row represents one subject. Note that the PIE face images have only illumination variation, whereas the Banca ones have illumination, pose, and expression variations. Moreover, the Banca images are captured in different scenarios with different cameras.

# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

# TABLE 3

# Classification Accuracy (%) of Different Methods with Different Training Set Sizes

|Dataset|#of training samples|PCA|Fisherface|PCA+NULL|LDAQR|LDAIFKT|MDAFKT|
|---|---|---|---|---|---|---|---|
|PIE|2|39.62(1.56)|88.95(1.23)|89.49(1.15)|85.74(1.66)|98.09(0.71)|95.03(1.16)|
| |10|59.60(2.07)|93.58(0.85)|93.78(0.94)|91.05(2.48)|99.74(0.32)|99.06(0.57)|
| |12|73.53(1.53)|96.18(0.38)|96.05(0.53)|94.70(1.60)|99.96(0.04)|99.77(0.23)|
| |10|82.61(1.62)|97.78(0.59)|97.41(0.63)|95.58(1.23)|99.97(0.03)|99.95(0.05)|
| |12|88.10(1.46)|98.58(0.66)|98.27(0.70)|97.57(1.39)|100.0(0.00)|99.91(0.08)|
| |2|38.30(1.57)|65.33(3.71)|65.62(2.72)|55.04(1.92)|70.45(1.74)|70.42(1.74)|
|Banca|2|54.45(1.78)|79.81(2.19)|68.32(2.09)|73.59(4.84)|85.33(1.67)|86.15(1.31)|
| |10|65.37(0.74)|90.15(0.94)|91.41(0.85)|80.23(4.73)|90.87(0.77)|91.06(1.13)|
| |12|73.63(1.04)|94.20(0.85)|94.33(0.56)|87.40(5.15)|93.85(0.56)|93.55(0.62)|
| |10|78.54(1.33)|95.93(0.79)|94.97(0.56)|88.52(3.10)|94.35(0.60)|94.77(0.56)|
| |12|82.34(1.31)|96.92(0.48)|96.01(0.44)|91.30(3.41)|95.12(0.35)|95.35(0.27)|

Note that, since MDA/FKT is not limited by the number of classes (unlike LDA), we may project the data onto a space whose dimension is greater than the number of classes. Fig. 9 shows a plot of accuracy versus projected dimensions for different numbers of training samples for both the PIE and Banca data sets. We observe the following:

Fig. 8. Face recognition by using PCA, MDA/FKT, and various LDA-based methods with different numbers of training samples per class. (a) PIE (67 classes). (b) Banca (52 classes). Note that, for visualization, we only show the mean of the accuracies from 10 runs. These plots are generated from Table 3.

Fig. 9. Face recognition by varying the number of training samples per class and the projected dimension. (a) MDA/FKT curves on PIE (67 classes). (b) MDA/FKT curves on Banca (52 classes). Note that, for visualization, we only show the mean of the accuracies from 10 runs.

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

training data can sample the underlying distribution more accurately than a smaller set. It works even if Sb = 0, which is where LDA-based methods fail. Furthermore, for Gaussian mixture pdf, it works better than LDA.

1. With fewer training samples per class, say, two or four, the highest accuracy is obtained at C - 1 projected dimensions (C = 67 for PIE, C = 52 for Banca).
2. However, with more training samples per class, say, six or more, we can obtain a high classification rate with fewer than C - 1 projected dimensions. For example, on the PIE data set, with eight or 10 training samples per class, we can obtain 98 percent accuracy by using only 30 projected dimensions (Fig. 9a). The curves remain flat with increasing dimensions. Thus, there is no incentive to use more than 30 dimensions.

Now, let us summarize our experiments on real data sets. First, MDA/FKT is comparable to LDA-based methods with respect to the accuracy. Second, LDA/FKT and MDA/FKT significantly outperform other LDA-based methods for small-sample-size problems.

# CONCLUSION

In this paper, we showed how FKT can provide valuable insights into LDA. We derived and proved the relationship between GSVD, FKT, and LDA and then unified different LDA-based methods. Furthermore, we proposed a new method—MDA/FKT—to handle the MDA problem. More precisely:

1. We decomposed the whole data space into four subspaces by using FKT. For the LDA problem, we proved that the GSVD is equivalent to the FKT.
2. We proved that the eigenvalue ratio of FKT and the square of the generalized singular value of GSVD are equal to the generalized eigenvalue of LDA, which is the measure of discriminability according to the Fisher Criterion. It unifies these three methods that were previously separately proposed.
3. We proposed a unified framework to understanding different methods, that is, Fisherface, PCA+NULL, LDA/QR, and LDA/GSVD. Our theoretical analyses showed how to choose the discriminant subspaces based on the generalized eigenvalue, the essential measure of separability.
4. We also compared some common LDA methods with LDA/FKT. Most of these methods are suboptimal in terms of the Fisher Criterion. More specifically, Fisherface, PCA+NULL, and LDA/QR all operate in different parts of the discriminative subspaces of LDA/FKT. We showed that LDA/GSVD and LDA/FKT are, in fact, equivalent, but our LDA/FKT is more efficient than LDA/GSVD with respect to computation.
5. We further presented MDA/FKT with the following properties:

# APPENDIX A

# PROOF OF LEMMA 1

Proof. GSVD = FKT.

Based on GSVD,

S = H H> = X > > b 0 X 1;

Sw = HwHw = X w X :

Thus,

X>(Sb + Sw)X = 0 0 :

Since > b + > w = I 2 IRrᵗ rᵗ, if we choose the first r columns of X as P, that is, P = X(d;rₜ), then P(Sb + Sw)P = I. This is exactly FKT. Meanwhile, we can obtain that b = > b and w = > w.

FKT = b w

Based on FKT P = UD,

Sb = P SbP = D U HbHb UD ;

Hence, Sb = V bV :

D 1/2U>H H>UD 1/2 = V V>:

In general, there is no unique decomposition on the above equation because HbH> = HbYY>H> for any orthogonal matrix Y 2 IRC C.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 10, OCTOBER 2007

By examining (48) and (49), we can see that

2NS = Nt + Nu.

Here, HU? = 0, and HU? = 0 because U? is the intersection of the null space of Sb and Sw. Similarly, we can get ZHX = [b; 0], where Z ∈ ℝn is an arbitrary orthogonal matrix, b ∈ ℝt, and w = wb. Since b + w = I and I ∈ ℝr×t is an identity matrix, it is easy to check that btb + wtw = I, which satisfies the constraint of GSVD.

Now, we have to prove X is nonsingular

XXt = [U; U? ] D1/2V; U?

Here, V ∈ ℝr and [U; U? ] are orthogonal matrices.

Note that U U? = 0 and U?U = 0. From the above equation, XXt can be eigendecomposed with positive eigenvalues, which means X is also nonsingular. This completes the proof.

# REFERENCES

1. ´re, S. Bengio, F. Bimbot, M. Hamouz, J. Kittler, J. E. Bailly-Baillie ´thoz, J. Matas, K. Messer, V. Popovici, F. Pore ´e, B. Ruı́z, and J.P. Thiran, “The Banca Database and Evaluation Protocol,” Proc. Fourth Int’l Conf. Audio and Video-Based Biometric Person Authentication, pp. 625-638, 2003.
2. P. Belhumeur, J. Hespanha, and D. Kriegman, “Eigenfaces versus Fisherfaces: Recognition Using Class Specific Linear Projection,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 7, July 1997.
3. H. Cevikalp, M. Neamtu, M. Wilkes, and A. Barkana, “Discriminant Common Vectors for Face Recognition,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 1, pp. 4-13, Jan. 2005.
4. S. Chakrabarti, S. Roy, and M. Soundalgekar, “Fast and Accurate Text Classification via Multiple Linear Discriminant Projections,” Proc. Int’l Conf. Very Large Data Bases, pp. 658-669, 2002.
5. R. Duda, P. Hart, and D. Stork, Pattern Classification, second ed. John Wiley & Sons, 2000.
6. F. de la Torre Frade and T. Kanade, “Multimodal Oriented Discriminant Analysis,” Proc. Int’l Conf. Machine Learning, Aug. 2005.
7. K. Fukunaga, Introduction to Statistical Pattern Recognition, second ed. Academic Press, 1990.
8. G.H. Golub and C.F. Van Loan, Matrix Computations, third ed. Johns Hopkins Univ. Press, 1996.
9. P. Howland and H. Park, “Generalizing Discriminant Analysis Using the Generalized Singular Value Decomposition,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 8, pp. 995-1006, Aug. 2004.
10. R. Huang, Q. Liu, H. Lu, and S. Ma, “Solving the Small Sample Size Problem of LDA,” Proc. IEEE Int’l Conf. Pattern Recognition, vol. 3, pp. 29-32, 2002.
11. X. Huo, “A Statistical Analysis of Fukunaga-Koontz Transform,” IEEE Signal Processing Letters, vol. 11, no. 2, pp. 123-126, Feb. 2004.
12. A.K. Jain, R.P.W. Duin, and J. Mao, “Statistical Pattern Recognition: A Review,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 4-37, Jan. 2000.
13. I.T. Jolliffe, Principal Component Analysis. Springer, 1986.
14. C.F. Van Loan, “Generalizing the Singular Value Decomposition,” SIAM J. Numerical Analysis, vol. 13, no. 1, pp. 76-83, 1976.
15. B. Moghaddam, “Principal Manifolds and Probabilistic Subspaces for Visual Recognition,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 24, no. 6, pp. 780-788, June 2002.
16. C.C. Paige and M.A. Saunders, “Towards a Generalized Singular Value Decomposition,” SIAM J. Numerical Analysis, vol. 18, no. 3, pp. 398-405, 1981.
17. T. Sim, S. Baker, and M. Bsat, “The CMU Pose, Illumination, and Expression Database,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 25, no. 12, pp. 1615-1618, Dec. 2003.
18. M. Turk and A. Pentland, “Eigenfaces for Recognition,” J. Cognitive Neuroscience, vol. 3, no. 1, 1991.
19. X. Wang and X. Tang, “Dual-Space Linear Discriminant Analysis for Face Recognition,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 564-569, June 2004.
20. J. Ye and Q. Li, “A Two-Stage Linear Discriminant Analysis via QR-Decomposition,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 6, pp. 929-942, June 2005.
21. H. Yu and H. Yang, “A Direct LDA Algorithm for High-Dimensional Data—with Application to Face Recognition,” Pattern Recognition, vol. 34, no. 10, pp. 2067-2070, 2001.
22. S. Zhang and T. Sim, “When Fisher Meets Fukunaga-Koontz: A New Look at Linear Discriminants,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 323-329, June 2006.

# APPENDIX B

# PROOF OF THEOREM

Since

1

X = (ai aj)t

i Ni Nj

L(aI) = L(aj)

E = N (ai aj)(ai aj)t

E L(ai) ≠ L(aj)

Then,

XX = NI + NE = (ai aj)(ai aj)t

i j i j

i j

XX = 2N (ai ai) + 2N (mm)

i

where m = (1/N) Σ ai is the total mean of the samples.

On the other hand,

St = (1/N) Σ (ai - m)(ai - m)t

= Σ ai ait - ai mt - m ait + m mt

= Σ ai ait - (1/N) m mt

i

# ZHANG AND SIM: DISCRIMINANT SUBSPACE ANALYSIS: A FUKUNAGA-KOONTZ APPROACH

Sheng Zhang received the bachelor’s degree from Zhejiang University, China, in 1998, the Master’s degree from the Institute of Automation, Chinese Academy of Sciences, in 2001, and the PhD degree from the School of Computing, National University of Singapore in 2006. He now works in the Department of Electrical and Computer Engineering, University of California at Santa Barbara (UCSB) as a postdoctoral researcher. His research interests include face recognition, computer vision, statistical pattern recognition, digital photography, and music processing. He chairs the Workgroup on Cross-Jurisdictional and Societal Aspects in the Biometrics Technical Committee, Singapore. He is a member of the IEEE.

Terence Sim received the BS degree in computer science and engineering from the Massachusetts Institute of Technology in 1990, the MS degree in computer science from Stanford University in 1991, and the PhD degree in electrical and computer engineering from Carnegie Mellon University in 2002. He is an assistant professor in the School of Computing, National University of Singapore. His research interests are in biometrics, face recognition, computer vision, digital photography, and machine learning. He is a member of the IEEE.

For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.

