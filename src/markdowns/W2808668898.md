# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

# A3NCF: An Adaptive Aspect Attention Model for Rating Prediction

Zhiyong Cheng1, Ying Ding2, Xiangnan He1, Lei Zhu3âˆ—, Xuemeng Song4, Mohan Kankanhalli1

1School of Computing, National University of Singapore, Singapore

2Vipshop US Inc., San Jose, CA, USA

3School of Information Science and Engineering, Shandong Normal University, China

4School of Computer Science and Technology, Shandong University, China

{jason.zy.cheng, leizhu0608}@gmail.com, mohan@comp.nus.edu.sg

# Abstract

Current recommender systems consider the various aspects of items for making accurate recommendations. Different users place different importance to these aspects which can be thought of as a preference/attention weight vector. Most existing recommender systems assume that for an individual, this vector is the same for all items. However, this assumption is often invalid, especially when considering a userâ€™s interactions with items of diverse characteristics. To tackle this problem, in this paper, we develop a novel aspect-aware recommender model named A3NCF, which can capture the varying aspect attentions that a user pays to different items. Specifically, we design a new topic model to extract user preferences and item characteristics from review texts. They are then used to 1) guide the representation learning of users and items, and 2) capture a userâ€™s special attention on each aspect of the targeted item with an attention network. Through extensive experiments on several large-scale datasets, we demonstrate that our model outperforms the state-of-the-art review-aware recommender systems in the rating prediction task.

# 1 Introduction

User ratings on E-commerce websites provide a good guidance for users to choose products. Therefore, rating prediction of products for users (who are new to those products) is a practical way to increase revenue for E-commerce companies, as it could guide the recommendation of products to potential customers. Matrix factorization (MF) [Koren et al., 2009] has achieved great success in this task, as demonstrated by the Netflix Prize contest [Bell and Koren, 2007]. Relying on the user-item rating matrix, this method represents usersâ€™ interests and itemsâ€™ features as latent factor vectors in a common latent space. However, a rating only reflects a userâ€™s overall satisfaction towards an item without explaining the underlying rationale. For example, a user may give a high rating to a phone because of its high-resolution camera or powerful battery, which cannot be told by the overall rating. As a result, MF methods cannot achieve fine-grained modeling of user preference on the various aspects of items, resulting in unexplained recommendations and the â€œcold-startâ€ problem of users with few ratings [McAuley and Leskovec, 2013; Wang et al., 2018; Cheng et al., 2018].

Motivated by this observation, we propose an Adaptive...

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

# Aspect Attention-based Neural Collaborative Filtering model

(or A3NCF for short), which can accurately capture the varying attentions that a user pays to each aspect of different items. In A3NCF, a new topic model is developed to extract both usersâ€™ preferences and itemsâ€™ characteristics simultaneously. It is different from previous topic-based methods [McAuley and Leskovec, 2013; Wang and Blei, 2011; Ling et al., 2014; Tan et al., 2016], which directly apply the LDA [Blei et al., 2003] model in reviews and can only extract itemsâ€™ features. For each pair of user and item, their representations learned from the topic model are subsequently used in a neural collaborative filtering network for (1) guiding the learning of their final latent factors and (2) capturing the attention vector of the user with respect to the various aspects of this particular item with an attention network. Finally, an unknown rating is predicted based on the attentive interaction of the userâ€™s and itemâ€™s final latent factors. To this end, we expect that our model could achieve better rating prediction performance, due to (1) the aspect-aware representation learning of users and items via the powerful non-linear neural networks and (2) the delicately designed attention network of adaptive aspect attention modeling for each user-item pair.

To evaluate the effectiveness of our model, we conduct comprehensive experiments on both Amazon product datasets and the Yelp Dataset 2017 to compare our model with several state-of-the-art methods, which utilize both reviews and ratings with different strategies. Results show that our model outperforms those competitors by a large margin and also verify the effectiveness of the attention mechanism.

# Contributions

- We propose an aspect-aware rating prediction method based on a novel adaptive aspect attention modeling design. In particular, a new topic model is developed to extract both user and item features from reviews to guide the aspect-aware representation learning.
- We introduce an attention network to capture the varying attention vectors of each specific user-item pair.
- We conduct comprehensive experiments on publicly accessible datasets to comparatively evaluate and demonstrate the effectiveness of the proposed method.

# Related Work

Many approaches have been developed to combine reviews and ratings for improving recommendation performance. In this section, we mainly review approaches falling into the topic-based and deep learning-based categories, which are closely related to our work.

# Topic-based

A general approach of these methods is to extract latent topics from reviews using topic models [McAuley and Leskovec, 2013; Ling et al., 2014; Zhang and Wang, 2016; Tan et al., 2016; Cheng et al., 2018] or non-negative matrix factorization (NMF) [Bao et al., 2014; Qiu et al., 2016] and learn latent factors from ratings using MF methods. Then, the latent topics and latent factors are combined in a way for final rating prediction. For example, HFT [McAuley 1A latent factor is regarded as an aspect in our context.]

# Our Model

# Preliminaries

Problem Setting. Let D be a review collection for an item (i.e., product) set I from a specific category (e.g., clothes) written by a set of users U, and each review du,i âˆˆ D comes with an overall rating ru,i to indicate the overall satisfaction of user u towards item i. The primary goal is to predict the unknown ratings of items that the users have not reviewed yet.

# Intuition

A user may place different importance to the various aspects of different items, or the attention weights on aspects are varied when facing different items. Sometimes, an important aspect can dominate a userâ€™s attitude towards an item. For example, a fan of the famous NBA player James Harden is willing to purchase Adidas basketball shoes just because they are endorsed by this player. But when purchasing other basketball shoes, he would like to carefully consider.

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

# Text Review

# Topic model

# User Feature (ğœ½ğ’–)

# Attention Network

User (u)

# Text Review

# Topic model

# Item Feature (ğ‹ğ’Š)

# Fusion

# ReLU

|ğ‘1ğ‘1|ğ‘1|ğ‘“â‚|
|---|---|---|
|ğ‘2ğ‘2|ğ‘2|ğ‘“â‚‚|
|ğ‘3ğ‘3|ğ‘3|ğ‘“â‚ƒ|

â€¦ Regression

Å·ui

# Embedding

User (u)

0
0
0
1
â€¦â€¦
User Embedding

# Fusion

Element

ReLU

Product

ğ‘â€¦â€¦ â€¦ â€¦

- ğ‘˜ğ‘ğ‘˜

ğ‘ğ‘˜

ğ‘“ğ‘˜

# Item (i)

0
0
1
0
â€¦â€¦
Embedding

# Input

# Feature Fusion

# Attentive Interaction

# Rating Prediction

Figure 1: The structure of our AÂ³NCF model.

other factors, such as whether the shoes are comfortable and how good is the cushioning. Based on this observation, we argue that it is important to capture the attention that a user pays to each aspect of a targeted item when evaluating his attitude towards this item. The problem is how to accurately model usersâ€™ attention weights towards different items.

When writing a review, users tend to write comments on either the aspects they care for or the most notable features of the targeted item. Therefore, we could extract a user uâ€™s preferences and an item iâ€™s characteristics of different aspects, denoted as Î¸u and Ï•i, respectively. The interaction between Î¸u and Ï•i will be used to estimate the attention vector au,i of user u for item i. As the mechanism behind the interaction could be very complex, we introduce an attention network to model it, since neural networks have shown strong ability in modeling the complex interactions.

3.2 A3 Neural Collaborative Filtering

Figure 1 shows the structure of our Adaptive Aspect Attention-based Neural Collaborative Filtering (A3NCF) model, which consists of four components.

The input part takes the reviews of users and items as well as their identities as inputs. Based on the reviews, a new topic model is applied to extract usersâ€™ preferences and itemsâ€™ characteristics, represented as Î¸u and Ï•i for user u and item i, respectively (described in Sect. 3.5). The identity of a user u or an item i is transformed to a binarized sparse vector with a one-hot encoding, which is then projected to a dense vector via an embedding layer. The embedding layer is a fully connected layer. Identity-based embedded features and review-based features of users and items are then passed to the next layer.

The fusion part aims to fuse the embedded features and review-based features for better representation learning. The strategy of combining rating- and review-based features has been widely adopted for boosting recommendation performance in previous works. Different fusion methods can be applied, such as concatenation, addition, or element-wise product. Here, we adopt the addition fusion method, which has been applied in RBLT and ITLFM and achieves good performance. The difference is that we add a fully-connected neural layer directly after the fusion step for more sophisticated effects. This layer adopts the non-linear ReLU activation function. In experiments, we found that this additional layer can substantially improve the performance.

The attentive interaction part is the core of A3NCF - capturing the targeted userâ€™s attention vector for different aspects of the targeted item. Let pu âˆˆ RK and qi âˆˆ RK be the representation vectors of user u and item i (learned from the fusion part), respectively. K is the dimension of latent vectors. The attentive interaction part outputs the representation of the user-item pair for the subsequent rating prediction. Let F = [f1, f2, ..., fK] denote the output representation of the user-item pair. F is obtained by:

F = au,i (pu qi)

where denotes the element-wise product, and au,i âˆˆ RK is the attention vector of user u for item i. From the equation, we can see fk = au,i,k Â· pu,k Â· qi,k, where fk denotes the k-th factor in F. It indicates that for the interaction of each factor between pu and qi, there is an attention weight au,i,k to capture the importance of this factor k of item i with respect to user u, namely, uâ€™s attention on the aspect k of item i. Therefore, au,i,k is unique for each user-item pair. The attention mechanism for au,i will be introduced in Section 3.3.

The rating prediction part. The obtained interaction feature vector F is fed into fully connected layers as follows:

zL = ÏƒL(WL(ÏƒLâˆ’1(WLâˆ’1 Â· Â· Â· Ïƒ1(W1F + b1)) + bLâˆ’1) + bL)

where L denotes the number of hidden layers; W, b, and Ïƒl are the weight matrix, bias vector, and activation function for the l-th layer, respectively. We adopt the ReLU activation function for all the layers. The predicted rating rË†u,i is obtained via a regression layer:

rË†u,i = W zL + b

3.3 Attention Mechanism

In this section, we introduce the attention mechanism in A3NCF for capturing a user uâ€™s specific attention on an aspect/factor k of an item i. Since the user preference and item characteristics can be explicitly observed in reviews, we rely on the review-based features (i.e., Î¸u and Ï•i) to capture a user uâ€™s attention on the various aspects of an item i.

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

Hong et al., 2016b] in reviews to extract only itemsâ€™ characteristics, we develop a new topic model, which is capable of extracting both usersâ€™ preferences and itemsâ€™ characteristics on different aspects simultaneously.

In our topic model, we assume that a set of latent topics (i.e., K topics) represent all the aspects that users discuss in the reviews. Î¸u is a probability distribution of latent topics, in which each value Î¸u,k denotes the relative importance of an aspect to the user. Similarly, Ï•i is the probability distribution of aspects in item iâ€™s characteristics, in which each value Ï•i,k denotes the importance of an aspect k to the item i. Î¸u is determined based on all the reviews written by u; and Ï•i is learned from the reviews of item i written by all users.

Figure 2: Graphical representation of the proposed topic model.

of the user-item pair, the attention vector is computed as:

au,i = vT ReLU(Wa[Î¸u;Ï•i;pu; qi] + ba)

where Wa and ba are respectively the weight matrix and bias vector that project the input into a hidden layer, and vT is the vector that projects the hidden layer into an output attention weight vector. [Î¸u;Ï•i;pu; qi] denotes the concatenation of the four feature vectors. ReLU is also used as the activation function here due to its effectiveness in neural attention networks [Cao et al., 2018; Song et al., 2018]. Following the standard setting of neural attention networks, au,i is obtained by a subsequent normalization with the softmax function.

au,i,k = exp(Ã¢u,i,k) / âˆ‘j=1K exp(Ã¢u,i,j)

It is worth mentioning that we also tried [Î¸u;Ï•i] or [Î¸u; qi] instead of [Î¸u;Ï•i;pu;qi] in Eq. 2 to calculate the attention weights, which leads to inferior performance. This also validates the assumption of combining them together is more effective.

# 3.4 Learning

Notice that user preferences and item features extracted from reviews are pre-processed by the proposed topic model (detailed in next subsection). Thus, the learning of A3NCF is the same as general deep learning networks. As our task is rating prediction, we treat it as a regression task and the square loss is used for training. The objective function is:

L = âˆ‘(ru,i)âˆˆD (ru,i - Ë†ru,i)2

where ru,i is the given rating in dataset D, and Ë†ru,i is the predicted rating. The stochastic gradient descent (SGD) algorithm is adopted for optimization.

# 3.5 Topical Feature Extraction

The features extracted from reviews for users and items are critical to our model, as they are not only related to the learning of the final representations for users and items, but also the core to compute the attention weights. As previous works have successfully applied topic models in extracting review features for rating prediction, such as [McAuley and Leskovec, 2013; Ling et al., 2014; Tan et al., 2016], we also use the topic modeling method. Different from those works which directly apply LDA [Blei et al., 2003; 3Here, a latent topic is regarded as an aspect as previous works.

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

# Datasets

|Dataset|# users|# items|# ratings|Sparsity|BMF|RMR|HFT|RBLT|TransNet|A3NCF|
|---|---|---|---|---|---|---|---|---|---|---|
|Baby|17,177|7,047|158,311|0.9987|1.176|1.152|1.117|1.119|1.098|1.082*|
|Grocery|13,979|8,711|149,434|0.9988|1.126|1.063|1.009|1.011|0.993|0.985|
|Home & Kitchen|58,901|28,231|544,239|0.9997|1.108|1.092|1.082|1.086|1.074|1.051*|
|Garden|1,672|962|13,077|0.9919|1.099|1.074|1.037|1.034|1.040|1.021*|
|Sports|31,176|18,355|293,306|0.9995|1.087|1.011|0.972|0.963|0.983|0.940*|
|Yelp2017|169,257|63,300|1,659,678|0.9998|1.284|1.263|1.185|1.204|1.183|1.152*|

Table 1: Statistics of the evaluation datasets. K = 25. The best performance is highlighted with bold face. The symbol * denotes a significance with p-value < 0.05 over the second best model based on a two-tailed paired t-test. â€œH & Kâ€ denotes â€œHome & Kitchenâ€.

Parameters need to be estimated including Î¸u, Ï•i, Ï†w and Ï€u. Different inference methods for parameter learning in topic models have been developed, such as variation inference [Blei et al., 2003] and collapsed Gibbs sampling [Cheng et al., 2016; Cheng and Shen, 2016; Hong et al., 2016a]. In this paper, we adopt the collapsed Gibbs sampling method for parameter inference.

# 4 Experiments

# 4.1 Experimental Setup

Datasets. We conducted experiments on two publicly accessible datasets: the Amazon Product Review dataset [McAuley and Leskovec, 2013] and the Yelp Dataset 2017. The first dataset contains reviews and metadata of diverse products from Amazon. It contains 24 product categories. We adopted five categories and took the 5-core version for experiments, where each user or item has at least 5 interactions. The five categories are of different sizes and sparsity degrees (as shown in Table 1). The Yelp dataset contains reviews of local businesses in 12 metropolitan areas across 4 countries. We preprocessed all the datasets by removing duplicated reviews of the same user-item pairs and keeping only users and items with at least 5 reviews. For each review in the datasets, we extracted â€˜userIDâ€, â€œitemIDâ€, the corresponding rating score (1 to 5 rating stars) and textual review for experiments. In addition, we cleaned the reviews by removing punctuation, stopwords, and infrequent terms appearing less than 10 times in the datasets. The basic statistics of the datasets is shown in Table 1.

Experimental Settings. We randomly split each dataset into training, validation, and testing sets with ratio 8:1:1 for each user as in [McAuley and Leskovec, 2013; Ling et al., 2014; Catherine and Cohen, 2017]. As each user has at least 5 reviews, we have at least 3 reviews per user for training, and at least 1 interaction per user for validation and testing. Note that we only used the review information in the training phase, because reviews are unavailable in validation and testing phase in real-world scenarios.

# 4.2 Experimental Results

Performance Comparison. Figure 3 shows the performance of all the adopted methods with respect to different numbers of predictive factors K. Due to the space limitation, we only show the results of four relatively larger datasets. We also report the concrete values of RMSE (based on K = 25) on all the six evaluation datasets in Table 2. Firstly, methods incorporating reviews all achieve better performance than BMF with a large margin, indicating the importance of utilizing reviews in preference modeling. Secondly, our method achieves the best performance over all datasets, significantly outperforming the state-of-the-art methods RBLT.

â€¢ BMF. It is a standard matrix factorization (MF) method with biased terms [Koren et al., 2009].

â€¢ HFT. It models ratings and review texts using MF and LDA [Blei et al., 2003], respectively. Then an exponential transform function is used to associate the latent topics and latent factors for rating prediction [McAuley and Leskovec, 2013].

â€¢ RMR. Different from HFT and CTR, which use MF to model rating, it uses a mixture of Gaussian distributions to model the ratings [Ling et al., 2014].

â€¢ RBLT. Similar to HFT, this method uses MF to model ratings and LDA to model review text. Different from HFT, it linearly combines the latent factors (learned from ratings) and latent topics (learned from reviews) to represent users and items [Tan et al., 2016]. This strategy is also adopted by ITLFM [Zhang and Wang, 2016]. Here, we use RBLT as a representative method for this strategy.

â€¢ TransNet. This method adopts a neural network framework for rating prediction. Reviews of users and items are passed into two CNNs respectively to learn the latent representations of users and items [Catherine and Cohen, 2017]. The latent representations of a targeted user and a targeted item are concatenated and passed through a regression layer to estimate the rating.

Evaluation Metrics. The root-mean-square error (RMSE) is used in evaluation. A smaller RMSE indicates better performance.

Parameter settings. The number of MLP layers L in the rating prediction part of A3NCF is set to 2 in our implementation. Besides, the dropout technique [Srivastava et al., 2014] is used to prevent overfitting and the dropout ratio is 0.5. The learning rate is set to 0.001 for all the datasets and the Adam optimization method [Kingma and Ba, 2014] is used.

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

|1.26|1.16|1.20|1.50|
|---|---|---|---|
|1.22|1.14|1.16|1.40|
|11.18|11.12|11.12|1|
|1.10|1.08|1.30|HFT|
|1.14|1.08|1.04| |
|1.10|1.06|1.00|1.20|
|0.96| | |A NCF|
|1.06|1.04|0.92|1.10|

5   10 15     20   25        5   10 15     20  25        5                       10 15 20 25  5  10   15 20 20

# factors                    # factor                                        # factor # factor

(a) Baby              (b) Home & Kitchen                                    (c) Sports (d) Yelp 2017

Figure 3: Performance of all competitors w.r.t. the number of latent factors on four relatively larger datasets.

|1.17|1.12|1.02|1.21|
|---|---|---|---|
|1.15|1.10|1.00|1.20|
|1.13|1|10.98|11.19|
|1.11|1.08| |1.18 A NCF|
|0.96| |1.17 NCF| |
|1.09|1.06|0.94|1.16 ANCF|
|1.07| | |1.15|
|1.05|1.04|0.92|1.14|

5   10   15   20   25       5    10   15   20   25       5                       10  15  20  25  5  10  15  20  20

# factors                  # factors                                         #factors # factors

(a) Baby               (b) Home & Kitchen                                    (c) Sports  (d) Yelp 2017

Figure 4: Performance of variants w.r.t. the number of latent factors on four larger datasets.

and TransNet. Based on the results in Table 2, the rel- Thus, only the one-hot encodings of a userâ€™s and an itemâ€™s relative improvement over RBLT and TransNet is 2.9% and 2.2% respectively with significance testing. The performance of RBLT, TransNet, and HFT is comparable across all the datasets, followed by RMR. Lastly, with the increase of num- ber of latent factors (#f actors), all the methods could ob- tain relatively better results.6 Compared to BMF, review- based methods are relatively stable as they could already achieve relatively good performance with a small #f actors. It demonstrates the benefits of considering review informa- tion in preference modeling, especially when #f actors is small. Compared to the baselines, our model achieves very good performance when #f actors is only 5.

The substantial improvement of our model over the base- lines could be credited to two reasons: (1) our model inte- grates features extracted from ratings and reviews via non- linear neural networks, and more importantly, applies more complicated interactions between usersâ€™ and itemsâ€™ latent vec- tors instead of a simple concatenation; and (2) our model uses an attention mechanism to capture usersâ€™ attention weights on different aspects of each item, and thus could achieve more accurate predictions.

# 5  Conclusions

In this paper, we presented a novel adaptive aspect attention- based neural collaborative filtering (A3NCF) model for rat- ing prediction. In this model, a new topic model is devel- oped to extract usersâ€™ preferences and itemsâ€™ characteristics on different aspects from reviews. Besides, an attention net- work is introduced in A3NCF to utilize the features extracted from reviews, aiming to capture the attention that a user pays to each aspect of the targeted item. This special design is due to the observation that a user may place different im- portance to the same aspect of different items. This is the first attempt to design a neural attention network based on reviews and ratings to tackle this problem. Experiments on 3753

# Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)

real-world datasets from Amazon show that our method significantly outperforms state-of-the-art methods. [Ling et al., 2014] Guang Ling, Michael R Lyu, and Irwin King. Ratings meet reviews, a combined approach to recommend. In ACM RecSys, pages 105â€“112, 2014.

# Acknowledgments

This research is supported by the National Research Foundation, Prime Ministerâ€™s Office, Singapore under its International Research Centre in Singapore Funding Initiative.

# References

- [Bao et al., 2014] Yang Bao, Hui Fang, and Jie Zhang. Topicmf: Simultaneously exploiting ratings and reviews for recommendation. In AAAI, volume 14, pages 2â€“8, 2014.
- [Bell and Koren, 2007] Robert M Bell and Yehuda Koren. Lessons from the netflix prize challenge. ACM SIGKDD Explorations Newsletter, 9(2):75â€“79, 2007.
- [Blei et al., 2003] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan):993â€“1022, 2003.
- [Cao et al., 2018] Da Cao, Xiangnan He, Lianhai Miao, Yahui An, Chao Yang, and Richang Hong. Attentive group recommendation. In ACM SIGIR, 2018.
- [Catherine and Cohen, 2017] Rose Catherine and William Cohen. Transnets: Learning to transform for recommendation. In ACM RecSys, pages 288â€“296, 2017.
- [Cheng and Shen, 2016] Zhiyong Cheng and Jialie Shen. On effective location-aware music recommendation. ACM Transactions on Information Systems (TOIS), 34(2):13, 2016.
- [Cheng et al., 2016] Zhiyong Cheng, Shen Jialie, and Steven CH Hoi. On effective personalized music retrieval by exploring online user behaviors. In ACM SIGIR, pages 125â€“134, 2016.
- [Cheng et al., 2018] Zhiyong Cheng, Ying Ding, Lei Zhu, and Kankanhalli Mohan. Aspect-aware latent factor model: Rating prediction with ratings and reviews. In WWW, pages 639â€“648, 2018.
- [Covington et al., 2016] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In ACM RecSys, pages 191â€“198, 2016.
- [He et al., 2015] Xiangnan He, Tao Chen, Min-Yen Kan, and Xiao Chen. Trirank: Review-aware explainable recommendation by modeling aspects. In ACM CIKM, pages 1661â€“1670, 2015.
- [He et al., 2017] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In WWW, pages 173â€“182, 2017.
- [Hong et al., 2016a] Richang Hong, Zhenzhen Hu, Ruxin Wang, Meng Wang, and Dacheng Tao. Multi-view object retrieval via multi-scale topic models. IEEE Trans. Image Process., 25(12):5814â€“5827, 2016.
- [Hong et al., 2016b] Richang Hong, Luming Zhang, Chao Zhang, and Roger Zimmermann. Flickr circles: aesthetic tendency discovery by multi-view regularized topic modeling. IEEE Trans. Multimed., 18(8):1555â€“1567, 2016.
- [Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
- [Koren et al., 2009] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):42â€“49, 2009.
- [Maas et al., 2013] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML, 2013.
- [McAuley and Leskovec, 2013] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In ACM RecSys, pages 165â€“172, 2013.
- [Nie et al., 2015] Liqiang Nie, Meng Wang, Luming Zhang, Shuicheng Yan, Bo Zhang, and Tat-Seng Chua. Disease inference from health-related questions via sparse deep learning. IEEE Trans. Knowledge Data Eng., 27(8):2107â€“2119, 2015.
- [Qiu et al., 2016] Lin Qiu, Sheng Gao, Wenlong Cheng, and Jun Guo. Aspect-based latent factor model by integrating ratings and reviews for recommender system. Knowledge-Based Systems, 110:233â€“243, 2016.
- [Song et al., 2018] Xuemeng Song, Fuli Feng, Xianjing Han, Xin Yang, Wei Liu, and Liqiang Nie. Neural compatibility modeling with attentive knowledge distillation. In ACM SIGIR, 2018.
- [Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929â€“1958, 2014.
- [Tang et al., 2015] Duyu Tang, Bing Qin, Ting Liu, and Yuekui Yang. User modeling with neural network for review rating prediction. In IJCAI, pages 1340â€“1346, 2015.
- [Wang and Blei, 2011] Chong Wang and David M Blei. Collaborative topic modeling for recommending scientific articles. In ACM KDD, pages 448â€“456, 2011.
- [Wang et al., 2018] Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. TEM: Tree-enhanced embedding model for explainable recommendation. In WWW, pages 1543â€“1552, 2018.
- [Zhang and Wang, 2016] Wei Zhang and Jianyong Wang. Integrating topic and latent factors for scalable personalized review-based rating prediction. IEEE Transactions on Knowledge and Data Engineering, 28(11):3013â€“3027, 2016.
- [Zhang et al., 2014] Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping Ma. Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. In ACM SIGIR, pages 83â€“92, 2014.
- [Zhang et al., 2016] Wei Zhang, Quan Yuan, Jiawei Han, and Jianyong Wang. Collaborative multi-level embedding learning from reviews for rating prediction. In IJCAI, pages 2986â€“2992, 2016.
- [Zhang et al., 2017] Yongfeng Zhang, Qingyao Ai, Xu Chen, and W. Bruce Croft. Joint representation learning for top-n recommendation with heterogeneous information sources. In ACM CIKM, pages 1449â€“1458, 2017.
- [Zheng et al., 2017] Lei Zheng, Vahid Noroozi, and Philip S Yu. Joint deep modeling of users and items using reviews for recommendation. In ACM WSDM, pages 425â€“434, 2017.

