# THE UNIVERSITY OF WARWICK

Original citation:

Zhang, Jun, Cormode, Graham, Procopiuc, Cecilia M., Srivastava, Divesh and Xiao, Xiaokui (2014) Privbayes: private data release via bayesian networks. In: ACM SIGMOD Conference, Salt Lake City, Utah, 22-27 Jun 2014. Published in: Proceedings of the 2014 ACM SIGMOD international conference on Management of data pp. 1423-1434.

Permanent WRAP url: http://wrap.warwick.ac.uk/63459

Copyright and reuse:

The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. Copyright © and all moral rights to the version of the paper presented here belong to the individual author(s) and/or other copyright owners. To the extent reasonable and practicable the material made available in WRAP has been checked for eligibility before being made available.

Copies of full items can be used for personal research or study, educational, or not-for-profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and/or URL is given for the original metadata page and the content is not changed in any way.

Publisher’s statement:

"© ACM, 2014. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in Proceedings of the 2014 ACM SIGMOD international conference on Management of data pp. 1423-1434. (2014) http://dx.doi.org/10.1145/2588555.2588573"

A note on versions:

The version presented here may differ from the published version or, version of record, if you wish to cite this item you are advised to consult the publisher’s version. Please see the ‘permanent WRAP url’ above for details on accessing the published version and note that access may require a subscription.

For more information, please contact the WRAP Team at: publications@warwick.ac.uk

warwickpublicationswrap

highlight your research

http://wrap.warwick.ac.uk

# PrivBayes: Private Data Release via Bayesian Networks

Jun Zhang1 &nbsp; &nbsp; Graham Cormode2 &nbsp; &nbsp; Cecilia M. Procopiuc3

1Nanyang Technological University

{jzhang027, xkxiao}@ntu.edu.sg

3AT&T Labs – Research

{magda, divesh}@research.att.com

# ABSTRACT

Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art goal for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless.

To address the deficiency of the existing methods, this paper presents PRIVBAYES, a differentially private method for releasing high-dimensional data. Given a dataset D, PRIVBAYES first constructs a Bayesian network N, which (i) provides a succinct model of the correlations among the attributes in D and (ii) allows us to approximate the distribution of data in D using a set P of low-dimensional marginals of D. After that, PRIVBAYES injects noise into each marginal in P to ensure differential privacy, and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in D. Finally, PRIVBAYES samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, PRIVBAYES circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in P instead of the high-dimensional dataset D. Private construction of Bayesian networks turns out to be significantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate PRIVBAYES on real data, and demonstrate that it significantly outperforms existing solutions in terms of accuracy.

# Categories and Subject Descriptors

H.2.7 [Database Administration]: Security, integrity & protection

# Keywords

Differential privacy; synthetic data generation; Bayesian network

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

SIGMOD’14, June 22–27, 2014, Snowbird, UT, USA.

Copyright 2014 ACM 978-1-4503-2376-5/14/06 ...$15.00.

Divesh Srivastava3 &nbsp; &nbsp; Xiaokui Xiao1

2University of Warwick

g.cormode@warwick.ac.uk

# 1. INTRODUCTION

The problem of privacy-preserving data publishing (PPDP) has become increasingly important in recent years. Often, we encounter situations where a data owner wishes to make available a data set without revealing private, sensitive information. For example, this arises in the case of revealing detailed demographic information about citizens (census), patients (health data), investors (financial data), and so on. In each example, there are many potential uses and users for the data: for social analysis, for medical research, for freedom-of-information and other legal disclosure reasons. The canonical case in PPDP is when the database can be modeled as a table, where each row may contain information about an individual (say, details of their medical status, or employment information). Then, the aim is to release some manipulated version of this information so that this can still be used for the intended purpose, but the privacy of individuals in the database is preserved.

Following many attempts to formally define the requirements of privacy, the current state-of-the-art solution is to seek the differential privacy guarantee [16]. Informally, this model requires that what can be learned from the released data is (approximately) the same, whether or not any particular individual was included in the input database. This model offers strong privacy protection, and does not make any limiting assumptions about the power of the notional adversary: it remains a strong model even in the face of an adversary with much background knowledge and reasoning power.

Putting differential privacy into practice remains a challenging problem. Since its proposal, there have been many efforts to develop mechanisms and processes for data release for different kinds of input database, and for different objectives for the end use. However, it seems that all existing techniques encounter difficulties when trying to release even moderately high-dimensional data – that is, an input table with half a dozen columns or more. The reasons for these problems are two-fold:

- Output Scalability: Most algorithms (see, e.g., [37]) either explicitly or implicitly represent the database as a vector x of size equal to the domain size, that is, the product of cardinalities of the attributes. For many natural data sets, the domain size m is orders of magnitude larger than the data size n [13]. Hence, these algorithms become inapplicable for any realistic dataset with a moderate-to-high number of attributes. For example, a million row table with ten attributes, each of which has 20 possible values, results in a domain size (and hence an output size) of m = 2010 ≈ 10T B, which is very unwieldy and slow to use compared to the input which can be measured in megabytes.
- Signal-to-noise ratio: When the high dimensional database is represented as a vector x, the average count in each entry, given by n/m, is typically very small. Once noise is added to x (or some transformation of it) to obtain another vector x*, the noise com-

pletely dominates the original signal, making the published vector x∗ next to useless. For example, if the table above has size n = 1M, the average entry count is n/m = 10−7. By contrast, the average noise injected to achieve, e.g., differential privacy with parameter ε = 0.1 has expected magnitude around 10. Even if the data is skewed in the domain space, i.e., there are some entries x[i] with high counts, such peaks are infrequent and so the vast majority of published values x∗[i] are useless.

# 1.1    Related Work

A full survey of methods to realize differential privacy is beyond the scope of this work. Here, we identify the most related efforts, and discuss why they cannot fully solve the problems above. Initial efforts released projections of the data on subsets of dimensions, via Fourier decompositions [5]. This reduces the impact of higher dimensionality, but requires the data owner to determine (somehow) which set of dimensions are of interest, and for the data to be mapped into a space where a Fourier decomposition can be computed. Subsequent work followed this template, for example by searching for meaningful “subcubes” of the datacube representation to release privately [15]. These can be aggregated to answer lower-dimensional cube queries, where the signal-to-noise ratio in each cell is significantly higher than in the original domain. A major limitation is that the running time is exponential in the dimensionality of the domain, making it inapplicable for all but small sets. Accuracy is improved by additional post-processing of the output to restore “consistency” of the counts mandated by the structure (e.g. using the fact that counts in a hierarchy should sum to the count of an ancestor) [5, 15, 21]; however, this improvement does not overcome the inherent error incurred in high dimensions.

Another approach is to use data reduction techniques to avoid dimensionality issues. For example, [13] proposes various sampling mechanisms to reduce the size of (and time to produce) the output x∗, while approximately preserving the accuracy of subset-sum queries. However, the accuracy guarantee is with respect to the accuracy of using the entire vector x∗, rather than the original vector x, which degrades rapidly with data dimensionality. The approach in [12] tries to keep the domain size of the output small, and the density of data within the new domain high, by adaptively grouping some of the attribute values. In the example above, suppose that on each of the ten attributes, we grouped the 20 values into two groups. Thus, we reduce the output size from 2010 to 210 ≈ 1MB. This coarser grid representation of the data loses precision, and in this example still leads to small average counts of the order of 1. Cormode et al. [12] address the latter problem by using spatial decompositions to define irregular grids over the domain of x, such that the count x[i] in each grid cell is sufficiently large. This makes sense only for range queries, and requires all attributes to have an ordering.

Other approaches find other ways to recast the input data and release it, so that the noise from the privacy transformation has less impact on certain statistics. In particular, Xiao et al. [37] make use of the wavelet transformation of data. This addresses range queries, and means that the noise incurred by a range query scales proportionately to the logarithm of its length, rather than to its length directly. The value of this is confined primarily to low-dimensional databases where range queries are anticipated. More generally, the matrix mechanism of Li and Miklau [25, 26] and subsequent related approaches [20, 38, 39] take in a query workload (expressed in terms of a weighted set of inner-product queries), and seek to release a version of the database that minimizes the noise for these queries. The cost of this optimization can be high, and critically assumes the foreknowledge of the query distribution.

The above discussion focuses on methods that produce an output in a general form that can be used flexibly for a variety of subsequent analyses. There is also a large class of results that instead generate a description of the output of a specific algorithm computed under privacy, for example the result of building a classifier for the data. Prominent examples include the work of McSherry and Mironov [30] to mask the preferences of individual raters in recommendation systems; Rastogi and Nath [34] to release time-series data based on leading Fourier coefficients; and McSherry and Mahajan [29] for addressing common queries over network trace data. Differential privacy has also been applied to other sophisticated data analysis tasks, e.g., coresets for summarizing geometric data [18], building classifiers in the form of decision trees [19], and support vector machines [35], and mining the occurrence of frequent patterns [6, 27].

Some frameworks have been proposed to solve a class of optimization problems. For example, Chaudhuri et al. [8, 9] and Kifer et al. [22] consider differentially private convex empirical risk minimization, which can be applied to a wide range of optimization problems (e.g., logistic regression and support vector machines). Zhang et al. [40] propose the PrivGene framework, which is a combination of genetic algorithms and an enhanced version of the exponential mechanism for differentially private model fitting. Smith et al. [33, 36] and Mohan et al. [32] respectively present and implement the sample and aggregate framework that can be used for any analysis task whose results are not affected by the number of records in the database. While this class of methods obtains generally good results for the target problem, it requires fixing this objective at the time of data release, and limits the applicability of the output for other uses.

# 1.2    Our Contributions

In this paper, we propose a powerful solution to the problem of publishing differentially private high-dimensional data. Unlike the bulk of prior work, which focuses on optimizing the output for specific workloads (e.g., range queries, cube queries), we aim to approximate the high-dimensional distribution of the original data with a data-dependent set of well-chosen low-dimensional distributions, in the belief that, for a sufficiently accurate approximation, the resulting data will maintain high accuracy for almost any type of (linear or non-linear) query. Since our approach is query-independent, many different queries can be evaluated (accurately) on the same set of released data. Query-independence means that our approach may be weaker than approaches that target a particular query set; however, we show empirically that the gap is small or non-existent in many natural cases. By working in low-dimensional spaces, we avoid the signal-to-noise problem. Although we compare to the full-dimensional distribution for evaluation purposes, our approach never needs to compute this explicitly, thus avoiding the scalability problem.

To achieve this goal, we start from the well-known Bayesian network model, which is widely studied in the statistical and machine learning communities [23]. Bayesian networks combine low-dimensional distributions to approximate the full-dimensional distribution of a data set, and are a simple but powerful example of a graphical model. Our algorithm, dubbed PRIVBAYES, consists of the following steps:

1. (Network learning) We describe how to compute a differentially private Bayesian network that approximates the full-dimensional distribution via the Exponential Mechanism (EM). This step requires new theoretical insights, which are described in Section 3. We improve on this basic approach by defining a new quality function for use in EM, which results in significantly better networks.

# Figure 1: A Bayesian network N1 over five attributes

being found. The definition and computation of this function are one of our main technical contributions; see Section 4.

1. (Distribution learning) We explain how to compute the necessary differentially private distributions of the data in the subspaces of the Bayesian network, via the Laplace Mechanism.
2. (Data synthesis) We show how to generate synthetic data from the differentially private Bayesian network, without explicitly materializing the global distribution.

In Section 5, we provide an extensive experimental evaluation of the accuracy of the synthetic datasets generated above, over workloads of linear and non-linear queries. In each case, we compare to prior methods specifically designed to optimize the accuracy for that type of workload. Our experiments show that PRIVBAYES is often more accurate than any prior method, even though it is not optimized for any specific type of query. When PRIVBAYES is less accurate than some prior method, the accuracy loss is small and, we believe, an acceptable tradeoff, since PRIVBAYES offers a generic solution that does not require prior knowledge of the workload and works well on many different types of queries.

# 2. PRELIMINARIES

This section reviews two concepts closely related to our work, namely, differential privacy and Bayesian networks.

# 2.1 Differential Privacy

Let D be a sensitive dataset to be published. Differential privacy requires that, prior to D’s release, it should be modified using a randomized algorithm G, such that the output of G does not reveal much information about any particular tuple in D. The formal definition of differential privacy is as follows:

DEFINITION 1 (ε-DIFFERENTIAL PRIVACY [17]). A randomized algorithm G satisfies ε-differential privacy, if for any two datasets D1 and D2 that differ only in one tuple, and for any possible output O of G, we have

Pr⎜G(D1) = O⎜ ≤ eε · Pr⎜G(D2) = O⎜,

where Pr[·] denotes the probability of an event.

In what follows, we say that two datasets are neighboring if they differ in only one tuple, i.e., the values of one tuple are changed while the rest are identical. While there are many approaches to achieving differential privacy, we rely on the two best known and most-widely used, namely, the Laplace mechanism [17] and the exponential mechanism [31].

The Laplace mechanism releases the result of a function F that takes as input a dataset and outputs a set of numeric values. Given F, the Laplace mechanism transforms F into a differentially private algorithm, by adding i.i.d. noise (denoted as η) into each output value of F. The noise η is sampled from a Laplace distribution Lap(λ) with the following pdf: Pr[η = x] = (1 / (2λ)) e-|x|/λ. Dwork et al. [17] prove that the Laplace mechanism ensures ε-differential privacy if λ ≥ S(F)/ε, where S(F) is the sensitivity of F:

**Table 1: The attribute-parent pairs in N1**
|i|Xi|Πi|
|---|---|---|
|1|age|/|
|2|education|{age}|
|3|workclass|{age}|
|4|title|{age, education, workclass}|
|5|income|{workclass, title}|

DEFINITION 2 (SENSITIVITY [17]). Let F be a function that maps a dataset into a fixed-size vector of real numbers. The sensitivity of F is defined as

S(F) = max ‖F(D1)− F(D2)‖1,

where ‖·‖1 denotes the L1 norm, and D1 and D2 are any two neighboring datasets. Intuitively, S(F) measures the maximum possible change in F’s output when we modify one arbitrary tuple in F’s input.

When F’s output is categorical instead of numeric, the Laplace mechanism does not apply, but the exponential mechanism [31] can be used instead. The exponential mechanism releases a differentially private version of F, by sampling from F’s output domain Ω. The sampling probability for each ω ∈ Ω is determined based on a user-specified score function fs, which takes as input any dataset D and any element ω ∈ Ω, and outputs a numeric score fs(D,ω) that measures the quality of ω: a larger score indicates that ω is a better output with respect to D. More specifically, given a dataset D, the exponential mechanism samples ω ∈ Ω with a probability proportional to exp(fs(D,ω)/2Δ), where Δ is a scaling factor that controls the degree of privacy protection. McSherry and Talwar [31] show that the exponential mechanism achieves ε-differential privacy if Δ ≥ S(fs)/ε, where S(fs) is defined as:

S(fs) = Dmaxω′ |fs(D1, ω′) − fs(D2,ω′)|,

for D1 and D2 any two neighboring datasets, and ω′ any element in Ω. For convenience, we also refer to S(fs) as the sensitivity of fs, as it is similar in form to sensitivity as defined above.

Both mechanisms can be applied quite generally; however, to be effective we seek to ensure that the noise introduced does not outweigh the signal in the data, and that it is computationally efficient to apply the mechanism. This requires a careful design of what functions to use in the mechanisms.

# 2.2 Bayesian Network

Let A be the set of attributes on a dataset D, and d be the size of A. A Bayesian network on A is a way to compactly describe the (probability) distribution of the attributes in terms of other attributes. Formally, a Bayesian network is a directed acyclic graph (DAG) that (i) represents each attribute in A as a node, and (ii) models conditional independence among attributes in A using directed edges. As an example, Figure 1 shows a Bayesian network over a set A of five attributes, namely, age, education, workclass, title, and income. For any two attributes X, Y ∈ A, there exist three possibilities for the relationship between X and Y:

Case 1: Direct dependence. There is an edge between X and Y, say, from Y to X. This indicates that for any tuple in D, its distribution on X is determined (in part) by its value on Y. We define Y as a parent of X, and refer to the set of all parents of X as its parent set. For example, in Figure 1, the edge from workclass to income indicates that the income distribution depends on the type of job (and also on title).

# Table 2: Table of notations

|Notation|Description|
|---|---|
|D|A sensitive dataset to be published|
|n|The number of tuples in D|
|A|The set of attributes in D|
|d|The number of attributes in A|
|N|A Bayesian network over A|
|Pr[A]|The distribution of tuples in D|
|PrN [A]|An approximation of Pr[A] defined by N|
|dom(X)|The domain of random variable X|

# Case 2: Weak conditional independence.

There is a path (but no edge) between Y and X. Assume without loss of generality that the path goes from Y to X. Then, X and Y are conditionally independent given X’s parent set. For instance, in Figure 1, there is a two-hop path from age to income, and the parent set of income is {workclass, title}. This indicates that, given workclass and job title of an individual, her income and age are conditionally independent.

# Case 3: Strong conditional independence.

There is no path between Y and X. Then, X and Y are conditionally independent given any of X’s and Y’s parent sets.

Formally, a Bayesian network N over A is defined as a set of d attribute-parent (AP) pairs, (X1, Π1), . . . ,(Xd, Πd), such that:

1. Each Xi is a unique attribute in A;
2. Each Πi is a subset of the attributes in A\{Xi}. We say that Πi is the parent set of Xi in N;
3. For any 1 ≤ i < j ≤ d, we have Xj ∉ Πi, i.e., there is no edge from Xi to Xj in N. This ensures that the network is acyclic, namely, it is a DAG.

We define the degree of N as the maximum size of any parent set Πi in N. For example, Table 1 shows the AP pairs in the Bayesian network N1 in Figure 1; N1’s degree equals 3, since the parent set of any attribute in N1 has a size at most three.

Let Pr[A] denote the full distribution of tuples in database D. The d AP pairs in N essentially define a way to approximate Pr[A] with conditional distributions Pr[X1 | Π1], Pr[X2 | Π2], . . . , Pr[Xd | Πd]. In particular, under the assumption that any Xi and any Xj ∈ A are conditionally independent given Πi, we have:

Pr[A] = Pr[X1, X2, . . . , Xd] = Pr[X1] · Pr[X2 | X1] · Pr[X3 | X1, X2] . . . Pr[Xd | X1, . . . , Xd−1]

= ∏i=1d Pr[Xi | Πi].

Let PrN[A] = ∏i=1d Pr[Xi | Πi] be the above approximation of Pr[A] defined by N. Intuitively, if N accurately captures the conditional independence among the attributes in A, then PrN[A] would be a good approximation of Pr[A]. In addition, if the degree of N is small, then the computation of PrN[A] is relatively simple as it requires only d low-dimensional distributions Pr[X1 | Π1], Pr[X2 | Π2], . . . , Pr[Xd | Πd]. Low-degree Bayesian networks are the core of our solution to release high-dimensional data. Table 2 shows notation that will be frequently used in this paper.

# 3. SOLUTION OVERVIEW

This section presents an overview of PRIVBAYES, our solution for releasing a high-dimensional dataset D in an ε-differentially private manner. PRIVBAYES runs in three phases:

# Algorithm 1 NoisyConditionals (D, N, k): returns P*

1. Initialize P* = 0
2. for i = k + 1 to d do
3. Materialize the joint distribution Pr[Xi, Πi]
4. Generate differentially private Pr*[Xi, Πi] by adding Laplace noise Lap(4·(d−k)/n·ε)
5. Set negative values in Pr*[Xi, Πi] to 0 and normalize;
6. Derive Pr*[Xi | Πi] from Pr*[Xi, Πi]; add it to P*
7. for i = 1 to k do
8. Derive Pr*[Xi | Πi] from Pr*[Xk+1, Πk+1]; add it to P*
9. return P*

1. Construct a k-degree Bayesian network N over the attributes in D, using an (ε/2)-differentially private method. (k is a small value that can be chosen automatically by PRIVBAYES.)

2. Use an (ε/2)-differentially private algorithm to generate a set of conditional distributions of D, such that for each AP pair (Xi, Πi) in N, we have a noisy version of the conditional distribution Pr[Xi | Πi]. (We denote this noisy distribution as Pr*[Xi | Πi].)

3. Use the Bayesian network N (constructed in the first phase) and the d noisy conditional distributions (constructed in the second phase) to derive an approximate distribution of the tuples in D, and then sample tuples from the approximate distribution to generate a synthetic dataset D*.

In short, PRIVBAYES utilizes a low-degree Bayesian network N to generate a synthetic dataset D* that approximates the high-dimensional input data D. The construction of N is highly non-trivial, as it requires carefully selecting AP pairs and the value of k to derive a close approximation of D without violating differential privacy. By contrast, the second and third phases of PRIVBAYES are relatively straightforward. In the following, we will clarify the details of these phases, and prove the privacy guarantee of PRIVBAYES; the algorithm for PRIVBAYES’s first phase will be elaborated in Section 4.

# Generation of Noisy Conditional Distributions.

Suppose that we are given a k-degree Bayesian network N. To construct the approximate distribution PrN[A], we need d conditional distributions Pr[Xi | Πi] (i ∈ [1, d]), as shown in Equation (4). Algorithm 1 illustrates how the distributions specified by our algorithm can be derived in a differentially private manner. In particular, for any i ∈ [k + 1, d], the algorithm first materializes the joint distribution Pr[Xi, Πi] (Line 3), and then injects Laplace noise into Pr[Xi, Πi] to obtain a noisy distribution Pr*[Xi, Πi] (Line 4-5). To enforce the fact that these are probability distributions, all negative numbers in Pr*[Xi, Πi] are set to zero, then all values are normalized to maintain a total probability mass of 1 (Line 5). After that, based on Pr*[Xi, Πi], the algorithm derives a noisy version of the conditional distribution Pr[Xi | Πi], denoted as Pr*[Xi | Πi] (Line 6). The scale of the Laplace noise added to Pr[Xi, Πi] is set to 4(d − k)/nε, which ensures that the generation of Pr*[Xi, Πi] satisfies (ε/2(d − k))-differential privacy, since Pr*[Xi, Πi] has sensitivity 2/n. Meanwhile, the derivation of Pr[Xi | Πi] from Pr*[Xi, Πi] does not incur any privacy cost, as it only relies on Pr*[Xi, Πi] instead of the input data D.

Overall, Lines 2-7 of Algorithm 1 construct (d − k) noisy conditional distributions Pr*[Xi | Πi] (i ∈ [k + 1, d]), and they satisfy (ε/2)-differential privacy, since each Pr[Xi | Πi] is (ε/2(d − k))-differentially private. This is due to the composability property of differential privacy.

# 4. PRIVATE BAYESIAN NETWORKS

This section presents our solution for constructing differentially private Bayesian networks. We will first introduce a non-private algorithm for Bayesian network construction (in Section 4.1), and then explain how the algorithm can be converted into a differentially private solution (in Sections 4.2 and 4.3).

# 4.1 Non-Private Methods

Suppose that we aim to construct a k-degree Bayesian network N on a dataset D containing a set A of attributes. Ideally, N should provide an accurate approximation of the tuple distribution in D, i.e., PrN[A] should be close to Pr[A]. A natural question is under what condition will PrN[A] closely approximate Pr[A]? We make use of standard notions from information theory to measure this. The entropy of a random variable X over its domain dom(X) is denoted by H(X) = −∑ Pr[X = x] log Pr[X = x], and I(·,·) denotes the mutual information between two variables as:

I(X, Π) = ∑∑ Pr[X = x, Π = π] log Pr[X = x] Pr[Π = π],

where Pr[X, Π] is the joint distribution of X and Π, and Pr[X] and Pr[Π] are the marginal distributions of X and Π respectively. The KL-divergence of PrN[A] from Pr[A] measures the difference between the two probability distributions, and is defined by:

KL(Pr[A], PrN[A]) = − ∑ I(Xi, Πi) + ∑ H(Xi) − H(A).

We seek a Bayesian network representation so that the KL-divergence between the original and the approximate distribution is small. In (6), the term ∑d H(Xi) − H(A) is solely decided by Pr[A], which is fixed once the input database D is given. Hence, the KL-divergence of PrN[A] from Pr[A] is small (in which case they closely approximate each other), if and only if ∑d I(Xi, Πi) is maximized. Therefore, the construction of N can be modeled as an optimization problem, where we aim to choose a parent set Πi for each attribute Xi in D to maximize ∑d I(Xi, Πi).

For the case when k = 1, Chow and Liu show that greedily picking the next edge based on the maximum mutual information is optimal, leading to the celebrated notion of Chow-Liu trees. However, as shown in [10], this optimization problem is NP-hard when k > 1. For this reason, heuristic algorithms (e.g., hill-climbing, genetic algorithms, and simulated annealing) are often employed in practice. In the context of differential privacy, however, a different calculus applies: these methods incur a high cost in terms of sensitivity and so incur a large amount of noise. That is, these algorithms make many queries to the data, so that making them differentially private entails large perturbations which lead to poor overall accuracy. Therefore, we seek a new method that will imply less noise, and so give a better overall approximation when the noise is added. Thus we propose a greedy algorithm that makes fewer probes to the data, by extending the Chow-Liu approach to higher degrees, described in Algorithm 2.

# Algorithm 2 GreedyBayes (D, k): returns N

1. Initialize N = 0 and V = 0
2. Randomly select an attribute X from A; add (X, 0) to N; add X1 to V
3. for i = 2 to d do
4. Initialize Ω = 0
5. For each X ∈ ⌈V⌉
6. Select a pair (A\V and each Π ∈ k, add (X, Π) to Ω
7. Add (Xi, Πi) to N; add Xi to V
8. return N

All logarithms used in this paper are to the base 2.

that contains all attributes whose parent sets have been fixed in the proof for brevity, but the maximum difference in mutual informa- partial construction of N. As a next step, the algorithm randomly selects an attribute (denoted as X1) from A, and sets its parent set Π to /

|X\Π|0|1|
|---|---|---|
|0|0|0|
|1|0|1|

The mutual information of the left distribution is 0, and that of the right one is 1 log n + n−1 log n.

1. |Π| ≤ k, which ensures that N is a k-degree Bayesian network. This is ensured by choosing Π only from ⌈V⌉, where ⌈V⌉ denotes the set of all subsets of V with size min(k,|Vk|) k

2. N contains no edge from Xi to Xj for any j (Lines 5-6).

tees that N is a DAG. We ensure this condition by requiring that in the beginning of any iteration, V only contains the attributes whose parent sets have been decided in the previous iterations (Line 7). In other words, the parent set of Xi can only be a subset of {X1,X2, . . . , Xi−1}, as a consequence of which N cannot contain any edge from Xi to Xj for any j < i.

Once the parent set of each attribute is decided, the algorithm terminates and returns the Bayesian network N (Line 9). The number of pairs considered in iteration i is (d − i)⌈i⌉, so summing over all iterations the cost is bounded by d∑i=1d⌈i⌉ = d⌈d+1⌉. This determines the asymptotic cost of the procedure. Note that when k = 1, the above algorithm is equivalent to Chow and Liu’s method [11] for constructing optimal 1-degree Bayesian networks.

# 4.2 A First-Cut Solution

Observe that in Algorithm 2, there is only one place where we interact directly with the input dataset D, namely, the greedy selection of an AP pair (Xi, Πi) in each iteration of the algorithm (Line 6). Therefore, if we are to make Algorithm 2 differentially private, we only need to replace Line 6 of Algorithm 2 with a procedure that selects (Xi,Πi) from Ω in a private manner. Such a procedure can be implemented with the exponential mechanism outlined in Section 2.1, using the mutual information function I as the score function. Specifically, we first inspect each AP pair (X,Π) ∈ Ω, and calculate the mutual information I(X,Π) between X and Π; after that, we sample an AP pair from Ω, such that the sampling probability of any pair (X, Π) is proportional to exp(I(X, Π)/2Δ), where Δ is a scaling factor.

The value of Δ is set as follows. As mentioned in Section 3, PRIVBAYES requires that the construction of the Bayesian network N should satisfy (ε/2)-differential privacy. Accordingly, we set Δ = 2(d − 1)S(I)/ε, where S(I) denotes the sensitivity of the mutual information function I (see Equation 3). This ensures that each invocation of the exponential mechanism satisfies (ε/2(d − 1))-differential privacy. Given the composability property of differential privacy [16] and the fact that we only invoke the exponential mechanism d − 1 times during the construction of N, it can be verified that the overall process of constructing N is (ε/2)-differentially private.

# 4.3 An Improved Solution

The method in Section 4.2 is simple and intuitive, but may not achieve the best results: Observe that S(I) > log n/n; this can be large compared to the range of I. For example, range(I) = 1 for binary distributions. As a consequence, the scaling factor Δ = 2(d − 1)S(I)/ε tends to be large, and so the exponential mechanism is still quite likely to sample (from Ω) an AP pair with a small mutual information. In that case, the Bayesian network N constructed using the exponential mechanism will offer a weak approximation of Pr[A], resulting in a low-quality output from PRIVBAYES. To improve over this solution, we propose to avoid using I as the score function in the exponential mechanism. Instead, we define a novel function F that maps each AP pair (X,Π) ∈ Ω to a score, such that

1. F’s sensitivity is small (with respect to the range of F).
2. If F(X, Π) is large, then I(X, Π) tends to be large.

The rationale is that since S(F) is small with respect to range(F), the scaling factor Δ = 2(d − 1)S(F)/ε will also be small, and hence, the exponential mechanism has a high probability to select an AP pair (X, Π) with a large F(X, Π). In turn, such an AP pair tends to have a large mutual information between X and Π, which helps improve the quality of the Bayesian network N.

In what follows, we will clarify our construction of F. To achieve property 2 above, we set F to its maximum value (i.e., 0) when I is greatest. To achieve property 1, we make F(X, Π) decrease linearly in proportion to the L1 distance from Pr[X,Π] to a distribution that maximizes F, since linear functions ensure that the sensitivity is controlled: the function does not change sharply anywhere in its domain. We first introduce the concept of maximum joint distribution, which will be used to define the peaks of F, and then characterize such distributions:

# DEFINITION 3 (MAXIMUM JOINT DISTRIBUTION)

Given an AP pair (X, Π), a maximum joint distribution Pr [X,Π] for X and Π is one that maximizes the mutual information between X and Π.

# LEMMA 2

Assume that |dom(X)| ≤ |dom(Π)|. A distribution Pr [X, Π] is a maximum joint distribution if and only if

1. Pr [X = x] = 1/|dom(X)|, for any x ∈ dom(X);
2. For any π ∈ dom(Π), there is at most one x ∈ dom(X) with Pr [X = x, Π = π] > 0.

DeProofs in this section are deferred to the appendix. We illustrate Definition 3 and Lemma 2 with an example:

# EXAMPLE 2

Consider a binary variable X with dom(X) = {0, 1} and a variable Π with dom(Π) = {a, b, c}. Consider two joint distributions between X and Π as follows:

|X\Π|a|b|c|
|---|---|---|---|
|0|0.5|0|0|
|1|0|0.5|0|

|X\Π|a|b|c|
|---|---|---|---|
|0|0|0|0.2|
|1|0.5|0|0|

By Lemma 2, both of the above distributions are maximum joint distributions, with I(X,Π) = 1.

Let (X ,Π) be an AP pair, and P [X, Π] be the set of all maximum joint distributions for X and Π. Our score function F (for evaluating the quality of (X, Π)) is defined as

1
F(X,Π) = − min ∥Pr ∈P ∥Pr[X, Π]− Pr [X ,Π]∥1.

If F (X,Π) is large, then Pr[X, Π] must have a small L1 distance to one of the maximum joint distributions in P [X, Π], and vice-versa. In turn, if Pr[X,Π] is close to a maximum joint distribution in P [X, Π], then intuitively, Pr[X,Π] is likely to give a large mutual information between X and Π. In other words, the value of F(X,Π) tends to be positively correlated with I(X,Π). This explains why F could be a good score function to replace I. In addition, F has a much smaller sensitivity than I, as shown in the following theorem:

# THEOREM 2.

S(F ) = 1/n.

This follows immediately from considering the L1 distance between neighboring distributions. Observe that S(F) &lt; S(I)/log n, where n is the number of tuples in the input data. Meanwhile, the ranges of F and I are comparable; for example, range(I) = 1 and range(F) = 0.5 for binary domains. Therefore, when n is large (as is often the case), the sensitivity-to-range ratio of F is significantly smaller than that of I, which makes F a favorable score function over I for selecting AP pairs in the Bayesian network N.

# 4.4 Computation of F

While (7) defines the function F, it still remains unclear how we can calculate F(X,Π) given Pr[X, Π]. In this subsection, we use dynamic programming to solve the problem for the case when all attributes in Π ∪ {X} have binary domains; we address the case of non-binary domains in Section 4.5.

Let (X, Π) be an AP pair where |Π| = k. Then, the joint distribution Pr[X, Π] can be represented by a 2×2k matrix where the sum of all elements is 1. For example, Table 3(a) illustrates a joint distribution Pr[X,Π] with |Π| = 2. To compute F(X,Π), we need to identify the minimum L1 distance between Pr[X,Π] and a maximum joint distribution Pr [X, Π] ∈ P [X, Π]. Table 3(b) illustrates one such maximum joint distribution, whose L1 distance to the distribution in Table 3(a) equals 0.4. To derive the minimum L1 distance, a naive approach is to enumerate all maximum joint distributions in P [X,Π]; nevertheless, as P [X, Π] may contain an infinite number of maximum joint distributions, a brute-force enumeration of P is infeasible. To address this issue, we will first introduce an exponential-time algorithm for computing F(X, Π), which will serve as the basis of our dynamic programming solution.

The basic idea of our exponential-time algorithm is to (i) partition the distributions in P into a finite number of equivalence classes, and then (ii) compute F(X,Π) by processing each equivalence class individually. By Lemma 2, any maximum joint distribution Pr [X, Π] has the following property: for any π ∈ dom(Π), either Pr [X = 0,Π = π] = 0 or Pr [X = 1, Π = π] = 0. In other words, for each column in the matrix representation of Pr [X,Π] (where a column corresponds to a value in dom(Π)), there should be at most one non-zero entry. For example, the gray cells in Table 3(b) indicate the positions of non-zeros in the given maximum joint distribution.

# Table 3: An example of joint distributions

|X\Π|00|01|10|11|
|---|---|---|---|---|
|0|0.6|0|0|0|
|1|0.1|0.1|0.1|0.1|

|X\Π|00|01|10|11|
|---|---|---|---|---|
|0|0.5|0|0|0|
|1|0|0.3|0.1|0.1|

To explain, consider a particular equivalence class E. Let Z be the set of pairs (x,π), such that Pr [X = x,Π = π] = 0 for any Pr [X, Π] ∈ E. That is, Z− captures the positions of all zero entries in the matrix representation of Pr [X = x, Π = π]. Similarly, we define the sets of non-zero entries in row X = 0 and X = 1 as

Z0+ = {(0, π) | Pr [X = 0, Π = π] > 0}, and Z1+ = {(1, π) | Pr [X = 1, Π = π] > 0}.

For convenience, we also abuse notation and define

Pr[Z−] = ∑(x,π)∈Z− Pr[X = x, Π = π],

Pr[Z0+] = ∑(x,π)∈Z0 Pr[X = x, Π = π],

Pr[Z1+] = ∑(x,π)∈Z1 Pr[X = x, Π = π].

By Lemma 2, we have Pr [Z−] = 0, Pr [Z0+] = 1/2, and Pr [Z1+] = 1/2 for any Pr [X,Π] ∈ E. Then, for any Pr[X, Π], its L1 distance to a distribution Pr [X,Π] ∈ E is bounded by:

∥Pr[X, Π] − Pr [X,Π]∥1 ≥ Pr[Z−] + |Pr[Z0+] − 2| + |Pr[Z1+] − 2|.

Let (x)+ denote max(0, x). Given that Pr[Z−] + Pr[Z0+] + Pr[Z1+] = 1, the above inequality can be simplified to

∥Pr[X, Π] − Pr [X,Π]∥1 ≥ 2 · (1/2 − Pr[Z0+] + 1/2 − Pr[Z1+]).

Furthermore, there always exists a Pr [X, Π] ∈ E that makes the equality hold. In other words, once the positions of the non-zero entries in Pr [X,Π] are fixed, we can use the above to derive the minimum L1 distance from any Pr[X,Π] to E, with a linear scan of the entries in the matrix representation of Pr[X,Π]. By enumerating all O(32k) equivalence classes of P, we can then derive F(X,Π).

The above procedure for calculating F is impractical when k ≥ 4, as the exhaustive search over all possible equivalence classes of P is prohibitive. To tackle this problem, we propose a dynamic-programming-based optimization that reduces computation costs by taking advantage of the fact that the distributions are induced by n items.

Based on the above, our target is to find a combination of Z0+ and Z1+ (which therefore determine Z−) that minimizes

1 − Pr[Z0+] + 1 − Pr[Z1+].

We define the probability mass associated with Z0+ and Z1+ as K0 and K1 respectively. Initially, K0 = K1 = 0. For each π ∈ dom(Π), we can either increase K0 by Pr[X = 0,Π = π] (by assigning (0,π) to Z0+) or increase K1 by Pr[X = 1,Π = π] (by assigning (1, π) to Z1+).

We index π ∈ dom(Π) as 1, 2, . . . , 2k. We use C(i,a,b) to indicate if K0 = a/n and K1 = b/n is reachable by using the first i π’s, i.e., π1, π2, . . . , πi. It can be verified that (i) C(i,a,b) = true if i = a = b = 0, (ii) C(i, a, b) = false if i < 0 or a < 0 or b < 0, and

(iii) otherwise,

C(i, a, b) = C(i −1, a−n Pr[X = 0, Π = πi],b) ∨ C(i−1,a,b −nPr[X = 1,Π = πi]).

Given an input dataset D with n tuple, each cell in Pr[X,Π] must be a multiple of 1/n. Thus, we only consider the case when a and b are integers in the range [0, n]. Thus, the total number of states C(i, a, b) is n2k. A direct traversal of all states takes O(n2k) time. To reduce this time complexity, we introduce the following concept:

# DEFINITION 4 (DOMINATEDSTATE).

A state C(i, a1, b1) is dominated by C(i,a2,b2) if and only if a1 ≤ a2 and b1 ≤ b2.

Note that a dominated state is always inferior to some other states, and hence, it can be ignored without affecting the correctness of final result. Consequently, we maintain the set of at most n non-dominated reachable states for each i ∈ [1,2k]. The function F can be calculated by

F (X,Π) = − min ⎛1 a ⎜ ⎛1 b⎜ C(2k,a,b)=true 2 n + 2 n +

As such, the total number of states that need to be traversed is n2k, and thus the complexity of the algorithm is reduced to O(n2k). Note that k is small in practice, since we only need to consider low-degree Bayesian networks.

# 4.5 Extension to General Domains

The dynamic programming approach in Section 4.4 assumes that all attributes in the input data D are binary. In this section, we extend our solution to the case when D contains non-binary attributes.

Following the common practice in the literature [38], our first solution converts each non-binary attribute in the dataset into a set of binary attributes. In particular, for each categorical attribute X whose domain size equals ℓ, we first encode each value in X’s domain into a binary representation with log ℓ bits; after that, we convert X into log(ℓ) binary attributes X1, X2, . . . , Xlogℓ, such that Xi corresponds to the i-th bit in the binary representation. Meanwhile, for each continuous attribute Y, we first discretize the domain of Y into a fixed number b of equi-width bins (we use b = 16), and then convert Y into log b binary attributes, using a similar approach to the transformation of X. After the transformation, D can be encoded to form a new database Db in the binary domain. After that, we apply PRIVBAYES on Db to generate a synthetic dataset D*, then decode it to get D* in the original domain.

A second solution tries to preserve the semantics of (discrete) attributes more directly, via a more complex search. Following the outline for the binary case, we can model the computation of F (X, Π) over non-binary attribute X and parent set Π as an optimization problem. Now the structure of X is more complex, the approach of dynamic programming does not apply. Instead, we can find a different combinatorial characterization of maximum distributions, and encode this with a set of (linear) constraints. The search for the minimum cost maximum distribution is an optimization problem over these constraints, which can be solved by an integer program. We postpone details of this approach to the full version of this paper; in our experiments, we show results using the binary encoding, which is effective enough for our purposes.

# 4.6 Choice of k and θ-usefulness.

We have discussed how to build a k-degree Bayesian network under differential privacy, where k is considered as a given input to the algorithm. However, k is usually unknown in real applications and should be chosen carefully. The choice of k is non-trivial for PRIVBAYES. Intuitively, a Bayesian network with a larger k keeps more information from the full dimensional distribution Pr[A], e.g., a (d − 1)-degree Bayesian network approximates Pr[A] perfectly without having any information loss. On the other hand, the downside of using large k is that it forces PRIVBAYES to anonymize a set of high-dimensional marginal distributions in the second phase, which are very vulnerable to noise due to their domains of large size. These noisy distributions are less useful after anonymization especially when the privacy budget ε is small, leading to a synthetic database full of random perturbation. With very small values of ε, the best choice may be to pick k = 0, i.e. to model all attributes as independent. Hence, the choice of k should balance the informativeness of a Bayesian network and the robustness of marginal distributions. This balancing act is affected by three parameters: the total privacy budget ε, the total number of tuples in database n, and usefulness of each noisy marginal distribution in the second phase θ. We quantify this in the following definition.

# DEFINITION 5 (θ-USEFULNESS).

A noisy distribution is θ-useful if the ratio of average scale of information to average scale of noise is no less than θ.

⌈ L EMMA 3. The noisy distributions in Algorithm 1 are (d − n· ε / (k + 3))·2k-useful.

PROOF. In Algorithm 1, each marginal distribution is (k + 1)-dimensional with a domain size 2k + 1. Therefore, the average scale of information in each cell is 1/2k + 1.

For the scale of noise, we have d − k marginal distributions to be anonymized and each of them consumes privacy budget ε/2(d − k). The sensitivity of each marginal distribution is 2/n. According to the Laplace mechanism, the Laplace noise N injected to each cell is drawn from distribution Lap(4(d − k)/nε) where the average scale of noise is E(|η|) = 4(d − k)/nε.

The notion of θ-usefulness provides a more intuitive way to choose k automatically without closely studying the specific instance of the input database. Generally speaking, we believe a 0.5-useful noisy distribution is not good because the scale of noise is twice as that of information, while a 5-useful one is more reliable due to its large information to noise ratio. In practice, we set up a threshold θ, then choose the largest positive integer k that guarantees θ-usefulness in parameter learning (note, this is independent of the data, as it depends only on the non-private values ε, θ, n and d). If such a k does not exist, k is set to the minimum value, 0. In the experimental section, we will show that there is a wide range of θ to choose to train a PRIVBAYES model for good performance.

# 5. EXPERIMENTS

# 5.1 Experimental Settings

Datasets. We make use of four real datasets in our experiments: (i) Adult [4], which includes the information of 45,222 individuals extracted from the 1994 US Census, (ii) NLTCS [2], which contains records of 21,574 individuals participated in the National Long Term Care Survey, (iii) TPC-E[3], 40,000 tuples obtained by joining four tables in the TPC-E benchmark: “Trade”, “Security”, “Security status” and “Trade type”, and (iv) BR2000 [1], which consists of 38,000 census records collected from Brazil in year 2000. Each of the four datasets contains both continuous and categorical attributes. Table 4 illustrates the properties of the datasets.

Tasks. We evaluate the performance of PRIVBAYES on two different tasks. The first task is to build all α-way marginals of a

# Table 4: Dataset characteristics.

|Dataset|Cardinality|Dimensionality|Domain size|F, k = 1|F, k = 2|F, k = 3|F, k = 4|F, k = 6|
|---|---|---|---|---|---|---|---|---|
|Adult|45,222|15|≈ 252|10|sum of mutual information|5|sum of mutual information| |
|NLTCS|21,574|16|≈ 216|8| |4.5| | |
|TPC-E|40,000|24|≈ 277|6| |4| | |
|BR2000|38,000|14|≈ 232|4| |3.5| | |

dataset [5]. For convenience, we use Qα to denote the set of all α-way marginals. We evaluate Q3 and Q4 on NLTCS, but examine Q2 and Q3 instead on the remaining three datasets, since each of those datasets leads to a prohibitively large number of queries in Q4. We measure the accuracy of each noisy marginal by the total variation distance [14] between itself and the noise-free marginal (i.e., half of the L1 distance between the two marginals, when both of them are treated as probability distributions). We use the average accuracy over all marginals as the final error metric for Qα.

The second task that we consider is to simultaneously train multiple SVM classifiers on a dataset, where each classifier predicts one attribute in the data based on all other attributes. Specifically, on Adult, we train four classifiers to predict whether an individual (i) is a female, (ii) holds a post-secondary degree, (iii) makes over 50K a year, and (iv) has never married, respectively. Meanwhile, on NLTCS, we construct four classifiers to predict whether a person (i) is unable to get outside, (ii) is unable to manage money, (iii) is unable to bathe, and (iv) is unable to travel, respectively. We omit the experiments on BR2000 and TCP-E for space reasons.

For each classification task, we use 80% of the tuples in the data as the training set, and the other 20% as the testing set. We apply PRIVBAYES on the training data to generate a synthetic dataset, and then use the synthetic data to construct SVM classifiers. The quality of each classifier is measured by its misclassification rate on the testing set, i.e., the fraction of tuples in the testing data that are incorrectly classified.

For each of the aforementioned tasks, we repeat each experiment on each method 50 times, and we report the average measurements in our experimental results.

# Baselines.

For the task of answering count queries in Qα, we compare PRIVBAYES with three approaches: (i) Laplace [17], which generates all α-way marginals of a dataset and then injects Laplace noise directly into each cell of the marginals, (ii) Fourier [5], which transforms the input data D into the Fourier domain, adds Laplace noise to a subset of Fourier coefficients and uses the noisy coefficients to construct α-way marginals, and (iii) Contingency, which first builds the noisy contingency table, and then projects it onto attribute subsets to compute marginals. However, Contingency is only applicable to NLTCS since its computational cost is proportional to the domain size of input data. We also considered several other existing approaches [15, 20, 24, 25, 39] for answering count queries under differential privacy, but find them inapplicable due to our datasets’ large domain size. For fair comparison, we adopt two consistency techniques to boost the accuracies of baselines: (i) non-negativity, which rounds all negative counts in a noisy marginal to 0, and (ii) normalization, which linearly rescales the counts in a noisy marginal to make them sum to n.

# 5.2 Effect of Quality Function F

In the first set of experiments, we evaluate the effectiveness of score function F (in Section 4.2) against the mutual information function I. Figure 2 illustrates the performance of PRIVBAYES when combined with F and I, respectively, using Adult and NLTCS. The performance of each combination evaluated by the sum of the mutual information of every AP pair in the Bayesian network N, i.e., ∑d I(Xi, Πi). Observe that F significantly outperforms I in almost all cases. This is consistent with our analysis in Section 5.2 that F helps improve the quality of the Bayesian network constructed by PRIVBAYES.

For small k values (i.e., k = 0, 1, 2), the time taken to construct a Bayesian network with F is less than 1 minute and is negligible. For larger values of k, the time taken is typically higher (a few hours in the case of k = 5). Note that this is not a major concern, since data release is not considered a real-time problem, and the computation of F for different combinations of attributes can be easily parallelized.

# 5.3 Choice of θ

Recall that PRIVBAYES has only one internal parameter: the degree of Bayesian network k. As discussed in Section 4.6, we adopt the θ-usefulness criterion to automatically select an appropriate.

# 5.4 α-way Marginals

This section compares PRIVBAYES with the Laplace and Fourier approaches on eight sets of marginals over four datasets, and Contingency on two sets over NLTCS. Figure 4 shows the average variation distance of each method for each query set Qα, varying the privacy budget ε.

PRIVBAYES clearly outperforms the other three methods in all cases. The relative superiority of PRIVBAYES is more pronounced when (i) ε decreases or (ii) the value of α increases. To explain, observe that when ε is small, PRIVBAYES chooses to construct a very low-degree Bayesian network (down to k = 0), due to the θ-usefulness criterion. As a consequence, the marginal distributions in the second phase of PRIVBAYES will be more robust against noise injection, which ensures the quality of the synthetic data will not degrade too significantly. In contrast, the performance of Laplace and Fourier is highly sensitive to ε, owing to which they incur considerable errors when ε decreases.

# 5.5 Multiple SVM Classifiers

In the last set of experiments, we evaluate different methods for SVM classification. As explained in Section 5.1, on each of Adult and NLTCS, we train four SVM classifiers simultaneously. For PRIVBAYES, we apply it to generate only one synthetic dataset D* from each training set, and then use D* to train all four classifiers required. The other differentially private methods (i.e., PrivateERM, PrivGene, and Majority) can only produce one classifier at a time. Therefore, for each of those methods, we evenly divide the privacy budget ε evenly into four parts, and use ε/4 budget to train each classifier. To illustrate the performance of PrivateERM when building a single classifier, we include an additional baseline referred to as “PrivateERM (Single)”. This baseline is identical to PrivateERM, expect that it uses a privacy budget of ε (instead of ε/4) in training each classifier.

Figure 5 shows the misclassification rate of each method as a function of the overall ε. The error of NoPrivacy remains unchanged for all ε, since it does not enforce ε-differential privacy—it represents the best case to aim for. The accuracy of Majority is insensitive to ε, since (i) it performs classification only by checking whether there exists more than 50% tuples in the training set with a certain label, and (ii) this check is quite robust against noise injection when the number of tuples in the training set is large (as is the case in our experiments).

As for the other methods, PRIVBAYES consistently outperforms PrivateERM and PrivGene on Adult, except for the case of ε = 1.6.

# Figure 4: α-way marginals on four datasets

|Privacy Budget ε|Privacy Budget ε|Privacy Budget ε|Privacy Budget ε|
|---|
|PrivBayes|Laplace|Fourier|Contingency|
|100% average variation distance|100% average variation distance|80% average variation distance|80% average variation distance|
|80%|80%|70%|70%|
|60%|60%|50%|50%|
|40%|40%|40%|40%|
|30%|30%|20%|20%|
|10%|10%|0%|0%|
|0% 0.05 0.1 0.2 0.4 0.8 1.6|0% 0.05 0.1 0.2 0.4 0.8 1.6|0% 0.05 0.1 0.2 0.4 0.8 1.6|0% 0.05 0.1 0.2 0.4 0.8 1.6|
|(a) Adult, Q2|(a) Adult, Q2|(a) Adult, Q2|(a) Adult, Q2|
|(b) Adult, Q3|(b) Adult, Q3|(b) Adult, Q3|(b) Adult, Q3|
|(c) NLTCS, Q3|(c) NLTCS, Q3|(c) NLTCS, Q3|(c) NLTCS, Q3|
|(d) NLTCS, Q4|(d) NLTCS, Q4|(d) NLTCS, Q4|(d) NLTCS, Q4|
|(e) TPC-E, Q2|(e) TPC-E, Q2|(e) TPC-E, Q2|(e) TPC-E, Q2|
|(f) TPC-E, Q3|(f) TPC-E, Q3|(f) TPC-E, Q3|(f) TPC-E, Q3|
|(g) BR2000, Q2|(g) BR2000, Q2|(g) BR2000, Q2|(g) BR2000, Q2|
|(h) BR2000, Q3|(h) BR2000, Q3|(h) BR2000, Q3|(h) BR2000, Q3|

# Figure 5: Multiple SVM classifiers on two datasets

| |NoPrivacy|PrivBayes|PrivateERM|PrivateERM (Single)|PrivGene|Majority| |
|---|---|---|---|---|---|---|---|
|50% misclassification rate|50% misclassification rate|45% misclassification rate|45% misclassification rate| | | | |
|45%|40%|40%|40%| | | | |
|40%|30%|35%|35%| | | | |
| | |35%| |30%|30%| | |
|30%|20%|25%|25%| | | | |
|25%|10%|20%|20%| | | | |
|20%| | | |15%| | | |
|15%|0.05|0.1|0.2|0.4|0.8|1.6| |
| |0.05|0.1|0.2|0.4|0.8|1.6| |
| |0.05|0.1|0.2|0.4|0.8|1.6| |
| |0.05|0.1|0.2|0.4|0.8|1.6| |

Meanwhile, on NLTCS, PRIVBAYES and PrivateERM are comparable, and they both outperform PrivGene. Interestingly, in Figure 5(c), the misclassification rate of PRIVBAYES increases when ε changes from 0.05 to 0.1. The reason is that we have tuned the parameter θ for PRIVBAYES based on count queries on Adult and NLTCS, and hence, our choice of θ (and thus, the choice of k) does not always guarantee the best performance for PRIVBAYES on classification tasks. Overall, PRIVBAYES is superior to both PrivateERM and PrivGene on this classification task.

On the other hand, PRIVBAYES is outperformed by PrivateERM (Single) in most cases. This is reasonable given that PrivateERM is designed solely for SVM classification, whereas PRIVBAYES does not specifically optimize for SVM classification when it generates the synthetic data. In general, the fact that PRIVBAYES can support multiple analytical tasks (without incurring extra privacy overhead) makes it highly favorable in the common case when the user does not have a specific task in mind and would like to conduct exploratory data analysis by experimenting with various tasks.

# 6. CONCLUDING REMARKS

The model of Bayesian networks has proven a powerful way to represent correlated data approximately. We have seen that it is also highly effective as a model to release data while respecting privacy. We see that data released this way is very accurate, and indeed offers better accuracy than customized mechanisms for particular objectives, such as classification. A crucial part of our approach is the crafting of a novel quality function F as a surrogate for mutual information, which dramatically improves the quality of the released data. This requires some effort in order to compute efficiently, although since this is part of the release process, we can afford to spend more time on this. Nevertheless, an open problem is to study functions which can substitute for mutual information and which are fast to compute.

# 7. REFERENCES

- [1] Integrated public use microdata series international. https://international.ipums.org.
- [2] Statlib. http://lib.stat.cmu.edu/.
- [3] Transaction processing performance council. http://www.tpc.org.
- [4] K. Bache and M. Lichman. UCI machine learning repository, 2013.
- [5] B. Barak, K. Chaudhuri, C. Dwork, S. Kale, F. McSherry, and K. Talwar. Privacy, accuracy, and consistency too: a holistic solution to contingency table release. In PODS, pages 273–282, 2007.
- [6] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta. Discovering frequent patterns in sensitive data. In KDD, pages 503–512, 2010.
- [7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. page 27, 2011.
- [8] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In NIPS, pages 289–296, 2008.
- [9] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069–1109, 2011.
- [10] D. M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of bayesian networks is NP-Hard. Journal of Machine Learning Research, 5:1287–1330, 2004.
- [11] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14:462–467, 1968.
- [12] G. Cormode, C. M. Procopiuc, E. Shen, D. Srivastava, and T. Yu. Differentially private spatial decompositions. In ICDE, 2012.

# APPENDIX

G. Cormode, C. M. Procopiuc, D. Srivastava, and T. T. L. Tran. Differentially private publication of sparse data. In ICDT, 2012.

# PROOF OF Lemma 2.

The maximum mutual information between variables X and Π is

maxI(X,Π) = min{maxH(X), maxH(Π)} = min{log |dom(X)|, log |dom(Π)|} = log|dom(X)|, given that |dom(X)| ≤ |dom(Π)|.

Therefore, the maximum joint distribution for X and Π should be a joint distribution for X and Π with mutual information log |dom(X)|.

Suppose that Pr(X, Π) is a joint distribution satisfying the two properties in Lemma 2. Given basic results in information theory, the two properties are equivalent to

1. H(X) = log |dom(X)|;
2. H(X | Π) = 0.

Thus, the mutual information of Pr(X, Π) is

I(X,Π) = H(X) − H(X | Π) = log |dom(X)|.

By definition, Pr(X, Π) is a maximum joint distribution. On the other hand, suppose that Pr(X, Π) is a maximum joint distribution with mutual information log|dom(X)|. The mutual information can be expressed as:

I(X, Π) = log|dom(X)| = H(X) − H(X | Π), where H(X) ≤ log|dom(X)| and H(X | Π) ≥ 0 always hold. Thus, we conclude that I is maximized in the (achievable) case that

1. H(X) = log |dom(X)|, which is achieved only by the uniform distribution over dom(X);
2. H(X | Π) = 0, which implies that there is an x for each π such that Pr[X = x | Π = π] = 1.

The above two conditions are equivalent to the properties in the statement of Lemma 2.

# PROOF OF Theorem 2.

Let F(X, Π) be the F function for variable X and Π given input database D, i.e,

FD(X, Π) = −2 min ‖PrD[X, Π] − Pr[X, Π]‖1.

Notice that P[X, Π] is independent of the input database D. Now consider a pair of neighboring databases D1 and D2. We have

‖PrD₁[X, Π] − PrD₂[X, Π]‖1 = 2/n.

Assume that Pr ∈ P[X, Π] is the closest maximum joint distribution to Pr[X, Π]. We have

FD(X, Π) = −1/2· ‖PrD[X, Π] − Pr‖1.

Combined with Equation 9, the L1 distance between PrD₂[X, Π] and Pr can be upper bounded using the triangle inequality:

‖PrD₂[X, Π] − Pr‖1 ≤ ‖PrD₁[X, Π] − Pr‖1 + ‖PrD[X, Π] − PrD₂[X, Π]‖1

= −2· FD₁(X, Π) + 2/n.

On the other hand, recall that Pr is a maximum joint distribution in P[X, Π]. Therefore,

FD(X, Π) = −1 min ‖PrD[X, Π] − Pr[X, Π]‖2.

Thus, FD₁(X, Π) − FD₂(X, Π) ≤ 1/n.

