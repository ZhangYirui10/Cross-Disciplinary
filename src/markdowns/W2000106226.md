# Mining Association Rules with Multiple Minimum Supports

# Bing Liu, Wynne Hsu and Yiming Ma

# School of Computing

# National University of Singapore

Lower Kent Ridge Road, Singapore 119260

{liub, whsu, maym}@comp.nus.edu.sg

# Abstract

Association rule mining is an important model in data mining. Its mining algorithms discover all item associations (or rules) in the data that satisfy the user-specified minimum support (minsup) and minimum confidence (minconf) constraints. Minsup controls the minimum number of data cases that a rule must cover. Minconf controls the predictive strength of the rule. Since only one minsup is used for the whole database, the model implicitly assumes that all items in the data are of the same nature and/or have similar frequencies in the data. This is, however, seldom the case in real-life applications. In many applications, some items appear very frequently in the data, while others rarely appear. If minsup is set too high, those rules that involve rare items will not be found. To find rules that involve both frequent and rare items, minsup has to be set very low. This may cause combinatorial explosion because those frequent items will be associated with one another in all possible ways. This dilemma is called the rare item problem. This paper proposes a novel technique to solve this problem. The technique allows the user to specify multiple minimum supports to reflect the natures of the items and their varied frequencies in the database. In rule mining, different rules may need to satisfy different minimum supports depending on what items are in the rules. Experiment results show that the technique is very effective.

# 1. Introduction

Association rules are an important class of regularities that exist in databases. Since it was first introduced in [2], the problem of mining associations has received a great deal of attention. The classic application is market basket analysis [2]. It analyzes how the items purchased by customers are associated. An example of an association rule is as follows, cheese -> beer [sup = 10%, conf = 80%]. This rule says that 10% of customers buy cheese and beer together, and those who buy cheese also buy beer 80% of the time. The basic model of association rules is as follows: Let I = {i1, i2, ..., im} be a set of items. Let T be a set of transactions (the database), where each transaction t (a data case) is a set of items such that t ⊆ I. An association rule is an implication of the form X -> Y, where X ⊆ I, Y ⊆ I, and X ∩ Y = ∅. The rule X -> Y holds in the transaction set T with confidence conf if c% of transactions in T that support X also support Y. The rule has support s% in T if s% of the transactions in T contains X ∪ Y.

Knowing that 0.5% of the customers buy the 4 items together is useless because all these items are frequently purchased in a supermarket. For this rule to be useful, the support needs to be much higher. This dilemma is called the rare item problem [9]. When confronted with this problem in applications, researchers either split the data into a few blocks according to the frequencies of the items and then mine association rules in each block with a different minsup [6], or group a number of related rare items together into an abstract item so that this abstract item is more frequently purchased.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists requires prior specific permission and/or a fee.

KDD-99 San Diego CA USA

Copyright ACM 1999 1-58113-143-7/99/08...$5.00

# 3. Mining Large Itemsets with Multiple MISs

that involve items across different blocks are difficult to find.

# 3.1 Downward closure property

Similarly, the second approach is unable to find rules involving individual rare items and the more frequent items. Clearly, both approaches are ad hoc and "approximate" [6].

This paper argues that using a single minsup for the whole database is inadequate because it cannot capture the inherent natures and/or frequency differences of the items in the database.

By the natures of items we mean that some items, by nature, appear more frequently than others. For example, in a supermarket, people buy food processors and cooking pans much less frequently than they buy bread and milk. In general, those durable and/or expensive goods are bought less frequently, but each of them generates more profit. It is thus important to capture those rules involving less frequent items.

Efficient algorithms for finding large itemsets are based on level-wise search [3]. Let k-itemset denote an itemset with k items. At level 1, all large 1-itemsets are generated: At level 2, all large 2-itemsets are generated and so on. If an itemset is not large at level k-1, it is discarded as any addition of items to the set cannot be large (downward closure property).

In this paper, we extend the existing association rule model to allow the user to specify multiple minimum supports to reflect different natures and/or frequencies of items. Specifically, the user can specify a different minimum item support for each item. Thus, different rules may need to satisfy different minimum supports depending on what items are in the rules. This new model enables us to achieve our objective of producing rare item rules without causing frequent items to generate too many meaningless rules.

An efficient algorithm for mining association rules in the model is also presented. Experiment results on both synthetic data and real-life data show that the proposed technique is very effective.

# 2. The Extended Model

In our extended model, the definition of association rules remains the same. The definition of minimum support is changed. In the new model, the minimum support of a rule is expressed in terms of minimum item supports (MIS) of the items that appear in the rule. That is, each item in the database can have a minimum item support specified by the user. By providing different MIS values for different items, the user effectively expresses different support requirements for different rules.

Let MIS(i) denote the MIS value of item i. The minimum support of a rule R is the lowest MIS value among the items in the rule: That is, a rule R, a1, a2, ..., ak → ak+1, ..., aN satisfies its minimum support if the rule's actual support in the data is greater than or equal to: min(MIS(a1), MIS(a2), ..., MIS(ak)).

# 3.2 The algorithm

The proposed algorithm generalizes the Apriori algorithm for finding large itemsets given in [3]. We call the new algorithm MSapriori. When there is only one MIS value (for all items), it reduces to the Apriori algorithm.

Example 2: Consider the following items in a database, bread, shoes, clothes. The user-specified MIS values are as follows:

|MIS(bread)|2%|
|---|---|
|MIS(shoes)|0.1%|
|MIS(clothes)|0.2%|

The following rule doesn’t satisfy its minimum support:

clothes → bread [sup = 0.15%, conf = 70%] because min(MIS(bread), MIS(clothes)) = 0.2%. The following rule satisfies its minimum support:

clothes → shoes [sup = 0.15%, conf = 70%] because min(MIS(clothes), MIS(shoes)) = 0.1%.

A key operation in the proposed algorithm is the sorting of the items in ascending order of their MIS values. This ordering is used in all subsequent operations of the algorithm.

Let L denote the set of large k-itemsets. Each itemset c is of the following form, &lt;c[1], c[2], ..., c[k]&gt;, which consists of items c[1], c[2], ..., c[k], where MIS(c[1]) &lt; MIS(c[2]) &lt; ... &lt; MIS(c[k]). The algorithm is given below:

# Algorithm MSapriori

1    M = sort(I, MS);                                   /* according to MIS(i's stored in MS */
2    F = init-pass(M, T);                /* make the first pass over T */
3    Lk = {c ∈ F | c.count ≥ MIS(c[1])};
4    for (k = 2; Lk-1 ≠ ∅; k++) do
5        if k = 2 then Ck = level2-candidate-gen(F)
6        else Ck = candidate-gen(Lk-1)
7        end
8        for each transaction t ∈ T do
9            C := subset(Ck, t);
10           for each candidate c ∈ C do c.count++;
11      end
12      Lk = {c ∈ Ck | c.count ≥ MIS(c[1])}
13   end
14   Answer = ∪k Lk;

Line 1 performs the sorting on I according to the MIS value of each item (stored in MS). Line 2 makes the first pass over the data using the function init-pass, which takes two arguments, the database T and the sorted items M to produce the seeds for generating the set of candidate large itemsets of length 2, i.e., C2:

init-pass has two steps:

1. It makes a pass over the data to record the actual support count of each item in M.
2. It then follows the sorted order to find the first item i in M that meets MIS(i). i is inserted into F. For each subsequent item j in M after i, if j.count ≥ MIS(j) then j is also inserted into F (j.count means the count of j).

Note that for simplicity, we use the terms support and count interchangeably (actually, support = count/|T|, where |T| is the size of the database T).

# Example 4

Let us follow Example 3 and the given MIS values of the four items. Assume our database has 100 transactions (not limited to the four items). After making one pass over the data, we obtain the following support counts: 3.count = 6, 4.count = 3, 1.count = 9 and 2.count = 25. Then, (in sorted order) F = {3, 1, 2}, and L1 = {<3>, <2>}.

Item 4 is not in F because 4.count < MIS(3) (= 5%), and <1> is not in L1 because 1.count < MIS(1) (= 10%).

Large 1-itemsets (L1) are obtained from F (line 3). It is easy to show that all large 1-itemsets are in L:

For each subsequent pass, say pass k, the algorithm performs operations. First, the large itemsets in Lk-1 found in the (k-1)th pass are used to generate the candidate itemsets Ck using the candidate-gen function (line 6). It then scans the data and updates various support counts of the candidates in Ck (line 8-11). After that, those new large itemsets are identified to form Lk (line 12).

However, there is a special case, i.e., when k = 2 (line 5), for which the candidate itemsets generation function is different. Both candidate generation functions level2-candidate-gen and candidate-gen are described below.

# 3.3 Candidate generation

level2-candidate-gen takes as argument F (not L1), and returns a superset of the set of all large 2-itemsets. The algorithm is as follows:

1   for each item f in F in the same order do
2      if f.count ≥ MIS(f) then
3          for each item h in F that is after f do
4             if h.count ≥ MIS(f) then
5                 insert <f, h> into C2

# Example 5

Let us continue with Example 4. We obtain, C2 = {<2, 3>, <3, 2>} and C1 = {<3, 1>, <3, 2>}.

<1, 2> is not a candidate 2-itemset because the support count of item 1 is only 9 (or 9%), which is less than MIS(1) (= 10%). Hence, <1, 2> cannot be large.

Note that we must use F rather than L1 because L1 does not contain those items that may satisfy the MIS of an earlier item (in the sorted order) but not the MIS of itself (see the difference between F and L1 in Example 4). Using F, the problem discussed in Section 3.1 is solved for C2.

Correctness of level2-candidate-gen: See [7].

Let us now present the candidate-gen function: It performs a similar task as apriori-gen in Apriori algorithm [3]. candidate-gen takes as argument Lk-1 (k > 2) the set of all large (k-1)-itemsets, and returns a superset of the set of all large k-itemsets. It has two steps, the join step and the prune step: The join step is the same as in the apriori-gen function. The prune step is, however, different. The join step is given below. It joins Lk-1 with Lk-1:

insert into Ck
select p.item1, p.item2, ..., p.itemk-1, q.itemk
from Lk-1 p, Lk-1 q
where p.item1 = q.item1, ..., p.itemk-1 = q.itemk-1

Basically, it joins any two itemsets in Lk-1 whose first k-2 items are the same, but the last items are different.

After the join step, there may still be candidate itemsets in Ck that are impossible to be large. The prune step removes these itemsets. This step is given below:

1      for each itemset c ∈ Ck do
2                for each (k-1)-subset s of c do
3              if (c[1] ∈ s) or (MIS(c[2]) = MIS(c[1])) then
4                if (s ∈ Lk-1) then delete c from Ck;

It checks each itemset c in Ck (line 1) to see whether it can be deleted by finding its (k-1)-subsets in Lk-1. For each (k-1)-subset s in Ck, if s is not in Lk-1, c can be deleted. However, there is an exception, which is when s does not include c[1] (there is only one such s). This means that the first item of c, which has the lowest MIS value, is not in s. Then, even if s is not in Lk-1, we cannot delete c because we cannot be sure that s does not satisfy MIS(c[1]), although we know that it does not satisfy MIS(c[2]), unless MIS(c[2]) = MIS(c[1]) (line 3).

# Example 6

Let L3 be {<1, 2, 3>, <1, 2, 5>, <1, 3, 4>, <1, 3, 5>, <1, 4, 5>, <1, 4, 6>, <2, 3, 5>}. Items in each itemset are in the sorted order. After the join step, C4 is {<1, 2, 3>, <1, 3, 4>, <1, 4, 5, 6>}.

The prune step deletes the itemset <1, 4, 5, 6> because the itemset <1, 5, 6> is not in L2. We are then left with C4 = {<1, 2, 3>, <1, 3, 4, 5>}. <1, 3, 4, 5> is not deleted although <3, 4, 5> is not in L3 because the minimum support for <3, 4, 5> is MIS(3), which may be higher than MIS(1). Although <3, 4, 5> does not satisfy MIS(3), we cannot be sure that it does not satisfy MIS(1) either. However, if we know MIS(3) = MIS(1), then <1, 3, 4, 5> can also be deleted.

Correctness of candidate-gen: See [7].

The problem discussed in Section 3.1 is solved for Ck (k > 2) because due to the sorting we do not need to extend a large (k-1)-itemset with any item that has a lower MIS value, but only an item with a higher (or equal) MIS value. Such itemsets are said to have satisfied the sorted closure property.

# 3.4 Subset function

The subset function checks to see which itemsets in Ck are in transaction T. Itemsets in Ck are stored in a tree similar to that in transaction T.

Each tree node contains an item (except the root). By depth-first traversing of the tree against t, we can find if an itemset is in candidate itemsets using the existing approach of a single t. At each node, we check whether the item in the node is in t. If so, we go down the tree. If not, we backtrack. When a leaf node is reached, we know that the itemset represented by the path is in t.

This method for finding C is different from that in [3]. The method in [3] uses each item in t to traverse the tree. In our extended model, this, however, requires the items in each transaction t to be sorted according to their MIS values in ascending order in order to achieve the sorted closure property. This computation can be substantial if the database is large and resides on hard disk. Most databases for association rule mining are very large. (This is, however, an alternative implementation).

# 4. Evaluation

The section evaluates the extended model. We show that the model allows us to find rules with very low supports (involving rare items) yet without generating a huge number of meaningless rules with frequent items.

# 4.1 Experiments with synthetic data

The synthetic test data is generated with the data generator in [3], which is widely used for evaluating association rule mining algorithms. For our experiments, we need a method to assign MIS values to items in the data set. We use the actual frequencies (or the supports) of the items in the data as the basis for MIS assignments. Specifically, we use the following formulas:

MIS(i) = M(i) > LSMIS(i) = 1 LS OtherwiseM(i) = f(i)

f(i) is the actual frequency (or the support expressed in percentage of the data set size) of item i in the data. LS is the user-specified lowest minimum item support allowed. p (0 < p < 1) is a parameter that controls how the MIS values for items should be related to their frequencies. Thus, to set MIS values for items we use two parameters, p and LS. If p = 0, we have only one minimum support, LS, which is the same as the traditional association rule mining. If p = 1 and LS, f(i) > LS, f(i) is the MIS value for i.

Example 7: Consider three items, 1, 2 and 3 in a data set, where f(1) = 1%, f(2) = 3% and f(3) = 10%. If we use LS = 2% and p = 0.3, then MIS(1) = 2%, MIS(2) = 2% and MIS(3) = 3%. For our experiments, we generated a number of data sets to test our model. Here, we use the results from one data set to illustrate. The others are similar and thus omitted. This data set is generated with 1000 items, and 10 items per transaction on average [3]. The number of transactions is 100,000. The standard deviation of the item frequencies of the data set is 1.14% (the mean is 1.17%, expressed in percentage of the total data set size). This shows that the frequencies of the items do not vary a great deal. (The synthetic data generator is designed for generating data used by mining algorithms with only one minsup.) For our experiment, we use three very low LS values, 0.1%, 0.2%, and 0.3%. Figure 1 shows the number of large itemsets found. The three thick lines give the numbers of large itemsets found using the existing approach of a single minsup at 0.1%, 0.2% and 0.3% respectively.

# 4.2 Application to real-life data

We tested the algorithm using a number of real-life data sets. Here, we only use one application data set. The results with the others are similar. Due to confidentiality agreement, we are unable to provide the details of the application. Here, we only give the characteristics of the data. The data set has 55 items and 700 transactions. Each of the data sets has been tested to show how p affects the number of large itemsets found by our method.

transaction has 14-16 items. Some items can appear in 500 rules can have one minsup, and at the level of milk, cheese, pork transactions, while some may only appear in 30 transactions; The and beef; there can be a different minsup. This model is standard deviation nf item frennenciec in the data ic 25.4% (the mean is 24.3%).

For this application, the user sets LS = 1%. The results are shown in Figure 3, which include both the numbers of candidate itemsets and large itemsets found; The two thick lines show the number of candidate itemsets and the number of large itemsets found respectively by the single minsup (= 1%) method. Our new method reduces the numbers dramatically: For this application, the user is happy with the large itemsets found at 0= 4. The number of large itemsets found by our method at a = 4 is only 8.5% of that found by the existing single minsup method. The drop in the number of candidate itemsets is even more drastic.

MSapriori is the sorting of items according to their MIS values in order to achieve the closure property. Although we still use level-wise search, each step of our algorithm is different from that of algorithm Apriori, from initialization, candidate itemsets generation to pruning of candidate itemsets.

# 6. Conclusion

This paper argues that a single minsup is insufficient for association rule mining since it cannot reflect the natures and frequency differences of the items in the database. In real-life applications, such differences can be very large. It is neither satisfactory to set the minsup too high, nor is it satisfactory to set it too low. This paper proposes a more flexible and powerful model. It allows the user to specify multiple minimum item supports. This model enables us to find rare item rules yet without producing a huge number of meaningless rules with frequent items. The effectiveness of the new model is shown experimentally and practically.

# References

- [1] Aggarwal, C., and Yu, P. “Online generation of association rules.” ICDE-98, 1998, pp. 402-411.
- [2] Agrawal, R., Imielinski, T., Swami, A. “Mining association rules between sets of items in large databases.” SIGMOD-1993, pp. 207-216.
- [3] Agrawal, R. and Srikant, R. “Fast algorithms for mining association rules.” VLDB-94, 1994.
- [4] Brin, S., Motwani, R., Ullman, J., and Tsur, S. “Dynamic Itemset counting and implication rules for market basket data.” SIGMOD-97, 1997, pp. 255-264.
- [5] Han, J. and Fu, Y. “Discovery of multiple-level association rules from large databases.” VLDB-95.
- [6] Lee, W., Stolfo, S. J., and Mok, K. W. “Mining audit data to build intrusion detection models.” KDD-98.
- [7] Liu, B., Hsu, W., and Ma, Y. “Mining association rules with multiple minimum supports.” SoC technical report, 1999.
- [8] Liu, B., Hsu, W., and Ma, Y. “Pruning and Summarizing the Discovered Associations.” KDD-99, 1999.
- [9] Mannila, H. “Database methods for data mining.” KDD-98 tutorial, 1998.
- [10] Ng, R. T., Lakshmanan, L., Han, J. “Exploratory mining and pruning optimizations of constrained association rules.” SIGMOD-98, 1998.
- [11] Park, J. S., Chen, M. S., and Yu, P. S. “An effective hash based algorithm for mining association rules.” SIGMOD-95, 1995, pp. 175-186.
- [12] Rastogi, R. and Shim, K. “Mining optimized association rules with categorical and numeric attributes.” ICDE-98.
- [13] Srikant, R. and Agrawal, R. “Mining generalized association rules.” VLDB-1995.
- [14] Srikant, R., Vu, Q., and Agrawal, R. “Mining association rules with item constraints.” KDD-97, 1997, pp. 67-73.

