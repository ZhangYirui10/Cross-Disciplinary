# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews

Zhiyong Cheng

National University of Singapore

jason.zy.cheng@gmail.com

Ying Ding

Vipshop Inc., USA

ian.yingding@gmail.com

Lei Zhu

Shandong Normal University, China

leizhu0608@gmail.com

Mohan Kankanhalli

National University of Singapore

mohan@comp.nus.edu.sg

# ABSTRACT

Although latent factor models (e.g., matrix factorization) achieve good accuracy in rating prediction, they suffer from several problems including cold-start, non-transparency, and suboptimal recommendation for local users or items. In this paper, we employ textual review information with ratings to tackle these limitations. Firstly, we apply a proposed aspect-aware topic model (ATM) on the review text to model user preferences and item features from different aspects, and estimate the aspect importance of a user towards an item. The aspect importance is then integrated into a novel aspect-aware latent factor model (ALFM), which learns user’s and item’s latent factors based on ratings. In particular, ALFM introduces a weighted matrix to associate those latent factors with the same set of aspects discovered by ATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, an aspect rating is weighted by an aspect importance, which is dependent on the targeted user’s preferences and targeted item’s features. Therefore, it is expected that the proposed method can model a user’s preferences on an item more accurately for each user-item pair locally. Comprehensive experimental studies have been conducted on 19 datasets from Amazon and Yelp 2017 Challenge dataset. Results show that our method achieves significant improvement compared with strong baseline methods, especially for users with only few ratings. Moreover, our model could interpret the recommendation results in depth.

# CCS CONCEPTS

• Information systems → Social recommendation; Personalization; Recommender systems; Collaborative filtering; • Computing methodologies → Topic modeling; Factor analysis;

# KEYWORDS

Aspect-aware, Matrix Factorization, Recommendation, Review-aware, Topic Model

This paper is published under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.

WWW 2018, April 23–27, 2018, Lyon, France

© 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.

ACM ISBN 978-1-4503-5639-8/18/04.

https://doi.org/10.1145/3178876.3186145

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

of restaurants, some users care more about the taste of food while others pay more attention to the ambience. Even for the same aspect, the preference of users could be different from each other. For example, in the food aspect, some users like Chinese cuisines while some others favor Italian cuisines. Similarly, the characteristics of items on an aspect could also be different from each other. Thus, our model avoids referring to external sentiment analysis tools for aspect rating prediction as in [12, 42]. The overall rating is obtained by a linear combination of the aspect ratings, which are weighted by the importance of corresponding aspects (i.e., aspect importance).

Note that the latent topics and latent factors in our model are not linked directly; instead, they are correlated via the aspects indirectly. Therefore, the number of latent topics and latent factors could be different and separately optimized to model reviews and ratings respectively, which is fundamentally different from the one-to-one mapping in previous models [2, 23, 26, 31, 32, 39]. Besides, our model could learn an aspect importance for each user-item pair, namely, assigning a different weight to each pu,k ∗ qi,k, and thus could alleviate the suboptimal local recommendation problem and achieve better performance.

A set of experimental studies has been conducted on 19 real-world datasets from Yelp and Amazon to validate the effectiveness of our proposed model. Experimental results show that our model significantly outperforms the state-of-the-art methods which also use both reviews and ratings for rating prediction. Besides, our model also obtains better results for users with few ratings, demonstrating the advantages of our model on alleviating the cold-start problem. Furthermore, we illustrate the interpretability of our model on recommendation results with examples.

# Main Contributions

- We propose a novel aspect-aware latent factor model, which could effectively combine reviews and ratings for rating prediction. Particularly, our model relaxes the constraint of one-to-one mappings between the latent topics and latent factors in previous models and thus could achieve better performance.
- Our model could automatically extract explainable aspects, and learn the aspect importance/weights for different user-item pairs. By associating latent factors with aspects, the aspect weights are integrated with latent factors for rating prediction. Thus, the proposed model could alleviate the suboptimal problem of MF for individual user-item pairs.
- We conduct comprehensive experimental studies to evaluate the effectiveness of our model. Results show that our model is significantly better than previous approaches on tasks of rating prediction, recommendation for sparse data, and recommendation interpretability.

# 2 RELATED WORK

In this paper, we focus on the problem of personalized rating prediction and attempt to tackle the above limitations together by utilizing reviews with ratings. Specifically, an Aspect-aware Topic Model (ATM) is proposed to extract latent topics from reviews, which are used to model users’ preferences and items’ features in different aspects. In particular, each aspect of users/items is represented as a probability distribution of latent topics.

Based on the results, the relative importance of an aspect (i.e., aspect importance) is represented. We broadly classify the approaches for the targeted task in three categories: (1)

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

sentiment-based, (2) topic-based, and (3) deep learning-based. Our approach falls into the second category.

# Table 1: Notations and their definitions

|Notation|Definition|
|---|---|
|D|corpus with reviews and ratings|
|du,i|review document of user u to item i|
|s|a sentence in a review du,i|
|U, I, A|user set, item set, and aspect set, respectively|
|M, N, A|number of users, items, and aspects, respectively|
|Nw,s|number of words in a sentence s|
|K|number of latent topics in ATM|
|y|an indicator variable in ATM|
|as|assigned aspect a to sentence s|
|πu|the parameter of Bernoulli distribution P(y =0)|
|η|Beta priors (η = {η0, η1})|
|αu,αi|Dirichlet priors for aspect-topic distributions|
|γu, γi|Dirichlet priors for aspect distributions|
|βw|Dirichlet priors for topic-word distributions|
|θu,a|user’s aspect-topic distribution: denoting user’s preference on a|
|ψi,a|item’s aspect-topic distribution: denoting item’s features on a|
|λu, λv|aspect distributions of user and item, respectively|
|ϕw|topic-text word distribution|
|fμ|number of latent factors in ALFM|
|b·|regularization coefficients|
|w·|bias terms, e.g., bu, bi, b0|
|p a q|weight vector for aspect a|
|ru,i|latent factors of user u and item i, respectively|
|ru,i|rating of user u to item i|
|u,i,a|aspect rating of user u towards item i on aspect a|
|ρu,i,a|aspect importance of a for u with respect to i|
|su,i,a|the degree of item i’s attributes matching user u’s preference on aspect a|

# 3 PROPOSED MODEL

# 3.1 Problem Setting

Let D be a collection of reviews of item set I from a specific category (e.g., restaurant) written by a set of users U, and each review comes with an overall rating ru,i to indicate the overall satisfaction of user u to item i. The primary goal is to predict the unknown ratings of items that the users have not reviewed yet. A review du,i is a piece of text which describes opinions of user u on different aspects a ∈ A towards item i, such as food for restaurants. In this paper, we only consider the case that all the items are from the same category, i.e., they share the same set of aspects A. Aspects that users care for items are latent and learned from reviews by our proposed topic model, in which each aspect is represented as a distribution of the same set (e.g., K) of latent topics. Table 1 lists the key notations. Before introducing our method, we would like to first clarify the concepts of aspects, latent topics, and latent factors.

- Aspect - it is a high-level semantic concept, which represents the attribute of items that users commented on in reviews, such as “food” for restaurant and “battery" for mobile phones.
- Latent topic & latent factor - in our context, both concepts represent a more fine-grained concept than “aspect". A latent topic or factor can be regarded as a sub-aspect of an item. For instance, for the “food" aspect, a related latent topic could be “breakfast" or “Italian cuisine". We adopt the terminology of.

# Track: User Modeling, Interaction and Experience on the Web

# 3.2 Aspect-aware Latent Factor Model

Based on the observations that (1) different users may care for different aspects of an item and (2) users’ preferences may differ from each other for the same aspect, we claim that the overall satisfaction of a user u towards an item i (i.e., the overall rating ru,i) depends on u’s satisfaction on each aspect a of i (i.e., aspect rating ru,i,a) and the importance of each aspect (of i) to u (i.e., aspect importance ρu,i,a). Based on the assumptions, the overall rating ru,i can be predicted as:

X   aspect importance

rˆ   =         z}|{

u,i           ρu,i,a    ru,i,a                   (1)

a∈A                |{z}

# 3.2.1 Aspect rating estimation.

Aspect rating (i.e., ru,i,a) reflects the satisfaction of a user u towards an item i on the aspect a. To receive a high aspect rating ru,i,a, an item should at least possess the characteristics/attributes in which the user is interested in this aspect. Moreover, the item should satisfy user’s expectations on these attributes in this aspect. In other words, the item’s attributes on this aspect should be of high quality such that the user likes it. Take the “food" aspect as an example, for a user who likes Chinese cuisines, to receive a high rating on the “food" aspect from the user, a restaurant should provide Chinese dishes and the dishes should suit the user’s taste. Based on user’s text reviews, we can learn users’ preferences and items’ characteristics on each aspect and measure how the attributes of an item i on aspect “a" suit a user u’s requirements on this aspect, denoted by su,i,a. We compute su,i,a based on results of the proposed Aspect-aware Topic Model (ATM) (described in Sect. 3.3), in which user’s preferences and item’s characteristics on each aspect are modeled as multinomial distributions of latent topics, denoted by θu,a and ψi,a, respectively. su,i,a ∈ [0,1] is then computed as:

su,i,a = 1 − JSD(θu,a,ψi,a )                   (2)

where JSD(θu,a ,ψi,a ) denotes the Jensen–Shannon divergence [13] between θu,a and ψi,a. Notice that a large value of su,i,a does not mean a high rating ru,i,a - an item providing all the features that a user u requires does not mean that it satisfies u’s expectations, since the provided ones could be of low quality. For instance, a restaurant provides all the Chinese dishes the user u likes (i.e., high score su,i,a), but these dishes taste bad from u’s opinion (i.e., low rating ru,i,a). Therefore, we can expect that for this restaurant: users discuss its Chinese dishes in reviews with negative opinions and thus give low ratings. Instead of analyzing the review sentiments for aspect rating estimation by using external NLP tools (such as [42]), we refer to the matrix factorization (MF) [21] technique.

MF maps users and items into a latent factor space and represents users’ preferences and items’ features by f-dimensional latent factor vectors (i.e., pu ∈ Rf ×1 and qi ∈ Rf ×1). The dot product of the user’s and item’s vectors (pT q) characterizes the user’s overall interests on the item’s characteristics, and is thus used to predict the rating ru,i. To extend MF for aspect rating prediction, we introduce a binary matrix W ∈ Rf ×A to associate the latent factors to different aspects, where A is the number of aspects considered. We call this model aspect-aware latent factor model (ALFM), in which the weight vector wa in the a-th column of W indicates which factors are related to the aspect a. Thus, pu,a = wa ⊙ pu denotes user’s interests in the aspect a, where ⊙ represents element-wise product between vectors. Therefore, (pu,a)T (qi,a) represents the aspect rating of user u to item i on aspect a. Finally, we integrate the matching results of aspects (i.e., su,i,a) into ALFM to estimate the aspect ratings:

ru,i,a = su,i,a · (wa ⊙ pu)T (wa ⊙ qi)                 (3)

As a high aspect rating ru,i,a requires large values of both su,i,a and (wa ⊙ pu)T (wa ⊙ qi), it is expected that the results learned from reviews could guide the learning of latent factors.

# 3.2.2 Aspect importance estimation.

We rely on user reviews to estimate ρu,i,a, as users often discuss their interest topics of aspects in reviews, such as different cuisines in the food aspect. In general, the more a user comments on an aspect in reviews, the more important this aspect is (to this user). Thus, we estimate the importance of an aspect according to the possibility of a user writing review comments on this aspect. When writing a review, some users tend to write comments from the aspects according to their own preferences, while others like commenting on the most notable features of the targeted item. Based on this consideration, we introduce (1) πu to denote the probability of user u commenting an item based on his own preference and (2) λu,a (Pa∈A λu,a = 1) to denote the probability of user u commenting on the aspect a based on his own preference. Accordingly, (1 − πu) denotes the probability of the user commenting from the item i’s characteristics (Pa∈A λi,a = 1), and λi,a is the probability of user u commenting item i from the item’s characteristics on the aspect a. Thus, the probability of a user u commenting an item i on an aspect a (i.e., ρu,i,a) is:

ρu,i,a = πuλu,a +(1− πu)λi,a                        (4)

λu,a, λi,a, and πu are estimated by ATM, which simulates the process of a user writing a review, as detailed in the next subsection.

# 3.3 Aspect-aware Topic Model

Given a corpus D, which contains reviews of users towards items {du,i|du,i ∈ D,u ∈ U,i ∈ I}, we assume that a set of latent topics (i.e., K topics) covers all the topics that users discuss in the reviews. λu is a probability distribution of aspects in user u’s preferences, in which each value λu,a denotes the relative importance of an aspect a to the user u. Similarly, λi is the probability distribution of aspects in item i’s characteristics, in which each value λi,a denotes the importance of an aspect a to the item i. As the K latent topics cover all the topics discussed in reviews, an aspect will only relate to some of the latent topics closely. For example, topic “breakfast" is closely related to aspect “food", while it is not related to aspects like “service" or “price". The relation between aspects and topics is also represented by a probabilistic distribution, i.e., θu,a for users and ψi,a for items. More detailedly, the interests of a user u in a specific aspect a is represented by θu,a, which is a multinomial distribution of the latent topics; the characteristics of an item i in a specific aspect a is represented by ψi,a, which is also a multinomial distribution of the same set of latent topics.

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# Algorithm 1: Generation Process of ATM

1  for each topic k = 1, ..., K do
2     Draw ϕk,w ∼ Dir (· |βw);
3  for each user u ∈ U do
4     Draw λu ∼ Dir (· |γu);
5  for each item i ∈ I do
6     Draw λi ∼ Dir(· |γi);
7  for each user u ∈ U, each aspect a ∈ A do
8     Draw θu,a ∼ Dir(· |αu);
9  for each item i ∈ I, each aspect a ∈ A do
10     Draw ψi,a ∼ Dir(· |αi);
11  for each review du,i,us ∈ U,i ∈ I do
12     for each sentence ∈ du,i do
13         Draw y ∼ Bernoulli(· |πu);
14         if ys == 0 then
15             Draw as ∼ Multi(λu) and then draw zs ∼ Multi(θu,as);
16         if ys == 1 then
17             Draw as ∼ Multi(λi) and then draw zs ∼ Multi(ψi,as);
18         foreach word w ∈ s do
19             Draw w ∼ Multi(ϕzs,w)

based on all the reviews {du,i|i ∈ I} of user u writing for items. ψi,a is learned from all the reviews {du,i |u ∈ U} of i written by users. A latent topic is a multinomial distribution of text words in reviews. Based on these assumptions, we propose an aspect-aware topic model ATM to estimate the parameters {λi, λi, θu,a, ψi,a, πu} by simulating the generation of the corpus D.

The graphical representation of ATM is shown in Fig. 1. In the figure, the shaded circles indicate observed variables, while the unshaded ones represent the latent variables. ATM mimics the processing of writing a review sentence by sentence. A sentence usually discusses the same topic z, which could be from user’s preferences or from item’s characteristics. To decide the topic zs for a sentence s, our model introduces an indicator variable y ∈ {0,1} based on a Bernoulli distribution, which is parameterized by πu. Specifically, when y = 0, the sentence is generated from user’s preference; otherwise, it is generated according to item i’s characteristics. πu is user-dependent, indicating the tendency to comment from u’s personal preferences or from the item i’s characteristics is determined by u’s personality.

The generation process of ATM is shown in Algorithm 1. Let as denote the aspect assigned to a sentence s. If y = 0, as is drawn from λu and zs is then generated from u’s preferences on aspect as: θu,aₛ; otherwise, if y = 1, as is drawn from λi and zs is then generated from i’s characteristics on aspect as: ψi,aₛ. Then all the words w in sentence s is generated from zs according to the word distribution: ϕzₛ,w.

In ATM, αu, αi, γu, γi, β, and η are pre-defined hyper-parameters and set to be symmetric for simplicity. Parameters need to be estimated including λi, λi, θu,a, ψi,a, and πu. Different approximate inference methods have been developed for parameter estimation in topic models, such as variation inference and collapsed Gibbs sampling. We apply collapsed Gibbs sampling to infer the parameters, since it has been successfully applied in many large scale applications of topic models. Due to the space limitation, we omit the detailed inference steps in this paper.

# 3.4 Model Inference

With the results of ATM, ρu,i,a and su,i,a can be computed using Eq. 4 and 2, respectively. With the consideration of bias terms (i.e., improvement has been observed.

In our experiments, we tried to normalize ρu,i,a or ρu,i,a · su,i,a in Eq. 5, but no improvement has been observed.

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# Table 2: Statistics of the evaluation datasets

|Datasets|#users|#items|#ratings|Sparsity|
|---|---|---|---|---|
|Instant Video|4,902|1,683|36,486|0.9956|
|Automotive|2,788|1,835|20,218|0.9960|
|Baby|17,177|7,047|158,311|0.9987|
|Beauty|19,766|12,100|196,325|0.9992|
|Cell Phones|24,650|10,420|189,255|0.9993|
|Clothing|34,447|23,026|277,324|0.9997|
|Digital Music|5,426|3,568|64,475|0.9967|
|Grocery|13,979|8,711|149,434|0.9988|
|Health|34,850|18,533|342,262|0.9995|
|Home & Kitchen|58,901|28,231|544,239|0.9997|
|Musical Instruments|1,397|900|10,216|0.9919|
|Office Products|4,798|2,419|52,673|0.9955|
|Patio|1,672|962|13,077|0.9919|
|Pet Supplies|18,070|8,508|155,692|0.9990|
|Sports & Outdoors|31,176|18,355|293,306|0.9995|
|Tools & Home|15,438|10,214|133,414|0.9992|
|Toys & Games|17,692|11,924|166,180|0.9992|
|Video Games|22,348|10,672|228,164|0.9990|
|Yelp 2017|169,257|63,300|1,659,678|0.9998|

# 4.2 Experimental Settings

For each dataset, we randomly split it into training, validation, and testing set with ratio 80:10:10 for each user as in [6, 23, 26]. Because we take the 5-core dataset where each user has at least 5 interactions, we have at least 3 interactions per user for training, and at least 1 interaction per user for validation and testing. Note that we only used the review information in the training set, because the reviews in the validation or testing set are unavailable during the prediction process in real scenarios. The number of aspect is set to 5 in experiments.

# 4.1 Dataset Description

We conducted experiments on two publicly accessible datasets that provide user review and rating information. The first dataset is Amazon Product Review dataset collected by [26], which contains product reviews and metadata from Amazon. This dataset has been widely used for rating prediction with reviews and ratings in previous studies [6, 23, 26, 31]. The dataset is organized into 24 product categories. In this paper, we used 18 categories (See Table 2) and focus on the 5-core version, with at least 5 reviews for each user or item. The other dataset is from Yelp Dataset Challenge 2017, which includes reviews of local business in 12 metropolitan areas across 4 countries. For the Yelp 2017 dataset, we also processed it to keep users and items with at least 5 reviews. From each review in these datasets, we extract the corresponding “userID", “itemID", a rating score (from 1 to 5 rating stars), and a textual review for experiments. Notice that for all the datasets, we checked and removed the duplicates, and then filtered again to keep them as 5-core. Besides, we removed the infrequent terms in the reviews for each dataset.

# 4.3 Research Questions

To validate the assumptions when designing the model and evaluate our proposed model, we conducted comprehensive experimental studies to answer the following questions:

- RQ1: How do the important parameters (e.g., the number of latent topics and latent factors) affect the performance of our model? More importantly, is the setting f = K optimal, which is a default assumption for many previous models?
- RQ2: Can our ALFM model outperform the state-of-the-art recommendation methods, which consider both ratings and reviews, on rating prediction?
- RQ3: Compared to other methods which also use textual reviews and ratings, how does our ALFM model perform on the cold-start setting when users have only few ratings?
- RQ4: Can our model explicitly interpret the reasons for a high or low rating?

# 4.4 Baselines

We compare the proposed ALFM model with the following baselines. It is worth noting that these methods are tuned on the validation dataset to obtain their optimal hyper-parameter settings for fair comparisons.

- BMF [21]. It is a standard MF method with the consideration of bias terms (i.e., user biases and item biases). This method only leverages ratings when modeling users’ and items’ latent factors. It is typically a strong baseline model in collaborative filtering [21, 23].
- HFT [26]. It models ratings with MF and review text with latent topic model (e.g., LDA [5]). We use it as a representative of the methods which use an exponential transformation function to link the latent topics with latent factors, such as TopicMF [2]. The topic distribution can be modeled on either users or items. We use the topic distribution based on items, since it achieves better results. Note that in experiments, we add bias terms into HFT, which can achieve better performance.
- CTR [32]. This method also utilizes both review and rating information. It uses a topic model to learn the topic distribution of items, which is then used as the latent factors of items in MF with an addition of a latent variable.

# Footnotes

3 http://jmcauley.ucsd.edu/data/amazon/

4 http://www.yelp.com/dataset_challenge/

5 The thresholds of infrequent terms varied across different datasets. For example, for the “Yelp 2017" dataset, which is relatively large, a term that appears less than 10 times in reviews is defined as an infrequent term; and the thresholds are smaller for relatively small datasets (e.g., the threshold is 5 for the “Music Instruments" dataset).

6 We tuned the number of aspects from 1 to 8 for all the datasets, and found that the performance does not change much unless setting the aspect number to 1 or 2.

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# 4.3 Effect of Important Parameters (RQ1)

In this subsection, we analyze the influence of the number of latent factors and the number of latent topics on the final performance of ALFM. As we know, in MF, more latent factors will lead to better performance unless overfitting occurs [20, 21]; while the optimal number of latent topics in topic models (e.g., LDA) is dependent on each aspect of a targeted item. The substantial improvement of ALFM over those baselines demonstrates the benefits of modeling users’ specific preferences on each aspect of different items.

# 4.4 Model Comparison (RQ2)

We show the performance comparisons of our ALFM with all the baseline methods in Table 3, where the best prediction result on each dataset is in bold. For fair comparison, we set the number of latent factors (f) and the number of latent topics (K) to be the same as f = K = 5. Notice that our model could obtain better performance when setting f and K differently. Still, ALFM achieves the best results on 18 out of the 19 datasets. Compared with BMF, which only uses ratings, we achieve much better prediction performance (16.49% relative improvement on average). More importantly, our model outperforms CTR and RMR with large margins - 6.28% and 8.18% relative improvements on average, respectively. Compared to the recently proposed RBLT and TransNet, ALFM can still achieve 3.37% and 4.26% relative improvement on average respectively with significance testing.

# Figure 2: Effects of factors and topics in our model.

| |0.987|0.981|0.975|0.969|0.963|
|---|---|---|---|---|---|
|RMSE|0.987|0.981|0.975|0.969|0.963|
|5|10|15|20|25| |

# Figure 3: Effects of #factors v.s. #topics.

|(a) Clothing|(b) Yelp 2017|
|---|---|
|1.0325|1.1553|
|1.033|1.1554|
|1.032|1.1552|
|1.031|1.155|

# References

- [2] HFT, TopicMF
- [23] RMR
- [31] RBLT
- [26] HFT
- [39] ITLFM
- [6] TransNet
- [1] LDA
- [4] Topic Models

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# Table 3: Comparisons of adopted methods in terms of RMSE with f = K = 5.

|Datasets|BMF|HFT|CTR|RMR|RBLT|TransNet|ALFM|Improvement(%)|
|---|---|---|---|---|---|---|---|---|
|Instant Video|1.162|0.999|1.014|1.039|0.978|0.996|0.967|g vs. a g vs. b g vs. c g vs. d g vs. e g vs. f|
|Automotive|1.032|0.968|1.016|0.997|0.924|0.918|0.885|14.26* 8.58** 12.86* 11.19* 4.24** 3.56*|
|Baby|1.359|1.112|1.144|1.178|1.122|1.110|1.076|20.83** 3.24 5.98* 8.66** 4.11 3.05*|
|Beauty|1.342|1.132|1.171|1.190|1.117|1.123|1.082|19.39** 4.47 7.65** 9.12** 3.18** 3.65**|
|Phones|1.432|1.216|1.271|1.289|1.220|1.207|1.167|18.47** 3.98* 8.18* 9.4** 4.33 3.27*|
|Clothing|1.073|1.103|1.142|1.145|1.073|1.064|1.032|3.8** 6.47** 9.65 9.9** 3.86** 2.96*|
|Digital Music|1.093|0.918|0.921|0.960|0.918|1.061|0.920|15.82 -0.15 0.13* 4.49** -0.15** 4.13**|
|Grocery|1.192|1.016|1.045|1.061|1.012|1.022|0.982|17.66** 3.36** 6.07 7.46** 3.01** 3.94*|
|Health|1.263|1.073|1.105|1.135|1.070|1.114|1.042|17.48* 2.83 5.65* 8.20 2.56** 6.46**|
|Home & Kitchen|1.297|1.083|1.123|1.149|1.086|1.123|1.049|19.16** 3.15** 6.62 8.7** 3.41** 6.61**|
|Musical Instruments|1.004|0.972|0.979|0.983|0.946|0.901|0.893|11.08 8.17** 8.83** 9.2** 5.61 0.95|
|Office Products|1.025|0.879|0.898|0.934|0.872|0.898|0.848|17.29** 3.55** 5.61* 9.26** 2.77** 5.67**|
|Patio|1.180|1.041|1.062|1.077|1.032|1.046|1.001|15.19** 3.84* 5.7* 7.07* 2.96 4.33*|
|Pet Supplies|1.367|1.137|1.177|1.200|1.139|1.149|1.099|19.64** 3.41* 6.67* 8.41 3.54** 4.38*|
|Sports & Outdoors|1.130|0.970|0.998|1.019|0.964|0.990|0.933|17.42** 3.8* 6.47 8.4* 3.2** 5.77*|
|Tools & Home|1.168|1.013|1.047|1.090|1.011|1.041|0.974|16.63** 3.90 6.98 10.68** 3.7** 6.51**|
|Toys & Games|1.072|0.926|0.948|0.974|0.923|0.951|0.902|15.81** 2.59* 4.82** 7.39** 2.3** 5.11*|
|Video Games|1.321|1.096|1.115|1.150|1.094|1.123|1.070|19.02* 2.43 4.03** 6.97* 2.24** 4.77*|
|Yelp 2017|1.415|1.174|1.233|1.266|1.202|1.190|1.155|18.35* 1.60** 6.33** 8.74* 3.88** 2.92*|
|Average|1.207|1.044|1.074|1.097|1.037|1.049|1.004|14.56** 2.84* 7.16** 8.31* 3.37** 4.26**|

The improvements with * are significant with p −value <  0.05, and the improvements with ** are significant with p −value < 0.01 with a two-tailed paired t-test.

Gain in RMSE

|0.35|0.35|0.35|
|---|---|---|
|0.30|0.30|0.30|
|0.25|0.25|0.25|
|0.20|0.20|0.20|
|0.15|0.15|0.15|
|0.10|0.10|0.10|
|0.05|0.05|0.05|
|0.00|0.00|0.00|

# training items

(a) Clothing
(b) Yelp 2017
(c) All Datasets
# Figure 4: Gain in RMSE for user with limited training data on two individual datasets and overall 19 datasets.

# 4.5 Cold-Start Setting (RQ3)

To demonstrate the capability of our model in dealing with users with very limited ratings, we randomly split the datasets into training, validation, and testing sets in ratio 80:10:10 based on the number of ratings in each set. In this setting, it is not guaranteed that a user has at least 3 ratings in the training set. It is possible that a user has no rating in the training set. For the users without any ratings in the training set, we also removed them in the testing set. Therefore, matrix factorization easily suffers from the cold-start problem. By integrating reviews in users’ and items’ latent factor learning, our model could alleviate the problem of cold-start to a great extent, since reviews contain rich information about user preferences and item features. Then we evaluate the performance of users who have the number of ratings from 1 to 10 in the training set. The value of Gain in RMSE is equal to the average RMSE of baselines minus that of our model (e.g., “BMF-TALFM"). A positive value indicates that our model achieves.

# Track: User Modeling, Interaction and Experience on the Web

# WWW 2018, April 23-27, 2018, Lyon, France

# Table 4: Top ten words of each aspect for a user (index 1511)

|Value|Comfort|Accessories|Shoes|Clothing|
|---|---|---|---|---|
|price|size|ring|socks|shirt|
|color|fit|pretty|foot|back|
|quality|wear|dress|boots|bra|
|worth|comfortable|time|comfort|top|
|cute|bra|beautiful|sandals|feel|
|comfortable|small|gift|walk|soft|
|fits|color|earrings|toe|black|
|ring|fits|compliments|pairs|jeans|
|dress|perfect|chain|hold|pants|
|shirt|material|jewelry|strap|tight|
|material|long|shoes|pockets|material|

# Table 5: Interpretation for why the “user 1511" rated “item 1" and “item 2" with 5 and 2, respectively, from Clothing.

|Aspects|Value|Comfort|Accessories|Shoes|Clothing|
|---|---|---|---|---|---|
|Importance (1)|0.621|0.042|0.241|0.001|0.095|
|Matching (1)|0.982|0.596|0.660|0.759|0.638|
|Polarity (1)|+|-|+|-|+|
|Importance (2)|0.621|0.042|0.241|0.001|0.094|
|Matching (2)|0.920|0.303|0.362|1.000|0.638|
|Polarity (2)|-|-|-|-|+|

# 5 CONCLUSIONS

In this paper, we proposed an aspect-aware latent factor model for rating prediction by effectively combining reviews and ratings. Our model correlates the latent topics learned from review text and the latent factors learned from ratings based on the same set of aspects, which are discovered from textual reviews. Accordingly, our model does not have the constraint of one-to-one mapping between latent factors and latent topics as previous models (e.g., HFT, RMR, RBLT, etc.), and thus could achieve better user preference and item feature modeling. Besides, our model is able to estimate aspect ratings and assign weights to different aspects. The aspect weight is dependent on each user-item pair, since it is estimated based on user’s personal preferences on the corresponding aspect towards an item. As we can see, our ALFM model substantially improves the prediction accuracy compared with the BMF model. Experimental results on 19 real-world datasets show that our model greatly improves the rating prediction accuracy compared to the state-of-the-art methods, especially for users who have few ratings. This demonstrates that our model is more effective in exploiting reviews and ratings, because it learns user’s preferences and item’s features in different aspects and is capable of estimating the aspect weights based on the targeted user’s preferences and targeted item’s features.

# ACKNOWLEDGMENTS

This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its International Research Centre in Singapore Funding Initiative. The authors would like to thank Rose Catherine Kanjirathinkal (from CMU)’s great help on fine-tuning the results of TransNet on all datasets.

# Track: User Modeling, Interaction and Experience on the Web

# REFERENCES

1. R. Arun, V. Suresh, CE V. Madhavan, and MN N. Murthy. 2010. On finding the natural number of topics with latent dirichlet allocation: Some observations. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 391–402.
2. Y. Bao, H. Fang, and J. Zhang. 2014. TopicMF: Simultaneously exploiting ratings and reviews for recommendation. In Proceedings of the 28th AAAI Conference on Artificial Intelligence. 2–8.
3. R. M Bell and Y. Koren. 2007. Lessons from the Netflix prize challenge. ACM SIGKDD Explorations Newsletter 9, 2 (2007), 75–79.
4. David M Blei. 2012. Probabilistic topic models. Commun. ACM 55, 4 (2012), 77–84.
5. D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research 3 (2003), 993–1022.
6. R. Catherine and W. Cohen. 2017. TransNets: Learning to Transform for Recommendation. In Proceedings of the 11th ACM Conference on Recommender Systems. ACM, 288–296.
7. Z. Cheng and J. Shen. 2016. On effective location-aware music recommendation. ACM Trans. Inf. Syst. 34, 2 (2016), 13:1–13:32.
8. Z. Cheng, J. Shen, L. Nie, T.-S. Chua, and M. Kankanhalli. 2017. Exploring user-specific information in music retrieval. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 655–664.
9. Z. Cheng, J. Shen, L. Zhu, M. Kankanhalli, and L. Nie. 2017. Exploiting music play sequence for music recommendation. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 3654–3660.
10. E. Christakopoulou and G. Karypis. 2016. Local item-item models for top-n recommendation. In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 67–74.
11. P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 191–198.
12. Q. Diao, M. Qiu, C.-Y. Wu, A. J Smola, J. Jiang, and C. Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 193–202.
13. D. M. Endres and J. E Schindelin. 2003. A new metric for probability distributions. IEEE Trans. Inf. Theory 49, 7 (2003), 1858–1860.
14. G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond the Stars: Improving Rating Predictions using Review Text Content. In Proceedings of the 12th International Workshop on the Web and Databases, Vol. 9. Citeseer, 1–6.
15. T. L Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences 101, Suppl 1 (2004), 5228–5235.
16. R. He and J. McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In Proceedings of the 30th AAAI Conference on Artificial Intelligence. 144–150.
17. X. He, T. Chen, M.-Y. Kan, and X. Chen. 2015. Trirank: Review-aware explainable recommendation by modeling aspects. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 1661–1670.
18. X. He and T.-S. Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 355–364.
19. X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web International World Wide Web Conferences Steering Committee, 173–182.
20. X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. 2016. Fast matrix factorization for online recommendation with implicit feedback. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 549–558.
21. Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30–37.
22. F. Li, S. Wang, S. Liu, and M. Zhang. 2014. SUIT: A supervised user-item based topic model for sentiment analysis. In Proceedings of the 28th AAAI Conference on Artificial Intelligence. 1636–1642.
23. G. Ling, M. Lyu, and I. King. 2014. Ratings meet reviews, a combined approach to recommend. In Proceedings of the 8th ACM Conference on Recommender systems. ACM, 105–112.
24. H. Ma, D. Zhou, C. Liu, M. Lyu, and I. King. 2011. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining. ACM, 287–296.
25. J. Mairal, F. Bach, J. Ponce, and G. Sapiro. 2010. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research 11, Jan (2010), 19–60.
26. J. McAuley and J. Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems. ACM, 165–172.
27. N. Pappas and A. Popescu-Belis. 2013. Sentiment analysis of user comments for one-class collaborative filtering over ted talks. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 773–776.
28. Š. Pero and T. Horváth. 2013. Opinion-driven matrix factorization for rating prediction. In International Conference on User Modeling, Adaptation, and Personalization. Springer, 1–13.
29. L. Qiu, S. Gao, W. Cheng, and J. Guo. 2016. Aspect-based latent factor model by integrating ratings and reviews for recommender system. Knowledge-Based Systems 110 (2016), 233–243.
30. Y. Shi, M. Larson, and A. Hanjalic. 2013. Mining contextual movie similarity with matrix factorization for context-aware recommendation. ACM Trans. Intell. Syst. Technol. 4, 1 (2013), 16.
31. Y. Tan, M. Zhang, Y. Liu, and S. Ma. 2016. Rating-boosted latent topics: Understanding users and items with ratings and reviews. In Proceedings of the 25th International Joint Conference on Artificial Intelligence. 2640–2646.
32. C. Wang and D. Blei. 2011. Collaborative topic modeling for recommending scientific articles. In Proceedings of the 17th ACM SIGKDD International conference on Knowledge Discovery and Data Mining. ACM, 448–456.
33. H. Wang, Y. Lu, and C. Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 618–626.
34. X. Wang, X. He, L. Nie, and T.-S. Chua. 2017. Item silk road: Recommending items from information domains to social users. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 185–194.
35. X. Wang, X. He, L. Nie, and T.-S. Chua. 2018. TEM: Tree-enhanced embedding model for explainable recommendation. In Proceedings of the 27th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee.
36. Y. Wu and M. Ester. 2015. FLAME: A probabilistic model combining aspect based opinion mining and collaborative filtering. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. ACM, 199–208.
37. H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S. Chua. 2016. Discrete collaborative filtering. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 325–334.
38. H. Zhang, Z.-J. Zha, Y. Yang, S. Yan, Y. Gao, and T.-S. Chua. 2014. Attribute-augmented semantic hierarchy: towards a unified framework for content-based image retrieval. ACM Transactions on Multimedia Computing, Communications, and Applications 11, 1s (2014), 21:1–21:21.
39. W. Zhang and J. Wang. 2016. Integrating topic and latent factors for scalable personalized review-based rating prediction. IEEE Trans. Knowledge Data Eng. 28, 11 (2016), 3013–3027.
40. W. Zhang, Q. Yuan, J. Han, and J. Wang. 2016. Collaborative multi-level embedding learning from reviews for rating prediction. In Proceedings of the 25th International Joint Conference on Artificial Intelligence. 2986–2992.
41. Y. Zhang, Q. Ai, X. Chen, and W. B. Croft. 2017. Joint representation learning for top-n recommendation with heterogeneous information sources. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 1449–1458.
42. Y. Zhang, G. Lai, M. Zhang, Y. Zhang, Y. Liu, and S. Ma. 2015. Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 1661–1670.
43. L. Zheng, V. Noroozi, and P. S Yu. 2017. Joint deep modeling of users and items using reviews for recommendation. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 425–434.

