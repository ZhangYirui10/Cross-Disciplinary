# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

Yeying Jinâˆ—, Beibei Linâˆ—, Wending Yan

National University of Singapore, Singapore, Singapore

e0178303@u.nus.edu, beibei.lin@u.nus.edu, yan.wending@huawei.com

Yuan Yuan, Wei Ye, Robby T. Tan

Huawei International Pte Ltd, Singapore, Singapore

yuanyuan10@huawei.com, yewei10@huawei.com, robby.tan@nus.edu.sg

Night Haze Ours Liu-22 [51] Wang-22 [74]

Figure 1: Our nighttime dehazing results compared to existing methods, we can handle night glow and low-light conditions.

# ABSTRACT

Visibility in hazy nighttime scenes is frequently reduced by multiple factors, including low light, intense glow, light scattering, and the presence of multicolored light sources. Existing nighttime dehazing methods often struggle with handling glow or low-light conditions, resulting in either excessively dark visuals or unsuppressed glow outputs. In this paper, we enhance the visibility from a single nighttime haze image by suppressing glow and enhancing low-light regions. To handle glow effects, our framework learns from the rendered glow pairs. Specifically, a light source aware network is proposed to detect light sources of night images, followed by the APSF (Angular Point Spread Function)-guided glow rendering. Our framework is then trained on the rendered images, resulting in glow suppression. Moreover, we utilize gradient-adaptive convolution, to capture edges and textures in hazy scenes. By leveraging extracted edges and textures, we enhance the contrast of the scene without losing important structural details. To boost low-light intensity, our network learns an attention map, then adjusted by gamma correction. This attention has high values on low-light regions and low values on haze and glow regions. Extensive evaluation on real nighttime haze images demonstrates the effectiveness of our method. Our experiments demonstrate that our method achieves a PSNR of 30.38dB, outperforming state-of-the-art methods by 13% on GTA5 nighttime haze dataset. Our data and code is available at: https://github.com/jinyeying/nighttime_dehaze.

# CCS CONCEPTS

â€¢ Computing methodologies â†’ Artificial intelligence; Computer vision; Computer vision tasks.

# KEYWORDS

nighttime, haze, glow, low-light, gradient, edge, texture, APSF

# ACM Reference Format:

Yeying Jin, Beibei Lin, Wending Yan, Yuan Yuan, Wei Ye, and Robby T. Tan. 2023. Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution. In Proceedings of the 31st ACM International Conference on Multimedia (MM â€™23), October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3581783.3611884

# 1 INTRODUCTION

Nighttime hazy or foggy images often suffer from reduced visibility. In addition to the common issues faced by night images, such as low light, noise, uneven light distribution, and multiple light colors, nighttime hazy or foggy images also exhibit a strong glow and particle veiling effect. Despite these challenges, addressing these issues is crucial for improving the quality of nighttime images.

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

them is crucial for many applications. These include self-driving cars, autonomous drones, and surveillance [3], as haze during the nighttime are natural phenomena that are frequent and inevitable.

Daytime haze removal methods cannot handle the unique challenges posed by nighttime haze. Traditional non-learning daytime dehazing methods (e.g., [4, 16, 23, 70]) rely on the haze imaging model [32]. However, this model is not valid at night due to the presence of artificial light sources and the complexity of illumination colors. As a result, unlike in daytime, we cannot assume a uniform atmospheric light color. Moreover, this daytime haze model does not account for the visual appearance of glow.

Existing nighttime dehazing methods produce unsatisfactory dark visuals or unmitigated glow effects. Non-deep learning methods (e.g., [45, 50, 74, 85]) introduce certain constraints on glow. However, they struggle with dark results due to the imprecise decomposition of glow and background layers or the use of dark channel prior [22] for dehazing. The main challenge faced by learning-based methods is the absence of real-world paired training data, as obtaining clear ground truth images of hazy nighttime scenes that include glow and multiple light sources is intractable. A learning-based method [95] has attempted to address this issue by utilizing synthetic data. However, this method is unable to effectively suppress glow since the synthetic dataset does not account for the glow effect. A semi-supervised deep learning-based network [84] suffers from artifacts and loss of low-frequency scene details.

In this paper, our goal is to enhance visibility in a single nighttime haze image by suppressing glow and enhancing low-light regions. Our glow suppression includes two main parts: APSF-guided glow rendering and gradient adaptive convolution. Our glow rendering method uses an APSF-guided approach to create glow effects for various light sources. We employ a light source aware network to detect the locations of light sources in images and then apply APSF-guided glow rendering to these sources. Our framework learns from the rendered images and thus can suppress glow effects in different light sources. Our gradient adaptive convolution captures edges and textures from hazy images. To be specific, edges are obtained by computing the pixel differences [69] between neighboring pixels, while the bilateral kernel [72] is used to extract textures of images. Both edges and textures are then fed into our framework to enhance the image details. To enhance the visibility of non-light regions, we introduce a novel attention-guided enhancement module. The hazy regions have low weights, while the dark regions have high weights in the attention map. As shown in Fig. 1, our method not only handles glow effects but also enhances the low-light regions.

Overall, our contributions can be summarized as follows:

- To our knowledge, our method is the first learning-based network that handles night glow and low-light conditions in one go.
- We present a light source aware network and APSF-guided glow rendering to simulate glow effects from different light sources. By learning from the APSF-guided glow rendering data, our framework effectively suppresses glow effects in real-world hazy images.
- Since night images contain less contrast, we employ gradient-adaptive convolution for edge enhancement and the bilateral kernel for texture enhancement.

Extensive experiments on nighttime images demonstrate the effectiveness of our approach in quantitative and qualitative evaluations. Our method achieves 30.38dB of PSNR, which outperforms existing nighttime dehazing methods by 13%.

# 2 RELATED WORK

Early dehazing methods utilized multiple images [46, 54] or priors for atmospheric light and transmission estimation [5, 16, 23, 70?]. With the advent of deep learning, numerous networks were proposed to estimate the transmission map [6, 61, 92] or output clean images end-to-end [18, 29, 37, 39, 60, 62, 78, 87, 89, 90]. Recent fully supervised [15, 19, 48, 59, 67, 77, 80, 97], semi-supervised [10, 38, 43, 63], zero-shot [35, 36], and unsupervised [17, 24, 47, 86, 96] methods have been developed. However, these methods struggle with nighttime haze due to non-uniform, multi-colored artificial light and the absence of clean ground truth data for training.

Optimization-based nighttime dehazing methods have followed the atmospheric scattering model (e.g., [1, 2, 57]), new imaging model (e.g., [51, 71, 93, 94]), etc. Pei and Lee [57] transfer the airlight colors of hazy nighttime images to daytime and use DCP to dehaze. Ancuti et al. [1, 2] introduce a fusion-based method and Laplacian pyramid decomposition to estimate local airlight. Zhang et al. [94] use illumination compensation, color correction and DCP to dehaze. Zhang et al. [93] propose maximum reflectance prior (MRP). Tang et al. [71] use Retinex theory and Taylor series expansion. Liu et al. [50, 52] use regularization constraints. Wang et al. [74] proposed the gray haze-line prior and variational model. Existing nighttime dehazing methods depend on local patch-based atmospheric light estimation, assuming uniformity within a small patch. Therefore, their performance is sensitive to the patch size. These methods are not adaptive and time-consuming in optimization. Unlike them, our method is learning-based, more efficient, practical and fast.

Recently, learning-based nighttime dehazing methods [49] have been proposed. Zhang et al. [95] train the network using synthetic nighttime hazy images through fully supervised learning. However, this approach does not account for glow, leaving it in the results. Yan et al. [84] propose a semi-supervised method employing high-low frequency decomposition and a grayscale network. However, their results tend to be dark, with lost details. This is because coarse frequency-based decomposition methods struggle to effectively separate glow, leading to reduced brightness and visibility of the scene. The DeGlow-DeHaze network [33] estimates transmission followed by DehazeNet [6]. However, the atmospheric light estimated by the DeHaze network is obtained from the brightest region and assumed to be globally uniform, which is invalid at nighttime [11â€“14, 26, 64â€“66]. In contrast, our results can suppress glow and, at the same time, enhance low-light regions.

The glow of a point source, referred to as the Atmospheric Point Spread Function (APSF), has been studied in various works. Narasimhan and Nayar [55] first introduced APSF and developed a physics-based model for the multiple scattered light. Metari et al. [?] model the APSF kernel for multiple light scattering. Li et al. [45] decompose glow from the input image using a layer separation method [42], constrain glow by its smooth attribute, and dehaze using DCP. Park et al. [56] and Yang et al. [85] follow the nighttime haze model and use weighted entropy and super-pixel to estimate.

# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

|pixel location|pixel difference|bilateral kernel|
|---|---|---|
|(4) Attention Guided Enhance|Real/Fake|eifer|
|(3) Gradient Adaptive Conv|Edge|Texture|
|Enhance|mc"er|meijer|
|Input Haze|Light Source Aware|Output Clean|
|Enhanced Output| | |

(1) Dehaze and Deglow

Guided APSF
Input Clean
Light Source Map
APSF2D
Output Haze

Figure 2: (1) Our deglowing framework ğºğ‘ have two inputs: one to learn from real haze images ğ¼â„ and the other to learn from real clean reference images ğ¼ğ‘. For input haze images ğ¼â„, ğºğ‘ output clean images ğ‘‚ğ‘. For input clean images ğ¼ğ‘, ğºğ‘ output clean images ğºğ‘(ğ¼ğ‘). (2) APSF guide glow generator ğºâ„ to generate glow ğ‘‚â„ on reference images ğ¼ğ‘. (3) the upper left is the gradient adaptive convolution, from the gradient convolution (the blue window), we obtain edges; from the adaptive bilateral kernel (the red), we enhance texture details. (4) the upper right is attention-guided enhancement module.

atmospheric light and transmission map. However, these methods, after glow removal, simply apply daytime dehazing to nighttime dehazing, which results in low visibility and color distortion in their outcomes. Previous works have primarily focused on optimization-based approaches, while our work is the first to incorporate the APSF prior into a nighttime learning network.

# 3 PROPOSED METHOD

Fig. 2 shows our pipeline, including glow suppression and low-light enhancement. Our glow suppression has two parts: deglowing network ğºğ‘ and glow generator ğºâ„. Our deglowing network ğºğ‘ transforms real haze images ğ¼â„ to clean images ğ‘‚ğ‘. We employ a discriminator ğ·ğ‘ to determine whether the generated clean images ğ‘‚ğ‘ and the reference image ğ¼ğ‘ are real or not. Our novelty in the pipeline lies in these 3 ideas: APSF-guided glow rendering, gradient-adaptive convolution, and attention-guided enhancement.

# 3.1 Light Source Aware Network

Nighttime scenes often contain active light sources such as streetlights, car headlights, and building lights. These sources can cause strong glow in hazy nighttime scenes. The appearance of haze and glow in nighttime scenes can be modeled as [45]:

ğ¼â„(ğ‘¥) = ğ¼ğ‘(ğ‘¥)ğ‘¡(ğ‘¥) + ğ´(ğ‘¥) (1 âˆ’ğ‘¡(ğ‘¥)) + ğ¿ğ‘  (ğ‘¥) âˆ—APSF,

where ğº(ğ‘¥) = ğ¿ğ‘  (ğ‘¥) âˆ— APSF, is the glow map, ğ¿ğ‘ (ğ‘¥) represents the light sources, and APSF stands for Atmospheric Point Spread Function, âˆ— denotes the 2D convolution. ğ¼â„ is an observed hazy image. ğ¼ğ‘ is the scene radiance (without haze). ğ´ is the atmospheric light.

Light, and ğ‘¡ is the transmission, modeled as ğ‘¡(ğ‘¥) = ğ‘’âˆ’ğ›½ğ‘‘(ğ‘¥), where ğ›½ is the extinction coefficient. Light sources play an important role and can be utilized in three ways: (1) to inform ğºğ‘ about the location of light sources (as shown in Fig. 4), (2) to guide ğºâ„ to generate glow output ğ‘‚â„, and (3) to render the glow data ğ¼ğ‘” using APSF (as shown in Fig. 5). For (1), we define light source consistency loss to keep the light source regions consistent in input and output images, therefore to maintain the same color and shape of these regions:

Lls = |ğ‘‚ğ‘ âŠ™ ğ‘€ âˆ’ ğ¿ğ‘ |1,

where ğ¿ğ‘  is the light source, ğ‘‚ğ‘ is the output clean image, ğ‘€ is the soft matting map, âŠ™ is element-wise multiplication. The process of obtaining them is depicted in Fig. 3 and Algorithm 1, which involves the following steps: First, we identify the regions that contain light sources. Next, an initial light source mask Ë†ğ‘€ is generated by thresholding the night image. To obtain a more accurate separation of the light sources from the surrounding areas and to ensure.

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# Yeying Jin et al.

# Pinhole Image Plane

| |70|60|60|50|50|40|40|30|30|20|10| | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|Oj1aE21|Point Source|Scatterer|10â‚…â‚€|100|50|50|100|150|200|150|100|50| | | | |
| | | | | |APSF2Dâµâ°|100|150|200| | | | | | | | |

Figure 3: We show Algorithm 1, light source map detection: (1) We first generate an initial light source mask Ë† M based on intensity, (2) then refine the mask using alpha matting [34] to obtain light source soft matting ğ‘€. (3) By multiplying the light source map ğ‘€ with the night clean image ğ¼ğ‘, we obtain the light source map ğ¿ğ‘ . After obtaining the light source, we show Algorithm 2, APSF-guided nighttime glow rendering: (1) Next, we perform APSF 2D convolution on the light source map to render glow ğº. (2) Finally, by combining the night clean and glow image, we obtain the rendered glow image ğ¼ğ‘”. More results are shown in Fig. 5.

# Walmart

Figure 4: We show the light source maps ğ¿ğ‘  of night haze.

# Algorithm 2 APSF-based Nighttime Glow Rendering

1. Compute the APSF function in Algorithm 3, ğ´ğ‘ƒğ‘†ğ¹ â† psfweight(ğœƒ,ğ‘‡ , ğ‘)
2. Convert the APSF function to an APSF2D, ğ´ğ‘ƒğ‘†ğ¹2ğ· â† get2D(ğ´ğ‘ƒğ‘†ğ¹)
3. Perform APSF2D convolution on the light source image, ğº â† ğ¿ğ‘  âˆ— ğ´ğ‘ƒğ‘†ğ¹2ğ·
4. Calculate the parameter for combining the clean and glow image, ğœ– âˆ¼ N (0,1), ğ›¼ â† 0.4196 Â· ğ‘™ğ‘–ğ‘”â„ğ‘¡_ğ‘ ğ‘§2 âˆ’4.258Â· ğ‘™ğ‘–ğ‘”â„ğ‘¡_ğ‘ ğ‘§ + 11.35 + 0.05Â· ğœ–
5. Combine the clean and glow image to render night glow, ğ¼ğ‘” â† 0.99Â· ğ¼ğ‘ + ğ›¼Â· ğº
6. Add Gaussian noise, ğ¼ğ‘” â† AddNoise(ğ¼ğ‘”)

In Fig. 5, we provide examples of paired nighttime clean ğ¼ğ‘ and glow ğ¼ğ‘” images, demonstrating the effectiveness of our approach.

In Fig. 3 (top left), we use the phase function ğ‘ƒ (cosğœƒ) [25] to approximate light scattering in nighttime glow. The scattering angle ğœƒ is defined as the angle between the incident (ğœƒâ€², ğœ‡â€²) and scattered light (ğœƒ, ğœ‡) directions, where ğœ‡â€² = cosğœƒâ€²and ğœ‡ = cosğœƒ represent the angular distribution of light. The scattered light intensity ğ¼(ğ‘‡ , ğœ‡), considering optical thickness (ğ‘‡), measures the attenuation of light due to fog. Here, ğ‘‡ = ğ›½ğ‘…, with ğ›½ representing the attenuation coefficient and ğ‘… representing the distance between the isotropic source and the pinhole camera. The shape of the phase function ğ‘ƒ depends on the size of the scatterer, and hence the weather conditions. For large particles, such as fog, ğ‘ƒ exhibits a strong peak in the direction of light incidence.

# APSF-Guided Nighttime Glow Rendering

After obtaining the light source maps, we present an APSF-based method for rendering nighttime glow effects, as shown in Fig. 3 and Algorithm 2. First, we compute the APSF function in Algorithm 3 and convert it into a 2D format, which allows us to perform 2D convolution on the light source maps, resulting in the glow.

# Algorithm 3

shows the APSF weight calculation process [55]. There are three input parameters: angle ğœƒ (-180â—¦ to 180â—¦), optical thickness ğ‘‡, and forward scattering parameter ğ‘ that indicates the.

# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# Algorithm 3 APSF Weights Calculation

(a) Vanilla Conv   : pixel location

Require:  ğœƒ, optical thicknessğ‘‡, forward scattering parameter ğ‘

(b) Gradient Conv  : pixel difference between  and

(c) Adaptive Conv  : bilateral kernel

Ensure: weights

1. ğœ‡ â† cos(ğœƒ)
2. if look-up table (LUT) for Legendre polynomials exists then
3. Load LUT
4. else
5. Generate LUT and save to file   (2ğ‘š+1) (1âˆ’ğ‘ğ‘šâˆ’1)
6. Define ğ›¼m(ğ‘š) â† ğ‘š + 1, ğ›½m(ğ‘š, ğ‘) â†   ğ‘š
7. Define ğ‘”m(ğ‘š, ğ›¼m, ğ›½m,ğ‘‡) â† ğ‘’(âˆ’ğ›½áµğ‘‡âˆ’ğ›¼áµ log(ğ‘‡ ))
8. Initializeğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘  to all zeros
9. for ğ‘š â† 1 to 200 (number of polynomials) do
10. if ğ‘š == 1 then
11. ğ¿m-1(ğœ‡) â† 1
12. else
13. ğ¿m-1(ğœ‡) â† ğ¿m(ğœ‡)
14. ğ¿m(ğœ‡) â† interpolate LUT for ğ‘š and ğœ‡
15. ğ‘ â† ğ‘”m(ğ‘š, ğ›¼m(ğ‘š), ğ›½m(ğ‘š, ğ‘),ğ‘‡)(ğ¿m-1(ğœ‡) + ğ¿m(ğœ‡)), Eq. (3).
16. ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘  â† ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘  + ğ‘
17. ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘  â† ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘ Â· ğ‘‡2 (normalize)

LL

2017áµ€áµ’áµˆáµˆPagePhocography

9

2017 BocdPage Photograpny

# Figure 6

We show using gradient adaptive convolution, can obtain edges (middle) and textures (below) in haze images.

# Figure 7

Our gradient adaptive convolution combine (b) gradient convolution (blue) to capture edge, and (c) adaptive bilateral kernel (red) to obtain texture.

# 3.3.1 Edge Capture using Gradient Convolution

We utilize nearby information to preserve the gradient information and improve contrast by extracting edges using gradient-adaptive convolution. The main idea of gradient convolution is to use local binary patterns (LBP) [58, 69] in a ğ‘˜ Ã— ğ‘˜ neighborhood, where the central pixel serves as a threshold and the values of the surrounding pixels are compared to the central pixel. If a neighboring pixelâ€™s value exceeds the central pixelâ€™s value, a position marker of 1 is assigned; otherwise, a value of 0 is assigned. The vanilla convolution and gradient convolutions are expressed as follows:

vâ€² =   w  pğ‘– âˆ’ pğ‘—  vğ‘—,         (Vanilla Conv)

ğ‘–

ğ‘—âˆˆÎ©(ğ‘–)

vâ€² =   w  pğ‘– âˆ’ pğ‘—  (vğ‘— âˆ’ vâ€²),  (Gradient Conv)

ğ‘–

ğ‘—

ğ‘—âˆˆÎ©(ğ‘–)

where v = (v1, . . . ,vğ‘›), vğ‘– âˆˆ Rğ‘ over ğ‘› pixels and ğ‘ channels, is input image features. w is the weight in the Î©(Â·), ğ‘˜ Ã— ğ‘˜ convolution window. We use [pğ‘– âˆ’ pğ‘—] to denote indexing of the spatial dimensions of an array with 2D spatial offsets. This convolution operation results in a ğ‘â€²-channel output, vâ€² âˆˆ Rğ‘â€², at each pixel ğ‘–.

In Fig. 7 (a) and (b), instead of using input pixel values in Eq. (4), we use the differences between neighboring pixels in Eq. (5) to capture the gradient information.

atmospheric conditions [53]. The phase function ğ‘ƒ can be expanded using Legendre polynomials series ğ¿ğ‘š (ğ‘š stand for order) [7]:

âˆ

ğ¼(ğ‘‡ , ğœ‡) =  ğ‘”  (ğ‘‡) (ğ¿  (ğœ‡) + ğ¿ (ğœ‡)),

(3)

ğ‘š      ğ‘šâˆ’1      ğ‘š

ğ‘š=1

where:ğ‘”ğ‘š(ğ‘‡) = ğ¼0ğ‘’âˆ’ğ›½ğ‘šğ‘‡âˆ’ğ›¼ğ‘š logğ‘‡ , ğ›¼ğ‘š = ğ‘š+1, ğ›½ğ‘š = 2ğ‘š+1  1 âˆ’ğ‘ğ‘šâˆ’1 .

# 3.3.2 Texture Capture using Bilateral Kernel

A single CNN may not be sufficient for effectively handling uneven light distribution within a nighttime haze image. Therefore, we propose using a bilateral kernel in adaptive convolution to extract texture details, as shown in Fig. 7 (c):

vâ€² =  ğ¾  f       ,f   w  p âˆ’ p   v ,  (Adaptive Conv)

ğ‘–                ğ‘–  ğ‘—  ğ‘–  ğ‘—  ğ‘—

ğ‘— âˆˆ Î©(ğ‘–)

MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

Yeying Jin et al.

where ğ¾ is bilateral kernel, depends on pixel features f [68]. We use color features f = (ğ‘Ÿ, ğ‘”, ğ‘), ğ¾ fğ‘–, fğ‘— = exp âˆ’ 1   fğ‘– âˆ’ fğ‘— ,

1 2 2ğ›¼1 Inbuue w  pğ‘– âˆ’ pğ‘— = exp âˆ’2ğ›¼â‚‚  pğ‘– âˆ’ pğ‘— .

Since the features obtained using the bilateral kernel are noise-reduced, edge-preserved, and detail-enhanced, they help extract high-frequency texture details that are less affected by haze and glow, as shown in Fig. 6 (bottom). To enforce the consistency between the input ğ¼â„ and the output ğ‘‚ğ‘ and improve the quality of the output, we define the self-supervised bilateral kernel loss as:

Lğ‘˜ = |ğ¾(ğ‘‚ğ‘) âˆ’ ğ¾(ğ¼â„)|1. (8)

In summary, our gradient-adaptive convolution captures edge information by considering the differences between neighboring pixels instead of their pixel values, as reflected in the gradient term (vğ‘— âˆ’vâ€²). Additionally, it accounts for spatially varying illumination and non-uniform haze distribution by utilizing the adaptive term ğ¾(fğ‘–, fğ‘—), which helps to preserve texture information and adapt more effectively to nighttime scenes.

# 3.4 Network and Other Losses

In Fig. 2, our glow suppression is CycleGAN [98]-based network. We use the deglowing network ğºğ‘, which is coupled with a discriminator ğ·ğ‘. To ensure reconstruction consistency, we have another haze generator ğºâ„ coupled with its discriminator ğ·â„. ğºğ‘ input hazy images ğ¼â„ and output clean images ğ‘‚ğ‘. Our haze generator ğºâ„ transforms the output images ğ‘‚ğ‘ to reconstructed clean images Ë†ğ¼â„. By imposing the cycle-consistency constraints, our deglowing network ğºğ‘ learns to remove the real-world glow. Thanks to the cycle-consistency constraints, we are allowed to use unpaired data to optimize our deglowing network, thus reducing the domain gap between the synthetic datasets and real-world glow images.

Besides the self-supervised light source consistency loss Lls, gradient loss Lğ‘” and bilateral kernel loss Lğ‘˜, with weights {1, 0.5, 5}, we followed [98], use other losses to train our network. They are adversarial loss Lğ‘ğ‘‘ğ‘£, cycle consistency loss Lğ‘ğ‘¦ğ‘, identity loss Lğ‘–ğ‘‘ğ‘’ğ‘›, with weights {1, 10, 10}.

# 3.5 Low-light Region Enhancement

Nighttime dehazing often leads to dark results due to low-light conditions [20, 27, 30, 31, 75, 76, 79]. To address this, we incorporate a low-light enhancement module to improve the visibility of object regions. Our approach involves generating an attention map that highlights these regions, allowing our method to focus on enhancing their intensity. We then apply a low-light image enhancement technique [21] to enhance the region with the assistance of the attention map, even in scenes with low light.

# Attention Map

To obtain the soft attention maps ğ´ shown in Fig. 8, we input the night haze images and refine the coarse map using [44]. The refined attention map ğ´ exhibits high values in object regions and low values in uniform regions, such as the sky. Therefore, we can distinguish between the object and the haze regions.

ğ‘‚  = (1 âˆ’ğ´) Â·ğ‘‚ + ğ´Â· ğ‘‚ğ›¾, (9)

# 4 EXPERIMENTAL RESULTS

# 4.1 Datasets

GTA5 [84] is a synthetic nighttime dehazing dataset, which is generated by the GTA5 game engine. It includes 864 paired images, where 787 paired images are used as the training set and the rest images are taken as the test set.

RealNightHaze is a real-world night dehazing dataset. It includes 440 night hazy images, where 150 images are from [95], 200 images are from [84] and the rest images are collected from the Internet.

# 4.2 Comparison on Synthetic Datasets

In this section, we compare our method with existing state-of-the-art methods, including Yan [84], Zhang [94], Li [45] Ancuti [1], Zhang [93], Yu [91], Zhang [95], Liu [50] and Wang [74]. The summary of the main differences is shown in Table 2. The experimental results are shown in Table 1. It can be observed that our method achieves a significant performance improvement. We adopt two widely used metrics PSNR, SSIM in generation [73] and restoration [8, 9, 28, 40, 41, 81â€“83, 88] tasks. Our method achieves a PSNR of 30.383 and a SSIM of 0.904, outperforming Yanâ€™s method [84] by.

**Table 1: Results on GTA5 nighttime haze dataset.**
|Input Image|PSNR â†‘|SSIM â†‘|
|---|---|---|
|Input Image|18.987|0.6764|
|Li [45]|21.024|0.6394|
|Zhang [93]|20.921|0.6461|
|Ancuti [1]|20.585|0.6233|
|Yan [84]|26.997|0.8499|
|CycleGAN [98]|21.753|0.6960|
|w/o Gradient Adaptive Conv|27.913|0.8673|
|w/o APSF Guided Render|28.913|0.8776|
|Our Result|30.383|0.9042|

**Table 2: Summary of comparisons between ours and existing learning-based nighttime dehazing methods. SL is short for supervised learning, SSL is semi-supervised learning.**
|Learning Methods|Glow|Edge, Texture|Low Light|Light Source|Uneven light|
|---|---|---|---|---|---|
|SSL Ours|âœ“|âœ“|âœ“|âœ“|âœ“|
|SL Kuanar [33]|âœ“|Ã—|Ã—|Ã—|Ã—|
|SSL Yan [84]|âœ“|âœ“|Ã—|Ã—|Ã—|
|SL Zhang [93]|Ã—|Ã—|Ã—|Ã—|Ã—|

# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

|(a) Input|(b) Ours|(c) Liu-22 [51]|(d) Wang-22 [74]|
|---|---|---|---|
|(e) Zhang [95]|(f) Yan-20 [84]|(g) Yu-19 [91]|(h) Zhang [93]|
|(a) Input|(b) Ours|(c) Liu-22 [51]|(d) Wang-22 [74]|
|(e) Zhang [95]|(f) Yan-20 [84]|(g) Yu-19 [91]|(h) Zhang [93]|
|(a) Input|(b) Ours|(c) Liu-22 [51]|(d) Wang-22 [74]|
|(e) Zhang [95]|(f) Yan-20 [84]|(g) Yu-19 [91]|(h) Ancuti-20 [2]|

Figure 9: Visual comparisons of different nighttime dehazing methods on real nighttime hazy scenes. Our results are more realistic and effective in nighttime dehazing. Zoom-in for better visualization.

2452

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# Yeying Jin et al.

# Table 3: User study evaluation on the real night images

Our method obtained the highest mean (the max score is 10), showing our method is effective in nighttime dehazing, deglowing and low-light enhancement. Our method is also visually realistic. The best result is in red whereas the second and third best results are in blue and purple, respectively.

|Aspects|Ours|Yan [84]|Zhang [94]|Li [45]|Ancuti [1]|Zhang [93]|Yu [91]|Zhang [95]|Liu-22 [51]|Wang-22 [74]|
|---|---|---|---|---|---|---|---|---|---|---|
|1.Dehazeâ†‘|9.1Â± 0.99|8.9 Â± 1.75|4.2Â± 2.42|6.1Â± 2.00|5.2Â± 2.35|4.4Â± 2.01|4.8Â± 2.26|4.6 Â±2.07|3.9Â± 2.12|5.5 Â± 1.91|
|2.Deglowâ†‘|9.1Â± 0.91|7.9 Â± 1.12|3.2Â± 1.96|6.2Â± 1.83|5.2Â± 2.14|4.0Â± 2.09|3.7Â± 2.08|4.4 Â±2.05|5.3Â± 1.99|5.7 Â± 1.76|
|3.Low-lightâ†‘|8.5Â± 1.30|7.9 Â± 1.93|7.1Â± 2.31|5.5Â± 2.33|5.4Â± 2.01|5.6Â± 1.82|6.5Â± 2.12|5.6 Â±1.75|5.5Â± 1.92|5.4 Â± 1.89|
|4.Realisticâ†‘|8.9Â± 0.94|8.0 Â± 1.28|4.6Â± 1.93|4.7Â± 2.07|6.7Â± 1.90|4.9Â± 1.93|5.7Â± 1.90|4.9 Â±1.97|3.8Â± 1.81|5.9 Â± 1.67|

14% and 5%, respectively. This is because our method learns from the APSF-guided glow rendering and thus effectively removes the glow effects. Another advantage is that we introduce a gradient-adaptive convolution to capture the edges and textures. The obtained edges and textures are then used to enhance the structural details of the enhanced images, leading to superior performance.

# 4.3 Comparison on Real-World Datasets

Input *Ih w/o Enhance Oc Attention A w/ Enhance Oe*

Fig. 9 show the qualitative results, including Liu [50], Wang [74], Zhang [95], Yan [84], Yu [91], Zhang [93] and our method. It can be found that our method significantly enhances the visibility of nighttime hazy images. Specifically, most state-of-the-art methods cannot sufficiently remove haze since their methods suffer from the domain gap between the synthetic datasets and real-world images. Yan et al [84] proposes a semi-supervised framework for nighttime foggy removal and can remove most hazy effects. However, their method over-suppresses hazy images, and thus their outputs become too dark.

In contrast, our method handles glow and low-light conditions. As shown in Fig. 9 (b), our method not only removes the haze of the input images but also enhances the light. For instance, the details of trees and buildings are clear. This is because our method simulates the glow rendering by utilizing Angular Point Spread Function (APSF) and thus can effectively remove haze or glow in a real-world night hazy image. Moreover, we propose a gradient-adaptive convolution to capture the edges and textures from hazy images. The captured edges and textures are then used to boost the details of images, leading to superior performance. Furthermore, we introduce an attention map to enhance the low-light regions. As a result, our method achieves a significant performance improvement.

# 5 CONCLUSION

In this paper, we have proposed a novel nighttime visibility enhancement framework, addressing both glow and low-light conditions. Our framework includes three core ideas: APSF-guided glow rendering, gradient-adaptive convolution and attention-guided low-light enhancement. Our framework suppresses glow effects via learning from the APSF-guided glow rendering data. Thanks to our APSF-guided glow rendering, we allow to use a semi-supervised method to optimize our network, thus handling glow effects in different light sources. Our gradient-adaptive convolution is proposed to capture edges or textures from a nighttime hazy image. Benefiting from the captured edges or textures, our framework effectively preserves the structural details. Our low-light region enhancement boosts the intensity of dark or over-suppressed regions via attention map. Both quantitative and qualitative experiments show that our method achieves a significant performance improvement. Moreover, the ablation study proves the effectiveness of each core idea.

# ACKNOWLEDGMENTS

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD/2022-01-037[T]).

# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# REFERENCES

1. Cosmin Ancuti, Codruta O Ancuti, Christophe De Vleeschouwer, and Alan C Bovik. 2016. Night-time dehazing by fusion. In 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2256â€“2260.
2. Cosmin Ancuti, Codruta O Ancuti, Christophe De Vleeschouwer, and Alan C Bovik. 2020. Day and night-time dehazing by local airlight estimation. IEEE Transactions on Image Processing 29 (2020), 6264â€“6275.
3. Sriparna Banerjee and Sheli Sinha Chaudhuri. 2021. Nighttime image-dehazing: a review and quantitative benchmarking. Archives of Computational Methods in Engineering 28, 4 (2021), 2943â€“2975.
4. Dana Berman, Shai Avidan, et al. 2016. Non-local image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1674â€“1682.
5. Dana Berman, Tali Treibitz, and Shai Avidan. 2018. Single image dehazing using haze-lines. IEEE transactions on pattern analysis and machine intelligence 42, 3 (2018), 720â€“734.
6. Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and Dacheng Tao. 2016. Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Processing 25, 11 (2016), 5187â€“5198.
7. Subrahmanyan Chandrasekhar. 2013. Radiative transfer. Courier Corporation.
8. Sixiang Chen, Tian Ye, Yun Liu, Erkang Chen, Jun Shi, and Jingchun Zhou. 2022. Snowformer: Scale-aware transformer via context interaction for single image desnowing. arXiv preprint arXiv:2208.09703 (2022).
9. Sixiang Chen, Tian Ye, Yun Liu, Taodong Liao, Jingxia Jiang, Erkang Chen, and Peng Chen. 2023. MSP-former: Multi-scale projection transformer for single image desnowing. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1â€“5.
10. Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu. 2021. PSD: Principled synthetic-to-real dehazing guided by physical priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7180â€“7189.
11. Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy. 2022. Flare7k: A phenomenological nighttime flare removal dataset. Advances in Neural Information Processing Systems 35 (2022), 3926â€“3937.
12. Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Yihang Luo, and Chen Change Loy. 2023. Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond. arXiv preprint arXiv:2306.04236 (2023).
13. Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Qingpeng Zhu, Qianhui Sun, Wenxiu Sun, Chen Change Loy, Jinwei Gu, Shuai Liu, et al. 2023. MIPI 2023 Challenge on Nighttime Flare Removal: Methods and Results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2852â€“2862.
14. Yuekun Dai, Yihang Luo, Shangchen Zhou, Chongyi Li, and Chen Change Loy. 2023. Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 20783â€“20791.
15. Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, and Ming-Hsuan Yang. 2020. Multi-scale boosted dehazing network with dense feature fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2157â€“2167.
16. Raanan Fattal. 2008. Single image dehazing. ACM transactions on graphics (TOG) 27, 3 (2008), 1â€“9.
17. Alona Golts, Daniel Freedman, and Michael Elad. 2019. Unsupervised single image dehazing using dark channel prior loss. IEEE Transactions on Image Processing 29 (2019), 2692â€“2701.
18. Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai, Weidong Zhang, and Chongyi Li. 2023. Underwater Ranker: Learn Which Is Better and How to Be Better. In Proceedings of the AAAI conference on artificial intelligence.
19. Chun-Le Guo, Qixin Yan, Saeed Anwar, Runmin Cong, Wenqi Ren, and Chongyi Li. 2022. Image Dehazing Transformer with Transmission-Aware 3D Position Embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5812â€“5820.
20. Lanqing Guo, Renjie Wan, Wenhan Yang, Alex Kot, and Bihan Wen. 2022. Enhancing low-light images in real world via cross-image disentanglement. arXiv preprint arXiv:2201.03145 (2022).
21. Xiaojie Guo, Yu Li, and Haibin Ling. 2016. LIME: Low-light image enhancement via illumination map estimation. IEEE Transactions on image processing 26, 2 (2016), 982â€“993.
22. Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence 33, 12 (2010), 2341â€“2353.
23. Kaiming He, Jian Sun, and Xiaoou Tang. 2011. Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence 33, 12 (2011), 2341â€“2353.
24. Lu-Yao Huang, Jia-Li Yin, Bo-Hao Chen, and Shao-Zhen Ye. 2019. Towards Unsupervised Single Image Dehazing With Deep Learning. In 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2741â€“2745.
25. Akira Ishimaru et al. 1978. Wave propagation and scattering in random media. Vol. 2. Academic press New York.
26. Yun Liu, Zhongsheng Yan, Sixiang Chen, Tian Ye, Wenqi Ren, and Erkang Chen. 2023. NightHazeFormer: Single Nighttime Haze Removal Using Prior Query.

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# References

1. Yun Liu, Zhongsheng Yan, Jinge Tan, and Yuche Li. 2023. Multi-Purpose Oriented Single Nighttime Image Haze Removal Based on Unified Variational Retinex Model. IEEE Transactions on Circuits and Systems for Video Technology 33, 4 (2023), 1643â€“1657. https://doi.org/10.1109/TCSVT.2022.3214430
2. Yun Liu, Zhongsheng Yan, Aimin Wu, Tian Ye, and Yuche Li. 2022. Nighttime image dehazing based on variational decomposition model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition Workshop. 640â€“649.
3. Yun Liu, Zhongsheng Yan, Tian Ye, Aimin Wu, and Yuche Li. 2022. Single nighttime image dehazing based on unified variational decomposition model and multi-scale contrast enhancement. Engineering Applications of Artificial Intelligence 116 (2022), 105373.
4. William Edgar Knowles Middleton. 1957. Vision through the atmosphere. Springer.
5. Srinivasa G Narasimhan and Shree K Nayar. 2000. Chromatic framework for vision in bad weather. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662), Vol. 1. IEEE, 598â€“605.
6. Srinivasa G Narasimhan and Shree K Nayar. 2003. Shedding light on the weather. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., Vol. 1. IEEE, Iâ€“I.
7. Dubok Park, David K Han, and Hanseok Ko. 2016. Nighttime image dehazing with local atmospheric light and weighted entropy. In 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2261â€“2265.
8. Soo-Chang Pei and Tzu-Yen Lee. 2012. Nighttime haze removal using color transfer pre-processing and dark channel prior. In 2012 19th IEEE International conference on image processing. IEEE, 957â€“960.
9. Matti PietikÃ¤inen. 2010. Local binary patterns. Scholarpedia 5, 3 (2010), 9775.
10. Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. 2020. FFA-Net: Feature fusion attention network for single image dehazing. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11908â€“11915.
11. Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced Pix2pix Dehazing Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8160â€“8168.
12. Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and Ming-Hsuan Yang. 2016. Single image dehazing via multi-scale convolutional neural networks. In European conference on computer vision. Springer, 154â€“169.
13. Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang. 2018. Gated fusion network for single image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3253â€“3261.
14. Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and Nong Sang. 2020. Domain Adaptation for Image Dehazing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2808â€“2817.
15. Aashish Sharma, Loong-Fah Cheong, Lionel Heng, and Robby T Tan. 2020. Nighttime Stereo Depth Estimation using Joint Translation-Stereo Learning: Light Effects and Uninformative Regions. In 2020 International Conference on 3D Vision (3DV). IEEE, 23â€“31.
16. Aashish Sharma and Robby T Tan. 2021. Nighttime Visibility Enhancement by Increasing the Dynamic Range and Suppression of Light Effects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11977â€“11986.
17. Aashish Sharma, Robby T Tan, and Loong-Fah Cheong. 2020. Single-Image Camera Response Function Using Prediction Consistency and Gradual Refinement. In Proceedings of the Asian Conference on Computer Vision.
18. Yuda Song, Zhuqing He, Hui Qian, and Xin Du. 2023. Vision transformers for single image dehazing. IEEE Transactions on Image Processing 32 (2023), 1927â€“1941.
19. Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, and Jan Kautz. 2019. Pixel-adaptive convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11166â€“11175.
20. Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti PietikÃ¤inen, and Li Liu. 2021. Pixel difference networks for efficient edge detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5117â€“5127.
21. Robby T Tan. 2008. Visibility in bad weather from a single image. In 2008 IEEE conference on computer vision and pattern recognition. IEEE, 1â€“8.
22. Qunfang Tang, Jie Yang, Xiangjian He, Wenjing Jia, Qingnian Zhang, and Haibo Liu. 2021. Nighttime image dehazing based on Retinex and dark channel prior using Taylor series expansion. Computer Vision and Image Understanding 202 (2021), 103086.
23. Carlo Tomasi and Roberto Manduchi. 1998. Bilateral filtering for gray and color images. In Sixth international conference on computer vision (IEEE Cat. No. 98CH36271). IEEE, 839â€“846.
24. Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby T Tan, and Haizhou Li. 2023. Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14653â€“14662.
25. Wenhui Wang, Anna Wang, and Chen Liu. 2022. Variational Single Nighttime Image Haze Removal With a Gray Haze-Line Prior. IEEE Transactions on Image Processing 31 (2022), 1349â€“1363.
26. Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. 2022. Low-light image enhancement with normalizing flow. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 2604â€“2612.
27. Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex C Kot, and Bihan Wen. 2023. ExposureDiffusion: Learning to Expose for Low-light Image Enhancement. arXiv preprint arXiv:2307.07710 (2023).
28. Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. 2021. Contrastive learning for compact single image dehazing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10551â€“10560.
29. Ruiqi Wu, Zhengpeng Duan, Chunle Guo, Zhi Chai, and Chongyi Li. 2023. RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
30. Wanyu Wu, Wei Wang, Zheng Wang, Kui Jiang, and Xin Xu. 2023. From Generation to Suppression: Towards Effective Irregular Glow Removal for Nighttime Visibility Enhancement. arXiv:2307.16783 [cs.CV]
31. Boxue Xiao, Zhuoran Zheng, Yunliang Zhuang, Chen Lyu, and Xiuyi Jia. 2022. Single UHD image dehazing via interpretable pyramid network. Available at SSRN 4134196 (2022).
32. Zeyu Xiao, Jiawang Bai, Zhihe Lu, and Zhiwei Xiong. 2023. A dive into sam prior in image restoration. arXiv preprint arXiv:2305.13620 (2023).
33. Zeyu Xiao, Xueyang Fu, Jie Huang, Zhen Cheng, and Zhiwei Xiong. 2021. Space-time distillation for video super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2113â€“2122.
34. Zeyu Xiao, Zhiwei Xiong, Xueyang Fu, Dong Liu, and Zheng-Jun Zha. 2020. Space-time video super-resolution using temporal profiles. In Proceedings of the 28th ACM International Conference on Multimedia. 664â€“672.
35. Wending Yan, Robby T Tan, and Dengxin Dai. 2020. Nighttime defogging using high-low frequency decomposition and grayscale-color networks. In European Conference on Computer Vision. Springer, 473â€“488.
36. Minmin Yang, Jianchang Liu, and Zhengguo Li. 2018. Superpixel-based single nighttime image haze removal. IEEE transactions on multimedia 20, 11 (2018), 3008â€“3018.
37. Yang Yang, Chaoyue Wang, Risheng Liu, Lin Zhang, Xiaojie Guo, and Dacheng Tao. 2022. Self-Augmented Unpaired Image Dehazing via Density and Depth Decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2037â€“2046.
38. Tian Ye, Sixiang Chen, Yun Liu, Yi Ye, Jinbin Bai, and Erkang Chen. 2022. Towards real-time high-definition image snow removal: Efficient pyramid network with asymmetrical encoder-decoder architecture. In Proceedings of the Asian Conference on Computer Vision. 366â€“381.
39. Tian Ye, Sixiang Chen, Yun Liu, Yi Ye, Erkang Chen, and Yuche Li. 2022. Underwater light field retention: Neural rendering for underwater imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 488â€“497.
40. Tian Ye, Yunchen Zhang, Mingchao Jiang, Liang Chen, Yun Liu, Sixiang Chen, and Erkang Chen. 2022. Perceiving and modeling density for image dehazing. In European Conference on Computer Vision. Springer, 130â€“145.
41. Hu Yu, Naishan Zheng, Man Zhou, Jie Huang, Zeyu Xiao, and Feng Zhao. 2022. Frequency and spatial dual guidance for image dehazing. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XIX. Springer, 181â€“198.
42. Teng Yu, Kang Song, Pu Miao, Guowei Yang, Huan Yang, and Chenglizhao Chen. 2019. Nighttime single image dehazing via pixel-wise alpha blending. IEEE Access 7 (2019), 114619â€“114630.
43. He Zhang and Vishal M Patel. 2018. Densely connected pyramid dehazing network. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3194â€“3203.
44. Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, and Chang Wen Chen. 2017. Fast haze removal for nighttime image using maximum reflectance prior. In Proceedings of the IEEE conference on computer vision and pattern recognition. 7418â€“7426.
45. Jing Zhang, Yang Cao, and Zengfu Wang. 2014. Nighttime haze removal based on a new imaging model. IEEE, 4557â€“4561.
46. Jing Zhang, Yang Cao, Zheng-Jun Zha, and Dacheng Tao. 2020. Nighttime dehazing with a synthetic benchmark. In Proceedings of the 28th ACM International Conference on Multimedia. 2355â€“2363.
47. Shiyu Zhao, Lin Zhang, Ying Shen, and Yicong Zhou. 2021. RefineDNet: A weakly supervised refinement framework for single image dehazing. IEEE Transactions on Image Processing 30 (2021), 3391â€“3404.
48. Zhuoran Zheng, Wenqi Ren, Xiaochun Cao, Xiaobin Hu, Tao Wang, Fenglong Song, and Xiuyi Jia. 2021. Ultra-high-definition image dehazing via multi-guided bilateral learning. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 16180â€“16189.
49. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In Computer Vision (ICCV), 2017 IEEE International Conference on.

# Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# Table 4: The comparison with dataset [95], for PSNR and SSIM, higher values indicate better performance.

We tested our method and Yan [84] on a GPU GTX 3090 using an image resolution of 512x512 to measure the runtime. Other numbers in the table are borrowed from [95].

|Type|Method|Venue|PSNRNHR â†‘|SSIM â†‘|NHM|PSNR â†‘|SSIM â†‘|NHC|PSNR â†‘|SSIM â†‘|Parameters|Time (s)|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|Opti.|Zhang NDIM [94]|ICIPâ€™14|14.31|0.53|14.58|0.56|11.12|0.29|-|-|5.63| |
|Opti.|Li GS [45]|ICCVâ€™15|17.32|0.63|16.84|0.69|18.84|0.55|-|-|22.52| |
|Opti.|Zhang FAST-MRP [93]|CVPRâ€™17|16.95|0.67|13.85|0.61|19.17|0.58|-|-|0.236| |
|Opti.|Zhang MRP [93]|CVPRâ€™17|19.93|0.78|17.74|0.71|23.02|0.69|-|-|1.769| |
|Opti.|Zhang OSFD [95]|MMâ€™20|21.32|0.80|19.75|0.76|23.10|0.74|-|-|0.576| |
|Opti.|Zhang NDNET [95]|MMâ€™20|28.74|0.95|21.55|0.91|26.12|0.85|-|-|0.0074| |
|Learning|Yan [84]|ECCVâ€™20|21.05|0.62|17.54|0.45|15.06|0.46|50M|-|0.97| |
|Learning|Ours|MMâ€™23|26.56|0.89|33.76|0.92|38.86|0.97|21M|-|1.20| |

# Figure 12:

We show with APSF, we can render glow ğ¼ğ‘” (bottom) on night clean ğ¼ğ‘ (top).

# Figure 13:

We show with APSF, we can render glow ğ¼ğ‘” (bottom) on night clean ğ¼ğ‘ (top).

# MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada

# Yeying Jin et al.

# Figure 14

We show with APSF, we can render glow ğ¼ğ‘” (bottom) on night clean ğ¼ğ‘ (top).

# BARE ELEGANCE

# Girls

# Figure 15

We show with APSF, we can render glow ğ¼ğ‘” (bottom) on night clean ğ¼ğ‘ (top).

2457

