# The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)

# 3D Human Pose Estimation Using Spatio-Temporal Networks with Explicit Occlusion Training

# Yu Cheng,1∗ Bo Yang,2* Bo Wang,2* Robby T. Tan1,3

1National University of Singapore, 2Tencent Game AI Research Center, 3Yale-NUS College

e0321276@u.nus.edu, {brandonyang, bohawkwang}@tencent.com, robby.tan@nus.edu.sg

# Abstract

Estimating 3D poses from a monocular video is still a challenging task, despite the significant progress that has been made in the recent years. Generally, the performance of existing methods drops when the target person is too small/large, or the motion is too fast/slow relative to the scale and speed of the training data. Moreover, to our knowledge, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional networks (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground truth data, we further utilize 2D video data to inject a semi-supervised learning capability to our network. Experiments on public data sets validate the effectiveness of our method, and our ablation studies show the strengths of our network’s individual submodules.

# Introduction

This paper focuses on 3D human pose estimation from a monocular RGB video. A 3D pose is defined as the 3D coordinates of pre-defined keypoints on humans, such as shoulder, pelvis, wrist, and etc. Recent top-down approaches (Hossain and Little 2018; Wandt and Rosenhahn 2019; Pavllo et al. 2019; Cheng et al. 2019) have shown promising results, where spatial features from individual frames are extracted to detect a target person and estimate the 2D poses, and temporal context is used to produce consistent 3D predictions.

Unlike most previous works (Newell, Yang, and Deng 2016; Pavllo et al. 2019) that only use the peaks in the heat maps, we encode these maps into a latent space to incorporate more spatial information. Then, we apply temporal convolutional networks (TCNs) (Pavllo et al. 2019) to these latent features with different strides, e.g., 1, 2, 4, and 8, and concatenate them together for prediction of the 3D poses. Figure 1 shows some examples of our results.

Moreover, to reduce the risk of invalid 3D poses, we also utilize a discriminator in our framework like many previous works.

∗Equal contribution

Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

# Input

# Heat Maps from Spatial

# Temporal Multi-Scale

# Training Stage Only

# Multi-Scale Features Training Stage Only

# Convolution

# KPTS

# Occlusion

# Embedding

# Augmentation

# TKCS

# KCS

# Pose

# Valid?

Figure 2: Illustration for our framework. We only show two different temporal strides for clarity purpose. KPTS is short for keypoints; KCS is Kinematic Chain Space; TKCS means Temporal KCS.

# Related Works

Within the last few years, pose estimation has been undergoing rapid development with deep learning techniques (Tompson et al. 2014; Toshev and Szegedy 2014; Newell, Yang, and Deng 2016; Cao et al. 2019; Mehta et al. 2017b). Researchers keep pushing the frontier of this field from different angles via better utilizing spatial or temporal information, learning human dynamics, pose regularization, and semi-supervised/self-supervised learning.

To better utilize spatial information, some recent works focused on cross stage feature aggregation or multi-scale spatial feature fusion to maintain the high resolution in the feature maps (Chen et al. 2018; Sun et al. 2019; Kanazawa et al. 2019). Although this helps to improve the 2D estimators, there is an inherent ambiguity for inferring 3D human structure from a single 2D image. To overcome this limitation, some researchers further utilized temporal information in video (Pavllo et al. 2019; Hossain and Little 2018; Cheng et al. 2019; Bertasius et al. 2019), and showed obvious improvement. However, their fixed temporal scales limit their performance on videos with different motion speeds from the ones in training.

However, our approach can handle both partial and total occlusion cases in individual frames or in a sequence of frames. Hence, our method is more general in handling human 3D pose estimation under occlusion. Moreover, the occlusion module allows us to do semi-supervised learning that utilizes both 3D and 2D datasets.

As a summary, our contributions are as follows:

- Incorporate multi-scale spatial and temporal features for robust pose estimation in video.
- Introduce a spatio-temporal discriminator to regularize the validity of a pose sequence.
- Perform diverse data augmentation for TCN to deal with different occlusion cases.

Experiments on public datasets show the efficacy of our contributions. To deal with partial occlusions, some techniques have been designed to recover occluded keypoints from unoccluded frames.

cluded ones according to the spatial or temporal context

Frame 60
Frame 70
Frame 80
(Radwan, Dhall, and Goecke 2013; Rogez, Weinzaepfel, and Schmid 2017; de Bem et al. 2018; Guo and Dai 2018; Cheng et al. 2019) or scene constraints (Zanﬁr, Mari- noiu, and Sminchisescu 2018; Zanﬁr et al. 2018). Some methods further introduced the concept of “human dynam- ics” (Kanazawa et al. 2019; Zhang et al. 2019), which pre- dicts future human poses according to single or multiple ex- isting frames in a video without any future frames. In real scenarios, we may have full, partial, or total occlusion for individual or continuous frames. Therefore, we introduce a method to integrate these two categories of methods into one uniﬁed framework by explicitly performing occlusion aug- mentation for all these cases during training. Due to lim- ited 3D human pose data, recent methods suggest to fur- ther utilize 2D human pose datasets in a semi-supervised or self-supervised fashion (Wandt and Rosenhahn 2019; Wang et al. 2019; Kocabas, Karagoz, and Akbas 2019; Chen et al. 2019). They project estimated 3D pose back to 2D image space so that 2D ground-truth can be used for loss computation. Such approaches reduce the risk of over-ﬁtting on small amount of 3D data. We also adopt this method and combine it with our explicit occlusion augmentation.

# Methodology

Our method belongs to the top-down pose estimation cat- egory. Given an input video, we ﬁrst detect and track the persons by any state-of-the-art detector and tracker, such as Mask R-CNN (He et al. 2017) and PoseFlow (Xiu et al. 2018). Subsequently, we perform the pose estimation for each person individually.

# Multi-Scale Features for Pose Estimation

Given a series of bounding boxes for a person in a video, we ﬁrst normalize the image within each bounding box to a pre-deﬁned ﬁxed size, e.g., 256 × 256, and then apply High Resolution Networks (HRNet) (Sun et al. 2019) to each nor- malized image patch to produce K heat maps, each of which indicates the possibility of certain human joint’s location. The HRNet conducts repeated multi-scale fusions by ex- changing the information across the parallel multi-scale sub- networks. Thus, the estimated heat maps incorporate spatial multi-scale features to provide more accurate 2D pose esti- mations.

We concatenate the K heat maps in each frame as a K- dimensional image mt, where t is the frame index, and ap- ply an embedding network fE to produce a low dimensional representation as rt = fE(mt). Such embedding incorpo- rates more spatial information from the whole heat maps than only using maps’ peaks as most previous works do. The effectiveness of the embedding is shown in the ablation study in Table 2.

# Spatio-Temporal KCS Pose Discriminator

To reduce the risk of generation of unreasonable 3D poses, we introduce a novel spatio-temporal discriminator to check the validity of a pose sequence, rather than just poses in individual frames like previous works (Yang et al. 2018; Wandt and Rosenhahn 2019; Chen et al. 2019).

Frame 81 Frame 100 Frame 134

Input Frame

Figure 4: Illustration for Temporal Kinematic Chain Space Augmentation (TKCS) between two neighboring bones.

with Occlusion

Among all single frame discriminators, the Kinematic Chain Space (KCS) used in (Wandt and Rosenhahn 2019) is one of the most effective methods. Each bone, defined as the connection between two neighboring human keypoints such as elbow and wrist, is represented as a 3D vector bm, indicating the direction from one keypoint to its neighbor. All such vectors form a 3 × M matrix B, where M is the predefined number of bones for a human structure. They use Ψ = BT B as the features for discriminator, where the diagonal elements in Ψ indicate the square of bone length and other elements represent the weighted angle between two bones as an inner production.

Data Augmentation for Occlusions

Inspired by their spatial KCS, we introduce a Temporal KCS (TKCS) defined as:

Φ = BT Bt+i − BTBt.

where i is the temporal interval between the KCS. The diagonal elements in Φ indicates the bone length changes, and other elements denote the change of angles between two bones. Figure 4 shows an example of two neighboring bones b1 and b2. The spatial KCS measures the lengths of b1 and b2 as well as angles between them, θ12. The temporal KCS measures the bone length changes between two frames with temporal interval i, i.e., differences between bt and bt+i as well as bt and bt+i and the angle change between neighboring bones, i.e., difference between θt and θt+i.

We concatenate the spatial KCS, temporal KCS, and the predicted keypoint coordinates, and then feed them to a TCN to build a discriminator. Such approach not only considers whether a pose is valid in individual frames, but also checks the validity of transitions across frames. We follow the procedure in the standard GAN to train the discriminator, and use it to produce a regularization loss for our predicted poses as Lgen.

In addition, to increase the robustness under different view angles, we introduce a rotational matrix as an augmentation to the generated 3D pose, as shown in the following equation:

L′ = Lgen(RX)

where R is a rotational matrix Rotation(α, β, γ), and α, β, γ are rotational angles along x, y, and z axis, respectively. As the rotational angles along x and z angles should be smaller compared with rotations along y for normal human poses, in our experiments, β is randomly sampled from [−π, π] while α and γ are sampled from [−0.2π,0.2π].

The overall loss function for our training is defined as:

L = L3d + w1Lmv + w2L2d + w3L′gen

Figure 5 demonstrates an example where occlusion augmentation helps to generate robust pose estimation results in a video clip where a target person is occluded.

# Experiments

|Emb|T Len|T Strides|T Intvl|P #1|P #2|
|---|---|---|---|---|---|
|64|64|1,2,3|1|58.3|44.2|
|128|64|1,2,3|1|46.7|36.1|
|256|64|1,2,3|1|43.1|33.8|
|512|64|1,2,3|1|42.6|33.4|
|1024|64|1,2,3|1|42.9|33.6|
|512|8|1,2,3|1|50.2|40.1|
|512|16|1,2,3|1|46.9|36.0|
|512|32|1,2,3|1|44.0|33.9|
|512|64|1,2,3|1|42.6|33.4|
|512|128|1,2,3|1|42.9|33.7|
|512|64|1|1|45.4|35.9|
|512|64|1,2|1|44.3|34.8|
|512|64|1,2,3|1|42.6|33.4|
|512|64|1,2,3,5|1|41.8|32.1|
|512|64|1,2,3,5,7|1|41.2|31.5|
|512|64|1,2,3|1|42.6|33.4|
|512|64|1,2,3|3|43.1|33.7|
|512|64|1,2,3|5|44.0|34.4|

Table 1: Parameter sensitivity test based on Protocol #1 and #2 of Human 3.6M dataset. Emb stands for embedding dimension, T Len stands for Temporal length, T Strides stands for temporal strides, T Intvl stands for the temporal interval for TKCS.

# Evaluation protocols

We apply a few common evaluation protocols in our experiments. Protocol #1 refers to the Mean Per Joint Position Error (MPJPE) which is the millimeters between the ground-truth and the predicted keypoints. Protocol #2, often called P-MPJPE, refers to the same error after applying alignment between the predicted keypoints and the ground-truth. Percentage of Correct 3D Keypoints (3D PCK) under 150mm radius is used for quantitative evaluation for MPI-INF-3DHP following (Mehta et al. 2017a). To compare with other human dynamics/pose forecasting methods, mean angle error (MAE) is used following (Jain et al. 2016).

# Hyper-Parameter Sensitivity Analysis

We conduct the sensitivity test of four hyper-parameters mentioned in this paper: embedding dimension for encoder, temporal length, temporal strides for TCN, and temporal interval for TKCS. The results are shown in Table 1. We find out that by adding more strides, the performance is improved and finally reaches 41.2mm with 5 strides compared to 45.4mm for single stride. We also test different temporal intervals for TKCS and observe interval 1 produces the best performance.

# Ablation Studies

We conduct ablation studies to analyze each component of the proposed framework as shown in Table 2. As the baseline, we build a TCN to regress the 3D keypoints’ positions based solely on their 2D coordinates (x, y), which are obtained from the peaks in heatmaps from 2D pose detector. During TCN training, the 3D skeletons are also rotated along x, y, z axes as mentioned before. We use the standard MSE loss for the training.

We then add the modules one-by-one to perform ablation studies, including heat maps embedding, multi-stride TCN, multi-view loss, spatial KCS, temporal KCS, and 2D data semi-supervised learning. We see that by adding more modules...

|Method|Protocol 1|Protocol 2|
|---|---|---|
|Base|51.7|40.5|
|+embedding|51.0|40.1|
|+multi-stride TCN|48.6|37.6|
|+Multi-view loss|47.3|36.9|
|+Spatial KCS|44.9|34.0|
|+Temporal KCS|41.2|31.5|
|+2D Data|40.1|30.7|

Table 2: Ablation study on Human3.6M dataset under Protocol #1 and #2. Best in bold.

|Method|Actions| | | | | | | | |Walk| |Avg| | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| |Direct|Disc.|Eat|Greet|Phone|Photo|Pose|Purch.|Sit|SitD.|Smoke|Wait|WalkD.|Walk|WalkT.| |
|Fang et al. AAAI (2018)|50.1|54.3|57.0|57.1|66.6|73.3|53.4|55.7|72.8|88.6|60.3|57.7|62.7|47.5|50.6|60.4|
|Yang et al. CVPR (2018)|51.5|58.9|50.4|57.0|62.1|65.4|49.8|52.7|69.2|85.2|57.4|58.4|43.6|60.1|47.7|58.6|
|Hossain & Little ECCV (2018)|44.2|46.7|52.3|49.3|59.9|59.4|47.5|46.2|59.9|65.6|55.8|50.4|52.3|43.5|45.1|51.9|
|Li et al. CVPR (2019)|43.8|48.6|49.1|49.8|57.6|61.5|45.9|48.3|62.0|73.4|54.8|50.6|56.0|43.4|45.5|52.7|
|Chen et al. CVPR (2019)|-|-|-|-|-|-|-|-|-|-|-|-|-|-|51.0| |
|Wandt et al. CVPR (2019) *|50.0|53.5|44.7|51.6|49.0|58.7|48.8|51.3|51.1|66.0|46.6|50.6|42.5|38.8|60.4|50.9|
|Pavllo et al. CVPR (2019)|45.2|46.7|43.3|45.6|48.1|55.1|44.6|44.3|57.3|65.8|47.1|44.0|49.0|32.8|33.9|46.8|
|Cheng et al. ICCV (2019)|38.3|41.3|46.1|40.1|41.6|51.9|41.8|40.9|51.5|58.4|42.2|44.6|41.7|33.7|30.1|42.9|
|Our result|36.2|38.1|42.7|35.9|38.2|45.7|36.8|42.0|45.9|51.3|41.8|41.5|43.8|33.1|28.6|40.1|

Table 3: Quantitative evaluation using MPJPE in millimeter between estimated pose and the ground-truth on Human3.6M under Protocol #1, no rigid alignment or transform applied in post-processing. Best in bold, second best underlined. * indicates ground-truth 2D labels are used.

|Method|Actions| | | | | | | | |Walk| |Avg| | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| |Direct|Disc.|Eat|Greet|Phone|Photo|Pose|Purch.|Sit|SitD.|Smoke|Wait|WalkD.|Walk|WalkT.| |
|Fang et al. AAAI (2018)|38.2|41.7|43.7|44.9|48.5|55.3|40.2|38.2|54.5|64.4|47.2|44.3|47.3|36.7|41.7|45.7|
|Yang et al. CVPR (2018)|26.9|30.9|36.3|39.9|43.9|47.4|28.8|29.4|36.9|58.4|41.5|30.5|29.5|42.5|32.2|37.7|
|Hossain & Little ECCV (2018)|36.9|37.9|42.8|40.3|46.8|46.7|37.7|36.5|48.9|52.6|45.6|39.6|43.5|35.2|38.5|42.0|
|Kocabas et al. CVPR (2019)|-|-|-|-|-|-|-|-|-|-|-|-|-|-|45.0| |
|Li et al. CVPR (2019)|35.5|39.8|41.3|42.3|46.0|48.9|36.9|37.3|51.0|60.6|44.9|40.2|44.1|33.1|36.9|42.6|
|Wandt et al. CVPR (2019) *|33.6|38.8|32.6|37.5|36.0|44.1|37.8|34.9|39.2|52.0|37.5|39.8|34.1|40.3|34.9|38.2|
|Pavllo et al. CVPR (2019)|34.1|36.1|34.4|37.2|36.4|42.2|34.4|33.6|45.0|52.5|37.4|33.8|37.8|25.6|27.3|36.5|
|Cheng et al. ICCV (2019)|28.7|30.3|35.1|31.6|30.2|36.8|31.5|29.3|41.3|45.9|33.1|34.0|31.4|26.1|27.8|32.8|
|Our result|26.2|28.1|31.1|28.4|28.5|32.9|29.7|31.0|34.6|40.2|32.4|32.8|33.1|26.0|26.1|30.7|

Table 4: Quantitative evaluation using P-MPJPE in millimeter between estimated pose and the ground-truth on Human3.6M under Protocol #2. Procrustes alignment to the ground-truth is used in post-processing. Best in bold, second best underlined. * indicates ground-truth 2D labels are used.

Quantitative Results

The experiment results on Human 3.6M are shown in Table 3 and Table 4 for Protocol #1 and #2, respectively. The MPJPE is reduced by 2.8mm compared to previous work and yields an error reduction of 6.5%. The P-MPJPE is reduced by 2.1mm and obtained 6.4% error reduction. The performance on actions which already have low error rates is not improved significantly, but for those actions such as photo capturing and sitting down, the errors are reduced by > 5mm. Since in these actions, occlusion happens frequently, more temporal information and effective pose regularization are needed for producing correct estimations. Considering existing methods almost get saturated on this dataset, our improvement is promising.

We also evaluate our model’s potential on human dynamics which is targeted to predict several future frames’ 3D skeleton. The performance is shown in Table 5. Note that, (Chiu et al. 2019) uses past 3D ground-truth keypoints as input for prediction, while our method does not use any ground-truth but takes images from video as input to estimate the keypoints first, and then predict the future 3D results, 157.0 and 80.1, reported in (Martinez et al. 2017; Kanazawa et al. 2019).

|Actions|Walking|Eating|Smoking|Discussion| | | | | | | | | | | | | | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|Milliseconds|80|160|320|560|1000|Milliseconds|80|160|320|560|1000|Milliseconds|80|160|320|560|1000|Milliseconds|80|160|320|560|1000|
|Ghosh et al. (2017)|1.00|1.11|1.39|1.55|1.39|1.31|1.49|1.86|1.76|2.01|0.92|1.03|1.15|1.38|1.77|1.11|1.20|1.38|1.53|1.73| | | |
|Martinez et al. (2017)|0.32|0.54|0.72|0.86|0.96|0.25|0.42|0.64|0.94|1.30|0.33|0.60|1.01|1.23|1.83|0.34|0.74|1.04|1.43|1.75| | | |
|Chiu et al. (2019)|0.25|0.41|0.58|0.74|0.77|0.20|0.33|0.53|0.84|1.14|0.26|0.48|0.88|0.98|1.66|0.30|0.66|0.98|1.39|1.74| | | |
|Our result|0.29|0.48|0.65|0.79|0.92|0.25|0.39|0.58|0.87|1.02|0.34|0.44|0.90|1.07|1.52|0.33|0.63|0.90|1.30|1.77| | | |

Table 5: Evaluation on Human3.6M dataset on human dynamics protocol. Mean angle error of predicted 3D poses after different time intervals is used following (Martinez, Black, and Romero 2017; Ghosh et al. 2017). The milliseconds is the set future time for checking the performance. Best in bold, second best underlined.

|Input Frames|Base Results|Multi-Scale Only Results|Our Final Results|
|---|---|---|---|
|Frame 54| | | |
|Frame 80| | | |
|Frame 88| | | |
|Frame 91| | | |
|Frame 532| | | |
|Frame 542| | | |
|Frame 550| | | |
|Frame 560| | | |

Figure 6: Examples of results from our whole framework compared with different baseline results. First row shows the images from two video clips; second row shows the 3D results that uses baseline approach described in Ablation Studies; third row shows the 3D results that uses multi-scale temporal features without occlusion augmentation and spatio-temporal KCS; last row shows the results of the whole framework. Wrong estimations are labeled in red circles.

|Method|Walking|Jogging|Avg|
|---|---|---|---|
|Pavlakos et al. (2018)*|18.8|12.7|29.2|
|Hossain et al. (2018)|19.1|13.6|43.9|
|Wang et al. (2019)|17.2|13.4|20.5|
|Pavllo et al. (2019)|13.4|10.2|27.2|
|Cheng et al. (2019)|11.7|10.1|22.8|
|Our result|10.6|11.8|19.3|

Table 6: Evaluation on HumanEva-I dataset under Protocol. Best in bold, second best underlined.

|Method|PCK|
|---|---|
|Mehta et al. 3DV (2017a)|72.5|
|Yang et al. CVPR (2018)|69.0|
|Chen et al. CVPR (2019)|71.1|
|Kocabas et al. CVPR (2019)|77.5|
|Wandt et al. CVPR (2019)|82.5|
|Our result|84.1|

Table 7: Evaluation on MPI-INF-3DHP dataset using 3D PCK. Best in bold, second best underlined. Only overlapped keypoints with Human3.6M are used for evaluation.

# Conclusion

In this paper, we present a new method based on three major components: multi-scale temporal features, spatio-temporal KCS pose discriminator, and occlusion data augmentation. Our method can deal with videos with various motion speeds and different types of occlusion. The effectiveness of each component is demonstrated through qualitative results.

component of our method is illustrated in the ablation studies. To compare with the state-of-the-art 3D pose estimation methods, we evaluate the proposed method on four public 3D human pose datasets with commonly used protocols and demonstrate our method’s superior performance. Comparison with the human dynamics methods is provided as well to show our method is versatile and potentially can be used for other pose tasks, like pose forecasting.

# References

Bertasius, G.; Feichtenhofer, C.; Tran, D.; Shi, J.; and Torresani, L. 2019. Learning temporal pose estimation from sparsely-labeled videos. In NIPS.

Cao, Z.; Hidalgo, G.; Simon, T.; Wei, S.-E.; and Sheikh, Y. 2019. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE TPAMI.

Chen, Y.; Wang, Z.; Peng, Y.; Zhang, Z.; Yu, G.; and Sun, J. 2018. Cascaded pyramid network for multi-person pose estimation. In CVPR.

Chen, C.-H.; Tyagi, A.; Agrawal, A.; Drover, D.; MV, R.; Stojanov, S.; and Rehg, J. M. 2019. Unsupervised 3d pose estimation with geometric self-supervision. In CVPR.

Cheng, Y.; Yang, B.; Wang, B.; Yan, W.; and Tan, R. T. 2019. Occlusion-aware networks for 3d human pose estimation in video. In ICCV.

Chiu, H.-k.; Adeli, E.; Wang, B.; Huang, D.-A.; and Niebles, J. C. 2019. Action-agnostic human pose forecasting. In WACV.

de Bem, R.; Arnab, A.; Golodetz, S.; Sapienza, M.; and Torr, P. 2018. Deep fully-connected part-based models for human pose estimation. In ACML, 327–342.

Fang, H.-S.; Xu, Y.; Wang, W.; Liu, X.; and Zhu, S.-C. 2018. Learning pose grammar to encode human body configuration for 3d pose estimation. In AAAI.

Ghosh, P.; Song, J.; Aksan, E.; and Hilliges, O. 2017. Learning human motion models for long-term predictions. In3DV, 458–466.

Guo, X., and Dai, Y. 2018. Occluded joints recovery in 3d human pose estimation based on distance matrix. In ICPR, 1325–1330.

He, K.; Gkioxari, G.; Dollár, P.; and Girshick, R. 2017. Mask r-cnn. In ICCV, 2961–2969.

Hossain, M. R. I., and Little, J. J. 2018. Exploiting temporal information for 3d human pose estimation. In ECCV, 69–86. Springer.

Ionescu, C.; Papava, D.; Olaru, V.; and Sminchisescu, C. 2014. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE TPAMI 36(7):1325–1339.

Jain, A.; Zamir, A. R.; Savarese, S.; and Saxena, A. 2016. Structural-rnn: Deep learning on spatio-temporal graphs. In CVPR.

Kanazawa, A.; Black, M. J.; Jacobs, D. W.; and Malik, J. 2018. End-to-end recovery of human shape and pose. In CVPR.

Kanazawa, A.; Zhang, J. Y.; Felsen, P.; and Malik, J. 2019. Learning 3d human dynamics from video. In CVPR.

Kocabas, M.; Karagoz, S.; and Akbas, E. 2019. Self-supervised learning of 3d human pose using multi-view geometry. In CVPR.

Li, C., and Hee Lee, G. 2019. Generating multiple hypotheses for 3d human pose estimation with mixture density network. In CVPR.

Martinez, J.; Hossain, R.; Romero, J.; and Little, J. J. 2017. A simple yet effective baseline for 3d human pose estimation. In ICCV.

Martinez, J.; Black, M. J.; and Romero, J. 2017. On human motion prediction using recurrent neural networks. In CVPR, 2891–2900.

Mehta, D.; Rhodin, H.; Casas, D.; Fua, P.; Sotnychenko, O.; Xu, W.; and Theobalt, C. 2017a. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 3DV.

Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.; Shafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; and Theobalt, C. 2017b. Vnect: Real-time 3d human pose estimation with a single rgb camera. ACM TOG 36(4):44.

Newell, A.; Yang, K.; and Deng, J. 2016. Stacked hourglass networks for human pose estimation. In ECCV, 483–499. Springer.

Pavlakos, G.; Zhou, X.; and Daniilidis, K. 2018. Ordinal depth supervision for 3d human pose estimation. In CVPR, 7307–7316.

Pavllo, D.; Feichtenhofer, C.; Grangier, D.; and Auli, M. 2019. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In CVPR.

Radwan, I.; Dhall, A.; and Goecke, R. 2013. Monocular image 3d human pose estimation under self-occlusion. In ICCV, 1888–1895.

Rogez, G.; Weinzaepfel, P.; and Schmid, C. 2017. Lcr-net: Localization-classification-regression for human pose. In CVPR.

Sun, K.; Xiao, B.; Liu, D.; and Wang, J. 2019. Deep high-resolution representation learning for human pose estimation. In CVPR.

Tompson, J. J.; Jain, A.; LeCun, Y.; and Bregler, C. 2014. Joint training of a convolutional network and a graphical model for human pose estimation. In NIPS, 1799–1807.

Toshev, A., and Szegedy, C. 2014. Deeppose: Human pose estimation via deep neural networks. In CVPR, 1653–1660.

von Marcard, T.; Henschel, R.; Black, M. J.; Rosenhahn, B.; and Pons-Moll, G. 2018. Recovering accurate 3D human pose in the wild using IMUs and a moving camera. In European Conference on Computer Vision (ECCV), 614–631.

Wandt, B., and Rosenhahn, B. 2019. Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation. In CVPR.

Wang, K.; Lin, L.; Jiang, C.; Qian, C.; and Wei, P. 2019. 3d human pose machines with self-supervised learning. IEEE TPAMI.

Xiu, Y.; Li, J.; Wang, H.; Fang, Y.; and Lu, C. 2018. Pose Flow: Efficient online pose tracking. In BMVC.

Yang, W.; Ouyang, W.; Wang, X.; Ren, J.; Li, H.; and Wang, X. 2018. 3d human pose estimation in the wild by adversarial learning. In CVPR, 5255–5264.

Zanfir, A.; Marinoiu, E.; Zanfir, M.; Popa, A.-I.; and Sminchisescu, C. 2018. Deep network for the integrated 3d sensing of multiple people in natural images. In NIPS, 8410–8419.

Zanfir, A.; Marinoiu, E.; and Sminchisescu, C. 2018. Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints. In CVPR, 2148–2157.

Zhang, J. Y.; Felsen, P.; Kanazawa, A.; and Malik, J. 2019. Predicting 3d human dynamics from video. In ICCV.

Zhang, W.; Zhu, M.; and Derpanis, K. G. 2013. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2248–2255.

