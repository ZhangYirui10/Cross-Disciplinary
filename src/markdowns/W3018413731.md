# Estimating geographic subjective well-being from Twitter: A comparison of dictionary and data-driven language methods

Kokil Jaidkaa,b,1, Salvatore Giorgiᶜ, H. Andrew Schwartzᵈ, Margaret L. Kernᵉ, Lyle H. Ungarᶜ, and Johannes C. Eichstaedtf,g,1

aDepartment of Communications and New Media, National University of Singapore, Singapore 117416; bCentre for Trusted Internet and Community, National University of Singapore, Singapore 117416; cDepartment of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104; dDepartment of Computer Science, Stony Brook University, Stony Brook, NY 11794; eMelbourne Graduate School of Education, The University of Melbourne, Parkville, VIC 3010, Australia; fDepartment of Psychology, Stanford University, Stanford, CA 94305; and gInstitute for Human-Centered Artificial Intelligence, Stanford University, Stanford, CA 94305

Edited by Tyler J. VanderWeele, Harvard T. H. Chan School of Public Health, Boston, MA, and accepted by Editorial Board Member Kenneth W. Wachter March 5, 2020 (received for review April 15, 2019)

Researchers and policy makers worldwide are interested in measuring the subjective well-being of populations. When users post on social media, they leave behind digital traces that reflect their thoughts and feelings. Aggregation of such digital traces may make it possible to monitor well-being at large scale. However, social media-based methods need to be robust to regional effects if they are to produce reliable estimates. Using a sample of 1.53 billion geotagged English tweets, we provide a systematic evaluation of word-level and data-driven methods for text analysis for generating well-being estimates for 1,208 US counties. We compared Twitter-based county-level estimates with well-being measurements provided by the Gallup-Sharecare Well-Being Index survey through 1.73 million phone surveys. We find that word-level methods (e.g., Linguistic Inquiry and Word Count [LIWC] 2015 and Language Assessment by Mechanical Turk [LabMT]) yielded inconsistent county-level well-being measurements due to regional, cultural, and socioeconomic differences in language use. However, removing as few as three of the most frequent words led to notable improvements in well-being prediction. Data-driven methods provided robust estimates, approximating the Gallup data at up to r = 0.64. We show that the findings generalized to county socioeconomic and health outcomes and were robust when poststratifying the samples to be more representative of the general US population. Regional well-being estimation from social media survey measures. On the other hand, we find that standard English word-level methods (such as Linguistic Inquiry and Word Count 2015’s Positive emotion dictionary and Language Assessment by Mechanical Turk) can yield estimates of county well-being inversely correlated with survey estimates, due to regional cultural and socioeconomic differences in language use. Some of the most frequent misleading words can be removed to improve the accuracy of these word-level methods.

Twitter | subjective well-being | language analysis | big data | machine learning

Many governments worldwide are incorporating subjective well-being measures as indicators of progress and success (1, 2) to complement traditional objective and economic metrics. Subjective well-being spans cognitive (i.e., life satisfaction), affective (positive and negative emotion), and eudaimonic dimensions (such as a sense of meaning and purpose) (3); most metrics are based on self-report surveys and interviews of individuals, which might be collected annually and aggregated to represent the well-being of regions or nations. Such metrics are time and resource intensive to gather, and there is a growing interest in identifying efficient methods to garner subjective well-being information (4). Concurrently, social and information exchange has increasingly migrated to digital contexts, including social media platforms. Through language posted online, people leave behind psychological traces that can be mined to address real-world problems. The public nature of Twitter offers a way to augment the theory and practice of psychology and medicine with large-scale data collection. For example, researchers have used Twitter.

Author contributions: K.J., H.A.S., L.H.U., and J.C.E. designed research; K.J., S.G., H.A.S., and J.C.E. performed research; K.J., S.G., H.A.S., and J.C.E. contributed new reagents/analytic tools; K.J., S.G., H.A.S., and J.C.E. analyzed data; and K.J., S.G., H.A.S., and J.C.E. wrote the paper.

The authors declare no competing interest.

This article is a PNAS Direct Submission. T.J.V. is a guest editor invited by the Editorial Board.

This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).

Data deposition: The data and materials for this paper have been made publicly available via the Open Science Framework (OSF) and can be accessed at https://osf.io/jqk6f. County language estimates are available in the World Well-Being Project (WWBP) GitHub repository (https://github.com/wwbp/county_tweet_lexical_bank).

1To whom correspondence should be addressed. Email: jaidka@nus.edu.sg or johannes.stanford@gmail.com.

This article contains supporting information online at https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1906364117/-/DCSupplemental.

First published April 27, 2020.

|Type|Method (source)|No. of features|Categories| |
|---|---|---|---|---|
|Word-level methods|LIWC 2015 (13)|1,364|Positive emotion, negative emotion, anxiety, anger, sadness| |
| |PERMA dictionary (14, 15)|402|Positive emotion, negative emotion| |
| |ANEW (16)|1,034|Valence| |
| |LabMTi (17)|10,218|Valence| |
|Data-driven methods|WWBP affect (18)|7,265|Affect| |
| |Swiss Chocolate (19)|7,168|Positive, neutral and negative emotion| |
| |WWBP life satisfaction (this study)|2,000|Cantril Ladder score| |
| |County life satisfaction (this study)|2,000|Cantril Ladder score| |

appearing in the dictionary. For example, Golder and Macy (20) cated our analyses on county-level health and socioeconomic applied the LIWC (2007) dictionaries to Twitter posts to track outcomes to show that the observed patterns generalize beyond longitudinal variation in affect. Other word-level methods, such self-reported well-being metrics. To account for sample differ- as the Language Assessment by Mechanical Turk (LabMT) word ences, we replicated the primary analysis after poststratifying the list (21) and the Affective Norms of English Words (ANEW) Gallup and Twitter samples to match census demographics in (16), ask raters to annotate words for their valence. For example, age, gender, education, and income. Across a subset of 373 coun- LabMT provides the average rater-determined valence (between ties, we examined the stability of the findings across time. To “sad” and “happy”) for the 10,000 most frequent words in the investigate the impact of ecological aggregation, we ran parallel English language. These crowdsourced ratings have been applied analyses across a sample of 2,321 Facebook users. In addition, we to geotagged Twitter language to estimate the mood of US states conducted a post hoc diagnosis to identify and suggest a solution and urban and metropolitan statistical areas (10).

Data-driven methods involve the use of machine learning Evaluation of Twitter-Based Estimates to identify associations between the linguistic information con- tained in the text and its emotional content. The emotional Table 2 summarizes the convergent validity from the differ- content of sentences or documents (rather than words in iso- ent methods against the Gallup county estimates. Unexpectedly, lation) is determined by annotation or based on a self-report among the word-level methods, higher positive emotion/valence survey. Natural language processing methods are used to extract estimated from LIWC 2015, ANEW, and LabMT* correlated language features, which are then used to predict emotional with lower subjective well-being. For example, both LIWC’s pos- content using supervised machine learning. itive emotion dictionary and LabMT correlated negatively (r = −0.21 and r = −0.27, P values < 0.001) with life satisfaction—

being? Previous results with word-level methods are inconsistent the most widely used measure of subjective well-being. Similarly, (22, 23). At the regional level, LabMT’s state-level happiness they correlated negatively with happiness and positively with estimates show inconsistent associations with life satisfaction sadness. The PERMA positive emotion dictionary (14, 15, 30) reported by the Centers for Disease Control and Prevention is limited to more unambiguous words and correlated with (CDC) (10), and at the city level, LabMT’s estimates of happi- subjective well-being in the expected direction.† (PERMA is ness were negatively correlated with measures of physical health Seligman’s construct of well-being, an acronym for positive emo- (24). The unexpected findings may arise from how people use tion, engagement, relationships, meaning, and accomplishment.) language and differ in their use of social media; alternatively, The LIWC and PERMA negative emotion dictionaries they could be an artifact of the demographic and geographic showed the expected pattern of correlations. Throughout word- effects of aggregating the language of individuals to represent level and data-driven methods, negative emotion estimates geographies. On the other hand, data-driven methods, which showed larger and more consistent correlations than their pos- train machine learning models on large corpora and then apply itive counterparts, suggesting that they more consistently cap- those models to other contexts, have been shown to offer per- tured the absence of well-being on Twitter than its presence. formance improvements over word-based methods for predictive None of the methods predicted worry well, which demonstrated problems (25–27).

In the current study, we compare methods for regional esti- In contrast to the word-level methods, the data-driven meth- mates of subjective well-being from social media language ods consistently produced estimates that correlated with the against survey-based ground truth measures of county-level eval- Gallup measures in the expected directions, with positive lan- uative and hedonic well-being (excluding eudaimonic aspects). guage scores predicting higher life satisfaction and happiness and We use over a billion geolocated tweets from 2009 to 2015 (28), lower worry and sadness. Data-driven methods thus appear more from which we extracted language features, normalized their robust than the word-level methods. Among the data-driven frequency distributions, and aggregated them to yield county- methods, the state-of-the-art sentiment model Swiss Chocolate level language estimates. From these, we extracted emotion/life (19) matched or outperformed the World Well-Being Project satisfaction estimates (Table 1).

We aggregated 1.73 million responses to the Gallup-Sharecare Well-Being Index from 2009 to 2015 to obtain county-level mea- sures of life satisfaction, happiness, worry, and sadness. In the *Following ref. 17, we removed “neutral” words with 4 < valence < 6, leaving 3,731 primary analysis, we determined the convergent validity between words. the language-based methods and the Gallup county-level out- †SI Appendix, Table S16 has details on the approaches, and SI Appendix, Table S3 has comes using an open-source Python codebase (29). We repli- extended results covering additional word- and sentence-level methods.

# Table 2.

Pearson correlations (r) between Twitter-based emotions and Gallup-Sharecare Well-Being Index estimates across 1,208 US counties

|N = 1,208 U.S. counties| |Word-level| |Data-driven| | | | | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| |LIWC 2015|PERMA|ANEW|LabMT|WWBP|Swiss Chocolate|Sentence-level|Person-level|Affect|Positive|Negative|WWBP|Direct Life Sat. prediction| |
|Life Satisfaction|-0.21|-0.06|-0.32|0.22|-0.37|-0.03|0.15|-0.27|0.01|0.29|0.24|-0.29|0.39|0.62|
|Happiness|-0.13|0.13|-0.27|0.27|-0.17|0.04|0.18|-0.07|0.16|0.23|0.24|-0.30|0.23|0.51|
|Worry|0.11|0.01|0.03|-0.01|0.02|0.03|-0.05|0.02|-0.04|0.00|-0.02|0.11|-0.03|0.52|
|Sadness|0.25|-0.01|0.22|-0.19|0.18|0.09|-0.10|0.19|-0.09|-0.18|-0.20|0.33|-0.23|0.64|

The gray column headers identify the modified LIWC (removed 3 words), LabMT (removed 15 words), and ANEW (removed 2 words) dictionaries (in the text).

The color indicates the direction and magnitude of correlation; white cells are nonsignificant, and all others are P < 0.05 corrected for multiple comparisons.

model that we trained in this study. Direct prediction, also compared counties with counties within the same states and regions. The pattern of correlations was unchanged. Up until this point, these findings suggested that the language-based well-being estimates are not merely attributable to demographic or state-by-state differences in unobserved variables. Finally, when we controlled for income and education, it largely reduced most language associations. This is likely because socioeconomic status was strongly associated with our dependent variable, subjective well-being (e.g., life satisfaction correlated r = 0.59 with an income/education index). We infer that the variance in the word-level methods overlaps with socioeconomic variance in language use. Some of the data-driven methods captured some variance in Gallup happiness over and above socioeconomic status.

Stability of Results over Time. We examined whether our findings were robust to the evolving use of Twitter and well-being trends over time. We repeated our analyses across two shorter windows of time (from 2012 to 2013 and from 2015 to 2016) across a smaller sample of 373 counties for which sufficient Gallup and Twitter data were available. The pattern of results was largely consistent with Table 2. We also evaluated how well models built on 2012 to 2013 Twitter language predicted 2015 to 2016 well-being, finding only a small reduction in performance.

Comparison with Individual-Level Language Analyses. To shed light on the ecological effects of community-level aggregation, we carried out an analogous comparison of language methods at the individual-level across a sample of 2,321 Facebook users who had answered the same survey questions as the Gallup sample. The associations of the LIWC 2015 positive emotion dictionary with well-being were weakly positive (r = 0.04, P = 0.050), which aligned with previous findings with LIWC 2007. In general, all but LabMT showed weak associations in the expected direction at the individual level. The data-driven methods again produced the expected pattern of correlations, albeit with reduced magnitudes compared with the county level (r values < 0.25).

Controlling for Demographic and Socioeconomic Confounds. In order to control for endogenous differences, we added sociodemographic covariates for age, gender, and race when evaluating the language models (SI Appendix, Table S10). The resulting pattern of coefficients showed small differences in magnitude when compared with the main results in Table 2. As a stronger test, we entered dummy variables for US states and regions into the regression equations to adjust for unobserved endogenous variables at the state or regional level. Thereby, we only conducted a set of post hoc diagnostic analyses, which suggested that the main sources of error in these word-level methods were.

SI Appendix, Table S5 has the detailed results.

SI Appendix, Table S6 has a general overview of the response biases.

SI Appendix, Table S18 has details.

Details on the model accuracies are in SI Appendix, Table S7.

Additional information is in SI Appendix, Table S11.

The poststratification process is validated in SI Appendix, Table S8.

SI Appendix, Table S13 has the full results.

Downloaded from https://www.pnas.org by 119.234.32.97 on March 10, 2025 from IP address 119.234.32.97.

# Correlates positively with happiness

# Correlates negatively with happiness

|yayy|fabulous|succesgating|lovethunh' true relaxin|
|---|---|---|---|
|"fungreatlepitha|LIWC|thanks|excitingvalues|
|loyalty|sweetheart|trust_freestyle|favorite|
|opportunity|okl|funnyrespect|Positive|
|excitedwinner|lol|promise,|ensupeiinteresting}|
|delciostic|Imao'|Emotion|ngkanddel|
|certaini hope|*|bless|excellentawesomet|
|award|Correlation|luv 'Toyallmfao|faithful|
|crappy|risks|Frequency|enemiesfuck|
|risKeirdtrupukgross|painfulbrutal|h1o: 109|haters|
|lyingfake|108|shitboredarguing,|LIWC|
|critical|dumpingdominated|hungover|bitclicrazyg"|
|'Wrong ,|Negative|defense|dangerousfrustrating"|
|hurting|bitchesmad|Emotion|dominalominatingtacks,|
|dargkarpuksing|hatebrokenastes|hating||avoid|
|rager| | |~smh|
|bad| | |fight|
| | | |mess|

Fig. 1. Sources of error in the LIWC positive and negative emotion dictionaries. The matrix illustrates the 25 most frequent words from the two dictionaries that were correlated as expected (green indicates true LIWC positives and true negatives) or opposite to expectation (red indicates false positives and false negatives) with the Gallup happiness item. The size of the word denotes the magnitude of its correlation (0.06 < r < 0.34; P < 0.05 corrected for multiple comparisons). The shade indicates the normalized frequency, with darker shades reflecting higher frequencies relative to other words.

due to a few highly frequent words and geographic and cultural variation in language use.

# Mapping False Positive Emotion Words

Fig. 2 illustrates the relative frequency of false LIWC positive emotion words (as in Fig 1, they were the positive emotion words that falsely had a negative correlation with Gallup happiness). The map suggests a geocultural divide: false LIWC positive emotion words were used more frequently in the South and the Southeast, which roughly corresponds with the Mason–Dixon Line. We infer that our Twitter-based LIWC positive emotion measurements captured how different regions of the United States use these words differently. Furthermore, these usage differences overlapped with the negatively correlated with happiness than the true positive socioeconomic gradients across the United States in ways that words. They comprise words that may have been synchronously produced the unexpected negative correlations with well-being. used on social media as markers of flirting, amusement, irony, sarcasm, interjections, and empathy (e.g., “lol,” “lmao,” and “lmfao”) (32). The more the highly frequent word “love” was mentioned, the lower the counties’ well-being [also observed in Eichstaedt et al. (8)] (compare with SI Appendix, Table S5).

# Context Effects

The LIWC positive emotion dictionary captures a heterogeneity of language use. To better understand it, we considered how many of the words contained in the LIWC positive emotion dictionary are also included in other LIWC dictionaries capturing different concepts (the overlapping dictionary words accounted for 1.1% [religion] to 26.6% [netspeak] of positive emotion word occurrences) (Table 4 and SI Appendix, Table S15).

This demonstrates that even a dictionary intended to measure a single construct (such as positive emotion or valence) may inadvertently aggregate over different types of language use and speech acts—which themselves may differ substantially in their geographic association with well-being and income. In the context of Fig. 2, we can infer that language related to “work” and professions was indicative of higher income in the North (34), thus explaining correlations of r = 0.33 (P < 0.001) with county-level life satisfaction and r = 0.57 (P < 0.001) with socioeconomic status (income and education).

# Discussion

We found these few highly frequent words to have negative correlations with both well-being and income (SI Appendix, Fig. S3). The psychological signal left behind in digital traces on social media makes it possible to unobtrusively monitor the well-being of regions (US counties in this case). Language analysis is the most widespread method to derive emotion or well-being estimates from such data. This study demonstrates that Twitter language can be used to measure the well-being of large populations if robust data-driven methods are used, which seem to circumvent errors associated with word-level methods. We found that removing them uniformly improved convergence with Gallup measures (gray columns in Table 2). For example, the modifications improved LIWC’s prediction of happiness from r = −0.13 to 0.13 and LabMT’s from r = −0.07 to 0.16.

More details are in SI Appendix, Fig. S3B and the discussion of SI Appendix, Fig. S4.

Here, we consider words with a LabMT valence more than six as positive following ref. 17.

The border between the Civil War North and South.

SI Appendix, SI Text, Fig. S3, and Table S14 has more details.

Additional information is in SI Appendix, Fig. S3B and Table S10.

Downloaded from https://www.pnas.org by 119.234.32.97 on March 10, 2025 from IP address 119.234.32.97.

# Fig. 2.

The relative frequency of false LIWC positive emotion words across the United States. States with a darker shade of red had relatively higher numbers of positive emotion words that correlated negatively with county Gallup happiness (Fig. 1, Upper Right) at P &lt; 0.05, controlling for multiple comparisons.

that data-driven well-being estimates also predicted US county shape the associations between world-level estimates, well-being, economic and health outcomes. They were largely unchanged when correcting for sample biases through poststratification, when including demographic covariates, or when comparing only counties to counties within states. We found that the pattern of correlations with county Gallup estimates was stable over time.

Regarding the choice of language analysis method, our study had three main findings.

# 1. First

word-level methods for subjective well-being measurement should be used with caution. One of the primary difficulties in estimating psychological states for geographies using social media arises from applying methods designed to measure the emotion of sentences of individuals to the language of regional populations. The language of regions differs culturally, such as the South using more religious language. When these cultural differences interact with socioeconomic gradients, these differences may invert the expected relationship between word-level estimates and well-being and health outcomes.

# 2. Second

most of the discrepancies observed for word-level methods seem to be driven by the use of a few frequent words (such as lol, love, and good). Stylistic markers such as lol can be used to convey a variety of emotions; they may also symbolize meanings that are specific to cultures and communities. Removing these words from LIWC, ANEW, and LabMT dictionaries reduced the negative associations with Gallup happiness and thus, improved the convergence with survey-reported county-level well-being.

# 3. Third

data-driven methods can capture the socioeconomic variance present in the samples on which they were trained. At times, these language associations deviate from the apparent valence of words outside their socioeconomic context. For example, individuals with higher socioeconomic status and well-being more frequently mention “taxes” and “penalty”—while negatively valenced for individuals, these are markers of relative prosperity at the county level. Similarly, “mortgages” are valid geographical estimates of well-being.

These models offer opportunities to augment other methods of spatial estimation by providing estimates with higher temporal resolution than annual surveys and by providing estimates for regions that are insufficiently covered by other sampling methods.

# Table 3.

Pearson correlations (r) between Facebook-based emotions and survey responses across 2,321 Facebook users

|N = 2,321|Word-level|Facebook users|LIWC 2015|PERMA|ANEW| |LabMT| | | |
|---|---|---|---|---|---|---|---|---|---|---|
| |Life Satisfaction|.04|(modified)| | |(modified)| | | | |
| |Happiness|.04|.07|-.26|.14|-.21|.09|.12|-.02|.00|
| |Worry|.07|.06|-.21|.11|-.18|.08|.09|.00|.00|
| |Sadness|.04|.01|.15|-.04|.14|.01|-.02|.07|.03|

The color indicates direction and magnitude of correlation; white cells are nonsignificant, and all others are P &lt; 0.05 corrected for multiple comparisons.

†††Additional information is in SI Appendix, Table S3C.

Jaidka et al. PNAS | May 12, 2020 | vol. 117 | no. 19 | 10169

PSYCHOLOGICAL AND COMPUTER SCIENCES

COGNITIVE SCIENCES

|Personal Concerns|Most frequent positive emotion words| | | | | |
|---|---|---|---|---|---|---|
| |Religion|Leisure|Work|Netspeak|Social| |
|bless*, faith*, heaven*, worship*, paradise*|play, fun, party*, playing, joke*|champ*, award*, success, challeng*|lol, haha*, lmao*, party*, trust*|love, welcom*, credit*, giving*| | |
|Life Satisfaction|-.11|.15|.33|-.13|-.32| |
|Happiness|-.12|.15|.23|-.25|-.17| |
|Worry|.08|-.04|-.05|.10|.12| |
|Sadness|.27|-.21|-.30|.23|.32| |
|Socioeconomic Index|-.33|.26|.57|-.33|-.50| |
|All Cause Mortality|.49|-.22|-.48|.30|.38| |
|Fair/Poor Health|.43|-.25|-.44|.42|.37| |
|Mentally Unhealthy Days|.24|-.13|-.23|.15|.25| |

Color indicates direction and magnitude of correlation; white cells are nonsignificant, and all others are P &lt; 0.05 corrected for multiple comparisons indicative of homeownership and socioeconomic status (37).

The findings reported in this paper are correlational and do not intend to make causal claims. They provide a snapshot of community health and well-being correlates, but as internet language evolves (32, 44, 45), the correlations between social media language features and well-being are likely to change over time. Although the data-driven methods in this paper, such as the WWBP affect model and the WWBP life satisfaction model, were trained on Facebook posts and then applied to Twitter, we do not expect this to have substantially affected their performance when applied to the county level (46, 47).‡ ‡ ‡

# Materials and Methods

Full methods are in SI Appendix.

# County Twitter Data

We used the County Tweet Lexical Bank from ref. 28, which comprises language estimates of US counties and corresponds in time to the Gallup well-being dataset. §§§

Gallup-Sharecare Well-Being Index. We included 1,208 counties that had at least 300 Gallup respondents and sufficient Twitter language. To facilitate secondary poststratification analyses, we limited the sample to respondents for whom age, gender, income, and education were available before aggregating the well-being estimates to the county-level, which reduced the sample by 1.6%. In total, we aggregated 1,727,158 Gallup survey responses. ¶¶¶

# Individual-Level Data

We recruited adults in the United States via Qualtrics for a well-being survey, which included the same well-being items as used by Gallup; 2,321 individuals consented to share their Facebook data and had posted at least 100 posts on Facebook. Emotion measurements based on word-level and data-driven methods were obtained and compared against self-reported well-being. This study was approved by the Institutional Review Board at the University of Pennsylvania. ###

# Data Availability

The Gallup-Sharecare Well-Being Index data are available by institutional subscription. County language estimates are available in the WWBP GitHub repository (https://github.com/wwbp/county tweet lexical bank) (48). Replication code and the WWBP life satisfaction model are contained in the Open Science Framework archive (https://osf.io/jqk6f/) (49).

# ACKNOWLEDGMENTS

We thank T.J.V., the PNAS editorial staff, the anonymous reviewers, and James W. Pennebaker for their generous and insightful suggestions. Support for this research was provided by a Nanyang Presidential Postdoctoral Award, an Adobe Research Award, a Robert Wood Johnson Foundation Pioneer Award, and Templeton Religion Trust Grant TRT0048.

Additional information is in SI Appendix, Supervised Person-Level Methods and §§§ Table S2.

SI Appendix and ref. 28 have further details on the language data extraction process.

¶¶¶ SI Appendix, Fig. S1 shows the inclusion criteria.

### Dataset statistics are provided in SI Appendix, Tables S1A and S12.

Downloaded from https://www.pnas.org by 119.234.32.97 on March 10, 2025 from IP address 119.234.32.97.

# References

1. H. Andrew Schwartz et al., “Characterizing geographic variation in well-being using tweets” in *Seventh International AAAI Conference on Weblogs and Social Media*, L. Specia, M. Post, M. Paul, Eds. (Association for Computational Linguistics, Copenhagen, Denmark, 2017), pp. 55–60.
2. E. Kiciman, N. B. Ellison, B. Hogan, P. Resnick, I. Soboroff, Eds. (Association for the Advancement of Artificial Intelligence, Cambridge, MA, 2013), pp. 583–591.
3. D. Quercia, D. O. Seaghdha, J. Crowcroft, “Talk of the city: Our tweets, our community happiness” in *Proceedings of the Sixth AAAI International Conference on Weblogs and Social Media*, J. Breslin, N. B. Ellison, J. G. Shanahan, Z. Tufekci, Eds. (Association for the Advancement of Artificial Intelligence, Dublin, Ireland, 2012), pp. 555–558.
4. J. W. Pennebaker, R. L. Boyd, K. Jordan, K. Blackburn, “The development and psychometric properties of LIWC2015” (University of Texas at Austin, Austin, TX, 2015).
5. M. E. Seligman, *Flourish: A Visionary New Understanding of Happiness and Well-Being* (Simon and Schuster, 2012).
6. H. Andrew Schwartz et al., “Choosing the right words: Characterizing and reducing error of the word count approach” in *Second Joint Conference on Lexical and Computational Semantics (* SEM)*, Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, M. Diab, T. Baldwin, M. Baroni, Eds. (Association of Computational Linguistics, Atlanta, GA, 2013), vol. 1, pp. 296–305.
7. M. M. Bradley, P. J. Lang, “Affective Norms for English Words (ANEW): Instruction manual and affective ratings” (Tech. Rep.C-1, The Center for Research in Psychophysiology, University of Florida, Gainesville, FL, 1999).
8. P. S. Dodds, K. D. Harris, I. M. Kloumann, C. A. Bliss, C. M. Danforth, Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter. *PloS One* 6, e26752 (2011).
9. D. Preoţiuc-Pietro et al., “Modelling valence and arousal in Facebook posts” in *Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis*, A. Balahur, E. van der Goot, P. Vossen, A. Montoyo, Eds. (Association for Computational Linguistics, San Diego, CA, 2016), pp. 9–15.
10. M. Jaggi, F. Uzdilli, M. Cieliebak, “Swiss-chocolate: Sentiment detection using sparse SVMs and part-of-speech n-grams” in *Proceedings of the 8th International Workshop on Semantic Evaluation SemEval 2014*, P. Nakov, T. Zesch, Eds. (Association for Computational Linguistics, Dublin, Ireland, 2014), pp. 601–604.
11. S. A. Golder, M. W. Macy, Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures. *Science* 333, 1878–1881 (2011).
12. P. S. Dodds et al., Human language reveals a universal positivity bias. *Proc. Natl. Acad. Sci. U.S.A.* 112, 2389–2394 (2015).
13. P. Liu, W. Tov, M. Kosinski, D. J. Stillwell, L. Qiu, Do Facebook status updates reflect subjective well-being? *Cyberpsychol. Behav. Soc. Netw.* 18, 373–379 (2015).
14. J. Sun, H. A. Schwartz, Y. Son, M. L. Kern, S. Vazire, The language of well-being: Tracking fluctuations in emotion experience through everyday speech. *J. Pers. Soc. Psychol.* 118, 364–387 (2019).
15. J. Gibbons et al., Twitter-based measures of neighborhood sentiment as predictors of residential population health. *PloS One* 14, e0219550 (2019).
16. H. A. Schwartz et al., Personality, gender, and age in the language of social media: The open-vocabulary approach. *PloS One* 8, e73791 (2013).
17. J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 (11 October 2018).
18. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving language understanding by generative pre-training. (2018). https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language understanding paper.pdf. Accessed 14 April 2019.
19. S. Giorgi et al., “The remarkable benefit of user-level aggregation for lexical-based population-level predictions” in *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, E. Riloff, D. Chiang, J. Hockenmaier, J. Tsujii, Eds. (Association for Computational Linguistics, Brussels, Belgium, 2018), pp. 1167–1172.
20. H. A. Schwartz et al., “Dlatk: Differential language analysis toolkit” in *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*: Jaidka et al.

PNAS | May 12, 2020 | vol. 117 | no. 19 | 10171

PSYCHOLOGICAL AND COMPUTER SCIENCES

COGNITIVE SCIENCES

