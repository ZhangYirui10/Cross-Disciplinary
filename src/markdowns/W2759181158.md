# ACL 2023

# The 61st Conference of the Association for Computational Linguistics

# Proceedings of the Conference

# Volume 2: Short Papers

# July 9-14, 2023

# The ACL organizers gratefully acknowledge the support from the following sponsors.

|Level|Event|Sponsor|
|---|---|---|
|Diamond-Level|Welcome Event|Google Research|
|Platinum|Baid EE| |
|ByteDance| | |

# DATAOCEAN AI

# YOUR GLOBAL DATA PARTNER

|Gold|ANT|JPMorgan|
|---|---|---|
|Silver| | |
|Bronze| | |

# Diversity & Inclusion Champions

©2023 Association for Computational Linguistics

Order copies of this and other ACL proceedings from:

Association for Computational Linguistics (ACL)

209 N. Eighth Street

Stroudsburg, PA 18360

USA

Tel: +1-570-476-8006

Fax: +1-570-476-0860

acl@aclweb.org

ISBN 978-1-959429-71-5

v

# Message from the General Chair

Welcome to ACL 2023, the 61st Annual Meeting of the Association for Computational Linguistics! The conference will be held in Toronto, Canada, July 9-14, 2023. Following the succession of the recent conferences in our field, ACL 2023 will adopt a hybrid format. While the impact of Covid has considerably diminished in terms of traveling, obtaining visas to Canada entails a very long process. Moreover, the global economic conditions pose challenges for many individuals to travel to conferences. Recognizing these circumstances, we know many participants may not be able to attend the conference in person. Therefore, we are committed to providing a great virtual platform so everyone has the opportunity to interact with other participants and enjoy the conference. Based on the current registered participants, approximately 30% have chosen to attend the conference virtually. Whether you join us in person or virtually, we sincerely hope everyone has a remarkable conference experience.

This General Chair’s message is where I express my gratitude to the many individuals who have made enormous contributions to the conference over the past year.

First and foremost, I am grateful for the tremendous efforts by the program chairs: Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. The rapid growth of our field is challenging from the perspective of organizing a conference. Program chairs have admirably handled a huge number of submissions and implemented novel review criteria to improve the quality of reviews and the paper decision process. They responded promptly after ChatGPT was launched and provided guidelines for using it in paper writing. Beyond their responsibilities as program chairs, they have assisted me with various other decisions. Their efforts have truly shaped the conference. Also, thanks to all the senior area chairs, area chairs, reviewers, and the best paper committee, whose commitment and dedication made paper review and selection possible.

Next, I would like to thank the entire organizing committee for their service. It has been an honor for me to collaborate with such a dedicated team. This includes:

- Industry track chairs: Beata Beigman Klebanov, Jason Williams, and Sunayana Sitaram. An addition to this year’s ACL is the introduction of a separate industry track. This is motivated by two factors. First, ACL is held in North America this year (and thus no NAACL), and NAACL has an established tradition of hosting an industry track. Second there was an increasing number of industry track submissions at EMNLP last year from previous years. We hope that a separate industry track can foster the dissemination of research on real-world applications in industry settings. Thanks to the industry track chairs for their efforts in coordinating all the logistics associated with this track.
- Demo chairs: Alan Ritter, Danushka Bollegala, and Ruihong Huang, who managed demo submissions and accepted 58 demos that will be presented in the main conference.
- Student research workshop (SRW) chairs: Gisela Vallejo, Vishakh Padmakumar, and Yao Fu, who showed remarkable enthusiasm and dedication in organizing the workshop. They selected 45 papers to be presented in the main conference program. Also thanks to the faculty advisors: Ivan Vulic and Lu Wang, for providing guidance to the SRW chairs and obtaining NSF support for the workshop.
- Workshop chairs: Annie Louis, Eduardo Blanco, and Yang Feng, who collaborated with EACL workshop chairs to select 22 workshops, and served as the vital link between the conference and individual workshop organizers.
- Tutorials chairs: Margot Mieskes, Siva Reddy, and Vivian Chen, who also worked with EACL chairs to select 6 high quality tutorials that cater to the interest and needs of our conference.

# Conference Acknowledgments

- Ethics chairs: Dirk Hovy and Yonatan Bisk, who checked papers flagged with ethics issues. Thanks for their meticulous work to ensure our papers uphold the ethical standards.
- Publication chairs: Ryan Cotterell, Chenghua Lin, Jesse Thomason, Lei Shu, and Lifu Huang, who prepared the conference handbook, ensured proper formatting of papers, and produced the conference proceedings.
- Virtual infrastructure chairs: Jiacheng Xu, Martín Villalba, and Pedro Rodriguez, who worked hard to develop a virtual platform to ensure an engaging conference experience for both in-person and remote participants. They also made various innovations and enhancements on top of the Underline platform, which the conference utilizes.
- Publicity and social media chairs: Devamanyu Hazarika, Eva Vanmassenhove, and Tong Xu, who communicated and publicized the conference through various social media channels, enhancing the visibility and reach of the conference.
- Website chairs: Jinho Choi and Zhongyu Wei, who updated and maintained the conference website to keep participants informed.
- Diversity and inclusion (D&I) chairs: Daniel Beck, Maryam Fazel-Zarandi, and Nedjma Djouhra Ousidhoum, who arranged support to participants facing financial hardships, and organized a diverse array of activities aimed at promoting diversity and inclusion in our community.
- Student volunteer chairs: Ayah Zirikly and Tao Yu, who reviewed applications and selected student volunteers for the conference.
- Sponsorship chairs: Alla Rozovskaya and Lei Li. Thanks to them and Chris Callison-Burch, the ACL sponsorship Director for their efforts in securing sponsorships and managing the relationship between sponsors and the conference. The generous support from our sponsors has played a crucial role in enabling us to maintain a reasonable registration cost for attendees, and the additional sponsorship for D&I initiatives helps our commitment to fostering a diverse and inclusive environment.
- Visa assistance team: Ayana Niwa, Qingwen Liu, Renxiang Zhang, Samridhi Choudhary, and Tao You. Many participants require visas to attend the conference, and we fully understand this lengthy process. This team has been diligently handling visa requests by sending out numerous invitation letters to facilitate visa applications.
- Infrastructure support from Softconf (Richard Gerber) and Underline (Damira Mrsic, Sol Rosenberg). Both platforms kindly accommodated our many, many requests and implemented several new features.

I also want to specially thank Jennifer Rachford, the ACL event director, who handled all the local arrangements for this conference. Though she was relatively new to the role, and oftentimes needed to juggle multiple ACL conferences, she remained well organized, and consistently provided all the necessary information to all members of the organizer committee. Her contributions ensure the success of this conference.

Thanks to previous ACL/EMNLP conference chairs for sharing their knowledge, tips, and best practices on organizing this conference, and ACL Exec for the support they provided throughout the entire planning and execution of this conference.

Lastly, I extend my appreciation to every participant. Regardless of your role, whether as authors or presenters, workshop organizers, tutorial speakers, student volunteers, session chairs, or simply attendees, your involvement is essential in creating a memorable conference.

Welcome everyone to the conference!

# ACL 2023 General Chair

Yang Liu

Alexa, Amazon

# Message from the Program Chairs

It’s hard to believe that we’re actually going to be seeing the program come together in Toronto. We’re really looking forward to it and to seeing you all there!

Most of the work of a program chair is behind the scenes: herding reviewers and chairs, wrangling data from various sources, and answering lots and lots of email. This is a volunteer position, so the only reward we get for this is our chance to make the process of submitting and reviewing papers to our conference better. This letter will outline some of those experiments.

First, we asked reviewers for two scores: soundness and excitement. Our goal was that any sound paper would be accepted to some ACL affiliated venue, but that the “main conference” distinction (limited by space) would be focused on the most exciting papers. Our hope was that soundness would be less noisy than a single “overall recommendation” score, which would help reduce the randomness of decisions. Judging by the exit surveys, this change was well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change.

Next, we developed a new process for matching papers to reviewers based on keywords for not only the subject matter of the paper, but also its type of contribution and target language(s). This allowed more fine-grained control over the paper-reviewer matches, and we were also able to provide the chairs with context for the paper-reviewer matches.

To improve review quality, we also updated the reviewer guidelines, and developed a system for the authors to flag specific types of issues with reviews. Finally, we have also proposed a new initiative for recognizing outstanding reviewers and chairs (73 awards at ACL’23).

Finally, we have tried to give more options for presentations. Findings papers now have an in-person presentation spotlight slot and virtual posters in addition to recording videos. Virtual posters have portals to link in-person attendees to virtual posters. We have also brought back Miniconf and RocketChat to allow for better virtual communication between papers (regardless of where the authors are).

This conference is a result of the joint efforts of over ten thousand people. We deeply thank them all, and apologize for the many nagging emails we had to send out. In particular:

- the general chair Yang Liu, who led the whole process;
- the incredible team of 70 SACs, 438 ACs, and 4490 reviewers, who were able to handle our record number of submissions;
- the 13,658 authors for their phenomenal scientific contributions, which we were honored to shepherd through the reviewing process;
- the ACL Executive (esp. Iryna Gurevych, Tim Baldwin, David Yarowsky, Yusuke Miyao, Emily M. Bender) for their support of many of our crazy ideas;
- 21 ethics committee reviewers, chaired by Dirk Hovy and Yonatan Bisk, for their hard work to uphold the ACL code of ethics;
- Our Best Paper Award committee (Jonathan Berant, Jose Camacho-Collados, Danqi Chen, Benjamin Van Durme, David Jurgens, Desmond Elliott, Sasha Luccioni, Jonathan May, Tom McCoy, Yusuke Miyao, Ekaterina Shutova, Emma Strubell, Jun Suzuki, Xiaojun Wan, Luke Zettlemoyer), who reviewed a record number of nominated papers under tight schedule;
- Our assistant Youmi Ma, for reducing our email and Softconf workload significantly and suggesting ideas to make the job run smoothly;
- Past ACL PCs, including Smaranda Muresan, Preslav Nakov and Aline Villavicencio (ACL 2022), Yoav Goldberg, Zornitsa Kozareva, Yue Zhang (EMNLP 2022), Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur (NAACL 2021), for their advice and suggestions;

# Acknowledgments

- Publication chairs Ryan Cotterell, Chenghua Lin, Jesse Thomason, Lei Shu, and Lifu Huang, who ensured the proper formatting of camera-ready papers;
- Emma Strubell, Ian Magnusson, and Jesse Dodge for their help in preparing publishable versions of Responsible NLP checklist;
- ACL Anthology director Matt Post;
- TACL editors-in-chief (Asli Celikyilmaz, Roi Reichart, Ani Nenkova) and CL Editor-in-Chief Hwee Tou Ng for coordinating TACL and CL presentations with us;
- Workshop chairs Annie Louis, Eduardo Blanco, and Yang Feng, for helping us to connect the Findings papers to possible presentation slots at workshops;
- Rich Gerber at Softconf, who answered countless emails and implemented several new features on our request;
- Kyle Lo and Semantic Scholar team, who kindly assisted us with data for paper-reviewer matching;
- Our virtual infrastructure chairs (Pedro Rodriguez, Jiacheng Xu, Martín Villalba) and Underline team (Damira Mrsic, Sol Rosenberg) for enabling a new kind of hybrid experience, combining miniconf and Underline;
- the ACL event director Jennifer Rachford and our visa support team (Ayana Niwa, Qingwen Liu, Renxiang Zhang, Samridhi Choudhary, and Tao You), who did everything possible to facilitate the Canada visa situation for ACL attendees.

# Submission and Acceptance

We had two routes to submit papers to ACL 2023: directly to the conference or through ACL Rolling Review (ARR). We received a record number of direct submissions (3601 long papers and 958 short papers) in January 2023. In addition, we received 305 commitments from ARR (271 long papers and 34 short papers) in March 2023. In total, we considered 4864 (3872 long and 992 short) papers with 70 senior area chairs, 438 area chairs, 4024 reviewers, 445 secondary reviewers, and 21 ethics reviewers in 27 tracks. We accepted 910 (23.50%) long and 164 (16.53%) short papers for the main conference, and 712 (41.89% including the long papers for the main conference) long and 189 (35.58% including the short papers for the main conference) short papers for Findings. To sum long and short papers, ACL 2023 accepted 1074 (22.08%) papers for the conference and 901 (40.60% including the papers for the main conference) papers for Findings. The ACL 2023 program also features 46 papers from the Transactions of the Association for Computational Linguistics (TACL) journal, and 7 from the Computational Linguistics (CL) journal.

# Limitations Section and Responsible NLP Checklist

Following EMNLP 2022 and EACL 2023, we required that each submitted paper must include an explicitly named Limitations section, discussing the limitations of the work. This was to counterbalance the practice of over-hyping the take-away messages of papers, and to encourage more rigorous and honest scientific practice. This discussion did not count towards the page limit, and we asked reviewers to not use the mentioned limitations as reasons to reject the paper, unless there was a really good reason to. In addition to the mandatory discussion of limitations, a new element at ACL 2023 is that the Responsible NLP Checklist for the accepted papers is not only considered by the reviewers, but also published together with the accepted papers as a special appendix, in an effort to improve transparency and accountability in the field.

# Areas

To ensure a smooth process, the submissions to ACL 2023 were divided into 26 areas. The areas mostly followed these of previous ACL, and more broadly ACL conferences, reflecting the typical divisions in the field. Following EMNLP 2022, we split the “Large Language Models” track away from “Machine learning in NLP”, reflecting the growth of submissions in the area. We also offered two new tracks (“Linguistic diversity” and “Multilingualism and Cross-Lingual NLP”). For the papers authored by SACs, the final recommendation decisions were made by a separate SAC team. The most popular areas (with over 250 submissions) were “Dialogue and Interactive Systems”, “Information Extraction”, “Large Language Models”, “Machine Learning for NLP”, and “NLP Applications”.

# Best Paper Awards

ACL’23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding. In total, 73 papers were nominated by the reviewers or area chairs for consideration for awards. These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 3 special awards (social impact, resource, reproduction), and several dozen outstanding papers. The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023.

# Presentation Mode

In ACL 2023, there is no meaningful distinction between oral and poster presentations in terms of paper quality. The composition of the oral sessions were proposed by the SACs of their respective tracks, so as to compose a thematically coherent set of papers on a shared topic or method, which would allow for an engaging discussion. The decisions were not based on the authors’ virtual or on-site attendance.

We hope you enjoy the program and the new elements we introduced (but let us know either way). We are looking forward to a great ACL 2023!

Anna Rogers (IT University of Copenhagen, Denmark)

Jordan Boyd-Graber (University of Maryland, USA)

Naoaki Okazaki (Tokyo Institute of Technology, Japan)

ACL 2023 Programme Committee Co-Chairs

# Program Committee

# Computational Social Science and Cultural Analytics

Walid Magdy, Daniel Preotiuc-Pietro, Md. Shad Akhtar, Nikolaos Aletras, Kalina Bontcheva, Kareem Darwish, Mai Elsherief, Kiran Garimella, Marco Guerini, Kokil Jaidka, Barbara Mcgillivray, Yelena Mejova, Usman Naseem, Bjorn Ross, James Thorne, Marco Viviani, Soroush Vosoughi, Ingmar Weber

# Dialogue and Interactive Systems

Y-Lan Boureau, Mary Ellen Foster, Minlie Huang, João Sedoc, Luciana Benotti, Paul Crook, Maryam Fazel-Zarandi, Michel Galley, Kallirroi Georgila, Alborz Geramifard, Devamanyu Hazarika, Baotian Hu, Wenqiang Lei, Gina-Anne Levow, Piji Li, Andrea Madotto, Fei Mi, Seungwhan Moon, Lili Mou, Natalie Parde, Baolin Peng, Oleg Rokhlenko, Samira Shaikh, Lei Shu, Kurt Shuster, Ruihua Song, Yiping Song, Shabnam Tafreshi, Ryuichi Takanobu, David Traum, Stefan Ultes, Charles Welch, Min Yang, Zhou Yu, Wei-Nan Zhang, Hao Zhou

# Discourse and Pragmatics

Christian Hardmeier, Jey Han Lau, Jacob Andreas, Chloé Braud, Luis Fernando D’haro, Junyi Jessy Li, Sharid Loaiciga, Nafise Sadat Moosavi, Anna Nedoluzhko, Juntao Yu, Amir Zeldes

# Ethics and NLP

Vinodkumar Prabhakaran, Diyi Yang, Kai-Wei Chang, Sunipa Dev, Karen Fort, Jack Hessel, Debora Nozza, Zeerak Talat, Yulia Tsvetkov

# Generation

Sebastian Gehrmann, Mohit Iyyer, Nina Dethlefs, Nan Duan, Greg Durrett, Angela Fan, Claire Gardent, Albert Gatt, Yeyun Gong, Srinivasan Iyer, Meng Jiang, Sujian Li, Ankur Parikh, Nanyun Peng, Lianhui Qin, Sudha Rao, Hannah Rashkin, Jinsong Su, Hiroya Takamura, John Wieting, Rui Yan, Jiajun Zhang

# Information Extraction

Lifu Huang, Chin-Yew Lin, Aaron White, Yixin Cao, Shiyu Chang, Muhao Chen, Brian Davis, Antoine Doucet, Xinya Du, Radu Florian, Xianpei Han, Filip Ilievski, Diana Inkpen, Reno Kriz, Lane Lawley, Manling Li, Kang Liu, Zhiyuan Liu, Bonan Min, Thien Nguyen, Qiang Ning, Alan Ritter, Benjamin Roth, Lei Sha, Jingbo Shang, Ge Shi, Xianzhi Wang, Wenpeng Yin, Mo Yu, Dongyan Zhao, Jun Zhao, Christos Christodoulopoulos

# Information Retrieval and Text Mining

Benjamin Piwowarski, Qifan Wang, Yi Fang, Fuli Feng, Yiqun Liu, Jian-Yun Nie, Xiaojun Quan, Yi Tay, Hongning Wang, Jingang Wang, Zenglin Xu, Grace Hui Yang

# Interpretability and Analysis of Models for NLP

Carolin Lawrence, Ana Marasovic, Chenhao Tan, Jasmijn Bastings, Dallas Card, Samuel Carton, Oana Cocarascu, Nadir Durrani, Jacob Eisenstein, Mor Geva, Ivan Habernal, Peter Hase, Alon Jacovi, Yangfeng Ji, Divyansh Kaushik, Piyawat Lertvittayakumjorn, Zaiqiao Meng, Pasquale Minervini, Isar Nejadgholi, Danish Pruthi, Abhilasha Ravichander, Roi Reichart, Swabha Swayamdipta, Martin Tutek, Elena Voita, Sarah Wiegreffe, Tongshuang Wu

# Language Grounding to Vision, Robotics, and Beyond

Zhongyu Wei, Mark Yatskar, Yoav Artzi, Yi Cai, Jingjing Chen, Zhihao Fan, Daniel Fried, Jiasen Lu, Lin Ma, Aishwarya Padmakumar, Zhaochun Ren, Freda Shi, Carina Silberer, Alessandro Suglia, Alane Suhr, Chen Sun, Hao Tan, Meng Wang, Tong Xu

# Large Language Models

Dipanjan Das, Bhuwan Dhingra, Mike Lewis, Xuezhe Ma, Miguel Ballesteros, Kenneth Church, Kumar Dubey, Orhan Firat, Marjan Ghazvininejad, Hila Gonen, Junxian He, Harsh Jhamtani, Mandar Joshi, Xiang Kong, Ni Lao, Moontae Lee, Bing Liu, Peter Liu, Eric Malmi, Huan Sun, Lijun Wu, Chunting Zhou

# Linguistic Diversity

Constantine Lignos, Emily Prud’hommeaux, Rebecca Knowles, Zoey Liu, Teresa Lynn, Lane Schwartz, Francis Tyers, Marcos Zampieri

# Linguistic Theories, Cognitive Modeling, and Psycholinguistics

Afra Alishahi, Najoung Kim, Lisa Beinborn, Abdellah Fourtassi, Nan-Jiang Jiang, R. Thomas McCoy, Aida Nematzadeh, Grusha Prasad

# Machine Learning for NLP

Marie-Francine Moens, Anna Rumshisky, Kevin Small, Heike Adel, Mikhail Burtsev, Giuseppe Castellucci, Trevor Cohn, Danilo Croce, Julian Eisenschlos, Francis Ferraro, Matthias Galle, Dan Goldwasser, Hannaneh Hajishirzi, Ricardo Henao, Estevam Hruschka, Pei Ke, Parisa Kordjamshidi, Omer Levy, Zemin Liu, André Martins, Ashutosh Modi, Ndapa Nakashole, Thanh Tam Nguyen, Giannis Nikolentzos, Barbara Plank, Steven Schockaert, Freda Shi, Vivek Srikumar, Jun Suzuki, Hao Tang, Lu Wang, Taro Watanabe, Ningyu Zhang

# Machine Translation

Markus Freitag, Tom Kocmi, Lei Li, Boxing Chen, Colin Cherry, George Foster, Roman Grundkiewicz, Francisco Guzman, Shujian Huang, Philipp Koehn, Qun Liu, Chi-Kiu Lo, Haitao Mi, Jan Niehues, Stephan Peitz, Maja Popović, Ricardo Rei, Felix Stahlberg, Zhaopeng Tu, David Vilar, Mingxuan Wang, Joern Wuebker, Tong Xiao, Jingjing Xu, François Yvon, Yue Zhang, Hao Zhou

# Multilingualism and Cross-Lingual NLP

A. Seza Doğruöz, Sunayana Sitaram, Muhammad Abdul-Mageed, David Ifeoluwa Adelani, Alham Fikri Aji, Antonios Anastasopoulos, Mikel Artetxe, Yoshinari Fujinuma, Dan Garrette, Shruti Rijhwani, Sebastian Ruder, Xinyi Wang

# NLP Applications

Sophia Ananiadou, Mark Dras, Jing Jiang, Makoto Miwa, Vincent Ng, Hadi Amiri, Riza Batista-Navarro, Jose Camacho-Collados, Fenia Christopoulou, Giovanni Da San Martino, Dina Demner-Fushman, Luigi Di Caro, Haibo Ding, Mariano Felice, Wei Gao, Sanda Harabagiu, Seung-Won Hwang, Naoya Inoue, Shafiq Joty, Ekaterina Kochmar, Mamoru Komachi, Wei Lu, Shervin Malmasi, David Mimno, Preslav Nakov, Maria Leonor Pacheco, Marek Rei, Kirk Roberts, Sara Rosenthal, Alla Rozovskaya, Tulika Saha, Hiroki Sakaji, Matthew Shardlow, Shuohang Wang, Jason Wei, Qianqian Xie, Jianfei Yu, Chrysoula Zerva, Aston Zhang, Arkaitz Zubiaga

# Phonology, Morphology, and Word Segmentation

Miikka Silfverberg, Ekaterina Vylomova, Ryan Cotterell, Xuanjing Huang, David R. Mortensen

# Question Answering

Eunsol Choi, Mrinmaya Sachan, Rishiraj Saha Roy, Priyanka Agrawal, Chitta Baral, Gianni Bar- lacchi, Hao Cheng, Danish Contractor, Pradeep Dasigi, Tushar Khot, Rik Koncel-Kedziorski, Bill Yuchen Lin, Bang Liu, Ismini Lourentzou, Sewon Min, Liangming Pan, Panupong Pasupat, Peng Qi, Ashish Sabharwal, Xiaoyu Shen, Veselin Stoyanov, Yu Su, Kai Sun, Mihai Surdeanu, Di Wang, Ziyu Yao, Yuhao Zhang

# Resources and Evaluation

Sarvnaz Karimi, Nathan Schneider, Karin Verspoor, Rachel Bawden, Asma Ben Abacha, Doina Caragea, Jennifer D’souza, Rotem Dror, Ondrej Dusek, Steffen Eger, Jorge Gracia, Udo Hahn, Lifeng Han, Radu Tudor Ionescu, David Janiszek, Sudipta Kar, Jin-Dong Kim, Jonathan Kummerfeld, John P. Lalor, Fabrice Lefèvre, Jochen Leidner, Roser Morante, Gabriella Pasi, Maja Popovi´c, German Rigau, Yves Scherrer, Manish Shrivastava, Sowmya Vajjala, Lucy Lu Wang

# Semantics: Lexical

Marianna Apidianaki, Gabriella Lapesa, Chris Biemann, Guy Emerson, Allyson Ettinger, Goran Glavaˇs, Dieuwke Hupkes, Nancy Ide, Andrey Kutuzov, Alessandro Lenci, Mohammad Taher Pilehvar, Yuval Pinter, Edoardo Maria Ponti, Vered Shwartz, Lonneke Van Der Plas, Ivan Vuli´c

# Semantics: Sentence-level Semantics, Textual Inference, and Other Areas

Yuki Arase, Roberto Navigli, Roy Schwartz, Tommaso Caselli, Simone Conia, Lei Cui, Li Dong, Lea Frermann, Atsushi Fujita, Christophe Gravier, Luheng He, Germán Kruszewski, Tommaso Pasini, Adam Poliak, Jakob Prange, Michael Roth, Keisuke Sakaguchi, Abulhair Saparov, Ji-Rong Wen, Wei Xu, Sho Yokoi, Chen Zhao

# Sentiment Analysis, Stylistic Analysis, and Argument Mining

Lun-Wei Ku, Henning Wachsmuth, Khalid Al Khatib, Elena Cabrio, Hao Fei, Anette Frank, Lin Gui, Yufang Hou, Ting-Hao Huang, Kentaro Inui, Anne Lauscher, John Lawrence, Saif Mohammad, Joonsuk Park, Shabnam Tafreshi, Orith Toledo-Ronen, Serena Villata, Shuai Wang

# Speech and Multimodality

Grzegorz Chrupała, Frank Rudzicz, Laurent Besacier, Manaal Faruqui, Sharon Goldwater, Florian Metze, Okko Rasanen, Andrew Rosenberg, Hao Tang, Wenwu Wang, Xin Wang, Shinji Watanabe

# Summarization

Chenghua Lin, Shashi Narayan, Reinald Kim Amplayo, Avi Caciularu, Chung-Chi Chen, Gong Cheng, Markus Dreyer, Xiaocheng Feng, Kathleen Mckeown, Stuart Middleton, Richard Yuanzhe Pang, Xiaojun Wan, Xingxing Zhang, Yao Zhao

# Syntax: Tagging, Chunking, and Parsing

Wanxiang Che, Djamé Seddah, Xinchi Chen, Leyang Cui, Lifeng Jin, Zhenghua Li, Joakim Nivre, Kenji Sagae, Meishan Zhang

# Theme: Reality Check

Ehud Reiter, Xiang Ren, Malihe Alikhani, Jan Buys, Jesse Dodge, Antske Fokkens, Robin Jia, Daniel Khashabi, Emiel Krahmer, Saad Mahamood, Margaret Mitchell, Richard Sproat, Byron Wallace, Adina Williams

# COI

Shay B. Cohen, Daisuke Kawahara

# Ethics

Yonatan Bisk, Dirk Hovy, Jin-Dong Kim, Zeerak Talat

# Best Paper Selection Committee

Jonathan Berant, Jose Camacho-Collados, Danqi Chen, Benjamin Van Durme, David Jurgens, Desmond Elliott, Sasha Luccioni, Jonathan May, Tom McCoy, Yusuke Miyao, Ekaterina Shutova, Emma Strubell

# Primary Reviewers

Amirhossein Abaskohi, Harika Abburi, Asad Abdi, Sadaf Abdul Rauf, Muhammad Abdul-Mageed, Kaori Abe, Omri Abend, Gavin Abercrombie, Sallam Abualhaija, Abdalghani Abujabal, Alafate Abulimiti, Lars Ackermann, Griffin Adams, Ife Adebara, David Ifeoluwa Adelani, Benedikt Adelmann, Tosin Adewumi, Jiban Adhikary, Suman Adhya, Yossi Adi, Somak Aditya, Vaibhav Adlakha, Noemi Aepli, Stergos Afantenos, Haithem Afli, Ankur Agarwal, Sanchit Agarwal, Shivam Agarwal, Rodrigo Agerri, Arshiya Aggarwal, Karan Aggarwal, Piush Aggarwal, Manex Agirre-zabal, Guy Aglionby, Aishwarya Agrawal, Ameeta Agrawal, Sweta Agrawal, Roee Aharoni, Wasi Uddin Ahmad, Sina Ahmadi, Natalie Ahn, Aman Ahuja, Chaitanya Ahuja, Kabir Ahuja, Lin Ai, Xi Ai, Ankit Aich, Annalena Aicher, Laura Aina, Salah Aït-Mokhtar, Akiko Aizawa, Alham Fikri Aji, Aswathy Ajith, Reina Akama, Pritom Saha Akash, Alan Akbik, Adewale Akinfaderin, Nader Akoury, Burak Aksar, Ibrahim Taha Aksu, Mousumi Akter, Arjun Akula, Ekin Akyurek, Hend Al-Khalifa, Hadeel Al-Negheimish, Hussein Al-Olimat, Rami Al-Rfou, Nora Al-Twairesh, Firoj Alam, Mehwish Alam, Alon Albalak, Abdullah Albanyan, Chris Alberti, Hanan Aldarmaki, Vasiliy Alekseev, Jan Alexandersson, Georgios Alexandridis, Mark Alfano, David Alfter, Robin Algayres, Raquel G. Alhama, Abdulaziz Alhamadani, Tariq Alhindi, Hamed Alhoori, Hassan Al-huzali, Badr Alkhamissi, Maxime Allard, Emily Allaway, Liesbeth Allein, Tiago Almeida, Khalid Alnajjar, Omar Alonso, Abdullah Alrajeh, Milad Alshomary, Maha Jarallah Althobaiti, Duygu Altinok, Fernando Alva-Manchego, Rami Aly, Chiara Alzetta, Bharat Ram Ambati, Maxime Amblard, Iqra Ameer, Saadullah Amin, Afra Amini, Silvio Amir, Maaz Amjad, Haozhe An, Jie An, Jisun An, Ashish Anand, Sophia Ananiadou, Raviteja Anantha, Rafael Anchiêta, Mark Anderson, Nicholas Andrews, Raghuram Annasamy, Diego Antognini, Jean-Yves Antoine, Maria Antoniak, Wissam Antoun, Rishita Anubhai, Xiang Ao, Emilia Apostolova, Mario Aragon, Erik Arakelyan, Jun Araki, Rahul Aralikatte, Ayme Arango Monnar, Oscar Araque, Matheus Araujo, John Arevalo, Arturo Argueta, Mozhdeh Ariannezhad, Hiba Arnaout, Akhil Arora, Piyush Arora, Siddhant Arora, Leila Arras, Ekaterina Artemova, Philip Arthur, Ron Artstein, Anjana Arunkumar, Saurav Aryal, Akari Asai, Ehsaneddin Asgari, Elliott Ash, Nicholas Asher, Md.sadek Hossain Asif, Arian Askari, Matthias Assenmacher, Zhenisbek Assylbekov, Berk Atil, Giuseppe Attanasio, Mohammed Attia, Aitziber Atutxa Salazar, Lauriane Aufrant, Tal August, Hayastan Avetisyan, Eleftherios Avramidis, Vera Axelrod, Hammad Ayyubi, Hosein Azarbonyad, Gorka Azkune, Aslan B. Wong, Bogdan Babych, Luca Bacco, Nguyen Bach, Sarkhan Badirli, Ebrahim Bagheri, Petra Bago, Parnia Bahar, Ashutosh Baheti, Vikas Bahirwani, Bing Bai, Fan Bai, He Bai, Jiaxin Bai, Long Bai, Xuefeng Bai, Yinhao Bai, Yu Bai, Yushi Bai, Jinyeong Bak, Amir Bakarov, Collin Baker, Vidhisha Balachandran, Mithun Balakrishna, Oana Balalau, Vevake Balaraman, Ananth Balashankar, Ramya Balasubramaniam, Gunjan Balde, Ioana Baldini, Timothy Baldwin, Simone Balloccu, Mohammadreza Banaei, Dibyanayan Bandyopadhyay, Debayan Banerjee, Pratyay Banerjee, Seojin Bang, Yejin Bang, Vinayshekhar Bannihatti Kumar, Hritik Bansal, Forrest Sheng Bao, Guangsheng Bao, Junwei Bao, Yu Bao, Yuwei Bao, Ankur Bapna, Kfir Bar, Roy Bar-Haim, Claire Barale, Mohamad Hardyman Barawi, Edoardo Barba, Adrien Barbaresi, Verginica Barbu Mititelu, M Saiful Bari, Loic Barrault, Alberto Barrón-Cedeño, Sabine Bartsch, Sabyasachee Baruah, Marco Basaldella, Pierpaolo Basile, Valerio Basile, Ali Basirat, Elisa Bassignana, Mohaddeseh Bastan, Kinjal Basu, Somnath Basu Roy Chowdhury, Tatiana Batura, Daniel Bauer, Timo Baumann, Ian Beaver, Björn Bebensee, Daniel Beck, Lee Becker, Maria Becker, Barend Beekhui.

# List of Contributors

zen, Dorothee Beermann, Gasper Begus, Melika Behjati, Shabnam Behzad, Andrei Stefan Bejgu, Nazar Beknazarov, Nuria Bel, Yonatan Belinkov, Eric Bell, Meriem Beloucif, Luca Benedetto, Martin Benjamin, Lauren Benson, Gábor Berend, Benjamin Bergen, Leon Bergen, Maria Berger, Nathaniel Berger, Rafael Berlanga, Gabriel Bernier-Colborne, Dario Bertero, Laurent Besacier, Chandra Bhagavatula, Rasika Bhalerao, Rohan Bhambhoria, Avanti Bhandarkar, Rishabh Bhardwaj, Aditya Bhargava, Pushpak Bhattacharyya, Satwik Bhattamishra, Bimal Bhattarai, Shohini Bhattasali, Anahita Bhiwandiwalla, Plaban Bhowmick, Rajarshi Bhowmik, Mukul Bhutani, Nikita Bhutani, Bin Bi, Guanqun Bi, Wei Bi, Giovanni Biancofiore, Adrien Bibal, Ann Bies, Laura Biester, Geetanjali Bihani, Yi Bin, Arne Binder, Jennifer Bishop, Debmalya Biswas, Yonatan Bitton, Johannes Bjerva, Henrik Björklund, Johanna Bjorklund, Philippe Blache, Nate Blaylock, Avi Bleiweiss, Terra Blevins, Rexhina Blloshmi, Su Lin Blodgett, Jelke Bloem, Michael Bloodgood, Carlos Bobed Lisbona, Victoria Bobicev, Ben Bogin, Bernd Bohnet, Ondřej Bojar, Huang Bojun, Valeriia Bolotova-Baranova, Necva Bülbül, Rishi Bommasani, Daniele Bonadiman, Alessandro Bondielli, Francesca Bonin, Logan Born, Mihaela Bornea, Emanuela Boros, Johan Bos, Digbalay Bose, Robert Bossy, Kaj Bostrom, Florian Boudin, Mohand Boughanem, Gerlof Bouma, Gosse Bouma, Zied Bouraoui, Andrey Bout, Johan Boye, Faeze Brahman, António Branco, Stephanie Brandl, Kiante Brantley, Pavel Braslavski, Adrian Brasoveanu, Daniel Braun, Jacob Bremerman, Jonathan Brennan, Chris Brew, Shaked Brody, Thomas Brovelli (meyer), Hannah Brown, Caroline Brun, Dominique Brunato, Yi Bu, Emanuele Bugliarello, Trung Bui, Paul Buitelaar, Razvan Bunescu, Laurie Burchell, Susanne Burger, Jill Burstein, Victor Bursztyn, Davide Buscaldi, Hendrik Buschmeier, Miriam Butt, Joan Byamugisha, Bill Byrne, Donna Byron, José G. C. De Souza, Michele Cafagna, Aoife Cahill, Samuel Cahyawijaya, Deng Cai, Han Cai, Hengyi Cai, Hongjie Cai, Pengshan Cai, Xiangrui Cai, Ruken Cakici, Iacer Calixto, Zoraida Callejas, Jesus Calvillo, Giovanni Campagna, Leonardo Campillos-Llanos, Niccolò Campolungo, Daniel Campos, Jon Ander Campos, Ricardo Campos, Burcu Can, M Abdullah Canbaz, Nicola Cancedda, Marie Candito, Ed Cannon, Erion Çano, Boxi Cao, Hailong Cao, Hejing Cao, Jiangxia Cao, Jie Cao, Kris Cao, Mengyun Cao, Pengfei Cao, Qingqing Cao, Qingxing Cao, Ruisheng Cao, Shuyang Cao, Yixuan Cao, Yu Cao, Yu Cao, Yuan Cao, Yuwei Cao, Ziqiang Cao, Cristian Cardellino, Rémi Cardon, Boaz Carmeli, Xavier Carreras, Paula Carvalho, Francisco Casacuberta, Fabio Casati, Helena Caseli, Pierluigi Cassotti, Sheila Castilho, Arie Cattan, Andrew Cattle, Paulo Cavalin, Roberto Centeno, Dumitru-Clementin Cercel, Christophe Cerisara, Mauro Cettolo, Sky Ch-Wang, Haixia Chai, Heyan Chai, Joyce Chai, Junyi Chai, Yekun Chai, Tuhin Chakrabarty, Megha Chakraborty, Tanmoy Chakraborty, Bharathi Raja Chakravarthi, Yllias Chali, Ilias Chalkidis, Nathanael Chambers, Hou Pong Chan, Zhangming Chan, Anshuma Chandak, Chandrahas, Raman Chandrasekar, Baobao Chang, Buru Chang, Ernie Chang, Haw-Shiuan Chang, Heng Chang, Kent Chang, Serina Chang, Shuaichen Chang, Tyler Chang, Yapei Chang, Yung-Chun Chang, Tai Chang-You, Guan-Lin Chao, Rajen Chatterjee, Akshay Chaturvedi, Iti Chaturvedi, Aditi Chaudhary, Vishrav Chaudhary, Subhajit Chaudhury, Geeticka Chauhan, Kushal Chawla, Chao Che, Ciprian Chelba, Emmanuel Chemla, Beiduo Chen, Berlin Chen, Bo Chen, Boli Chen, Canyu Chen, Catherine Chen, Chacha Chen, Chen Chen, Deli Chen, Derek Chen, Dongsheng Chen, Francine Chen, Fuxiang Chen, Guanhua Chen, Guanliang Chen, Guanyi Chen, Hanjie Chen, Howard Chen, Huiyuan Chen, Hung-Ting Chen, Jia Chen, Jiaao Chen, Jiangjie Chen, Jiaze Chen, Jifan Chen, Jingye Chen, John Chen, Junfan Chen, Junyang Chen, Kehai Chen, Kezhen Chen, Lei Chen, Lichang Chen, Lihu Chen, Lin Chen, Linqing Chen, Long Chen, Lu Chen, Luoxin Chen, Maximillian Chen, Mei-Hua Chen, Meiqi Chen, Meng Chen, Mingda Chen, Nuo Chen, Pei Chen, Qian Chen, Qiang Chen, Qianglong Chen, Qin Chen, Qipin Chen, Qiyuan Chen, Ruey-Cheng Chen, Sanxing Chen, Shijie Chen, Shizhe Chen, Sihao Chen, Tao Chen, Tongfei Chen, Xiaojun Chen, Xiaoli Chen, Xiaoyin Chen, Xilun Chen, Xingran Chen, Xinhong Chen, Xiuyi Chen, Xiuying Chen, Yang Chen, Yangbin Chen, Yangyi Chen, Yanping Chen, Yen-Chun Chen, Yiming Chen, Ying Chen, Yongjun Chen, Yu Chen, Yubo Chen, Yubo Chen, Yue Chen, Yue Chen, Yulong Chen, Yun Chen, Yunmo Chen, Zeming Chen, Zhibin Chen, Zhihong Chen, Zhijun Chen, Zhiyu Chen, Zhiyu Chen, Zhuang Chen.

# Cheng, Liying

# Cheng, Lu

# Cheng, Myra

# Cheng, Pengxiang

# Cheng, Qinyuan

# Cheng, Shanbo

# Cheng, Sijie

# Cheng, Weiwei

# Cheng, Yong

# Cheng, Yu

# Cheng, Zhi-Qi

# Cheng, Vijil

# Chenthamarakshan, Joe

# Cheri, Artem

# Chernodub, Emmanuele

# Chersoni, Jackie

# Chi Kit Cheung, Jianfeng

# Chi, Zewen

# Cheng-Han Chiang, David

# Chiang, Patricia

# Chiril, Nadezhda

# Chirkova, Luis

# Chiruzzo, Billy

# Chiu, Javier

# Chiyah-Garcia, Hyunchang

# Cho, Hyundong

# Cho, Hyunsoo

# Cho, Sangwoo

# Cho, Seunghyuk

# Cho, Sungjun

# Cho, Sungzoon

# Cho, Won Ik

# Cho, Young Min

# Choi, Daejin

# Choi, Jihun

# Choi, Jinho D.

# Choi, Seungtaek

# Choi, Yejin

# Choi, Yunseok

# Chollampatt, Shamil

# Choo, Jaegul

# Chopra, Shubham

# Choshen, Leshem

# Choubey, Prafulla Kumar

# Choudhury, Monojit

# Chowdhury, Md Faisal Mahbub

# Chowdhury, Shammur Absar

# Christ, Lukas

# Chu, Chenhui

# Chu, Yun-Wei

# Chu, Zewei

# Chu, Zhendong

# Chuang, Yung-Sung

# Chun, Jayeol

# Chung, Jin-Woo

# Chy, Abu Nowshed

# Cignarella, Alessandra Teresa

# Cimiano, Philipp

# Ciosici, Manuel

# Civera Saiz, Jorge

# Clark, Christopher

# Clark, Elizabeth

# Claveau, Vincent

# Clifton, Ann

# Coavoux, Maximin

# Cocos, Anne

# Cohen, Daniel

# Cohen, Raphael

# Coll Ardanuy, Mariona

# Colla, Davide

# Collins, Marcus

# Colon-Hernandez, Pedro

# Coman, Andrei

# Constant, Mathieu

# Cook, Paul

# Cooper Stickland, Asa

# Corazza, Anna

# Corcoglioniti, Francesco

# Cordeiro, João

# Cornille, Nathan

# Correia, Gonc

# Crabb, Erin

# Crabbé, Benoit

# Creutz, Matthias

# Cripwell, Liam

# Cromieres, Fabien

# Crouse, Maxwell

# Cuayahuitl, Heriberto

# Cui, Ganqu

# Cui, Haotian

# Cui, Peng

# Cui, Shaobo

# Cui, Shiyao

# Cui, Wanyun

# Cui, Xia

# Cui, Yiming

# Cunha, Rossana

# Cunha, Washington

# Da, Jeff

# Da Cunha, Iria

# Dabre, Raj

# Dagan, Gautier

# Dahl, Deborah

# Dahlmann, Leonard

# Dahlmeier, Daniel

# Dai, Damai

# Dai, Hongliang

# Dai, Qin

# Dai, Wenliang

# Dai, Xiang

# Dai, Yi

# Dai, Yinpei

# Dai, Yong

# Dakota, Daniel

# Dalvi, Fahim

# Damonte, Marco

# Dandapat, Sandipan

# Dangovski, Rumen

# Dankers, Verna

# Dara, Aswarth Abhilash

# Das, Amitava

# Das, Anubrata

# Das, Ayan

# Das, Debopam

# Das, Dipankar

# Das, Mithun

# Das, Sarkar Snigdha Sarathi

# Das, Souvik

# Das Gupta, Mithun

# Dash, Sarthak

# Datta, Debajyoti

# Daudaravicius, Vidas

# Davidson, Sam

# Davis, Forrest

# Davison, Joe

# De Bruyne, Luna

# De Chalendar, Gael

# De Clercq, Orphee

# De Gispert, Adria

# De Jong, Michiel

# De Kuthy, Kordula

# De La Clergerie, Éric

# De Lichy, Cyprien

# De Luca, Ernesto William

# De Mori, Renato

# De Varda, Andrea

# Debnath, Alok

# Dehouck, Mathieu

# Del, Maksym

# Del Corro, Luciano

# Delbrouck, Jean-Benoit

# Delcroix, Marc

# Delecraz, Sebastien

# Deleger, Louise

# Dell’orletta, Felice

# Delobelle, Pieter

# Demberg, Vera

# Dementieva, Daryna

# Demeter, David

# Demir, Seniz

# Demszky, Dorottya

# Deneefe, Steve

# Deng, Haolin

# Deng, Mingkai

# Deng, Shumin

# Deng, Xiang

# Deng, Xun

# Deng, Yang

# Deng, Yuntian

# Deng, Zhi-Hong

# Denis, Pascal

# Denkowski, Michael

# Derczynski, Leon

# Deriu, Jan

# Deutsch, Daniel

# Devanbu, Premkumar

# Devarakonda, Murthy

# Develder, Chris

# Devinney, Hannah

# Dey, Suvodip

# Deyoung, Jay

# Dhar, Prajit

# Di, Zonglin

# Di Eugenio, Barbara

# Di Gangi, Mattia

# Di Liello, Luca

# Di Nunzio, Giorgio Maria

# Diao, Shizhe

# Dias, Gaël

# Diaz, Alberto

# Dimitrov, Dimitar

# Dinan, Emily

# Ding, Bosheng

# Ding, Chenchen

# Ding, Jie

# Ding, Kaize

# Ding, Keyang

# Ding, Liang

# Ding, Ning

# Ding, Shuoyang

# Ding, Wenjian

# Ding, Wentao

# Ding, Yangruibo

# Ding, Yuning

# Ding, Zeyuan

# Ding, Zixiang

# Dinu, Anca

# Dinu, Liviu P.

# Dirix, Peter

# Divakaran, Ajay

# Dixit, Kalpit

# Dixit, Tanay

# Djuric, Nemanja

# Dligach, Dmitriy

# Dodda- paneni, Sumanth

# Dolin, Pavel

# Domingo, Miguel

# Dong, Chenhe

# Dong, Haoyu

# Dong, Meixing

# Dong, Mengxing

# Dong, Ming

# Dong, Minghui

# Dong, Qianqian

# Dong, Qingxiu

# Dong, Xiangjue

# Dong, Xin

# Doran, Christine

# Dossou, Bonaventure F. P.

# Dou, Longxu

# Dou, Zhicheng

# Dou, Zi-Yi

# Doughman, Jad

# Dragut, Eduard

# Drozd, Aleksandr

# Du, Jinhua

# Du, Li

# Du, Li

# Du, Mengnan

# Du, Pan

# Du, Tianyu

# Du, Wanyu

# Du, Yangkai

# Du, Yulun

# Du, Yupei

# Dua, Dheeru

# Duan, Hanyu

# Duan, Jiali

# Duan, Jiaxin

# Duan, Junwen

# Duan, Pengfei

# Duan, Sufeng

# Duan, Xiangyu

# Duboue, Pablo

# Dufter, Philipp

# Dugan, Liam

# Dugue, Nicolas

# Duh, Kevin

# Dunn, Jonathan

# Duseja, Tejas

# Dusell, Brian

# Dutta, Sourav

# Dwojak, Tomasz

# Dyer, William

# E, Haihong

# Eberle, Kurt

# Ebert, Sebastian

# Echizen’ya, Hiroshi

# Edman, Lukas

# Edmiston, Daniel

# Edwards, Aleksandra

# Egea Gómez, Santiago

# Egg, Markus

# Eguchi, Koji

# Ehara, Yo

# Ehrmann, Maud

# Eiselen, Roald

# Eisner, Jason

# Ekbal, Asif

# El Maarouf, Ismail

# El-Beltagy, Samhaa R.

# Elangovan, Aparna

# Elazar, Yanai

# Elbayad, Maha

# Elfardy, Heba

# Elgaar, Mohamed

# Elhadad, Michael

# Ell, Basil

# Elliott, Desmond

# Elsafoury, Fatma

# Elsner, Micha

# Emezue, Chris Chinenye

# Enayati, Saman

# Enguehard, Joseph

# Eo, Sugyeong

# Ermakova, Liana

# Ernst, Ori

# Ernst, Patrick

# Erzin, Engin

# Escolano, Carlos

# Eshghi, Arash

# Espa˜na-Bonet, Cristina

# Espinosa, Luis

Anke, Dominique Estival, Kawin Ethayarajh, Kilian Evang, Kenneth Ezukwoke, Saad Ezzini, Alex Fabbri, Marzieh Fadaee, Michael Faerber, Guglielmo Faggioli, Fahim Faisal, Agnieszka Falenska, Neele Falk, Tobias Falke, James Fan, Jungwei Fan, Yao-Chung Fan, Yimin Fan, Yue Fan, Zhihao Fan, Hui Fang, Qingkai Fang, Tianqing Fang, Yihao Fang, Yimai Fang, Yuwei Fang, Hossein Fani, Ana C Farinha, Nawshad Farruque, Amany Fashwan, Mehwish Fatima, Adam Faulkner, Benoit Favre, Amir Feder, Marc Feger, Zichu Fei, Guy Feigenblat, Nils Feldhus, Sergey Feldman, Virginia Felkner, Jianzhou Feng, Jiazhan Feng, Shangbin Feng, Shi Feng, Shutong Feng, Steven Y. Feng, Weixi Feng, Xiachong Feng, Yang Feng, Yansong Feng, Yu Feng, Yunhe Feng, Zhangyin Feng, Paulo Fernandes, Nigel Fernandez, Ramon Fernandez Astudillo, Javier Fernandez-Cruz, Daniel Fernández-González, Elisa Ferracane, Javier Ferrando, Rafael Ferreira, Besnik Fetahu, Alejandro Figueroa, Matthew Finlayson, Mauajama Firdaus, Mark Fishel, Margaret Fleck, Michael Flor, Jose Fonollosa, Marco Aurelio Fonseca, Tommaso Fornaciari, Karen Fort, Jennifer Foster, Abdellah Fourtassi, Robert Frank, Kathleen C. Fraser, Flavius Frasincar, Diego Frassinelli, Dayne Freitag, André Freitas, Simona Frenda, Victor Fresno, Dan Friedman, Annemarie Friedrich, Jason Fries, Francesca Frontini, Guohong Fu, Jie Fu, Lisheng Fu, Liye Fu, Peng Fu, Xiyan Fu, Yao Fu, Nancy Fulda, Kotaro Funakoshi, Pascale Fung, Yi Fung, Martin Funkquist, Hagen Fürstenau, Richard Futrell, Matteo Gabburo, Kata Gábor, Marco Gaido, Amit Gajbhiye, Dimitris Galanis, Olivier Galibert, Lukas Galke, Ramiro H. Gálvez, Mihaela Gaman, Leilei Gan, Yujian Gan, Sudeep Gandhe, Ashwinkumar Ganesan, Balaji Ganesan, Ananya Ganesh, Varun Gangal, Debasis Ganguly, William Gantt, Chang Gao, Chongyang Gao, Cuiyun Gao, Ge Gao, Hongyang Gao, Jiahui Gao, Jinhua Gao, Jun Gao, Lingyu Gao, Pengzhi Gao, Qiaozi Gao, Shen Gao, Tianyu Gao, Wei Gao, Xin Gao, Yifan Gao, Yingbo Gao, Cristina Garbacea, Marcos Garcia, Aitor García Pablos, Leibny Paola Garcia Perera, Iker García-Ferrero, Diego Garcia-Olano, Krishna Garg, Muskan Garg, Sarthak Garg, Siddhant Garg, Aina Garí Soler, Ekaterina Garmash, Łukasz Garncarek, Nicolas Garneau, Federico Gaspari, Judith Gaspers, Itai Gat, Susan Gauch, Eric Gaussier, Tanja Gaustad, Dipesh Gautam, Mengshi Ge, Suyu Ge, Xiou Ge, Yixiao Ge, Zhaocheng Ge, Michaela Geierhos, Christian Geishauser, Ruiying Geng, Ariel Gera, Felix Gervits, Luke Gessler, Hamidreza Ghader, Sahar Ghannay, Sarik Ghazarian, Mozhdeh Gheini, Deepanway Ghosal, Amur Ghose, Sayan Ghosh, Sayontan Ghosh, Soumitra Ghosh, Sourav Ghosh, Sreyan Ghosh, Sucheta Ghosh, Filip Ginter, John Giorgi, Salvatore Giorgi, Voula Giouli, Mario Giulianelli, Ameya Godbole, Nathan Godey, Pranav Goel, Rahul Goel, Vaibhava Goel, Anne Göhring, Koldo Gojenola, Tejas Gokhale, Yoav Goldberg, Seraphina Goldfarb-Tarrant, Sujatha Das Gollapalli, Olga Golovneva, Luís Gomes, Jose Manuel Gomez-Perez, Carlos Gómez-Rodríguez, Hugo Goncalo Oliveira, Marcos Goncalves, Teresa Goncalves, Lovedeep Gondara, Hongyu Gong, Jiaying Gong, Linyuan Gong, Shansan Gong, Zhuocheng Gong, Jeff Good, Michael Goodman, Senthilkumar Gopal, Karthik Gopalakrishnan, Jonathan Gordon, Philip John Gorinski, Isao Goto, Yanjie Gou, Antoine Gourru, Cyril Goutte, Venkata Subrahmanyan Govindarajan, Edward Gow-Smith, Thamme Gowda, Kartik Goyal, Navita Goyal, Palash Goyal, Prasoon Goyal, Natalia Grabar, Mario Graff, Damien Graux, David Griol, Milan Gritta, Loïc Grobol, Stig-Arne Grönroos, David Gros, Adam Grycner, Jia-Chen Gu, Jiasheng Gu, Shuhao Gu, Yue Gu, Yuxian Gu, Saiping Guan, Yong Guan, Nuno M. Guerreiro, Liangke Gui, Vincent Guigue, Bruno Guillaume, Adrien Guille, Kalpa Gunaratna, James Gung, Tunga Gungor, Sharath Chandra Guntuku, Biyang Guo, Fengyu Guo, Han Guo, Jiang Guo, Jiaqi Guo, Jinyang Guo, Junliang Guo, Lin Guo, Meiqi Guo, Qipeng Guo, Quan Guo, Ruocheng Guo, Shaoru Guo, Shu Guo, Wangzhen Guo, Xin Guo, Xinnan Guo, Yanzhu Guo, Yinpeng Guo, Zhijiang Guo, Abhirut Gupta, Akshat Gupta, Amulya Gupta, Anchit Gupta, Ankit Gupta, Ankita Gupta, Ashim Gupta, Jai Gupta, Nitish Gupta, Prakhar Gupta, Raghav Gupta, Rishabh Gupta, Sonu Gupta, Sparsh Gupta, Umang Gupta, Vivek Gupta, Ximena Gutierrez-Vasques, Jeremy Gwinnup, Loitongbam Gyanendro Singh, Le An Ha, Nizar Habash, Kais Haddar, Katharina Haemmerl, Christopher Hahn, Joonghyuk Hahn, Michael Hahn, Zhen Hai, Jan Hajič, Eva Hajicova, Hossein Hajipour, Sherzod Hakimov, Kishaloy Halder, Anaïs Halftermeyer, Harald Hammarström, Michael Hammond, Thierry Hamon, Chengcheng Han, Chi Han, Hojae Han, Kelvin Han, Ridong Han, Rujun Han.

Ting Han, Xiaochuang Han, Xiaohui Han, Xu Han, Xudong Han, Yo-Sub Han, Yu Han, Zhen Han, Zhongyuan Han, Chung-Wei Hang, Viktor Hangya, Jie Hao, Junheng Hao, Tianyong Hao, Rejwanul Haque, Syed Haque, Tatsuya Harada, David Harbecke, Momchil Hardalov, Daniel Hardt, Hardy Hardy, Keith Harrigian, William Hartmann, John Harvill, Sadid A. Hasan, Maram Hasanain, Taku Hasegawa, Chikara Hashimoto, Sabit Hassan, Bradley Hauer, Claudia Hauff, Shreya Havaldar, William Havard, Adi Haviv, Hiroaki Hayashi, Yoshihiko Hayashi, Amir Hazem, Ben He, Guoxiu He, Jacqueline He, Jianfeng He, Jiangen He, Jiayuan He, Jinzheng He, Kai He, Keqing He, Ru He, Shizhu He, Tianxing He, Wanwei He, Wei He, Xiaodong He, Xingwei He, Xuanli He, Yifan He, Yunjie He, Zexue He, Zhongjun He, Michael Heck, Behnam Hedayatnia, Michael Hedderich, Stefan Heindorf, Johannes Heinecke, Jindřich Helcl, William Held, Oliver Hellwig, Chadi Helwe, Christian Hempelmann, Lisa Anne Hendricks, Iris Hendrickx, Cui Hengbin, Leonhard Hennig, Yu-Jung Heo, David Herel, Delia Irazu Hernandez Farias, Christian Herold, Daniel Hershcovich, Jonathan Herzig, Christian Heumann, John Hewitt, Gerhard Heyer, Christopher Hidey, Derrick Higgins, Stefan Hillmann, Tsutomu Hirao, Tatsuya Hiraoka, Namgyu Ho, Cong Duy Vu Hoang, Cuong Hoang, Julia Hockenmaier, Chris Hokamp, Samuel Hollands, Nora Hollenstein, Pavan Holur, Christopher Homan, Takeshi Homma, Ukyo Honda, Giwon Hong, Pengyu Hong, Zhi Hong, Mark Hopkins, Ales Horak, Andrea Horbach, Sho Hoshino, Tom Hosking, Md Mosharaf Hossain, Mohammad Javad Hosseini, Pedram Hosseini, Rasa Hosseinzadeh, Lei Hou, Yifan Hou, Phillip Howard, David M. Howcroft, Cheng-Yu Hsieh, Chao-Chun Hsu, Chun-Nan Hsu, I-Hung Hsu, Yi-Li Hsu, Phu Mon Htut, Chi Hu, Dou Hu, Guangneng Hu, Guimin Hu, Hai Hu, Hailin Hu, Han Hu, Hexiang Hu, Jinyi Hu, Linmei Hu, Mengting Hu, Minda Hu, Songbo Hu, Xiang Hu, Xiaodan Hu, Xuming Hu, Yibo Hu, Yuchen Hu, Yushi Hu, Zhe Hu, Zhiwei Hu, Zhiyuan Hu, Zikun Hu, Ziniu Hu, Hang Hua, Wenyue Hua, Xinyu Hua, Chao-Wei Huang, Chen Huang, Chieh-Yang Huang, Fei Huang, Hen-Hsen Huang, Hui Huang, Jen-Tse Huang, Jiaxin Huang, Jie Huang, Jimin Huang, Jimmy Huang, Jin-Xia Huang, Junjie Huang, Kuan-Hao Huang, Kung-Hsiang Huang, Luyang Huang, Qingbao Huang, Quzhe Huang, Rongjie Huang, Shaohan Huang, Tenghao Huang, Xinting Huang, Yinya Huang, Yongjie Huang, Youcheng Huang, Zhiqi Huang, Zhongqiang Huang, Luwen (vivian) Huangfu, Patrick Huber, John Hudzina, Pere-Lluís Huguet Cabot, Mans Hulden, Chia-Chien Hung, Fantine Huot, Ali Hürriyetoğlu, Tin Huynh, Rebecca Hwa, Dae Yon Hwang, Jena D. Hwang, Dongmin Hyun, Ignacio Iacobacci, Muhammad Okky Ibrohim, Adrian Iftene, Ryu Iida, Gabriel Ilharco, Nikolai Ilinykh, Kenji Imamura, Ayyoob Imanigooghari, Joseph Marvin Imperial, Hirofumi Inaguma, Mert Inan, Svanhvít Lilja Ingólfsdóttir, Koji Inoue, Takashi Inui, Hitoshi Isahara, Tatsuya Ishigaki, Etsuko Ishii, Aminul Islam, Tunazzina Islam, Masaru Isonuma, Takumi Ito, Abe Ittycheriah, Hamish Ivison, Tomoya Iwakura, Ran Iwamoto, Kenichi Iwatsuki, Vivek Iyer, Peter Izsak, Bassam Jabaian, Aashi Jain, Alankar Jain, Parag Jain, Rishabh Jain, Milos Jakubicek, Masoud Jalili Sabet, Shoaib Jameel, Richard James, Abhik Jana, Eugene Jang, Hyeju Jang, Myeongjun Jang, Youngsoo Jang, Sepehr Janghorbani, Peter Jansen, Maarten Janssen, Sujay Kumar Jauhar, Tommi Jauhiainen, Inigo Jauregi Unanue, Ganesh Jawahar, Sébastien Jean, Fran Jelenić, Sungho Jeon, Minwoo Jeong, Myeongho Jeong, Young-Seob Jeong, Kevin Jesse, Elisabetta Jezek, Akshita Jha, Prince Jha, Sneha Jha, Bin Ji, Haozhe Ji, Seunghyun Ji, Shaoxiong Ji, Ziwei Ji, Chen Jia, Qi Jia, Zixia Jia, Yiren Jian, Aiqi Jiang, Chao Jiang, Feng Jiang, Hang Jiang, Hao Jiang, Jie Jiang, Jiyue Jiang, Junfeng Jiang, Jyun-Yu Jiang, Lan Jiang, Lavender Jiang, Ming Jiang, Ridong Jiang, Tianwen Jiang, Tianyu Jiang, Wenbin Jiang, Xiaotong Jiang, Xuhui Jiang, Yichen Jiang, Yong Jiang, Yuxin Jiang, Zhengbao Jiang, Zhiwei Jiang, Zhiying Jiang, Zhuoren Jiang, Zhuoxuan Jiang, Cathy Jiao, Wenxiang Jiao, Yizhu Jiao, Zhanming Jie, Bernal Jimenez Gutierrez, Di Jin, Li Jin, Lisa Jin, Mali Jin, Qiao Jin, Shuning Jin, Woojeong Jin, Xiaomeng Jin, Yiping Jin, Zhi Jin, Zhijing Jin, Zijian Jin, Hwiyeol Jo, Richard Johansson, Kristen Johnson, Michael Johnston, Erik Jones, Kenneth Joseph, Abhinav Joshi, Aditya Joshi, Brihi Joshi, Nitish Joshi, Rishabh Joshi, Xincheng Ju, Yiming Ju, Zeqian Ju, Jaap Jumelet, Kyomin Jung, Myong Chol Jung, Taehee Jung, Juraj Juraska, David Jurgens, Raquel Justo, Prathyusha Jwalapuram, Preethi Jyothi, Vimal Kumar K, Kishan K C, Besim Kabashi, Srikanth Doss Kadarundalagi Raghuram Doss, Kaxix

zuma Kadowaki, Andrea Kahn, Magdalena Kaiser, Ivana Kajic, Tomoyuki Kajiwara, Mihir Kale, Oren Kalinsky, Laura Kallmeyer, Aikaterini-Lida Kalouli, Katikapalli Subramanyam Kalyan, Abu Raihan Kamal, Ehsan Kamalloo, Nishant Kambhatla, Hidetaka Kamigaito, Jaap Kamps, Hiroshi Kanayama, Kamil Kanclerz, Masahiro Kaneko, Gi-Cheon Kang, Jaewook Kang, Minki Kang, Yoshinobu Kano, Diptesh Kanojia, Pinar Karagoz, Giannis Karamanolakis, Siddharth Karamcheti, Mladen Karan, Akbar Karimi, Younes Karimi, Payam Karisani, B¨orje Karlsson, Shubhra Kanti Karmaker Santu, Sanjeev Kumar Karn, Constantinos Karouzos, Marzena Karpinska, Omid Kashefi, Zdenˇek Kasner, Aly Kassem, Anisia Katinskaia, Yoav Katz, David Kauchak, Pride Kavumba, Noriaki Kawamae, Hideto Kazawa, Ashkan Kazemi, Ghazaleh Kazeminejad, Amirhossein Kazemnejad, Zixuan Ke, Akhil Kedia, Sedrick Scott Keh, Katherine Keith, Amr Keleg, Frank Keller, Casey Kennington, Tom Kenter, Roman Kern, Santosh Kesiraju, Lee Kezar, Shahram Khadivi, Muhammad Khalifa, Salam Khalifa, Anant Khandelwal, Dinesh Khandelwal, Shima Khanehzar, Simran Khanuja, Kyung Seo Ki, Mert Kilickaya, Halil Kilicoglu, Bugeun Kim, Gangwoo Kim, Gene Kim, Geonmin Kim, Gunhee Kim, Gyuhak Kim, Harksoo Kim, Hong Kook Kim, Hyoung-hun Kim, Hyunjae Kim, Hyunwoo Kim, Jaeyoung Kim, Jihyuk Kim, Jongwon Kim, Joo-Kyung Kim, Joshua Y. Kim, Jung-Jae Kim, Kangil Kim, Kyungho Kim, Minsoo Kim, Sungdong Kim, Taeuk Kim, Yekyung Kim, Young Jin Kim, Youngwoo Kim, Yu Jin Kim, Yasutomo Kimura, Milton King, Tracy Holloway King, Svetlana Kiritchenko, Christo Kirov, Denis Kiselev, Hirokazu Kiyomaru, Shun Kiyono, Christopher Klamm, Ayal Klein, Tassilo Klein, Jan-Christoph Klie, Roman Klinger, Julien Kloetzer, Miyoung Ko, Goro Kobayashi, Hayato Kobayashi, Thomas Kober, Elena Kochkina, Jan Kocon, Prashant Kodali, Jordan Kodner, Arne Koehn, Rob Koeling, Svetla Koeva, Jing Yu Koh, Mare Koit, Noriyuki Kojima, Stanley Kok, Daan Kolkman, Anton Kolonin, Kazunori Komatani, Kanako Komiya, Grzegorz Kondrak, Cunliang Kong, Lingkai Kong, Miloslav Konopík, Ioannis Konstas, Selcuk Kopru, Michalis Korakakis, Katerina Korre, Ana Kotarcic, Suraj Kothawade, Fajri Koto, Neema Kotonya, Alexander Kotov, Manolis Koubarakis, Anna Koufakou, Vasiliki Kougia, Punit Singh Koura, Venelin Kovatchev, Ivan Koychev, Michael Kranzlein, Matthias Kraus, Simon Krek, Brigitte Krenn, Amrith Krishna, Kalpesh Krishna, Kundan Krishna, Adit Krishnan, Nikhil Krishnaswamy, Canasai Kruengkrai, Udo Kruschwitz, Anna Kruspe, Da Kuang, Andrei Kucharavy, Ilia Kulikov, Aditya Prakash Kulkarni, Ashish Kulkarni, Atharva Kulkarni, Vivek Kulkarni, Ashutosh Kumar, Puneet Kumar, Ritesh Kumar, Sachin Kumar, Sawan Kumar, Shankar Kumar, Shanu Kumar, Sumeet Kumar, Varun Kumar, Sadhana Kumaravel, Anoop Kunchukuttan, Adhiguna Kuncoro, Tsung-Ting Kuo, Yuri Kuratov, Murathan Kurfalı, Tatsuki Kuribayashi, Mikko Kurimo, Shuhei Kurita, Ugur Kursuncu, Guy Kushilevitz, Mucahid Kutlu, Ilia Kuznetsov, Haewoon Kwak, Sunjun Kweon, Yeonsu Kwon, Moreno La Quatra, Philippe Laban, Sofie Labat, Matthieu Labeau, Yanis Labrak, Faisal Ladhak, Katrien Laenen, Allison Lahnala, Huiyuan Lai, Kenneth Lai, Viet Lai, Yi-An Lai, Yuxuan Lai, Veronika Laippala, Surafel M. Lakew, Kushal Lakhotia, Yash Kumar Lal, Tsz Kin Lam, Wai Lam, Hemank Lamba, Vasileios Lampos, Gerasimos Lampouras, Nur Lan, Yunshi Lan, Lukas Lange, Maurice Langner, Mateusz Lango, Mirella Lapata, Issam Laradji, Samuel Larkin, Mikel Larra˜naga, Stefan Larson, Samuel L¨aubli, Frances Adriana Laureano De Leon, Alberto Lavelli, Alexandra Lavrentovich, Dawn Lawrie, Phong Le, Joseph Le Roux, Kevin Leach, Gianluca Lebani, Lynda Lechani, Andrew Lee, Bruce W. Lee, Deokjae Lee, Dong-Ho Lee, Donghun Lee, Dongkyu Lee, Dongyub Lee, Fei-Tzin Lee, Gibbeum Lee, Grandee Lee, Hung-Yi Lee, Hwaran Lee, I-Ta Lee, Jackson Lee, Jae Hee Lee, Jae Sung Lee, Jay Yoon Lee, Jeong Min Lee, Ji-Ung Lee, Jihwan Lee, Jinhyuk Lee, John Lee, Jongwuk Lee, Jun-Min Lee, Koanho Lee, Kyumin Lee, Lung-Hao Lee, Mina Lee, Minho Lee, Minwoo Lee, Mong Li Lee, Nayeon Lee, Roy Ka-Wei Lee, Sang-Woo Lee, Seolhwa Lee, Wonkee Lee, Yongjae Lee, Yoonjoo Lee, Young-Suk Lee, Younghun Lee, Els Lefever, Jo¨el Legrand, Jens Lemmens, Yves Lepage, Leo Lepp¨anen, Pietro Lesci, Chun Wa Leung, Gregor Leusch, Ran Levy, Sharon Levy, Alexander Hanbo Li, Baoli Li, Bei Li, Belinda Z. Li, Bin Li, Bo Li, Bobo Li, Boyang Li, Changmao Li, Cheng Li, Cheng-Te Li, Chengming Li, Chenliang Li, Chong Li, Dianqi Li, Fangtao Li, Fei Li, Guanlin Li, Haizhou Li, Haochen Li, Haonan Li, Haoqi Li, Haoran

Li, Haoran

Li, Irene

Li, Jiacheng

Li, Jialu

Li, Jiangnan

Li, Jiangtong

Li, Jiaqi

Li, Jiaxuan

Li, Jieyu

Li, Jing

Li, Jinpeng

Li, Jiyi

Li, Juanhui

Li, Juncheng

Li, Junyi

Li, Junyi

Li, Keyi

Li, Lei

Li, Li

Li, Erran

Li, Liangyou

Li, Linjie

Li, Linyang

Li, Liunian Harold

Li, Maoxi

Li, Margaret

Li, Miao

Li, Miaoran

Li, Mingda

Li, Mingjie

Li, Mukai

Li, Peifeng

Li, Peiguang

Li, Peng

Li, Qian

Li, Qintong

Li, Ru

Li, Rui

Li, Ruifan

Li, Ruizhe

Li, Sha

Li, Shaobo

Li, Sheng

Li, Shengjie

Li, Shimin

Li, Shiyang

Li, Shuangyin

Li, Shujun

Li, Shuyang

Li, Si

Li, Siyan

Li, Tao

Li, Tianjian

Li, Wei

Li, Wei

Li, Wenyan

Li, Xia

Li, Xiang

Li, Xiang Lisa

Li, Xiao

Li, Xiaonan

Li, Ximing

Li, Xin

Li, Xintong

Li, Xinxin

Li, Xue

Li, Yafu

Li, Yanran

Li, Yanyang

Li, Yanzeng

Li, Yanzhou

Li, Yaoyiran

Li, Yinghui

Li, Yingjie

Li, Yingya

Li, Yitong

Li, Yiyuan

Li, Yu

Li, Yuan-Fang

Li, Yucheng

Li, Yuliang

Li, Yuncong

Li, Yunji

Li, Zekun

Li, Zhenhao

Li, Zhi

Li, Zhongli

Li, Zongxi

Li, Zuchao

Li, Vladislav Lialin

Li, Yixin Lian

Liang, Bin

Liang, Chao-Chun

Liang, Davis

Liang, Di

Liang, Hongru

Liang, Junjie

Liang, Miya

Liang, Paul Pu

Liang, Ping

Liang, Sheng

Liang, Yaobo

Liang, Zheng-zhong

Liang, Zhenwen

Liang, Zhicheng

Liao, Baohao

Liao, Lizi

Liao, Peiyuan

Liao, Siyu

Libovický, Jindˇ rich

Liesaputra, Veronica

Likhobaba, Daniil

Lim, Gilbert

Lim, Heuiseok

Lim, Jungwoo

Lim, Kwan Hui

Lim, Tomasz

Limisiewicz, Nut

Limsopatham, Bingqian

Lin, Bo

Lin, Chuan-Jie

Lin, Hongyu

Lin, Huan

Lin, Kevin

Lin, King Ip

Lin, Li

Lin, Lucy

Lin, Nankai

Lin, Peiqin

Lin, Qika

Lin, Sheng-Chieh

Lin, Ting-En

Lin, Victoria

Lin, Wei

Lin, Weizhe

Lin, Xiang

Lin, Xinshi

Lin, Yankai

Lin, Ying-Jia

Lin, Yu-Hsiang

Lin, Zeqi

Lin, Zhaojiang

Lin, Zhenxi

Lin, Zhouhan

Lin, Zi

Lindemann, Matthias

Ling, Jeffrey

Ling, Zhenhua

Linzen, Tal

Lippi, Marco

Lison, Pierre

Litman, Diane

Litschko, Robert

Litvak, Marina

Liu, Alisa

Liu, Ao

Liu, Bing

Liu, Boyang

Liu, Chen

Liu, Chi-Liang

Liu, Dayiheng

Liu, Dexi

Liu, Emmy

Liu, Fangyu

Liu, Fenglin

Liu, Guangliang

Liu, Guisheng

Liu, Han

Liu, Haokun

Liu, Hui

Liu, Hui

Liu, Jiacheng

Liu, Jiangming

Liu, Jiawei

Liu, Jiduan

Liu, Jie-Jyun

Liu, Jinglin

Liu, Jingzhou

Liu, Junhao

Liu, Lei

Liu, Linlin

Liu, Linqing

Liu, Luyang

Liu, Ming

Liu, Minqian

Liu, Nayu

Liu, Nelson F.

Liu, Peng

Liu, Qian

Liu, Qian

Liu, Qianying

Liu, Shuaiqi

Liu, Siyang

Liu, Song

Liu, Tianyuan

Liu, Wenqiang

Liu, Xianggen

Liu, Xiangyang

Liu, Xiao

Liu, Xiao

Liu, Xiaoyuan

Liu, Xingxian

Liu, Xuebo

Liu, Xuye

Liu, Yang

Liu, Yang Janet

Liu, Ye

Liu, Ye

Liu, Yijia

Liu, Yiren

Liu, Yixin

Liu, Yizhu

Liu, Yong

Liu, Yongbin

Liu, Yongfei

Liu, Yonghao

Liu, Yongkang

Liu, Yuanxin

Liu, Zechun

Liu, Zeming

Liu, Zequn

Liu, Zeyu

Liu, Zhe

Liu, Zhenghao

Liu, Zhengyuan

Liu, Zhengzhong

Liu, Zhijian

Liu, Zihan

Liu, Zitao

Liu, Zuozhu

Ljubeˇ si´ c, Nikola

Lo, Kuan-Chieh

Lo, Kyle

Logan Iv, Robert L

Logeswaran, Lajanugen

Kashyap, Abhay Lokesh

Lolive, Damien

Long, Guodong

Long, Shangbang

Long, Yunfei

Lopes, Lucelene

Lopes, Marcos

Lopes Cardoso, Henrique

Lopez De Lacalle, Oier

Lopez Monroy, Adrian Pastor

Lorge, Isabelle

Lou, Chao

Lou, Jian-Guang

Lou, Renze

Louka-chevitch, Natalia

Loukina, Anastassia

Loureiro, Daniel

Lourie, Nicholas

Loyola, Pablo

Lu, Di

Lu, Hongyuan

Lu, Jianqiao

Lu, Jinghui

Lu, Jinliang

Lu, Junru

Lu, Pan

Lu, Peng

Lu, Weiming

Lu, Wenpeng

Lu, Xiaolei

Lu, Yao

Lu, Yaojie

Lu, Yu

Lu, Yu

Lu, Yujie

Lubis, Nurul

Lukin, Stephanie M.

Luu, Gunnar

Luo, Cheng

Luo, Haoran

Luo, Haozheng

Luo, Hongyin

Luo, Jiaming

Luo, Jiebo

Luo, Junyu

Luo, Ling

Luo, Man

Luo, Renqian

Luo, Ruipu

Luo, Wencan

Luo, Zhunchen

Luo, Ziyang

Luu, Kelvin

Lv, Qi

Lyu, Chenyang

Lyu, Weimin

Lyu, Yajuan

Lyu, Yougang

M’hamdi, Meryem

Ma, Chenkai

Ma, Chunpeng

Ma, Congbo

Ma, Danni

Ma, Huifang

Ma, Kaixin

Ma, Longxuan

Ma, Mingyu Derek

Ma, Qianli

Ma, Ruotian

Ma, Tengfei

Ma, Wei-Yun

Ma, Weizhi

Ma, Xinyin

Ma, Yubo

Ma, Yukun

Ma, Zhanyu

Ma, Ziqiao

Macháˇ cek, Dominik

Macherey, Wolfgang

Macina, Jakub

Madaan, Aman

Madasu, Avinash

Maddela, Mounica

Madureira, Brielen

Mager, Manuel

Magnini, Bernardo

Mahendra, Rahmad

Maheshwari, Ayush

Mahowald, Kyle

Maier, Wolfgang

Maillard, Jean

Majumder, Bodhisattwa Prasad

Majumder, Navonil

Mak-rai, Márton

Malakasiotis, Prodromos

Mali, Ankur

Malkiel, Itzik

Malko, Anton

Malykh, Valentin

Mamou, Jonathan

Mandal, Arpan

Maneriker, Pranav

Manning, Emma

Manotas, Irene

Mansimov, Elman

Mansour, Saab

Manuvinakurike, Ramesh

Manzoor, Emaad

Mao, Jiaxin

Mao, Kelong

Mao, Rui

Mao, Wenji

Mao, Yuning

Mao, Zhendong

Mao, Zhiming

Mao, Zhuoyuan

Mardziel, Piotr

Margatina, Katerina

Marin, Alex

Marras, Mirko

Marrese-Taylor, Edison

Marro, Santiago

Martelli, Federico

Martínez Cámara, Eugenio

Martínez Garcia, Eva

Martínez Lorenzo, Abelardo Carlos

Fernando Martínez-Plumed, Juan Martinez-Romo, Bruno Martins, Pedro Henrique Martins, David Martins De Matos, Luisa M¨ arz, Laura Mascarell, Lambert Mathias, Sandeep Mathias, Sergio Matos, Yuichiroh Matsubayashi, Yuji Matsumoto, Takuya Matsuzaki, Evgeny Matusov, Borislav Mavrin, Jonathan May, Tobias Mayer, Joshua Maynez, Amir Mazaheri, Sahisnu Mazumder, Alessandro Mazzei, R. Thomas McCoy, Nick Mckenna, Paul Mcnamee, Quentin Meeus, Alexander Mehler, Ninareh Mehrabi, Nikhil Mehta, Sanket Vaibhav Mehta, Clara Meister, Dheeraj Mekala, Julia Mendelsohn, Erick Mendez Guzman, Arul Menezes, Telmo Menezes, Chuan Meng, Rui Meng, Yu Meng, Yuanliang Meng, Zhao Meng, Samuel Mensah, William Merrill, Mohsen Mesgar, Kourosh Meshgi, Eleni Metheniti, Lars Meyer, Adam Meyers, Ivan Vladimir Meza Ruiz, Yisong Miao, Alessio Miaschi, Antonio Valerio Miceli Barone, Timothee Mickus, Lesly Miculicich, Margot Mieskes, Todor Mihaylov, Nandana Mihindukulasooriya, Simon Mille, Timothy Miller, Hye-Jin Min, Koji Mineshima, Gosse Minnema, Andrei Mircea, Seyedabolghasem Mirroshandel, Paramita Mirza, Maryam Sadat Mirzaei, Abhijit Mishra, Pushkar Mishra, Shubhanshu Mishra, Siddhartha Mishra, Kanishka Misra, Masato Mita, Mitch Mithun, Ashish Mittal, Sarthak Mittal, Vibhu Mittal, Yasuhide Miura, Tong Mo, Yijun Mo, Daichi Mochihashi, Daniela Moctezuma, Ali Modarressi, Sandip Modha, Hans Moen, Aditya Mogadala, Nikita Moghe, Hosein Mohebbi, Behrang Mohit, Mrinal Mohit, Afroz Mohiuddin, Tasnim Mohiuddin, Michael Mohler, Luis Mojica De La Vega, Negar Mokhberian, Diego Molla, Nicholas Monath, Sneha Mondal, Helena Moniz, Ali Montazeralghaem, Manuel Montes, Johanna Monti, Hyeonseok Moon, Jihyung Moon, Lori Moon, Raymond Mooney, Jared Moore, Richard Moot, Mehrad Moradshahi, Goncalo Mordido, Erwan Moreau, Antonio Moreno-Ortiz, Antonio Moreno-Sandoval, Mathieu Morey, Yusuke Mori, Véronique Moriceau, Emmanuel Morin, Gaku Morio, Makoto Morishita, John Morris, Marius Mosbach, Larry Moss, Xiangyang Mou, Maximilian Mozes, Frank Mtumbuka, Jesse Mu, Aaron Mueller, David Mueller, Aldrian Obaja Muis, Shashank Mujumdar, Animesh Mukherjee, Rajdeep Mukherjee, Matthew Mulholland, Benjamin Muller, Mathias M¨ uller, Philippe Muller, Max M¨ uller-Eberstein, Emir Munoz, Rafael Mu˜ noz Guillena, Saliha Muradoglu, Koji Murakami, Deepak Muralidharan, Yugo Murawaki, Kenton Murray, Rudra Murthy, Shikhar Murty, Karthik Murugadoss, Skatje Myers, Agnieszka Mykowiecka, Sheshera Mysore, Anandhavelu N, Seung-Hoon Na, Nona Naderi, Seema Nagar, Masaaki Nagata, Aakanksha Naik, Saeed Najafi, Tetsuji Nakagawa, Yukiko Nakano, Yuta Nakashima, Hideki Nakayama, Christoforos Nalmpantis, Sung-jin Nam, Marcin Namysl, Subhrangshu Nandi, Abhilash Nandy, Tarek Naous, Diane Napolitano, Jason Naradowsky, Sharan Narasimhan, Tahira Naseem, Sudip Naskar, Alexis Nasr, Vivi Nastase, Borja Navarro-Colorado, Tapas Nayak, Mojtaba Nayyeri, Claire Nedellec, Carina Negreanu, Preksha Nema, Joshua Nemecek, Graham Neubig, Guenter Neumann, Aurélie Névéol, Mariana Neves, Hwee Tou Ng, Axel-Cyrille Ngonga Ngomo, Cam Tu Nguyen, Dang Tuan Nguyen, Dat Quoc Nguyen, Dong Nguyen, Duc-Vu Nguyen, Huy Nguyen, Huyen Nguyen, Kiet Nguyen, Nhung Nguyen, Thanh Nguyen, Thanh-Tung Nguyen, Trang Nguyen, Truc-Vien T. Nguyen, Trung Hieu Nguyen, Vincent Nguyen, Hoang-Quoc Nguyen-Son, Ansong Ni, Jianmo Ni, Jingwei Ni, Minheng Ni, Zhaoheng Ni, Eric Nichols, Garrett Nicolai, Massimo Nicosia, Feng Nie, Ping Nie, Shaoliang Nie, Zhijie Nie, Sofia Nikiforova, Dmitry Nikolaev, Nikola I. Nikolov, Vassilina Nikoulina, Iftitahu Nimah, Lasguido Nio, Noriki Nishida, Masaaki Nishino, Sergiu Nisioi, Malvina Nissim, Tong Niu, Xing Niu, Yulei Niu, Zheng-Yu Niu, Bill Noble, Mariana Noguti, Tadashi Nomoto, Armineh Nourbakhsh, Jekaterina Novikova, Pierre Nugues, Diarmuid Ó Séaghdha, Alexander O’connor, Brendan O’connor, Tim O’gorman, Stephen Obadinma, Jose Ochoa-Luna, Kemal Oflazer, Maciej Ogrodniczuk, Kelechi Ogueji, Tolulope Ogunremi, Alice Oh, Shin Ah Oh, Mayumi Ohta, Kiyonori Ohtake, Atul Kr. Ojha, Oleg Okun, Eda Okur, Amy Olex, Anais Ollagnier, Ali Omrani, Byung-Won On, Donovan Ong, Ethel Ong, Yasumasa Onoe, Juri Opitz, Abigail Oppong, Matan Orbach, Hadas Orgad, Riccardo Orlando, John E. Ortega, Pedro Ortiz Suarez, Yohei Oseki, Naoki Otani, Zhijian Ou, Hiroki Ouchi, Nedjma Ousidhoum, Nedjma Ousidhoum, Jessica Ouyang, Iris Oved, Lilja Øvrelid, Kehinde Owoeye, Deepak P, Trilok Padhi, Ankur Padia, Vishakh Padmakumar, Gustavo Paetzold, Artidoro Pagnoni, Vardaan Pahuja, Santanu Pal, Vaishali

Pal, Shriphani Palakodety, Chester Palen-Michel, Alexis Palmer, Alessio Palmero Aprosio, Shramay Palta, Junshu Pan, Xiang Pan, Xiaoman Pan, Yi-Cheng Pan, Youcheng Pan, Yu Pan, Yudai Pan, Artemis Panagopoulou, Alexander Panchenko, Mugdha Pandya, Liang Pang, Sheena Panthaplackel, Alessandro Panunzi, Isabel Papadimitriou, Pinelopi Papalampidi, Alexandros Papangelis, Nikos Papasarantopoulos, Paolo Papotti, Nikolaos Pappas, Emerson Paraiso, Bhargavi Paranjape, Letitia Parcalabescu, Antonio Pareja-Lora, Tanmay Parekh, Shantipriya Parida, Pierre-Henri Paris, Chaehun Park, Chan Young Park, Jun-Hyung Park, Jungsoo Park, Kunwoo Park, Seong-Bae Park, Seongmin Park, Seongsik Park, Shinwoo Park, Sunghyun Park, Sungjoon Park, Yannick Parmentier, Patrick Paroubek, Ankita Pasad, Lucia Passaro, Rebecca Passonneau, Ramakanth Pasunuru, Arkil Patel, Raj Patel, Roma Patel, Sapan Patel, Braja Gopal Patra, Jasabanta Patro, Parth Patwa, Manasi Patwardhan, Siddharth Patwardhan, Debjit Paul, Indraneil Paul, Shounak Paul, Adam Pauls, Nikita Pavlichenko, Ellie Pavlick, John Pavlopoulos, Siddhesh Pawar, Justin Payan, Pavel Pecina, Jiahuan Pei, Jiaxin Pei, Weiping Pei, Hao Peng, Hao Peng, Qianqian Peng, Qiwei Peng, Siyao Peng, Tao Peng, Wei Peng, Wei Peng, Wenjun Peng, Xutan Peng, Yifan Peng, Gerald Penn, Oren Pereg, Ethan Perez, Juan Antonio Perez-Ortiz, Gabriele Pergola, Charith Peris, Stanislav Peshtetliev, Denis Peskoff, Ben Peters, Slav Petrov, Miriam R. L. Petruck, Pavel Petrushkov, Maxime Peyrard, Sandro Pezzelle, Jonas Pfeiffer, Quang Nhat Minh Pham, Thang Pham, Jason Phang, Maciej Piasecki, Massimo Piccardi, Matúš Pikuliak, Nisha Pillai, Tiago Pimentel, Juan Pino, Leticia Pinto-Alva, Irina Piontkovskaya, Telmo Pires, Flammie Pirinen, Jakub Piskorski, Lidia Pivovarova, Daniel Platt, Laura Plaza, Flor Miriam Plaza-Del-Arco, Lahari Poddar, Massimo Poesio, Thierry Poibeau, Lucie Polakova, Marco Polignano, Senja Pollak, Maria Pontiki, Simone Paolo Ponzetto, Andrei Popescu-Belis, Maja Popović, Beatrice Portelli, Rafał Poświatek, Martin Potthast, Christopher Potts, Amir Pouran Ben Veyseh, Rohit Prabhavalkar, Shrimai Prabhumoye, Aniket Pramanick, Soumajit Pramanik, Animesh Prasad, Radityo Eko Prasojo, Adithya Pratapa, Pavel Pribáň, Prokopis Prokopidis, Piotr Przybyła, Michal Ptaszynski, Dongqi Pu, Ratish Surendran Puduppully, Rajkumar Pujari, Stephen Pulman, Hemant Purohit, Alberto Purpura, Matthew Purver, James Pustejovsky, Valentina Pyatkin, Ehsan Qasemi, Fanchao Qi, Ji Qi, Jianzhong Qi, Jingyuan Qi, Shuhan Qi, Siya Qi, Wang Qi, Weizhen Qi, Chen Qian, Hongjin Qian, Jing Qian, Yujie Qian, Zhong Qian, Yaqiong Qiao, Bosheng Qin, Bowen Qin, Chuan Qin, Jinghui Qin, Kechen Qin, Libo Qin, Yujia Qin, Jielin Qiu, Liang Qiu, Long Qiu, Xinying Qiu, Zhaopeng Qiu, Zimeng Qiu, Chen Qu, Tingyu Qu, Rakesh R. Menon, Ella Rabinovich, Alexandre Rademaker, Daniele Radicioni, Alessandro Raganato, Preethi Raghavan, Dinesh Raghu, Afshin Rahimi, Sunny Rai, Vyas Raina, Nishant Raj, Navid Rajabi, Hossein Rajaby Faghihi, Dheeraj Rajagopal, Kanagasabai Rajaraman, Taraka Rama, Heri Ramampiaro, Naveen Raman, Giulia Rambelli, Owen Rambow, Abhinav Ramesh Kashyap, Sahana Ramnath, Rita Ramos, Alan Ramponi, Tharindu Ranasinghe, Surangika Ranathunga, Priya Rani, Yanghui Rao, Okko Rasanen, Mohammad Sadegh Rasooli, Fedor Ratnikov, Vikas Raunak, Andrea Amelio Ravelli, Shauli Ravfogel, Manikandan Ravikiran, Srinivas Ravishankar, Bhanu Pratap Singh Rawat, Vipula Rawte, Soumya Ray, Jishnu Ray Chowdhury, Manny Rayner, Anastasiia Razdaibiedina, Yasaman Razeghi, Evgeniia Razumovskaia, Livy Real, Traian Rebedea, Gabor Recski, Hanumant Redkar, Michael Regan, Ines Rehbein, Georg Rehm, Machel Reid, Markus Reiter-Haas, Navid Rekabsaz, Da Ren, Feiliang Ren, Haopeng Ren, Liliang Ren, Pengjie Ren, Ruiyang Ren, Shuhuai Ren, Steven Rennie, Christian Retoré, Kiamehr Rezaee, Mehdi Rezagholizadeh, Ryokan Ri, Eugénio Ribeiro, Leonardo F. R. Ribeiro, Giuseppe Riccardi, Kyle Richardson, Caitlin Richter, Martin Riedl, Stefan Riezler, Davide Rigoni, Mattia Rigotti, Shruti Rijhwani, Matı̄ss Rikters, Fabio Rinaldi, Ruty Rinott, Annette Rios, Anthony Rios, Elijah Rippeth, Andrey Risukhin, Yara Rizk, Brian Roark, Alvaro Rodrigo, Melissa Roemmele, Morteza Rohanian, Mukesh Kumar Rohil, Mahdin Rohmatillah, Paul Roit, Lina M. Rojas Barahona, Roland Roller, Julia Romberg, Salvatore Romeo, Julien Romero, Srikanth Ronanki, Md Rashad Al Hasan Rony, Tanya Roosta, Rudolf Rosa, Domenic Rosati, Guy Rosin, Alexis Ross, Robert Ross, Sophie Rosset, Paolo Rosso, Guy Rotman, Hossein Rouhizadeh, Dmitri Roussinov, Rachel Edita Roxas, Aurko Roy, Shamik Roy, Soumyadeep Roy, Sumegh Roychowdhury, Jos Rozen, Antoine Roze

nknop, Yu-Ping Ruan, Susanna R¨ ucker, Koustav Rudra, Amina Rufai, Federico Ruggeri, Ramon Ruiz-Dolz, Mukund Rungta, Josef Ruppenhofer, Benjamin Ruppik, Thomas Ruprecht, Alexander Rush, Irene Russo, Piotr Rybak, Maciej Rybinski, Maria Ryskina, Hadeel Saadany, Arkadiy Saakyan, Caroline Sabty, Devendra Sachan, Fatiha Sadat, Farig Sadeque, Arka Sadhu, Philipp Sadler, Sahar Sadrizadeh, Mehrnoosh Sadrzadeh, Niloofar Safi Samghabadi, Sylvie Saget, Alsu Sagirova, Amrita Saha, Punyajoy Saha, Sougata Saha, Swarnadeep Saha, Tanay Kumar Saha, Tulika Saha, Saurav Sahay, G¨ ozde S ¸ ahin, Nihar Sahoo, Sovan Kumar Sahoo, Sunil Kumar Sahu, Surya Kant Sahu, Ananya Sai B, Oscar Sainz, Tarek Sakakini, Sakriani Sakti, Ander Salaberria, Julian Salazar, Elizabeth Salesky, Jonne Saleva, Avneesh Saluja, Tanja Samardˇ zi´ c, Rajhans Samdani, Younes Samih, I˜ naki San Vicente, Abhilasha Sancheti, Vicente Ivan Sanchez Carmona, Danae Sánchez Villegas, Víctor M. Sánchez-Cartagena, German Sanchis-Trilles, Mario S¨ anger, Ananth Sankar, Chinnadhurai Sankar, Scott Sanner, Sashank Santhanam, Andrea Santilli, Diana Santos, Rodrigo Santos, Bishal Santra, Sebastin Santy, Soumya Sanyal, Maarten Sap, Naomi Saphra, Ruhi Sarikaya, Efsun Sarioglu Kayi, Anoop Sarkar, Kamal Sarkar, Ritesh Sarkhel, Prathusha K Sarma, Prof. Shikhar Kumar Sarma, Gabriele Sarti, Kengatharaiyer Sarveswaran, Sheikh Sarwar, Felix Sasaki, Minoru Sasaki, Shota Sasaki, Ryohei Sasano, Giorgio Satta, Danielle Saunders, Ketki Savle, Guergana Savova, Apoorv Saxena, Michael Saxon, Asad Sayeed, Shigehiko Schamoni, Wout Schellaert, Frank Schilder, David Schlangen, Viktor Schlegel, Michael Sejr Schlichtkrull, J¨ org Schl¨ otterer, Helmut Schmid, Robin Schmidt, Patricia Schmidtova, Martin Schmitt, Tyler Schnoebelen, Stephanie Schoch, Annika Marie Schoene, Mirco Schoenfeld, Lenhart Schubert, Hendrik Schuff, William Schuler, Sabine Schulte Im Walde, Claudia Schulz, Hannes Schulz, Elliot Schumacher, Raphael Schumann, Sebastian Schuster, Ineke Schuurman, Jackson Scott, Kyle Seelman, Ethan Selfridge, Thibault Sellam, David Semedo, Nasredine Semmar, Cansu Sen, Srinivasan Sengamedu Hanumantha Rao, Ayan Sengupta, Shubhashis Sengupta, Rico Sennrich, Jaehyung Seo, Ronald Seoh, Yeon Seonwoo, Royal Sequiera, Sofia Serrano, Mahsa Shafaei, Stephen Shaffran, Simra Shahid, Omar Shaikh, Igor Shalyminov, Chao Shang, Mingyue Shang, Chenze Shao, Wei Shao, Yijia Shao, Yutong Shao, Ori Shapira, Aditya Sharma, Ashish Sharma, Piyush Sharma, Ser- ge Sharoff, Tatiana Shavrina, Shuaijie She, Artem Shelmanov, Aili Shen, Hua Shen, Jiaming Shen, Jianhao Shen, Sheng Shen, Shiqi Shen, Siqi Shen, Tianhao Shen, Xudong Shen, Yatian Shen, Ying Shen, Yongliang Shen, Yuming Shen, Zejiang Shen, Zhengyuan Shen, Emily Sheng, Qiang Sheng, Ashish Shenoy, Tom Sherborne, Botian Shi, Bowen Shi, Chen Shi, Jihao Shi, Kaize Shi, Ning Shi, Peng Shi, Tian Shi, Tianze Shi, Weijia Shi, Xiao Shi, Yangyang Shi, Zhan Shi, Zhouxing Shi, Tomohide Shibata, Hidetoshi Shimodaira, Jamin Shin, Seungjae Shin, Kazutoshi Shinoda, Takahiro Shinozaki, Keiji Shinzato, Prashant Shiralkar, Yow-Ting Shiue, Harry Shomer, Ziyi Shou, Mohit Shridhar, Ritvik Shrivastava, Dimitar Shterionov, Kai Shu, Raphael Shu, Kai Shuang, Zeren Shui, Alexander Shvets, Chenglei Si, Suzanna Sia, Anthony Sicilia, A.b. Siddique, Melanie Siegel, Ingo Siegert, Alejandro Sierra-Múnera, Ankur Sikarwar, Sandipan Sikdar, Andrew Silva, João Ricardo Silva, Danilo Silva De Carvalho, Fabrizio Silvestri, Stefano Silvestri, Robert Sim, Michel Simard, Patrick Simianer, Dharani Simma, Dan Simonson, Edwin Simpson, Jyotika Singh, Mayank Singh, Pranaydeep Singh, Thoudam Doren Singh, Sneha Singhania, Priyanka Sinha, Olivier Siohan, Amy Siu, Inguna Skadina, Gabriel Skantze, Victor Skobov, Aviv Slobodkin, Alisa Smirnova, David Smith, Noah A. Smith, Vésteinn Snæbjarnarson, Felipe Soares, Marco Antonio Sobrevilla Cabe- zudo, Artem Sokolov, Luca Soldaini, Amir Soleimani, Ilia Sominsky, Pia Sommerauer, Junyoung Son, Seonil (simon) Son, Youngseo Son, Haiyue Song, Haoyu Song, Hyeonho Song, Hyun-Je Song, Kai Song, Kaiqiang Song, Kaitao Song, Linfeng Song, Ran Song, Wei Song, Xiaohui Song, Yan Song, Yangqiu Song, Yifan Song, Zhenqiao Song, Sarvesh Soni, Shashank Sonkar, Taylor Sorensen, Ionut-Teodor Sorodoc, Alexey Sorokin, Daniil Sorokin, Anna Sotnikova, Xabier Soto, Sajad Sotudeh, Gerasimos Spanakis, Manuela Speranza, Andreas Spitz, Richard Sproat, Rachele Sprugnoli, Makesh Narsimhan Sreedhar, Mukund Srinath, Kavya Srinet, Balaji Vasan Srinivasan, Tejas Srinivasan, Vijay Srinivasan, Ankit Srivastava, Saurabh Srivastava, Efstathios Stamatatos, Dominik Stammbach, Karolina Stanczak, Marija Stanojevic, Gabriel Stanovsky, Katherine Sta-

saski, Manfred Stede, Julius Steen, Michal ˇ

Stefánik, Shane Steinert-Threlkeld, Georg Stemmer, Evgeny Stepanov, Zachary Stine, Regina Stodden, Niklas Stoehr, Alessandro Stolfo, Matthew Sto- ne, Shane Storks, Kevin Stowe, Marco Antonio Stranisci, Karl Stratos, Kristina Striegnitz, Phillip Str¨ obel, David Strohmaier, Jannik Str¨ otgen, Tomek Strzalkowski, Sara Stymne, Dan Su, Hsuan Su, Qi Su, Qinliang Su, Ruolin Su, Xin Su, Ying Su, Yixuan Su, Yusheng Su, Nishant Subramani, Katsuhito Sudoh, Saku Sugawara, Hiroaki Sugiyama, Kazunari Sugiyama, Yoshi Suhara, Zhifang Sui, Octavia S ¸ ulea, Elior Sulem, Md Arafat Sultan, Aixin Sun, Changzhi Sun, Chengjie Sun, Chenkai Sun, Guangzhi Sun, Haipeng Sun, Hao Sun, Hao Sun, Haohai Sun, Jian Sun, Jiao Sun, Ming Sun, Mingwei Sun, Qingfeng Sun, Renliang Sun, Shichao Sun, Simeng Sun, Tianxiang Sun, Weiwei Sun, Zewei Sun, Zhaoyue Sun, Zhiqing Sun, Dhanasekar Sundararaman, Mujeen Sung, Yi-Lin Sung, Yoo Yeon Sung, Hanna Suominen, Marek Suppa, Benjamin Suter, Mirac Suzgun, Sandesh Swamy, Stan Szpakowicz, Piotr Szyma´ nski, Ana¨ ıs Tack, Oyvind Tafjord, Shabnam Tafreshi, Dima Taji, Sho Takase, Ece Takmaz, George Tambouratzis, Aleˇ s Tamchyna, Aniruddha Tammewar, Akihiro Tamura, Chao-Hong Tan, Haochen Tan, Hongye Tan, Samson Tan, Wei Tan, Xiao Tan, Ryota Tanaka, Karan Taneja, Buzhou Tang, Chengguang Tang, Gongbo Tang, Hao Tang, Jialong Tang, Raphael Tang, Shuai Tang, Tianyi Tang, Wei Tang, Xiangyun Tang, Xuemei Tang, Xunzhu Tang, Yun Tang, Yun Tang, Zheng Tang, Simon Tannert, Chaofan Tao, Wei Tao, Allahsera Auguste Tapo, Shiva Taslimipoor, Sandeep Tata, Michiaki Tatsubori, Marta Tatu, Simone Tedeschi, Selma Tekir, Serra Sinem Tekiro˘ glu, Zhiyang Teng, Ian Tenney, Alberto Testoni, Joel Tetreault, Martin Teuffenbach, Kapil Thadani, Katherine Thai, Urmish Thakker, Surendrabikram Thapa, Avijit Thawani, Anton Thielmann, Krishnaprasad Thirunarayan, Brian Thompson, Jana Thompson, Craig Thomson, Sam Thomson, David Thulke, Chang Tian, Yuanhe Tian, Zhiliang Tian, Zuoyu Tian, J¨ org Tiedemann, Christoph Tillmann, Tiago Timponi Torrent, Prayag Tiwari, Amalia Todirascu, Nadi Tomeh, Nicholas Tomlin, Antonio Toral, Cagri Toraman, Manabu Torii, Kentaro Torisawa, Juan-Manuel Torres-Moreno, Lucas Torroba Hennigen, Shubham Toshniwal, Samia Touileb, Yannick Toussaint, Benjamin Towle, Amine Trabelsi, Khanh Tran, Trang Tran, Marcos Treviso, Jan Trienes, Bayu Distiawan Trisedya, Harsh Trivedi, Enrica Troiano, Chen-Tse Tsai, Adam Tsakalidis, Bo-Hsiang Tseng, Ioannis Tsiamas, Masaaki Tsuchida, Oren Tsur, Satoshi Tsutsui, Jingxuan Tu, Kewei Tu, Lifu Tu, Yunbin Tu, Yi-Lin Tuan, Marco Turchi, Ferhan Ture, Elena Tutubalina, Rutuja Ubale, Ana Sabina Uban, Adrian Ulges, Eddie Ungless, Bhargav Upadhyay, Kartikeya Upasani, Olga Uryupina, Asahi Ushio, Dmitry Ustalov, Ahmet ¨ Ust¨ un, Masao Utiyama, Venktesh V, Saujas Vaduguru, Ashwini Vaidya, Marco Valentino, Gisela Vallejo, Jannis Vamvas, Tim Van De Cruys, Antal Van Den Bosch, Rob Van Der Goot, Daan Van Esch, Josef Van Genabith, Emiel Van Miltenburg, Rik Van Noord, Vincent Vandeghinste, Keith Vanderlinden, David Vandyke, Natalia Vanetik, Eva Vanmassenhove, Daniel Varab, Francielle Vargas, Siddharth Varia, Neeraj Varshney, Rossella Varvara, Siddharth Vashishtha, Jake Vasilakes, Eva Maria Vecchi, Nikhita Vedula, Aswathy Velutharambath, Giulia Venturi, Gaurav Verma, Rakesh Verma, Yannick Versley, Anvesh Rao Vijjini, David Vilares, Jesús Vilares, Manuel Vilares Ferro, Martina Vilas, Veronika Vincze, Lucas Vinh Tran, Sami Virpioja, Juraj Vladika, Nikolai Vogler, Rob Voigt, Pius Von D¨ aniken, Spencer Von Der Ohe, Nikos Voskarides, Ali Vosoughi, Pavlos Vougiouklis, Thuy Vu, Thuy-Trang Vu, Yogarshi Vyas, Akifumi Wachi, Takashi Wada, Joachim Wagner, Jan Philip Wahle, Hiromi Wakaki, David Wan, Stephen Wan, Xingchen Wan, Yao Wan, Yu Wan, Ante Wang, Bailin Wang, Bang Wang, Baoxin Wang, Baoxun Wang, Beilun Wang, Benyou Wang, Bin Wang, Bin Wang, Bingqing Wang, Bingyu Wang, Bo Wang, Bo Wang, Boxin Wang, Chao Wang, Chengyi Wang, Chengyu Wang, Chuan-Ju Wang, Chunliu Wang, Cunxiang Wang, Dingquan Wang, Fei Wang, Guangrun Wang, Guoyin Wang, Hai Wang, Han Wang, Han Wang, Han Wang, Hanrui Wang, Hao Wang, Haobo Wang, Haoyu Wang, Haoyu Wang, Haoyu Wang, Heyuan Wang, Hong Wang, Hongfei Wang, Hsin-Min Wang, Huimin Wang, Jiaan Wang, Jian Wang, Jianing Wang, Ji-anyu Wang, Jianzong Wang, Jiayi Wang, Jie Wang, Jin Wang, Jin Wang, Jinpeng Wang, Jue Wang, Jun Wang, Lei Wang, Liang Wang, Lidan Wang, Lingzhi Wang, Longshaokan Wang, Longyue Wang, Meiqi Wang, Peifeng Wang, Pidong Wang, Ping Wang, Qiang Wang, Qingyun Wang, Qiqi

xxv

# Wang, Rui

Wang, Rui Wang, Runze Wang, Ryan Wang, Shufan Wang, Shuhe Wang, Shuo Wang, Sijia Wang, Sirui Wang, Tao Wang, Tianduo Wang, Tianlu Wang, Wei Wang, Wen Wang, Wenping Wang, Wenxuan Wang, Wenya Wang, Xiangdong Wang, Xiao Wang, Xiaojie Wang, Xiaolin Wang, Xiaozhi Wang, Xin Wang, Xindi Wang, Xing Wang, Xingjin Wang, Xintong Wang, Xinyi Wang, Xinyu Wang, Xuewei Wang, Xun Wang, Yan Wang, Yanlin Wang, Yanshan Wang, Ye Wang, Ye Wang, Yibo Wang, Yifan Wang, Yigong Wang, Yihan Wang, Yiwei Wang, Yizhong Wang, Yue Wang, Yun Cheng Wang, Yuxuan Wang, Zekun Wang, Zhaowei Wang, Zhen Wang, Zheng Wang, Zheng Wang, Zhenhailong Wang, Zhenyi Wang, Zhichun Wang, Zhiguang Wang, Zhilin Wang, Zhiqiang Wang, Zhiwei Wang, Zhuoer Wang, Zhuoyi Wang, Zifeng Wang, Zihan Wang, Zihan Wang, Zihao Wang, Zijian Wang, Zijie Wang, Zilong Wang, Zirui Wang, Prashan Wanigasekara, Leo Wanner, Alex Warstadt, Cedric Waterschoot, Julia Watson, Bonnie Webber, Leon Weber, Albert Webson, Kellie Webster, Tharindu Cyril Weerasooriya, Chengkun Wei, Jerry Wei, Lingwei Wei, Penghui Wei, Tianxin Wei, Xiangpeng Wei, Xiaochi Wei, Shira Wein, Nathaniel Weir, Henry Weld, Orion Weller, Marion Weller-Di Marco, Simon Wells, Bingyang Wen, Haoyang Wen, Jiaxin Wen, Liang Wen, Lijie Wen, Rongxiang Weng, Lukas Wertz, Peter West, Matthijs Westera, Jennifer C. White, Richard Wicentowski, Michael Wiegand, Ethan Wilcox, Rodrigo Wilkens, Bram Willemsen, Ronald Wilson, Shomir Wilson, Steven Wilson, Grégoire Winterstein, Shuly Wintner, Sam Wiseman, Guillaume Wisniewski, Emilia Wisnios, Tomer Wolfson, Marcin Woliński, Diedrich Wolter, Derek F. Wong, Ka Ho Wong, Raymond Wong, Tak-Lam Wong, Alina Wróblewska, Anna Wroblewska, Anne Wu, Bowen Wu, Changxing Wu, Chen Wu, Chen Henry Wu, Chien-Sheng Wu, Chuhan Wu, Di Wu, Di Wu, Fangzhao Wu, Hua Wu, Hui Wu, Junda Wu, Ledell Wu, Lianwei Wu, Linzhi Wu, Shengqiong Wu, Shih-Hung Wu, Sixing Wu, Stephen Wu, Te-Lin Wu, Tianxing Wu, Ting-Wei Wu, Weibin Wu, Wenhao Wu, Winston Wu, Xian Wu, Xianchao Wu, Xin Wu, Xixin Wu, Yang Wu, Yangjun Wu, Yaoyao Wu, Yike Wu, Yimeng Wu, Youzheng Wu, Yu Wu, Yuanbin Wu, Yuexin Wu, Yunfang Wu, Yuting Wu, Yuxiang Wu, Zeqiu Wu, Zhaofeng Wu, Zhen Wu, Zhijing Wu, Zhiyong Wu, Zhiyong Wu, Zhizheng Wu, Zhuofeng Wu, Zihao Wu, Zixiu Wu, Jian Xi, Zhaohan Xi, Fei Xia, Menglin Xia, Mengzhou Xia, Patrick Xia, Qingrong Xia, Yingce Xia, Anhao Xiang, Jiannan Xiang, Suncheng Xiang, Changrong Xiao, Chaojun Xiao, Chunyang Xiao, Jinfeng Xiao, Jing Xiao, Jinghui Xiao, Min Xiao, Yanghua Xiao, Zhaomin Xiao, Jun Xie, Kaige Xie, Ning Xie, Ruobing Xie, Shangyu Xie, Yiqing Xie, Yuqing Xie, Yuxi Xie, Zhiwen Xie, Zhouhang Xie, Ji Xin, Chen Xing, Linzi Xing, Zhenchang Xing, Bo Xiong, Chao Xiong, Jing Xiong, Kai Xiong, Wenhan Xiong, Binfeng Xu, Boyan Xu, Canwen Xu, Chen Xu, Chenchen Xu, Chunpu Xu, Dongfang Xu, Fan Xu, Fangyuan Xu, Frank F. Xu, Guandong Xu, Guangyue Xu, Hanzi Xu, Hongfei Xu, Hongzhi Xu, Jiacheng Xu, Jiashu Xu, Jin Xu, Jinan Xu, Jitao Xu, Jun Xu, Kang Xu, Keyang Xu, Kun Xu, Lei Xu, Lu Xu, Mingbin Xu, Mingzhou Xu, Nan Xu, Peng Xu, Peng Xu, Qiongkai Xu, Ruifeng Xu, Ruochen Xu, Shicheng Xu, Wang Xu, Weiran Xu, Weiwen Xu, Wenda Xu, Wenduan Xu, Xiao Xu, Xinnuo Xu, Yan Xu, Yang Xu, Yang Xu, Yi Xu, Yige Xu, Yiheng Xu, Yumo Xu, Zhen Xu, Zhenhui Xu, Zhichao Xu, Zhiyang Xu, Fuzhao Xue, Nianwen Xue, Shan Xue, Deshraj Yadav, Prateek Yadav, Yadollah Yaghoobzadeh, Bryce Yahn, Ikuya Yamada, Ivan Yamshchikov, An Yan, Hang Yan, Hanqi Yan, Jianhao Yan, Jun Yan, Lingyong Yan, Xifeng Yan, Xu Yan, Zhao Yan, Hitomi Yanaka, An Yang, Cheng Yang, Dejie Yang, Eugene Yang, Fan Yang, Guanqun Yang, Haoran Yang, Jian Yang, Jian Yang, Jianing Yang, Jie Yang, Jingfeng Yang, Jun Yang, Kexin Yang, Li Yang, Liner Yang, Linyi Yang, Liu Yang, Longfei Yang, Nan Yang, Sen Yang, Songlin Yang, Tsung-Yen Yang, Wei Yang, Wenmian Yang, Xianjun Yang, Xiaocong Yang, Yaqin Yang, Yazheng Yang, Yiben Yang, Yinfei Yang, Yuanhang Yang, Yue Yang, Zhao Yang, Zixiaofan Yang, Zonglin Yang, Ken Yano, Tae Yano, Barry Yao, Bingsheng Yao, Liang Yao, Peiran Yao, Zijun Yao, Mahsa Yarmohammadi, Bingyang Ye, Fanghua Ye, Hai Ye, Jiacheng Ye, Jiasheng Ye, Junjie Ye, Muchao Ye, Qinyuan Ye, Rong Ye, Seonghyeon Ye, Wei Ye, Wenting Ye, Xi Ye, An-Zi Yen, Jinyoung Yeo, Yu Ting Yeung, Jingwei Yi, Xiaoyuan Yi, Wen-Wai Yim, Seid Muhie Yimam, Chuantao Yin, Congchi Yin, Fan Yin, Kayo Yin, Qingyu Yin, Wenjie Yin, Xuwang Yin, Yu Yin, Yuwei Yin, Jiahao Ying, Anssi Yli-Jyra, Michael Yoder, Hikaru Yokono, Zheng Xin

# Secondary Reviewers

Sharon Adar, Sneha Agarwal, Utkarsh Agarwal, Akiko Aizawa, Christopher Akiki, Ilseyar Ali-

mova, Falah Amro, Miriam Anschütz, William Armstrong, Yuya Asano, Md Rabiul Awal, Ansar Aynetdinov, Andrea Bacciu, Yinhao Bai, Oliver Baumann, Alessandro De Bellis, Guillaume Le Berre, Marie Bexte, Hanoz Bhathena, Abari Bhattacharya, Mukul Bhutani, Verena Blaschke, Moritz Blum, Marc Brinner, Reynier Ortega Bueno, Kishan K C, Mingchen Cai, Yucheng Cai, Paul Caillon, Eduardo Calò, Marco Casavantes, Giulia Cassara, Roman Castagné, Brittany Cates, Amanda Chan, Ayon Chattopadhyay, Huiyao Chen, Liang Chen, Pei Chen, Tianyu Chen, Tongfei Chen, Weidong Chen, Xi Chen, Xingyu Chen, Yuan Chen, Yue Chen, Zhenghan Chen, Zhi Chen, Zhijia Chen, Zhikai Chen, Zifeng Cheng, Jae Sook Cheong, Lin Lee Cheong, Yan Kin Chi, Hanjun Cho, Eunsenog Choi, Sahil Chopra, Rennan Cordeiro, Matthias Cosler, Adrian Cosma, Liam Cripwell, Yudivián Almeida Cruz, Israel Cuevas, Shih-Chieh Dai, Yinpei Dai, Parag Dakle, Niklas Deckers, Zhongfen Deng, Sourabh Deoghare, Simma Dharani, Harshita Diddee, Qiuyu Ding, Yuning Ding, Zixiang Ding, Mingwen Dong, Kefei Duan, Fanny Ducel, Tobias Eder, Pavel Efimov, Suilan Estevez-Velarde, Saad Ezzini, Maurice Falk, Meng Fan, Ziwei Fan, Qingkai Fang, Mohsen Fayyaz, James Finch, Sarah Finch, Sheema Firdous, Martina Forster, Cady Gansen, Alberto Gasparin, Qiming Ge, Shiping Ge, Kinga Gémes, Lei Geng, Yaroslav Getman, Sadaf Ghaffari, Sarvjeet Singh Ghotra, Lukas Gienapp, Jonas Golde, Mahsa Goodarzi, Shuhao Gu, Gael Guibon, Mika Hämäläinen, Kelvin Han, Shiyi Han, Yu Han, Sami Ul Haq, Bradley Hauer, Hui He, Junyi He, Yunjie He, Zhiwei He, Julien Heitmann, Alexander Henlein, Ondřej Herman, Xanh Ho, Julian Hoellig, Chun-Cheng Hsieh, Echo Hu, Langlin Huang, Shih-Cheng Huang, Shuyan Huang, Yerin Hwang, Radu Cristian Alexandru Iacob, Etsuko Ishii, Itay Itzhak, Adam Ivankay, Nazanin Jafari, Anubhav Jangra, Seongjun Jeong, Tianbo Ji, Qi Jia, Yiren Jian, Chengyue Jiang, Junfeng Jiang, Yiwei Jiang, Hailong Jin, Omisa Jinsi, Richard Jonker, Minjoon Jung, Danial Kamali, Jeongwoo Kang, Beatrice Kanyi, Abhinav Ramesh Kashyap, Prachuryya Kaushik, Joschka Kersting, Shamir Khandaker, Aditi Khandelwal, Niama El Khbir, Sopan Khosla, Mohammad Khosravani, Hajung Kim, Hyunjong Kim, Jeonghwan Kim, Jiwoo Kim, Seungone Kim, Yongil Kim, Youngbin Kim, Chaitanya Kirti, Xenia Klinge, Erik Körner, Ádám Kovács, Vojtěch Kovář, Shachi H Kumar, Vivek Kumar, Gitanjali Kumari, Maddalen López De Lacalle, Jack Lanchantin, Loic De Langhe, Anna Laskina, Chaeeun Lee, Dongryeol Lee, Kang-Il Lee, Sunkyung Lee, Yongjae Lee, Els Lefever, Zhihong Lei, Elisa Leonardelli, Hang Li, Jiazhao Li, Junlong Li, Mengyu Li, Minghan Li, Senyu Li, Shiyang Li, Shuqin Li, Wenyan Li, Xinhang Li, Yan Li, Yichen Li, Yichuan Li, Yunshui Li, Zekun Li, Zhaoqun Li, Zhuoqun Li, Zitong Li, Zhenwen Liang, Ruotong Liao, Boda Lin, Jiuheng Lin, Hali Lindsay, Alisa Liu, Andy T. Liu, Hong Liu, Hongyi Liu, Huijun Liu, Mengying Liu, Zhexiong Liu, Alessandro Locaputo, Roberto López, Sebastian Lopez-Cot, Yuze Lou, Xuantao Lu, Kamile Lukosiute, Gunnar Lund, Chu Fei Luo, Haoran Luo, Xin Lv, Congbo Ma, Da Ma, Andrew Mackey, Hiren Madhu, Daniele Malitesta, Oscar Mañas, Fabienne Marco, Salima Mdhaffar, Marek Medveď, Nikhil Mehta, Di Mei, Althis Mendes, Augusto Mendes, Stefano Menini, Elena Merdjanovska, Hossein Mohammadi, Samraj Moorjani, Yusuke Mori, Durgesh Nandini, Gaurav Negi, Hoang Nguyen, Vincent Nguyen, Feng Nie, Anna Nikiforovskaya, Jingcheng Niu, Gibson Nkhata, Rik Van Noord, Michael Ogezi, Olubusayo Olabisi, Katrina Olsen, Talgat Omarov, Andreas Opedal, Junshu Pan, Suehyun Park, Daraksha Parveen, Maya Pavlova, Diogo Pernes, Jan Pfister, Alejandro Piad-Morffis, Max Ploner, Alexander Podolskiy, Dejan Porjazovski, Pradyot Prakash, Adrien Pupier, Maarten De Raedt, Pétur Orri Ragnarsson, Sai Krishna Rallabandi, Leonardo Ranaldi, Abhinav Rao, Anton Razzhigaev, Sebastian Reimann, Raphael Reinauer, François Remy, Jiaqian Ren, Siyu Ren, Akseli Reunamo, Valentin Richard, Ruty Rinott, Elsa Rizk, Giulia Rizzi, Sean Robertson, Cristian Rodriguez, Sudipta Singha Roy, Susanna Rücker, Elena Sofia Ruzzetti, Tasnim Kabir Sadik, Joy Sain, Jose Ignacio Abreu Salas, Hossein Salemi, Mufan Sang, Twisampati Sarkar, Simone Scaboro, Felix Schmidt, Frederik Schmitt, Christopher Schröder, Simeon Schüz, Nina Seemann, Vincent Segonne, Yasas Senarath, Ashish Seth, Silvio Severino, Lele Sha, Stephen Shaffran, Anastassia Shaitarova, Hee Ming Shan, Kai Shen, Xingyu Shen, Shuqian Sheng, Kaize Shi, Ke Shi, Yuanjun Shi, Yuxuan Shu, Lucas Dos Santos Silva, Harmanpreet Singh, Pranaydeep Singh, Salam Michael Singh, Iustin Sirbu, Sonish Sivarajkumar, Mohamed Soliman.

# Chenyang Song, Kunzhe Song, William Soto, Florian Steuber, Manuel Stoeckel, Vit Suchomel, Bin Sun, Changzhi Sun, Cong Sun, Jingdong Sun, Qiujie Sun, Xiaohui Sun, Xueyao Sun, Shahbaz Syed, Zhaoxuan Tan, Shaowen Tang, Ziming Tang, Kumar Tanmay, Jingxuan Tu, Sichang Tu, Xiao Chi Tu, Mehmet Deniz Turkmen, Sagar Uprety, Hannah Vanderhoeven, Julien Velcin, Elad Venezian, Radhakrishnan Venkatakrishnan, Ivo Vigan, Fedor Vitiugin, Nikolas Vitsakis, Xiangpeng Wan, An Wang, Bingyu Wang, Cong Wang, Haoran Wang, Hu Wang, Junlin Wang, Junting Wang, Ke Wang, Lei Wang, Lingzhi Wang, Qianli Wang, Ruofan Wang, Shih-Heng Wang, Teng Wang, Weizhi Wang, Xinyou Wang, Yigong Wang, Yiming Wang, Yueguan Wang, Zihao Wang, Haitian Wei, Martyna Wiacek, Ronald Wilson, Moritz Wolf, Haibin Wu, Jay Zhangjie Wu, Jian Wu, Yexin Wu, Yuan-Kuei Wu, Siyuan Xiang, Yang Xiao, Yao Xiao, Zhouhang Xie, Benfeng Xu, Chenwei Xu, Kaishuai Xu, Yuzhuang Xu, Zhichao Xu, Zhiyang Xu, Bo Xue, Siyuan Xue, Xiaojun Xue, Baosong Yang, Kaiqi Yang, Shiping Yang, Yanjie Yang, Yinguan Yang, Jiarui Yao, Bingyang Ye, Yongjing Yin, Yuwei Yin, Tarik Yousef, Guoxin Yu, Nan Yu, Tiezheng Yu, Zhengqing Yuan, Klim Zaporojets, Urchade Zaratiana, Omnia Zayed, Weihao Zeng, Ge Zhang, Hanlei Zhang, Jingyu Zhang, Le Zhang, Mian Zhang, Qi Zhang, Ruike Zhang, Songyang Zhang, Tao Zhang, Weijia Zhang, Yidan Zhang, Yunan Zhang, Zhiling Zhang, Ziheng Zhang, Ziqiing Zhang, Ziqing Zhang, Honghong Zhao, Jiahao Zhao, Jinman Zhao, Siyang Zhao, Wei Zhao, Xingmeng Zhao, Yingxiu Zhao, Yu Zhao, Gui Zhen, Kangjie Zhen, Kai Zheng, Kaiwen Zhou, Terry Zhou, Zhengping Zhou, Zhijie Zhou, Ming Zhu, Zhihong Zhu, Haojie Zhuang, Anni Zou

# Keynote Talk: Two Paths to Intelligence

# Geoffrey Hinton

# University of Toronto (emeritus)

Monday, July 10 – Time: 9:30 - 10:30 EDT – Room: Metropolitan

# Abstract:

I will briefly describe the forty year history of neural net language models with particular attention to whether they understand what they are saying. I will then discuss some of the main differences between digital and biological intelligences and speculate on how the brain could implement something like transformers. I will conclude by addressing the contentious issue of whether current multimodal LLMs have subjective experience.

# Bio:

Geoffrey Hinton received his PhD in Artificial Intelligence from Edinburgh in 1978. After five years as a faculty member at Carnegie-Mellon he became a fellow of the Canadian Institute for Advanced Research and moved to the University of Toronto where he is now an emeritus professor. He is also the Chief Scientific Adviser at the Vector Institute.

He was one of the researchers who introduced the backpropagation algorithm and the first to use backpropagation for learning word embeddings. His other contributions to neural network research include Boltzmann machines, distributed representations, time-delay neural nets, mixtures of experts, variational learning and deep learning. His research group in Toronto made major breakthroughs in deep learning that revolutionized speech recognition and object classification.

He is a fellow of the UK Royal Society and a foreign member of the US National Academy of Engineering, the US National Academy of Sciences and the American Academy of Arts and Sciences. His awards include the David E. Rumelhart prize, the IJCAI award for research excellence, the Killam prize for Engineering, the Royal Society Royal Medal, the NSERC Herzberg Gold Medal, the IEEE James Clerk Maxwell Gold medal, the NEC C&C award, the BBVA award, the Honda Prize and the Turing Award.

# Keynote Talk: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models

# Alison Gopnik

# University of California at Berkeley

# Wednesday, July 12 – Time: 14:00 - 15:00 EDT – Room: Metropolitan

# Abstract

Its natural to ask whether large language models like LaMDA or GPT-3 are intelligent agents. But I argue that this is the wrong question. Intelligence and agency are the wrong categories for understanding them. Instead, these Al systems are what we might call cultural technologies, like writing, print, libraries, internet search engines or even language itself. They are new techniques for passing on information from one group of people to another. Cultural technologies arent like intelligent humans, but they are essential for human intelligence. Many animals can transmit some information from one individual or one generation to another, but no animal does it as much as we do or accumulates as much information over time. New technologies that make cultural transmission easier and more effective have been among the greatest engines of human progress, but they have also led to negative as well as positive social consequences. Moreover, while cultural technologies allow transmission of existing information cultural evolution, which is central to human success, also depends on innovation, exploration and causal learning. Comparing LLM’s responses in prompts based on developmental psychology experiments to the responses of children may provide insight into which capacities can be learned through language and cultural transmission, and which require innovation and exploration in the physical world. I will present results from several studies making such comparisons.

# Bio

Alison Gopnik is a professor of psychology and affiliate professor of philosophy at the University of California at Berkeley, and a member of the Berkeley AI Research Group. She received her BA from McGill University and her PhD. from Oxford University. She is a leader in the study of cognitive science and of children’s learning and development and was one of the founders of the field of “theory of mind”, an originator of the “theory of cognitive development”, and the first to apply Bayesian probabilistic models to children’s learning. She has received both the APS Lifetime Achievement Cattell and William James Awards, the Bradford Washburn Award for Science Communication, and the SRCD Lifetime Achievement Award for Basic Science in Child Development. She is an elected member of the Society of Experimental Psychologists and the American Academy of Arts and Sciences and a Cognitive Science Society, American Association for the Advancement of Science, and Guggenheim Fellow. She was 2022-23 President of the Association for Psychological Science.

She is the author or coauthor of over 140 journal articles and several books including “Words, thoughts and theories” MIT Press, 1997, and the bestselling and critically acclaimed popular books “The Scientist in the Crib” William Morrow, 1999, “The Philosophical Baby; What children’s minds tell us about love,

truth and the meaning of life” 2009, and “The Gardener and the Carpenter” 2016, Farrar, Strauss and Giroux, the latter two won the Cognitive Development Society Best Book Prize in 2009 and 2016. Since 2013 she has written the Mind and Matter column for the Wall Street Journal and she has also written widely about cognitive science and psychology for The New York Times, The Economist, The Atlantic, The New Yorker, Scientific American, The Times Literary Supplement, The New York Review of Books, New Scientist and Slate, among others. Her TED talk on her work has been viewed more than 5.2 million times. She has frequently appeared on TV, radio and podcasts including “The Charlie Rose Show”, “The Colbert Report”, “Radio Lab” and “The Ezra Klein Show”. She lives in Berkeley with her husband Alvy Ray Smith and has three children and five grandchildren.

xxxii

# The Future of Computational Linguistics in the LLM Age

# Panel Discussion

Chair: Iryna Gurevych

Technische Universität at Darmstadt

Tuesday, July 11 - Time: 14:45-15:45

This is a panel discussion with:

- Dan Klein (UC Berkeley)
- Meg Mitchell (Hugging Face)
- Roy Schwartz (the Hebrew University of Jerusalem)

They will present short statements (5 to 7 min.) related to the main topic of the panel:

- New opportunities (e.g., artificial general intelligence, responsible NLP);
- Technical challenges (e.g., multimodality, instruction-tuning, etc.);
- Real life problems & societal implications (e.g., hallucinations, biases, future job market);
- LLMs and the future of NLP; and
- Open-science vs. commercial LLMs.

Followed by discussion with the panel and audience.

# Memorial: Dragomir Radev

Tuesday, July 11, 2023 - Room: Metropolitan - Time: 13:00–13:30

Dragomir Radev, the A. Bartlett Giamatti Professor of Computer Science at Yale University, passed away this year on Wed, March 29th. Drago contributed in substantial ways to research in NLP, to the organization of the ACL and to mentoring the next generation of computational linguists. Drago’s role in our ACL community spans four decades. He was recognized for his work over this period through his selection as an ACL Fellow in 2018 for his significant contributions to text summarization and question answering, and through his receipt of the Distinguished ACL Service Award in 2022. In this session, speakers from different time periods of his life will discuss his contributions to the field and the impact his life had on so many of us.

xxxiv

# Ethics Panel

Kar¨ en Fort, Min-Yen Kan and Yulia Tsvetkov, Luciana Benotti, Mark Dredze, Pascale Fung, Dirk Hovy, Jin-Dong Kim, Malvina Nissim

Tuesday, July 11, 2023 - Room: Pier 4&5 - Time: 16:15–17:45

We present our ACL Ethics Committee’s progress over the last few years. Of core interest, we will present the results of the ACL stakeholder survey about the role of ethics and ethics training exposure. Results from the survey respondents indicate that ethics is of primary interest to the community and that there is a mandate for the further creation and dissemination of ethics related training for authors, reviewers and event organisers. We will briefly review the survey results and feature a lengthed question and answer session in support of extended dialogue with our community. Our session will culminate through a dialogue with our session’s participants in a moderated panel that includes participation from the entire ethics committee.

xxxv

# Transitioning to Rolling Review Discussion

Mausam, Professor, IIT Delhi (ARR EIC), Jonathan K. Kummerfeld, Assistant Professor, University of Sydney (ARR CTO)

Tuesday, July 11, 2023 - Room: Metropolitan - Time: 14:15–14:45

This session will contain a presentation on progress in ARR over the past year and provide an opportunity for community questions and discussion.

xxxvi

# Program Chairs’ Report on Peer Review at ACL 2023

Anna Rogers♢

Marzena Karpinska♡

Jordan Boyd-Graber♠

Naoaki Okazaki♣

♢IT University of Copenhagen

♡University of Massachusetts Amherst

♠University of Maryland

♣Tokyo Institute of Technology

arog@itu.dk

mkarpinska@cs.umass.edu

jbg@umiacs.umd.edu

okazaki@c.titech.ac.jp

# Abstract

We present a summary of the efforts to improve conference peer review that were implemented at ACL’23. This includes work with the goal of improving review quality, clearer workflow and decision support for the area chairs, as well as our efforts to improve paper-reviewer matching for various kinds of non-mainstream NLP work, and improve the overall incentives for all participants of the peer review process. We present analysis of the factors affecting peer review, identify the most problematic issues that the authors complained about, and provide suggestions for the future chairs. We hope that publishing such reports would (a) improve transparency in decision-making, (b) help the people new to the field to understand how the *ACL conferences work, (c) provide useful data for the future chairs and workshop organizers, and also academic work on peer review, and (d) provide useful context for the final program, as a source of information for meta-research on the structure and trajectory of the field of NLP.

# 1 Introduction

With the continued growth of our field and the rising number of conference submissions, peer review draws more and more attention from the community—as an application area (Hua et al., 2019; Anjum et al., 2019; Stelmakh et al., 2019, inter alia), in meta-research (Rogers and Augenstein, 2020; Church, 2020, inter alia), in initiatives to organize and release peer review data (Kang et al., 2018; Jecmen et al., 2022; Dycke et al., 2022, inter alia), and, of course, in the regular heated social media discussions during submission deadlines, review release dates, and acceptance notifications. It is unlikely that peer review will ever be perfect – it remains ‘the least bad system’ we have for ensuring the quality of scientific publications (Smith, 2010). Still, with each iteration we should learn a little more about what works better for organizing peer review at such scale, and in a community so diverse in expertise and experience.

As a step in that direction, ACL’23 makes its peer review report public and an official part of the conference proceedings, complementing the introduction and other administrative materials. The goal is to increase the visibility of the results of the conference process, as well as any incidental findings from conference organizations and the lessons learned the hard way that may be useful to the future chairs and workshop organizers. Such publications also provide extra incentives for the future program chairs to invest more effort in the analysis of their process, and they provide a useful background to the composition of the final program that may be useful for meta-science research (since they essentially document the selection process for that program). Last but not least, such publications will improve the transparency of the *ACL conference process, which may be useful to the researchers who are new to the field.

We present the core statistics per track (§2), analysis of resubmissions (§3) and core demographics (§4), our efforts for improving peer review quality (§5), improving decision support for the chairs (§6), our analysis of various factors contributing to review scores and final decisions (§7), ethics review and best paper selection (§8), and our efforts towards improving incentives for the authors, reviewers and chairs (§9). We conclude with overall recommendations for future conference organizers (§10). The materials we developed will be available at a dedicated repository 1.

The results presented here are based on the analysis of internal data of ACL’23, as well as exit surveys that we sent to the chairs, authors and reviewers. We received responses from 25 senior area chairs (SACs).

# Tracks and Acceptance Statistics

|Track| |Direct submissions| | |ARR submissions|Submitted Main Findings|Submitted Main Findings|
|---|---|---|---|---|---|---|---|
|Computational Social Science and Cultural Analytics|113|22.12|19.47|10|90.00|10.00| |
|Dialogue and Interactive Systems|269|24.54|15.24|19|21.05|42.11| |
|Discourse and Pragmatics|52|21.15|34.62|1|100.00|0.00| |
|Ethics and NLP|54|22.22|31.48|7|42.86|42.86| |
|Generation|175|25.71|20.57|6|66.67|16.67| |
|Information Extraction|279|25.45|16.13|33|24.24|36.36| |
|Information Retrieval and Text Mining|94|14.89|21.28|9|44.44|0.00| |
|Interpretability and Analysis of Models for NLP|189|24.34|28.04|20|35.00|55.00| |
|Language Grounding to Vision, Robotics, and Beyond|147|24.49|21.77|5|40.00|40.00| |
|Large Language Models|252|28.17|21.03|10|50.00|30.00| |
|Linguistic Diversity|18|27.78|22.22|1|0.00|100.00| |
|Linguistic Theories, Cog. Modeling & Psycholinguistics|38|23.68|23.68|8|50.00|37.50| |
|Machine Learning for NLP|313|21.09|23.32|37|56.76|2.70| |
|Machine Translation|198|25.25|18.18|7|0.00|57.14| |
|Multilingualism and Cross-Lingual NLP|85|20.00|30.59|12|25.00|16.67| |
|NLP Applications|354|22.88|19.77|25|52.00|8.00| |
|Phonology, Morphology, and Word Segmentation|21|28.57|19.05|0| | | |
|Question Answering|197|18.78|18.78|22|45.45|18.18| |
|Resources and Evaluation|213|28.17|19.72|23|56.52|0.00| |
|Semantics: Lexical|54|25.93|25.93|3|66.67|33.33| |
|Semantics: Sentence-level Semantics|81|27.16|11.11|9|22.22|22.22| |
|Sentiment Analysis, Stylistic Analysis, Arg. Mining|107|17.76|30.84|10|30.00|0.00| |
|Speech and Multimodality|72|27.78|36.11|7|57.14|14.29| |
|Summarization|139|23.02|21.58|12|33.33|8.33| |
|Syntax: Tagging, Chunking, and Parsing|69|23.19|21.74|5|20.00|20.00| |
|Theme: Reality Check|110|26.36|30.91|1|100.00|0.00| |
|Total|4559|20.73|18.36|305|42.30|20.98| |

Table 1: Number of submissions and acceptance rates per track for direct and ARR submissions to ACL’23.

(35.7% response rate), 134 area chairs (ACs) (30.5% response rate), 510 reviewers (11.4% response rate), and 556 authors (4.07% response rate of all authors2).

ACL’23 had 26 tracks, most of which have also been offered at other recent NLP conferences. At the suggestion of EMNLP 2022 chairs, we kept their separation of “Large Language Models”3 track from “Machine Learning for NLP” track. At community requests we added the following tracks: “Linguistic Diversity” and “Multilingualism and Cross-lingual NLP”. Each track had at least two Senior Area Chairs (SACs), who then recruited area chairs (ACs) for that track. The full list of senior chairs per track is available at the conference website.4

Internally, in the START system there were also two special tracks: “Ethics review” track (which handled the reviews of papers that were flagged for ethical issues), and “Conflicts of interest” (COI) track, which handled the papers with which the SACs of the relevant tracks had a COI.

ACL’23 implemented a hybrid process, in which it was possible to submit papers either directly to the START system (to be reviewed through ACL’23 internal peer review process to be described in this report), or commit it through ACL ROlling Review (ARR) with reviews already performed at ARR. Most submissions to ACL’23 were direct submissions (4559), and 305 more came through ACL Rolling Review (ARR). Table 1 shows acceptance for each type of submission and in each track.

Assuming that in most cases at most one author per paper responded to the survey, the upper bound on the response rate for author feedback per paper would be 11.4% of all direct and ARR submissions that were reviewed. 37.9% of the authors who responded to the survey indicated that they disagreed with the outcome for their submission.

The EMNLP original name was Language Modeling and Analysis of Language Models. In our version it was simply Large Language Models, as they are the most frequent topic currently, but in retrospect the original version is preferable as it is more inclusive.

https://2023.aclweb.org/committees/program/

# Findings

| | |EACL|Direct-New|Withdrawn| | |
|---|---|---|---|---|---|---|
| | |33%|16%| | | |
| | | | |28%|78%| |
| |EMNLP|29%|0%| | | |
| |Arr-Resubmissions| |17%|15%| | |
| | | |Main|13%|39%| |
| |Direct-Resubmissions|4%|6%| | | |
| | |11%|3%| | | |
| | |Other| | | | |

# Figure 1: Resubmissions at ACL’23

ACL Rolling Review (ARR). Table 1 shows that in most tracks, ARR submissions had a much higher acceptance rate, sometimes twice higher. This is to be expected because ARR submissions self-select for high scores and positive reviews before committing to ACL.

Since in the hybrid process ARR submissions and direct submissions directly compete for acceptance, a question arises to what extent this is a fair competition. We asked that question to our SACs. 58.3% believe that this process is fair enough, 12.5% - that it is unfair to the direct submissions, and 29.6%—that it is unfair to the ARR submissions. Of 17 SACs who believed that this situation is unfair in some way, 23.5% suggested that they should have separate acceptance rate, 41.2%—that they should have a separate process and acceptance criteria, and 47.1%—that there should be some other solution (many comments pointing to the confusion, the apples-to-oranges comparisons of reviews performed with different evaluation, the less-than-ideal import of openreview data into START (browsing attachments takes more time). Many expressed a preference for a non-hybrid process.

As program chairs, our biggest challenge with ARR was that by design it provides reviews and meta-reviews, but the acceptance decisions are then made by our SACs—who generally do not provide extra feedback to either direct submissions or ARR submissions (nor can they be expected to: some tracks had over 300 papers per 3 SACs). For direct submissions, nobody expects SAC-level feedback. But to ARR authors, who likely self-selected for high scores and positive reviews, to be rejected without explanation is more frustrating, and we received a lot of angry emails demanding extra feedback (even though neither we nor ARR promised that). It seems that by design, a process where there are acceptance quotas, and decisions are fully decoupled from feedback, will necessarily leave the majority of authors rejected without explanation—and hence disappointed and unsure what they could do to improve their work (and we agree that this would indeed be frustrating to the authors).

The above factors could transform into a bigger problem in the future. We only had 305 ARR submissions, but if a majority of our submissions came with high scores and positive reviews—this just would not be a useful signal anymore. The acceptance odds of direct submissions would decrease (as compared to a process where everyone starts at the same stage of peer review). The SAC-ing would become harder (since selecting among high-quality papers is less easy than among papers of varying quality), and the authors would be disappointed because many would be rejected with high scores and no idea what they could do differently.

# 3 Resubmissions

Among the 4559 direct submissions to ACL’23, 754 indicated that they were resubmissions (see fig. 1a). The biggest “donors” were EACL5 (296), EMNLP (258), ICLR (103), AAAI (52), and ACL Rolling Review6 (39). Although the selectivity of top-tier conferences means that the majority of papers are

5 Because our submission deadline was shortly before EACL and ICLR notification deadlines, we made an exception to no-cross-submission policy and allowed their submissions to be also submitted to ACL. After their respective notifications many such papers withdrew from our pool, which explains the high withdrawal rate in Figure 1c.

6 There were 11 resubmissions from October 2022, 6 from September, and 1-3 from many other months of 2022.

# 1.0 Role

|4000|0.8|Author|0.8|0.8|
|---|---|---|---|---|
|Reviewer|Chair|3000|0.6|0.6|
|Gender|Female|0.6|0.6|Male|
|Not specified|Ratio|Other|0.4|0.4|
|2000|0.4|0.4|1000|0.2|
|0|0.0|0.0|Faculty|Other|
|Industry|Postdoc|Postdoc|Postdoc|0.0|
|PhD Student|Government|Industry|Government|Industry|
|Government|Author|Chair|Reviewer|Masters Student|
|PhD Student|Masters Student|Masters Student|Gender per role|Gender per affiliation|

# (a) Affiliation types distribution

# (b) Gender distribution

40003500300025002000count150010005000
# Country

China
USA
UK
Other
Korea
India
Canada
Japan
France
Italy
Israel
Spain
Taiwan
Russia
Brazil
Poland
Ireland
Belgium
Sweden
Iran
Germany
Singapore
Australia
Hong Kong
Denmark
Portugal
Viet Nam
Switzerland
Netherlands
Bangladesh
Czech Republic
# (c) Top 30 countries listed in the ACL author and reviewer profiles

Figure 2: Author and reviewer pool at ACL’23*

* All information is self-reported, not independently verified, and does not correspond to any specific definition of affiliation, gender, or country (e.g., some authors from Edinburgh may elect to list their country as “Scotland” rather than “UK”.)

rejected, the bulk of the ACL’23 submissions are new, which means that at this point the burden of re-reviewing is relatively low. It is possible that this is due to the wider acceptance of Findings as a publication channel, as more ACL conferences continue to offer this option.

Moreover, ACL’23 authors had the option to submit previous reviews as an attachment, but only 243 submissions used this option, which suggests that most resubmitters preferred to have a completely new set of reviewers. ARR allows that option within ARR, but the ARR submissions themselves did not have a high rate of revise-and-resubmit (only 8/305), as shown in fig. 1b.

Intuitively, one could expect that resubmissions have a higher chance of acceptance, since these are the papers that have received feedback and had a chance to revise. But fig. 1c suggests otherwise. See more analysis in §7.3.

# 4 Authors and Reviewers at ACL’23

We received a record 4864 submissions (4559 direct, 305 from ARR) from the total of 13,658 authors, reviewed by 4490 reviewers. This section reviews our recruitment process and the three demographic variables (country, affiliation type, and gender) to which we had access in the global START profiles of all participants of ACL peer review process.

# Reviewer recruitment.

We initially sent review invitations to the reviewer list which we had received from the organizers of previous conferences. We also required the authors of all submissions to nominate at least one experienced reviewer, whom we also sent invitations.

As we elicited reviewer data, we found that for a quarter of our reviewers there is no reliable Semantic Scholar publication history data that can be used for paper-reviewer matching. For conferences that fully rely on automated paper–reviewer matching based on publication history, this factor obviously sets a bound on their possible performance. Often the author pages exist because Semantic Scholar automatically created them, but the authors did not claim them and did not clean them up, which.

Out of the reviewers who filled in our sign-up forms, only 75.4% confirmed that their Semantic Scholar profile is accurate and can actually be used to estimate their areas of interest and expertise. In addition to that, 8.9% reviewers listed in START did not specify their Semantic Scholar IDs in their profiles.

may result in the addition of publications by namesake authors (e.g. the automatically created profile for “Anna Rogers” originally had contributions from at least three researchers with that name.) This is particularly worrying because at this point many venues have used this information for paper-reviewer matching, and urged the NLP community to maintain their Semantic Scholar profiles. We also specifically reminded about this, but still a quarter of our sign-up pool stated that their publication history is not accurate. In addition to this problem, matching based on publication history has the issue with establishing expertise of different authors on multi-author publications. Hence, we developed an alternative matching approach described in §5.2.

# 5 Efforts towards improving review quality

This section describes the following steps that ACL’23 proposed and implemented within its peer review process to improve review quality: review tutorials (§5.1), Area-Contribution-Language paper-reviewer matching (§5.2), flagging of review issues by the authors (§5.3). The efforts to improve the overall incentives are described in §9.2 and §9.3.

# Affiliation types

Figure 2a presents the overall distribution of the affiliations of our authors and reviewers (as stated in START profiles). The biggest group of authors, reviewers, and chairs are academic faculty. The second biggest group (by absolute numbers) in all three categories is industry, which is relevant to the recent concerns about the influence of industry on academic NLP research (Abdalla et al., 2023). Furthermore, students form at least 26% of reviewer pool (Ph.D. 22.7%, M.Sc. 3.3%). This was also our experience as area chairs at other recent conferences, and it highlights the need to continue the reviewer training efforts.

# Gender distribution

Based on the information in softconf profile, about 20% of ACL peer review participants in all roles did not answer the question about their gender (Figure 2b). For a part of this population this is likely a deliberate choice, but judging by how many other fields in the START profiles were not accurately filled in or updated, in many cases this likely signals simply the lack of desire to fill in forms, especially for the new authors who had to register in START last minute in order to make a submission. Considering only those profiles that responded to this question, we see a heavy imbalance for “male”, in agreement with the reports on under-representation of women in Computer Science (Jaccheri et al., 2020; Pantic and Clarke-Midura, 2019), where a lot of NLP research is currently happening. This underscores the need to continue the Diversity and Inclusion efforts.

# Top contributing countries

The analysis of the countries of all authors and reviewers suggests that the balance between reviewing and submitting papers is considerably off for many locations, and particularly China. We believe that this is at least partly due to the fact that our recruitment efforts started with the pool of the previous conferences. That pool needs to be deliberately expanded by more active and targeted reviewer recruitment efforts among Chinese institutions.

Church (2020) estimates that at 20% acceptance rate the authors of published papers “owe” the community at least 15 reviews per each publication (3 for their own paper, and 4x3 for the papers that didn’t get in). While some dis-balance between the author and reviewer list is to be expected (e.g., since many junior authors are not yet qualified to review, and many senior authors perform other organization roles)—we clearly need to decrease it in order to decrease the reviewer load. Our default quota was six papers per reviewer, in line with most recent conferences. This is a significant workload, and it can hardly be expected to improve the quality of reviews. Moreover, the more reviewers are in the pool, the smaller the trade-off between optimizing for best matches or smaller workload per reviewer.

In absolute numbers: 3881 authors vs 1271 reviewers for China (ratio 3.05, absolute difference 2610). For the US: 2608 authors, 1809 reviewers (ratio 1.4, absolute difference 799). While the reviewer:author ratios are also high for India (2.6) and Korea (2.64), from the point of view of a conference organizer China stands out due to the sheer volume of submissions.

We gave the reviewers a chance to request a lighter load at sign-up, and respected those quotas in our automated assignments, but there were still some over-assignments due to manual corrections of assignments by the chairs.

# 5.1 Reviewer training

As part of reviewer training, we prepared the following public materials (as a revision of an earlier tutorial10, developed by Anna Rogers and Isabelle Augenstein for ARR):

- ACL’23 Peer Review Process: the general tutorial about review process for novice reviewers, that covers the basic structure of ACL peer review process, author response, and discussion period, as well as tips for planning the time, reporting conflicts of interest and assessing whether to ask for reassignment. These materials were optional for experienced reviewers, and could be used across different ACL venues as is.
- ACL’23 Peer Review Policies: the tutorial explaining our review form and responsible NLP checklist (§9.1), as well as our peer review policy: specific, professional reviews with scores supported by the text. Our list of reviewer heuristics such as “reject if not SOTA” currently contains 14 heuristics (continued from the original eight heuristics pioneered at EMNLP 2020 (Cohn et al., 2020)). We asked even experienced reviewers to read this tutorial. The future chairs could reuse parts of this tutorial, with necessary updates to the review form description and review policies.

# Feedback

The exit survey indicates that the reviewers found the materials clear (43% respondents rated them as at 4 out of 4 and 40.5% - as 3 out of 4 on 4-point scale). One avenue of improvement suggested in many free comments was adding examples of good reviews.

We also asked the reviewers about their preferences for alternative formats, and the self-paced text-based tutorial was the majority choice (62.5% vs 13% preferring video tutorials and 9.6% preferring interactive tutorial with quizzes). But 13.4% respondents said that they would probably never be able to spend time on reviewer training, no matter what format it is offered in. This suggests that reviewer training, while valuable, will not help in all cases, and could perhaps be interpreted as an upper bound on the effect of any reviewer training.

# 5.2 ACL paper-reviewer matching: Area-Contribution-Language

One of the peer review issues that authors (and chairs) often complain about is “meh” reviews: the reviewer does not really find any significant problems with methodology or execution of the paper, but the overall recommendation is middling. This could be a symptom of paper-reviewer mismatch: the reviewer just is not sufficiently interested in the overall topic or approach, and hence no matter how good the paper is, it would not elicit much enthusiasm. In a recent survey (Thorn Jakobsen and Rogers, 2022) of authors, reviewers and ACs about their prior experience at NLP venues, many reviewers stated that “the area match was right, but... the subject of the paper was not interesting to me (e.g. I would prefer another NLP task, model, or data)” (54%), or the paper was not asking a research question that would be interesting for me” (45%). At the same time, over 27% of the author respondents in that survey reported that they had experience of reviews where the reviewer was not interested in the subject of the paper.

Most recent ACL conferences and ARR work with some version of an automated paper-reviewer matching system that computes affinity scores between the abstract and title of the submission and the candidate reviewer, based on their publication history. Interestingly, the same survey by Thorn Jakobsen and Rogers (2022) found that both authors, reviewers, and ACs generally considered these scores to be the least important factor for paper-reviewer matching. Besides the limitations of the current systems, one factor here is probably the noise in the reviewer publication history data (only 75% of our reviewers indicated that their Semantic Scholar profiles were accurate enough to use for review assignments, see §4). Then there is also the inherent difficulty with establishing level of expertise on a particular topic in multi-author papers.

A traditional alternative to affinity scores, that also addresses the issue with reviewer interest, is bidding: the reviewers explicitly say which papers they would be interested in. But this process is rather laborious: for a big track, a reviewer would need to indicate their interest for hundreds of papers. It also opens up the possibility of collusion rings (Littman, 2021). In our experience, many reviewers do not even respond to bidding calls on time, which once again leads to some part of assignments being essentially random.

10https://aclrollingreview.org/reviewertutorial

|Match by area|Match by contribution|Match by language|Review count|Review %|
|---|---|---|---|---|
|✓|✓|English|8996|71.36|
|n/a*|n/a|n/a|1052|8.35|
|✗|✓|English|691|5.48|
|✓|✗|English|558|4.43|
|✓|✓|✓|476|3.78|
|✓|✓|✗|345|2.74|
|✗|✓|✓|164|1.3|
|✗|✗|English|142|1.13|
|✗|✗|✓|52|0.41|
|✓|✗|✓|50|0.40|

Table 2: The number of reviews matched to submission by different combinations of ACL (Area-Contribution-Language) criteria. The ’n/a’ row corresponds to manual assignments by ACs, for which we do not have the match information.

Thus, we experimented with a new workflow that we dub ACL (Area-Contribution-Language) paper-reviewer-matching. It is a keywords-based matching process that explicitly targets three dimensions of submissions: track sub-areas (topical match), contribution types (match by focus/methodology), and target language (for submissions not focusing on English). To the extent possible, the paper-reviewer matching aimed to provide matches across all these dimensions. This approach further enabled us to provide the ACs with explanations for the specific matches (see §6.3).

# Track sub-areas.

Each track at ACL 2023 had an associated set of keywords describing its potential sub-areas. The goal was to describe the biggest expected sub-areas, and hopefully provide the authors with a better idea of the kind of work that the track was inviting. The full list of our keywords is publicly available in our blog post.11 Our keywords were provided by the SACs of all tracks independently, but the future chairs may wish to take a more top-down approach to editing this list, and to ask their SACs to check that the list still describes the sub-areas for which the most submissions are expected, and the individual keywords are sufficiently clear for the authors.

# Language(s).

Due to the “default” status of English (Bender, 2019), submissions targeting other languages may be perceived as “niche” by reviewers. Additionally, the lack of expertise in a language may make it harder for reviewers to spot potential issues. Hence, for papers on languages other than English, we endeavoured to also maximize reviewer matches along this dimension.

# Contribution types.

The contribution types cross-cut tracks, and we hope they would help to decrease the amount of cases where the reviewer just fundamentally does not recognize a certain type of work (Bawden, 2019) and hence scores it down, or has unreasonable expectations (e.g. experimental results in a position paper). For example, the category of compute/data-efficiency creates a de-facto equivalent of efficiency track spread across all tracks.

Our contribution types are based on COLING 2018 classification (Bender and Derczynski, 2018), which we extended as follows: (1) NLP engineering experiment (most papers proposing methods to improve state-of-the-art), (2) approaches for low-compute settings, efficiency, (3) approaches for low-resource settings, (4) data resources, (5) data analysis (6) model analysis & interpretability, (7) reproduction studies, (8) position papers, (9) surveys, (10) theory, (11) publicly available software and pre-trained models.

# Implementation.

To collect the information for this kind of matching, we asked the authors at submission time to specify their preferred track (up to two), the best-matching keywords in that track (multiple selection possible, or “other” option with free text entry), the best matching contribution type(s) and target language(s). Correspondingly, at reviewer recruitment stage we asked the reviewers to fill in a form specifying their preferences for the tracks, keywords, contribution types, and the language(s) the work on which they could review. The matching itself was based on Integer Linear Programming, aiming to maximize matches across the three keyword types (with more types of matching being more valuable than

11https://2023.aclweb.org/blog/reviewer-assignment/

e.g. more matches only by area). As a fallback, we also retrieved Semantic Scholar profile data for the reviewers and computed the similarity between submission abstracts to the abstracts in the publication history of candidate reviewers, but this factor was given the lowest priority in the assignment strategy.

The Area-Contribution-Language matches, as well as the most similar paper of the reviewer, then also became the basis for the rationales for the match (see §6.3). The SACs were given the opportunity to selectively check and adjust the matches as described in §6.2 (although few of them did), and the ACs and SACs were able to see the rationales for the matches when considering the reviews.

From the analysis of the final 12606 reviews in START, 1052 (8.3%) did not have the match information (due to manual reviewer reassignment by the chairs, most likely emergency reviewers). Of the remaining 93.7% reviews made by our criteria, only 1.13% reviews with automated assignment were assigned based on the similarity scores from publication history, after exhausting the possible keywords-based matches in the reviewer pool. 82.9% reviews had at least one match by the type of area, 84.97% - by contribution type. Importantly for DEI efforts and development of NLP for languages other than English, we had 1167 reviews for submissions that specified at least one target language other than English – and we were able to provide a reviewer matching by (at least one) language in 63.58% such reviews.

# Feedback

When asked to rate on 4-point scale how well the paper-reviewer matching worked for them, 85.5% ACL’23 reviewers rated it positively (35.7% at 4/4, 49.8% at 3/4). When asked for the kinds of mismatch, if any, 28.4% pointed at the topic, 13.7% at the methods, 10.4% at the type of contribution, 4.5% at languages, and 5.7% at other kinds of mismatch.

We conclude that Area-Contribution-Language assignments are overall a promising direction that can contribute to DEI efforts in the field and diversity of its contributions (see also §7). The matches could be further refined by (a) revising the area keywords12, and (b) more targeted reviewer recruitment to include speakers of various languages. One of our SACs suggested providing a glossary together with the list of keywords. We also recommend investing effort into a dedicated interface for checking reviewer assignments that would enable ACs to help with reviewer assignment checks while seeing the up-to-date reviewer availability information, and highlighting the possible problems with the current assignments (such as imperfect matches, rare types of contributions or languages that may need extra attention, insufficient pool for a area or a contribution that turns out to be more popular this year).

# 5.3 Review issue flagging

Even with all the above efforts, we anticipated that there would still be problematic and mismatched reviews. Given that the only people with the incentive to read the reviewer guidelines and enforce them are the authors, we developed a way for them to flag reviews for specific issues, which the ACs could be given specific instructions about, and be able to address more systematically.

Unfortunately, the START system does not have an editor for the author response form or meta-review form. Hence we had to provide the authors and ACs with the list of possible issues, and ask them to specify their type and rationale in plain text form, as shown in Figure 3. As could be expected, even with a template there were many format errors. We recommend that the future conferences use a form with a multi-selector, per each reviewer.

The authors actively used this feature at ACL’23, flagging 12.9% of all reviews. This is reassuring: judging by the intensity of online discussions of peer review at each review release day, most reviews are bad). The frequency of various reported issues is shown in Table 3. The biggest reported problem is the heuristics such as “not novel”, “not surprising”, “too simple”, and “not SOTA”. Particularly concerning are the rude/unprofessional reviews: even though there are only 1.69%, they have the most potential to impact the mental health of the authors, and we should strive for that number to be 0.

The author-reported issues should be interpreted as a lower bound on the number of review issues, because of 100 papers were reviewed but withdrew before the final decisions. It is possible that they did because they (a) agreed with the criticism and wished to revise the paper, or (b) that they disagreed but did not see a chance to persuade the reviewers. Assuming the latter, and that all their reviews were problematic, this would raise the upper bound of problematic reviews to 15.3%. But it is unlikely that all

12 In particular, our Language Grounding SACs indicated that their keywords should be revised and clarified.

# Response to Chairs

In rare cases reviews may be of unacceptably low quality, which violates the conference peer review policy: If this happened to You, you can use the box below to report the type of the issue and explain your rationale to the chairs. This mechanism should only be used for serious issues. It is not in the authors' interest to make their meta-reviewers investigate cases where the authors disagree with the reviewers but the reviewers have done due diligence and provide their arguments, evidence, references.

The following types of issues are known from past conferences:

- A. The review is not specific enough, e.g. missing references are not specified.
- B. The review exhibits one of the heuristics discussed in the ACL23 review policy post, such as "not novel", "not surprising", "too simple", "not SOTA". Note that these criticisms may be legitimate, if the reviewer explains their reasoning, and backs up the criticism with arguments, evidence, references. Please flag only the cases where you believe that the reviewer has not done due diligence.
- C. The scores do not match the review text. Note that in ACL23, the "soundness" score is meant to reflect the technical merit of the submission, and low soundness should be backed up with serious objections to the work; the "excitement" score is more subjective and its justification may not be reflected in the text.
- D. The review is rude/unprofessional.
- E. The review does not evince expertise (incl. texts that seem to be synthetic and not based on a deep understanding of the submission).
- F. The review does not match the paper type (e.g. short paper expected to produce more experiments than is necessary to support the stated claim).
- G. The review does not match the type of contribution (e.g. experimental work expected of a paper a different kind of contribution).
- H. The review is missing or too short and uninformative.
- I. The review was late and could not be addressed in the author response.
- J. Other (please explain).

If you feel that you have such a problem, please use the following format to report it in the text box below (without the #comment lines, 250 words max):

In this example Reviewer 1 had issue A (unspecific review) and Reviewer 2 had issues C and D (rude review; scores don't match the text):

#review problem type(s), as & capital letter corresponding to the issue type in the above list of possible issues. If there is more than one, list them comma-separated (e.g: A; I)

R1: A

# explanation

R1 states [reviewer statement]; which we believe corresponds to the review issue type A. It is unreasonable in this case because [rationale]:

R2: C,D

R2 states [reviewer statement]:

# Submit

Figure 3: Review issue flagging: minimal plain-text implementation in START withdrawn papers were of the (b) type, and the comments from ACs also suggest that many issues were not fully justified.

# Feedback

When asked to rate the utility of this system at ACL’23 on a 4-point scale, with 4 being the highest score, 42.1% of the authors in our exit survey rated it at 4/4, and 40.3% - at 3/4. We interpret it as overwhelming support, and recommend that this feature is maintained in the future conferences. However, the qualitative analysis of the authors’ comments suggests that in some cases the ACs did not respond to the flagged issues properly, which entails the need for further training and monitoring by the SACs.

Our follow-up analysis suggests that ACs reported addressing the author-flagged issues in at least 30.59% submissions (judging by their using a similar template to Figure 3 in the “confidential notes to chairs” in the meta-review). This should be interpreted as a lower bound: since the interface was very clunky, it is possible that some ACs did consider the flagged issues, but did not report their actions. But, clearly, many issues were not properly addressed, and there is much room for improvement and further training of ACs. Still, given that this is the first implementation of this system, this is a promising approach and it should improve in the future.

# 5.4 Reviewer discussion

Similarly to most of the recent ACL conferences, we implemented the author response period: a week during which the authors have the opportunity to read the reviews and send their response. The goal of this process is improving the quality of the reviews, and we supplemented that goal with the above new option for the authors to flag specific types of review issues (§5.3). The authors could (but didn’t have to) provide a response and flag review issues; this was done for 88.3% of reviewed submissions. In 57.3% review forms the reviewers indicated that they read the response (it is possible that more did read the response but did not fill in the form).

Those comments were seen by the ACs, not the reviewers. The ACs had the option to initiate reviewer discussions for the cases where they saw significant disagreements, quality issues, or misunderstandings. Each paper had an associated “forum” on START, where the reviewers could communicate in an.

|Type of issue|Number of reviews|% of reviews|
|---|---|---|
|A: The review is not specific enough|272|2.16|
|B: Review heuristics such as “not novel”, “not surprising”, “too simple”, “not SOTA”|678|5.38|
|C: The scores do not match the review text|448|3.55|
|D: The review is rude/unprofessional|213|1.69|
|E: The review does not evince expertise|542|4.3|
|F: The review does not match the paper type|98|0.78|
|G: The review does not match the type of contribution|152|1.21|
|H: The review is missing or too short|205|1.63|
|I: The review was late|12|0.1|
|J: Other|162|1.29|

Table 3: Review issue statistics

Anonymized fashion (as R1, R2, R3). The ACs were provided with instructions and suggested starter message template.

In total, out of 4559 direct submissions to ACL, 4069 had received reviews, and for 2901 out of those the ACs initiated discussions. In total, ACL review process generated 8553 messages (3879 by the ACs). However, only 2107 discussions (72.63%) had at least one response from at least one reviewer. Somewhat consistently, the discussions were overall initiated by 77.4% of all ACs. We conclude that both AC and reviewer involvement have room for improvement.

We reviewed one case of a strong paper that ended up being rejected. The AC could have been persuaded by a “champion” reviewer, and there was one such expert in the set who was surprised by the final outcome—but they did not engage in the forum discussion. We followed up with the reviewer, and they explained that since their review was already positive, they did not feel that they needed to be “on the case” anymore. We cannot establish how common this misconception is, but we would urge all reviewers to always read all reviews and author response, and when certain of the merit of a paper—to try to make sure that the AC is convinced.

# 6 Improving decision support for the chairs

In addition to the efforts for improving the quality of peer review (§5), we implemented the following steps for facilitating the decision support by ACs and SACs: revised SAC and AC guidelines (§6.1), guidance for assignment checking (§6.2), match rationales (§6.3), Soundness/Excitement scores (§6.4).

# 6.1 Updated SAC and AC guidelines

We updated the SAC/AC guidelines that we received from the program chairs of ACL’21 in following ways. We reformatted it to Markdown to utilize the ecosystem of GitHub (e.g., version control, asynchronous collaboration among PCs, automated deployment). The guides were built by Sphinx 13 with MyST extension 14, which enables to use Markdown and variables (making it easy to keep the consistency of dates and external URLs between SAC and AC guides and for the future chairs to adapt to their timeline). We also adjusted the existing instructions and created new instructions to incorporate everything we developed, from the new reviewer guidelines to guidelines for making recommendations. We shared the guides before the review process so that SACs and ACs can be prepared for the tasks and workloads.

Feedback. 83.3% SACS and 90.3% ACs rated the clarity of instructions at 3/4 or 4/4. Some of the free-text comments indicated a preference for shorter guidelines, but since the process is complex, and the guidelines need to serve both new and experienced chairs, there are limits to how much they can be shortened.

# 6.2 Support for checking assignments

As mentioned above, the usual workflow in large conferences is that the assignments are made automatically based on affinity scores between candidate reviewers’ publication history and submissions. Usually, the automated assignments are then shown to the ACs and SACs to check manually, but this is very difficult in practice: SACs cannot process such a large volume on their own, so they need to rely on ACs. But ACs, at least on START, do not have access to the list of possible reviewers together with their current number of assignments and all their COIs, which means that even if they spot an error—it is difficult for them to identify and recommend an available alternative. Providing the up-to-date quota and COI information on all reviewers in track to the ACs is not possible in the current START platform. There are also no detailed guidelines for this step, which means that even if ACs had the reviewer information, everybody would be suggesting alternatives based on different criteria.

In our experience as SACs in previous conferences, although the automated assignments are not perfect, very few ACs actually report the problems or propose alternatives. To see whether this was widespread, we asked our SACs in the exit about whether, in their experience, the ACs asked to check the automated assignments usually recommend many changes. Only 9 of our respondents previously served as SACs in this set-up, but most of them (6/9) concurred with our experience, reporting that ACs adjust very few assignments. When asked why the ACs do not recommend more changes, 33.3% SACs stated that there are no adjustments because the ACs don’t really check, 29.9%—that it happens because the automated assignments are already good enough, 29.2%—because of the difficulty with sharing up-to-date reviewer availability information with them, and 20.8%—that there are no better candidates even if the ACs check. 37.5% indicated that there are also other issues contributing to the ACs not recommending more changes.

We interpret these results as pointing to the fundamental issue of systematically sharing up-to-date reviewer availability information together with their preferences, experience, and profile information, in a way that would make it easy for the ACs to perform such checks and recommend alternatives.

Given that the above factors make it unrealistic to adjust assignments with help of ACs, and that the volume of assignments to check was too large for SACs, we experimented with an alternative approach: since we had the “explanations” for the matches and also the quantitative information about different types of contributions, languages and area keywords, this information would make it possible for SACs to identify the types of submissions most in need of extra checks, and to focus on those. This way the workload would remain manageable, and the SACs would be able to do that while having full access to the latest reviewer availability data. To assist in this process, we developed Jupyter notebooks with quantitative analysis per track (identifying which keywords, types of contributions and languages were rare and could need extra attention)—as well as reviewer lookup functionality by preferred keywords, languages or types of contribution (or any combination thereof). This solution was better than nothing, but admittedly clunky and could be much improved.

Feedback. 66.7% of SACs stated that they believed selective checking to be overall sufficient given sufficiently strict reviewer pool criteria (although in our specific case not all reviewers in our pool were up to all SAC’s standards).

Caveat: we encountered difficulty with uploading the final automated assignments due to dynamic computation of conflicts-of-interest in START. Because of that, several hundred automated assignments had to be redone manually at the last minute. For the conferences based on START, we strongly recommend that this computation is frozen after the main part of reviewers and chairs are added to the tracks.

# 6.3 Paper-reviewer match rationales

Given the information for the paper-reviewer matches that we had collected (§5.2), we were able to provide the ACs with a list of rationales for each match (except for those reviewers who were added manually by the chairs, and for whom we did not have this information.) A sample “explanation” for a match is shown in Figure 4a. The idea was to provide the AC with not only the general information about the reviewer, but also what are their interests that match this submission. Importantly, we highlighted the cases where the author-stated type of contribution or language was not among the reviewer’s stated.

# Basis for this assignment

Match by track subarea: corpus creation, reproducibility

No match by contribution types: The authors specified: approaches for low-compute settings, efficiency

Match by target language (non-English): French

Most similar paper score: 0.744

Most similar paper:

Reviewer highest degree: PhD

Reviewer affiliation type: Academia

Reviewer publication history: scholar profile

(a) Example of paper-reviewer match rationales. The most similar paper titles directly link to the papers (based on Semantic Scholar). For contributions and languages, the rationales either show the match, or alert to the lack of the match, so that the AC could take that into account.

|Area|Contribution|Language|Most similar paper|Reviewer profile| | | | | |
|---|---|---|---|---|---|---|---|---|---|
|50|60|50|40|40|30|30|20|20|10|
|0|0|Very helpful|Not helpful|Very helpful|Not helpful|Somewhat helpful|Somewhat helpful|Somewhat unhelpful|Somewhat unhelpful|
|Rating (AC)|Rating (SAC)| | | | | | | | |

(b) Chair feedback on which features of the match explanation they found the most useful.

Figure 4: Example explanation for paper-reviewer matches, and AC utility ratings for individual features displayed.

Feedback. This feature received overwhelming support from the chairs: 87.5% SACs and 73.9% ACs rated its utility at 3 or 4 out of 4 (Figure 4b). Among the suggestions for the future improvement, the SACs suggested indicating whether the reviewer was an emergency reviewer, and how late the review was, as well as some elements of reviewer history (e.g. whether they were late for other conferences). The numerical similarity scores were less useful than the titles of the most similar papers. While predominantly the ACs were very positive about easily accessible links to reviewer profiles (Figure 4b), some ACs raised fair concerns about the effect of this feature on reviewer deanonymization: the reviewers are already visible to ACs since they need this information for chasing late reviews, but providing links to reviewer profiles increases the saliency of the reviewers’ identities, and hence may by itself increase bias against, for instance, student reviewers.

# 6.4 Soundness/Excitement scores

While most of the experimental aspects of the ACL 2023 process was focused on matching reviewers to papers more effectively, a larger change visible to authors and reviewers was the introduction of two new scores on the review form to replace the Overall Recommendation that was previously the centerpiece of *CL review forms.

We asked reviewers for two scores: Soundness and Excitement. Our goal was that any sound paper would be accepted to some ACL affiliated venue (i.e., Findings), but that the “main conference” distinction (limited by space) would be focused on the most exciting papers. Our hope was that Soundness, as a more specific rubric with more objective criteria, would be less noisy than a single Overall Recommendation score, which would help reduce the randomness of decisions. The AC guidelines had explicit instructions for how these scores should map to their recommended status.

One more factor motivating our proposal was that the Soundness/Excitement distinction could help with the author-reviewer communication during the author response. When a reviewer points out issues with

15 See our definitions and rubrics for the review form and extra explanation here.

Soundness, the authors generally have a fair chance to clear any misunderstandings or issues with review quality, and the chairs are interested in this kind of discussion. The Excitement, however, is subjective, and the authors do not have a fair chance to convince reviewers that their general views or research agenda are wrong. The Soundness/Excitement distinction helps to focus the response on the Soundness issues, and hence have a more productive discussion.

Feedback. Judging by the exit surveys, this change was overall well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change. 38.1% authors, 35.1% reviewers and 29.9% ACs indicated that while the idea was good, it could be better executed. Among the named issues was the clarity of communication about what these scores meant, the difference in granularity (our scale for Excitement had 9 points, and Soundness only 5), and the wording could be adjusted to remove the semblance to Overall recommendation score. We made these recommendations to the program chairs of EMNLP 2023, who decided to keep this system.

From the communication with the authors who expressed dislike for this system, our impression is that one of the factors here is the mistaken impression that the final decisions are overall based on scores, and the papers with similar scores should be guaranteed the same outcome—whereas in reality the chairs know that scores can be noisy and miscalibrated, and hence the final decisions are made on case-by-case basis, with the full view of the reviews and meta-review, and also taking into account the acceptance quotas and their editorial priorities.16 The Soundness/Excitement scores were rather intended to make it harder for the chairs to just sort by the scores.

# 7 What Factors Contribute to ACL Peer Review Outcome?

Here we present the results of statistical analysis of ACL’23 data, with the goal of explicating what factors contributed to the final decisions and to the quality of individual reviews. We hope that this process both improves the transparency around chair decision-making, and highlights the potential biases and points of improvement for future conferences.

For the new authors, we should explain the general process for the acceptance decisions at ACL’23. First, the reviewers contribute their reviews. At the author response the authors see the reviews and have an opportunity to respond: a process mostly intended to clarify any misunderstandings (we disallowed submitting new results). Then the ACs initiate the reviewer discussion, with the goal to clarify misunderstandings and improve the quality of the reviews. Based on the final reviews and their own expertise, they write the meta-reviews and make recommendations for acceptance (Main track or Findings) or rejection. They are not concerned with the acceptance quotas. Their recommendations and meta-reviews (as well as reviews and author response if necessary) are then considered by the SACs, who have the constraint of the target acceptance quota (which we set at about 22% for the main track and 35% for Findings). Their decisions are based on three main factors: meta-reviews, quotas, and editorial priorities (with case-by-case consideration as needed). If they run out of their quota, they may additionally rank more papers by priority that may be accepted to main/track Findings if there is space (e.g., because some tracks did not use their quota fully). The final step is that the program chairs confirm the SAC decisions, and try to fit in as many papers of the ranked “maybes” as possible. In our case, that resulted in accepting more Findings papers than we originally planned based on prior conferences.

# 7.1 Review Scores: Overall Distribution

We start by exploring the overall distribution of the new Excitement and Soundness scores (described in §6.4) and how they mapped to the three possible decision outcomes (Rejection, acceptance to the Main track, or Findings). Both Excitement and Soundness are ordinal variables, and we use the mean as a rough estimate of the central tendency. Figure 5a shows that for both scores the means are higher for main track than for Findings, and for Findings they are higher than for rejections. For Excitement this is fully in line with our instructions to the chairs. For the main track, this suggests that higher (above 3) Soundness scores16 This is a general problem, and we imagine this would have also happened in the case of an Overall recommendation score. The drawback of the Soundness plus Excitement system is that less noisy decision cutoffs make outliers more salient.

# Figure 5: Soundness and Excitement scores per acceptance status

| |Accept−Main|Accept−Findings|Reject|Accept−Main|Accept−Findings|Reject|
|---|---|---|---|---|---|---|
|5|5| |1500| | | |
|4|4| |2000| | | |
|3|3| |1000| | | |
|Rating|Rating| |Count| | | |
|2|2| |1000| | | |
|1|1| | | | | |

# Table 4: Coefficients and Standard Errors (SE) for the Multinomial Logistic Regression Model predicting the final acceptance decisions given the mean scores and AC/SAC recommendations.

| |Findings Coeff|Main Coeff|Findings SE|Main SE|
|---|---|---|---|---|
|(Intercept)|-1.48|3.77|0.79|1.43|
|Soundness Mean|0.71|0.76|0.22|0.37|
|Excitement Mean|0.61|0.03|0.23|0.42|
|AC Recommendation (L)|2.66|4.50|0.50|0.94|
|AC Recommendation (Q)|-1.16|-0.05|0.43|0.81|
|AC Recommendation (C)|-0.04|0.10|0.31|0.58|
|AC Recommendation (^4)|0.04|-0.27|0.19|0.37|
|SAC Recommendation (L)|5.84|28.26|0.47|0.71|
|SAC Recommendation (Q)|-1.06|13.59|0.34|0.77|
|SAC Recommendation (C)|1.18|7.82|0.60|0.82|
|SAC Recommendation (^4)|1.52|4.48|0.64|0.74|

# 7.2 Factors Impacting the Final Acceptance Decisions

# 7.2.1 Reviewer Scores and Chair Recommendations

To establish the odds of a paper being accepted into Findings or the Main track vs it being Rejected, based only on reviewer and chair recommendations, we fit a multinomial log-linear model with multinom() function from the NNET package in R (Venables and Ripley, 2002). The dependent variable (DV) is the Outcome coded as a three-layer categorical variable (Main track, Findings, or Reject) with Reject being set as the reference level. The independent variables (IVs) are AC Recommendation (ordinal), SAC Recommendation (ordinal), mean Soundness score (interval), and mean Excitement score (interval). The analysis is performed on the papers submitted directly to the conference as the ARR submissions were reviewed through a different process and had different scores. The model coefficients are shown in Table 4. The model is a good fit for the data with McFadden’s pseudo-R2 of 0.777 (McFadden, 1973).

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

While ordinal regression would be more fit to represent the ordinal order of the possible outcome (Main track > Findings > Reject) we use the multinomial model as it does not have the proportional odds assumption.

Note both, the Excitement and Soundness are ordinal variables. Here, we employ the mean to obtain a rough estimate of the central tendency.

Please note the pseudo-R2 for logistic models cannot be directly interpreted as the proportion of variance explained as in linear models. Nevertheless, the high value observed here signifies a good fit to the data. We also report Cox and Snell.

| | |LR Chisq|Df|Pr(>Chisq)| |
|---|---|---|---|---|---|
| |Soundness Mean|10.88|2|0.0043|**|
| |Excitement Mean|9.67|2|0.0080|**|
| |AC Recommendation|209.71|8|0.0000|***|
| |SAC Recommendation|1438.12|8|0.0000|***|

Table 5: Type III Analysis of Deviance for Multinomial Logistic Regression in Table 4.17

To obtain the significance values for each IV (Table 5), we use the ANOVA() function in R on the fitted model (Type III Anova). As expected, all four IVs are significant (p&lt;0.05) but at different levels. The SAC Recommendation (χ2(8) = 1438.12, p&lt; 0.001) and AC Recommendation (χ2(8) = 209.71, p&lt; 0.001) significantly predict the Outcome with the SAC Recommendation appearing to be a better predictor (as expected, since AC recommendation are made without regards to the acceptance quotas). The mean Soundness score (χ2(2) = 10.88, p= 0.0043) and mean Excitement score (χ2(2) = 9.67, p = 0.0080) are also significant at p&lt;.05.

To establish the exact contributions of mean Soundness and Excitement scores to acceptance decisions for the Main track and Findings, we can look at Table 4 again. Note that since it is a multinomial regression model, the coefficients indicate an increase in log odds rather than directly interpretable odds (for which the coefficients need to be exponentiated). The “Findings Coeff” and “Main Coeff” correspond to the log-odds of being accepted into the Findings and Main track as opposed to being rejected.

# Soundness

In the case of the mean Soundness score the coefficient is positive for both Findings (0.71) and the Main track (0.76). This means that for one unit increase in the mean Soundness score the log-odds of being accepted as opposed to being rejected increase by 0.71 for Findings and 0.76 for the Main track. By taking the exponential of these values, we see that for one unit increase in the mean Soundness score the odds to be accepted increase 2.03 times for Findings and 2.14 times for the Main track.

# Excitement

Similarly, both coefficients are positive for the mean Excitement score for both Findings (0.61) and the Main track (0.03). This means that for one unit increase in the mean Excitement score the log-odds of being accepted vs rejected increase by 0.61 for Findings and 0.03 for the Main track. By taking the exponential of these values we see that for one unit increase in the mean Excitement score the odds of being accepted increase 1.84 times for Findings and 1.03 times for the Main track. While the values are still positive, this increase is much lower than for the mean Soundness scores, especially for the Main track. The overall distribution of these scores per acceptance status is shown in Figure 5b.

# AC Recommendations

Since AC Recommendation is an ordinal variable, it is coded using polynomial contrast, so the L indicates linear effect, Q a quadratic effect, C a cubic effect, and so on. Here we look mostly at the linear effect since it has a direct (linear) effect on the outcome. We see that both coefficients are positive, indicating that with an increase of one unit, the log-odds of being accepted vs being rejected increase by 2.66 units for Findings and 4.50 units for the Main track. By taking the exponential of these values we see that one unit increase in AC Recommendation corresponds to a 14.30-fold increase in the odds of being accepted into Findings (vs being rejected) and 90.02-fold increase in the odds of being accepted into the Main track (vs being rejected).

# SAC Recommendations

SAC Recommendation is also an ordinal variable, hence we see the same types of coefficients. However, the magnitude of the SAC’s decision appears to be much greater with a greater effect on the final outcome. With one unit increase in SAC Recommendation the log-odds of being accepted vs being rejected increase by 5.84 units for Findings, and 28.26 units for the Main track.

pseudo-R²=0.794 (Cox and Snell, 1989) and Nagelkerke pseudo-R²=0.913 (Nagelkerke, 1991).

21χ2 denotes likelihood ratio chi-square statistic.

22This latter finding seems counter-intuitive, given that our AC guidelines stressed that Findings is a venue for all sound work, while “sound & exciting” would be the basis for recommendations to the main track—but even among the papers accepted to the main track 39% have at least one “negative” Excitement score. At the same time, even among the Findings papers, only 49% have predominantly negative Excitement ratings, so there is a preference for at least some Excitement. This could be related to the confusion about the meaning of the scores in the initial iteration (see subsection 6.4).

|Paper Type|LR Chisq|Df|Pr(>Chisq)| |
|---|---|---|---|---|
|12.47|2|0.0020|**| |
|Review Issues|43.61|2|0.0000|***|
|Preprinted|47.96|2|0.0000|***|
|Previous Submissions|4.38|2|0.1120| |
|Languages Number|0.57|2|0.7528| |
|Languages not only English|3.53|2|0.1711| |
|Contribution: Efficiency|1.18|2|0.5540| |
|Contribution: Resource|4.34|2|0.1139| |
|Contribution: Reproduction|16.59|2|0.0002|***|
|Contribution: Theory|7.70|2|0.0213|*|
|Contribution: Software|19.62|2|0.0001|***|

Table 6: Type III Analysis of Deviance for Multinomial Logistic Regression, predicting submission Outcome (Main, Findings, Reject) conditioned on the variables listed in the table.24

Converting these values to their exponentials, we see that one unit increase in SAC Recommendation corresponds to a 343.78-fold increase in the odds of being accepted into the Findings (vs being rejected) and a massive increase of 1.88 × 1012 for the odds of acceptance into the Main track (vs being rejected). The model hence shows that the SAC recommendation is a much stronger predictor than the AC recommendation, which helps to explain why it is possible for a paper to be rejected even with a positive meta-review. AC recommendations are made without regards to the acceptance quotas, and SACs necessarily have to override them in many cases.

# 7.3 The Impact of Other Submission Properties

There are many properties of submissions that could systematically make a difference to their final outcome. In this section we investigate the possible effect of the type of contribution, the target languages, whether the reviews were problematic (as reported by the authors), and whether the paper was available as a preprint. To establish the importance of these factors, we fit another multinom() model, similarly to what we did in Table 4, and obtain the significance levels for each variable using Type III Anova. While the ordinal model would potentially better preserve the natural order of the final outcome (rejection being the worst and acceptance to the main track being the best outcome), the fitted model violated the assumptions of the ordinal model.

Since this model does not include strong predictors such as reviewer scores and chair recommendations, the fit of this model is relatively poor23 compared to the model in Table 4, which has a McFadden’s pseudo-R2 of approximately 0.80 (indicating a substantial improvement over the null model). In contrast, this model has a McFadden’s pseudo-R2 of approximately 0.01, suggesting that it barely improves upon the null model. Nevertheless, this model can still be used to establish the individual contributions of the submission-level properties, which likely interact in complex ways in the scores and recommendations. Statistically significant factors are also not necessarily strong predictors by themselves.

The results of this experiment are shown in Table 6. According to this analysis, the following factors have a statistically significant impact on submission outcome: low-quality reviews, preprinting, short/long paper type, and three types of contributions (software, reproduction, and theory).

To also assess the relative importance of our predictors in forecasting the final outcome, we employed a Random Forest algorithm (Liaw and Wiener, 2002). The results are shown in Figure 6. The most crucial predictor was Review Issues (i.e., author complaints about reviews25) with a Mean Decrease Gini value of 46.09. This suggests that this predictor played the most significant role in reducing the Gini impurity, and therefore, in improving the precision of our model. The second factor with the biggest Mean Decrease Gini is Preprinting (22.84). This analysis does not state the absolute importance of any factor (e.g., that 23 Its 3-class accuracy is 52%, vs 90% for the model shown in Table 4. This is the accuracy of the model on the withheld test set when the model is fitted with 70% of the data. The accuracy of the model on all data is about 1% higher.

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

The number of author complaints likely reflects (at least) two factors: the reviews that were truly problematic, and simply negative reviews since the authors are more likely to complain about those. In the latter case the leading cause for rejection is the negative review.

# Variable Importance

# Review issues

# Preprinted

# Contribution software

# Languages number

# Contribution resource

# Significance

# Paper Type

Not Significant

# Previous Submissions

Significant

# Contribution reproduction

# Contribution efficiency

# Contribution theory

# Languages not only english

0
10
20
30
40
Mean Decrease Gini

Figure 6: The importance of predictors in predicting the Outcome, ranked by mean decrease in Gini impurity. Predictor significance is indicated by color, with dark purple for not significant and dark green for significant predictors as per levels of significance indicated in Table 6.

# Contribution type

|Contribution type|% submissions|Match|Mismatch|Match-Mismatch|
|---|---|---|---|---|
|Efficiency|9.62|50.27|46.56|3.71|
|NLP engineering experiment|61.5|46.66|47.33|-0.67|
|Software and pre-trained models|12.14|56.75|45.56|11.19|
|Data resources|19|49.25|46.37|2.88|
|Data analysis|10.48|48.14|46.78|1.36|
|Reproduction studies|2.08|66.25|46.51|19.74|
|Approaches for low-resource settings|18.22|49.79|46.28|3.51|
|Surveys|1.64|44.44|46.96|-2.52|
|Interpretability|25.29|51.8|45.27|6.52|
|Theory|3.8|56.85|46.53|10.32|
|Position papers|2.57|53.54|46.74|6.8|

Table 7: Acceptance rate among direct submissions that were reviewed and considered for acceptance, with (Match) and without (Mismatch) given contribution types. The average acceptance rate in this pool is 46.92%. Preprinting increases the chances of acceptance by X%), and we are not claiming that these effects are independently large—but they do appear to be statistically significant. We will discuss these factors further: short/long papers in §7.3.1, contribution types in §7.3.2, review issues in §7.5.5, preprints in §7.5.7.

# 7.3.1 Short/long papers

Short papers have had significantly lower acceptance rates at most recent *ACL conferences. To mitigate that, we highlighted the problem in the reviewer instructions, had a separate Soundness formulation for short papers, and asked the SACs to consider the short and long papers separately, with their own target acceptance quotas. Despite all that, the significant effect of paper type (Table 6) is obvious: the long papers had 23.50% acceptance rate to main track vs 16.53% for short, and for Findings, the rate was respectively 41.89% vs 35.58%. The core reason seems to be that the source reviewer scores are systematically lower, despite all calls to not expect 120% thoroughness of short papers.

# 7.3.2 Types of contribution

We were pleasantly surprised to find a significant positive effect for the contributions of theory, reproductions, and pre-trained models and software (Table 6). The two latter types are in line with the findings by (Magnusson et al., 2023) who report that reproducibility efforts are rewarded. This effect is also visible from simply considering the differences in acceptance rates for papers with and without these contribution types, shown in Table 7. In fact, the “average” acceptance rate of 46.92% is the closest to the most “mainstream” type of contribution (NLP engineering experiment, 61.5% submissions) – and all other contribution types except surveys have the acceptance rate at least slightly higher than that.

|Submissions subset|Contribution type|% submissions|Match|Mismatch|Match-Mismatch|
|---|---|---|---|---|---|
|Resources & Evaluation|Resource|5.48|48.39|48.21|0.18|
|All tracks without Resources & Evaluation|Resource|94.52|49.48|46.34|3.14|
|Interpretability and Analysis of Models|Interpretability|4.89|52.69|57.14|-4.45|
|All tracks without Interpretability|Interpretability|95.11|51.61|45.18|6.43|

Table 8: Acceptance rate among direct submissions inside and outside tracks that targeted a resources and interpretability contributions, with (Match) and without (Mismatch) given contribution types. The average acceptance rate in this pool is 46.92%.

| |Accepted papers only|Rejected papers only|All papers| |
|---|---|---|---|---|
|Ordinal|Soundness|20.72 0.093[0.047,0.137]|17.68 0.116[0.076,0.156]|19.10 0.318[0.294,0.340]|
| |Excitement|12.68 0.120[0.075,0.169]|10.65 0.134[0.094,0.173]|23.23 0.311[0.287,0.334]|
|Categorical|Soundness|77.28 0.032[−0.052,0.112]|37.39 0.092[0.064,0.119]|53.80 0.221[0.194,0.248]|
| |Excitement|37.11 0.087[0.055,0.120]|49.60 0.074[0.039,0.114]|43.74 0.233[0.212,0.255]|

Table 9: Inter-reviewer agreement on soundness and excitement scores, measured as raw % agreement (%) and Krippendorff’s alpha (α) with 95% confidence interval [CI]. We consider only direct submissions to ACL’23 that were fully reviewed, and for which the final decisions were made: 3847 in total, 1805 “accept” (to either Main track of Findings), and 2042 “reject”.

The lack of a visible disadvantage in acceptance rates for non-mainstream types of contributions is a very positive finding. Consider the case of efficiency-oriented papers: they did not have a dedicated track, but their acceptance rate was not lower (and even a bit higher) than for the average in the pool (where the majority of engineering-oriented submissions focuses on performance). In effect, every track was an efficiency track, allowing both access to the area expertise and reviewers with interest in this type of contribution. We cannot establish to what extent this is due to Area-Contribution-Language matching or an overall increased interest in the need for efficient NLP solutions. But as long as such contributions are in the minority, we would recommend ensuring the matches by this criterion.

A complication for our analysis arises for two contribution types that also had large associated tracks: resources and interpretability. In this case, it is possible that the lack of difference in acceptance rate is due to the extra effort of ensuring the reviewers with matching interests through the track mechanism. To check for that, we compare the acceptance rates for these types of contributions inside and outside of the dedicated tracks (Table 8). We find that in all cases the match between tracks and contribution types yields a 3-6% increase above the average acceptance rate of 46.92%. An interesting case is interpretability and model analysis, which has a 4.45% higher acceptance rate outside of its dedicated track (probably indicating an appreciation for papers that perform analysis in addition to some other type of contribution).

# 7.4 How Much do ACL Reviewers Agree?

The issues with consistency of peer review were recently highlighted in the ML community by the two NeurIPS experiments (Price, 2014; Cortes and Lawrence, 2021; Beygelzimer et al., 2021). By treating peer review as an annotation problem (Rogers and Augenstein, 2020), we can apply the existing methodology for analyzing inter-annotator agreement (IAA). We consider three reviewers (annotators) per paper, discarding the rare cases of 4 reviews (from emergency assignments). We compute Krippendoff’s α (Krippendorff, 2011) on the Soundness and Excitement scores (Table 9). We treat these scores as ordinal data. We also experiment with mapping both scores to binary “positive/negative” categories (3–5 > “sound" for Soundness and 3.5–5 > “exciting” for Excitement, since the borderline scores were 2 for Soundness was 2 and 3 for Excitement).

“Ordinal” refers to the α coefficient computed using raw scores treated as ordinal variables. The percentage agreement for Soundness was computed using the raw scores (5-point scale). In order to match the scale length the percentage agreement for Excitement was computed on the rounded scores (i.e., 3.5 was treated as 4.0, etc.). “Categorical” denotes scores converted into either positive or negative decisions based on the given threshold (3.0 for Soundness and 3.5 for Excitement).

# Findings

# Ratio of review scores per acceptance status

| |Excitement| |Soundness| | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|
|Reject|7%|17%|24%|27%|18%|6%|11%|44%|39%|5%|
|Findings|18%|29%|27%|16%|6%|35%|49%|13%|2.5| |
|Main|6%|39%|31%|15%|5%|8%|59%|30%|4%| |

# Ratio of positive (+) and negative (-) score combinations per acceptance status

| |Excitement| |Soundness| | | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|
|Reject|3%|13%|37%|47%|22%|38%|29%|11%| | | |
|Findings|11%|40%|37%|13%|65%|30%|5%|− − −|− − +|+ + −|+ + +|
|Main|48%|39%|12%| |88%|11%| | | | | |

# Total number of submissions with different combinations of positive (+) and negative (-) scores

|Soundness|+ + + + + - - - + - - -|556|33|7|0|600| |
|---|---|---|---|---|---|---|---|
| |+ + +|500|457|461|246|23|400|
| |+ + -|300|631|296|36|7| |

# Number of accepted submissions with different combinations of positive (+) and negative (-) scores

|Excitement|- - -|181|339|354|205|100|
|---|---|---|---|---|---|---|
| | |65|46|11|2|100|

Figure 7: Review scores vs acceptance outcome. “Positive” scores (+) refer to the above-borderline scores (Soundness >=3, Excitement >=3.5), and “negative” (-) - to the number of scores below borderline.

Consistent with the general perception of inconsistency in peer review, α shows a level of IAA that seems far too low (the rule of thumb is that “substantial” agreement is in the range of 0.6-0.8 (Artstein and Poesio, 2008; Paun et al., 2022)). However, the raw agreement for the accepted papers (in the categorical view, i.e. as sound/unsound, exciting/unexciting) is almost twice higher for Soundness than for Excitement. We interpret this as an indication that although the scores are still noisy, it helps to ask more specific questions with more objective criteria. The much lower raw agreement on the Excitement is also in line with our point that this is overall a less relevant direction for the author response and reviewer discussion. Arguably we do not even want a high agreement on Excitement: everybody interested in the same thing could indicate that the field is ossifying and stagnating.

As a sanity check, we also analyzed IAA for the raw reviewer scores of EMNLP 2022 and EACL 2023. Both of these conferences used a single “overall recommendation” score, formulated differently for short and long papers. In EMNLP 2022, for 3092 observations for 3 reviewers (discarding R4 data), with scores treated as ordinal data, we got α 0.316 for the short papers, 0.31 for long, and 0.318 for the whole distribution – which is almost exactly the same as our α for both our scores (in the ordinal case). In EACL 2023, for 1121 subjects for 3 reviewers we got α 0.317 for the short papers, 0.34 for long, and 0.348 for the whole distribution.

A related question is “what kind of disagreements do we actually have?” Figure 7a shows the distribution of individual score values for all papers in a given acceptance status, which suggests that even papers accepted to the main conference had some very negative reviews. Figure 7b breaks down the scores into “positive” (Soundness >= 3, Excitement >= 3.5) and “negative”, and considers the combinations of three reviews as “all positive” (+ + +), “all negative” (- - -), “2 positive, 1 negative” (+ + -) and “2 negative, 1 positive” (- - +). We can see that despite disagreements on the exact scores, the papers accepted to the main track have a high ratio of “positive” review combinations for Soundness (88%, only 11% papers with one negative Soundness score). But for Excitement our SACs accepted to the main track 39% papers with one negative Excitement score, and 37% papers with a single “champion” reviewer. For Findings, they even accepted 37% papers which only 1 reviewer was excited about. Figure 7c shows the total number of submissions with various combinations of positive and negative Soundness and Excitement scores, and Figure 7d shows the same categories, but with the number of accepted papers with that score combination. Our data indicates that despite noisy scores and high disagreement, the mechanism of ACs and SACs does “rescue” many papers with one negative review, and at least the raw agreement does improve for the more specific Soundness score. Judging by the community feedback (§5), in this first implementation there was a lot of confusion about what the scores meant, and we expect that in future iterations the agreement could improve further.

# 7.5 Analysing Reviews and Review Scores

In this section, we take a step back from the final acceptance decisions and look only at the individual reviews and their scores, rather than the final outcome of the submission.

# 7.5.1 Do the Area-Contribution-Language matches impact reviewer scores?

To answer this question, Figure 8 shows the distributions of the individual reviewer scores for Soundness, Excitement, reviewer Confidence, and Reproducibility for all cases where the reviews were or weren’t matched by the area, contribution type, or language. The biggest visible impact is in reviewer Confidence, where the contributions are not matched by area: the ratio of reviews with high scores (4+) is decreased by about 14%. A worrying observation is that there is a 5% increase in high Confidence scores for the submissions where the reviewer is not matched by language and could be expected to feel less rather than more confident. We also observe an 11% increase in Soundness ratings 3+ from reviewers matched by language vs those mismatched, and 7% in Reproducibility.

# 7.5.2 Do the Area-Contribution-Language matches impact the reviewer activity?

To establish whether Area-Contribution-Language matching had any effect on reviewer activity, we counted the reviewers as “active” if they had at least one forum message or more than one review edit. The distributions of active/inactive reviewers that are/aren’t well-matched to submissions by Area-Contribution-Language criteria are shown in Figure 9. At a glance, there are a lot more matched & active reviewers, but since generally a lot more reviewers were matched than mismatched (see Table 2), we would generally expect that to be the case even by chance.

To establish whether there are any statistically significant effects, we first fit a generalized linear model (GLM) using the glm() function in R. The dependent variable was binary (the activity of the reviewer). The predictors were a contribution match (binary variable), a studied language match (three-layer categorical variable), and an area match (binary variable), all of which were treated as categorical variables (at least one matching keyword of the correct type). The link function was logit, corresponding to a binomial distribution of the response variable (logistic regression).

To validate the assumptions of the GLM, we examined the variance inflation factors (VIFs) using the vif() function in R to assess multicollinearity among predictors. The VIFs were all close to 1, suggesting that multicollinearity was not a concern. We also visually inspected residual plots to assess the model fit and did not find any obvious deviations from homoscedasticity or linearity.

For the language we consider three categories: (1) non-English language match, (2) non-English language mismatch, and (3) match only by English; under the assumption that all reviewers will be familiar with English.

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

# Match area

# Match contribution

# Match language

|True|2%|27%|42%|26%|3%| | | | |
|---|---|---|---|---|---|---|---|---|---|
|True|3%|27%|41%|25%|3%| | | | |
| | |English|2%|28%|42%|25%|3%| | |
| | | | |Soundness| | | | | |
|True| |1|3%|24%|44%|26%|3| | |
|False| | | | | | | | | |
|3%|29%|40%|25%|3%| | | | | |
|2%|27%|41%|26%|4%| | | | | |
| | |False|4%|22%|34%|33%|7%| | |
| | |False| |5| | | | | |
|0%|25%|50%|75%|100%| | | | | |

# (a) Soundness

| |Match area|Match contribution|Match language|Excitement| | | | |
|---|---|---|---|---|---|---|---|---|
|True| |English|17%|22%|22%|19%|12%| |
|True|16%| |22%|22%|19%|12%| | |
|True| | |2|15%|21%|22%|21%|12%|
|False|19%|22%|19%|22%|10%| | | |
|False| |17%|21%|21%|21%|12%| | |
|False| |3.5|16%|18%|16%|18%|17%|8%|
|False| |4| | | | | | |
|0%|25%|50%|75%|100%| | | | |

# (b) Excitement

|Match area|Match contribution|Match language| | | | | |
|---|---|---|---|---|---|---|---|
|True|11%|55%|29%|5%| | | |
|True|11%|54%|29%|6%| | | |
| | |English|11%|54%|29%|6%| |
| | | |Rev. Conf.| | | | |
|True| |8%|54%|32%|6%| | |
|False|7%|45%|35%|11%| | | |
|False|11%|53%|28%|7%| | | |
| | |False| |4| | | |
|0%|25%|50%|75%|100%| | | |

# (c) Reviewer confidence

|Match area|Match contribution|Match language| | | |
|---|---|---|---|---|---|
|True|12%|46%|30%|10%| |
|True|12%|46%|31%|10%| |
|English|13%|46%|30%|9%| |
| |Reproducibility| | | | |
|True|10%|47%|31%|10%| |
|False|13%|45%|30%|10%| |
|False|13%|49%|27%|9%| |
|False| |4| | | |
|0%|25%|50%|75%|100%| |

# (d) Reproducibility

Figure 8: Area-Contribution-Language Matches impact on reviewer scores. In each plot, True/False refers to the reviews where the submissions were/weren’t matched by area, contribution or language.

The results of the GLM (see Table 10) suggest that contribution match is a significant predictor of the reviewer’s activity (β = 0.16, SE = 0.08, z = 1.97, p = 0.048). Since the estimates relate to log-odds we consider the exponential of the reported value (1.178) which suggests that the odds of the reviewer being active when the contribution type is well-matched are 1.178 times higher than when the contribution does not match the reviewer’s expertise. The remaining variables, that is language match and area match, are not significant predictors in this model (p > 0.05).

Finally, we considered the language match as a binary variable, excluding English language papers. We conduct a Chi-square test (χ2) to examine the association between the language match (excluding English) and reviewer activity Table 11. The test reveals no significant association between the language match and reviewer activity (χ2(1)=0.73432, p = 0.3915). The chi-square test was performed using Pearson’s Chi-squared test with Yates’ continuity correction with the chisq.test() function in R.

We conclude that of the Area-Contribution-Language matching rubrics, only the contribution type contributes to improvement in reviewer activity. Although the effect is modest (1.178 times increase in likelihood of reviewer activity), given that reviewer activity post-submission is very important, and its level needs to be improved (§5.4), we would urge the future chairs to consider this criterion in the assignments. It also provides a quick and interpretable way to consider the variety of the types of work.

McFadden‘s pseudo-R2 of the model is 0.0008231973, which is very low. This suggests that our model does not explain much of the variability in the data. However, it is important to note that in the context of generalized linear models, the interpretation of pseudo-R² is not as straightforward as it is in ordinary least squares regression. The pseudo-R² is not necessarily a measure of the proportion of variance explained by the model in the data. Instead, it is a measure of the likelihood improvement per observation relative to the null model. Despite the low pseudo-R², our model could still provide valuable insights into the relationships between the independent variables (match type) and the reviewer‘s activity.

# 7.5.3 Do reviewer confidence scores reflect their experience?

START profiles contain self-reported reviewer experience labels (“never”, “first time”, “3 or fewer events”, “4 events and more”. We explored the relationship between this data and reviewer Confidence scores but found no strong effect. We do observe a small (about 4%) increase in the volume of 4+ Confidence scores for the most experienced reviewers, and it’s significant according to the ordinal logistic regression model. But the effect is quite small, and judging by this data we don’t recommend relying on confidence as a proxy for reviewer experience. Moreover, we observe no relation between this reviewer experience data and the number of review issues reported by the authors. This is a rather depressing finding from the perspective of reviewer training, and we hope that it is rather due to START profiles not being updated by the reviewers.

# 7.5.4 Do the reviewer scores correlate with length of the reviews?

The ACL review form had the following text input fields: summary, reasons to accept, reasons to reject, questions to the authors, missing references, suggestions&typos, and confidential notes to the chairs. We roughly estimated the length of these inputs by splitting on the whitespace, and computed Spearmans correlation (Spearman, 1987) between these variables and reviewer scores for Soundness, Excitement.

We fit model in R using the polr() function from the MASS package (Venables and Ripley, 2002) with reviewer’s confidence as an ordinal DV and experience as a three-layer categorical IV. We compare this model to an intercept-only model using the Anova() function. While the difference between these models is significant, McFadden’s pseudo-R² is extremely low (4.247533 × 10−4).

# Figure 9: Area-Contribution-Language matches vs reviewer activity.

|Reviewer Active|FALSE|TRUE| | | | | | |
|---|---|---|---|---|---|---|---|---|
|8000|8009|8000| | | | | | |
|6000| |6000| | | | | | |
|4000| |4000| | | | | | |
|Count|2446| |2468|2409| | | | |
|2000|2000|2000| | | | | | |
|239|860|217|626| | | | | |
| | | | | |107|318|169|573|
|0| |0| | | | | | |

# Table 10: Generalized linear model (GLM) estimates for predicting reviewer activity using match categories.

| |Estimate|Std. Error|z value|Pr(>|z|)|
|---|---|---|---|---|
|(Intercept)|1.1511|0.1012|11.38|0.0000|
|Match Contribution (True)|0.1638|0.0830|1.97|0.0484|
|Match Language (False)|-0.1076|0.1142|-0.94|0.3461|
|Match Language (True)|0.0114|0.0921|0.12|0.9015|
|Match Area (True)|-0.1151|0.0786|-1.46|0.1432|

# Table 11: Results of Pearson’s Chi-squared test with Yates’ continuity correction for the effect of language match (excluding English) on the reviewer’s activity

|Test|Chisq|df|p-value|
|---|---|---|---|
|Pearson’s Chi-squared (Yates’ correction)|0.73432|1|0.3915|

# Confidence

| |Confidence|Soundness|Excitement|Reproducibility|Reasons to Accept|Reasons to Reject|Questions|References|Suggestions|Confidential|
|---|---|---|---|---|---|---|---|---|---|---|
|Confidence|1|Soundness|-0.08|1|Excitement| | | | | |
|Excitement|-0.11|0.68|1|Reproducibility| | | | | | |
|Reproducibility|0.09|0.29|0.26|1|Reasons to Accept| | | | | |
|Reasons to Accept|-0.02|0.28|0.37|0.15|1|Reasons to Reject| | | | |
|Reasons to Reject|0.14|-0.36|-0.35|-0.11|0.17|1|Questions| | | |
|Questions|0.03|-0.01|0.04|-0.02|0.12|0.06|1|References| | |
|References|0.15|-0.08|-0.08|-0.01|0.03|0.15|0.14|1|Suggestions| |
|Suggestions|-0.03|-0.04|0.03|-0.03|0.14|0.14|0.27|0.16|1|Confidential|
|Confidential|0.01|-0.03|-0.03|-0.04|-0.01|0.04|0.04|0.05|0.09|1|

0.05).">

Confidence, and Reproducibility. The results are shown in Figure 10. As could be expected, we observe a significant negative correlation (-0.35-0.36) between the length of Reasons to Reject and both Soundness and Excitement scores, and the opposite trend for the Reasons to Accept (0.28-0.37). Interestingly, the length of Reasons to Accept also correlates positively with the Reproducibility score, indicating that the community appreciates this factor (0.15). Confidence has a similar correlation with the length of missing references. Finally, there is a high correlation between the length of “questions to the authors” and “suggestions”, indicating that the reviewers who engage with the submission deeply use both of these fields.

The highest positive correlation is between our Soundness and Excitement scores32 (0.68), which is in line with the intuition that unsound work would probably not be found exciting either.

# 7.5.5 What factors are associated with review issues?

As discussed in §5.3, we introduced a mechanism for the authors to flag specific types of issues with reviews, and we received such flags for 12.9% of the reviews. Figure 11 shows the ratio of reviews with complaints (True) and without (False). For both Soundness and Excitement there is a clear trend towards more complaints with lower scores, but there are also complaints for high scores (e.g., 43.1% of reviews which the authors complained about had Soundness 4). This makes more sense if we consider the figure Figure 11d, which shows that 95% complaints are made about reviews where at least one of the scores is 3 or less. This suggests that reported review issues are associated with negative reviews, even for Excitement (although we tried to make it clear that this score is subjective and does not need arguing).

To explore other possible factors that could make the reviews more likely to be reported we fit a GLM model using the glm() function in R. The dependent variable is the presence or absence of reported issues (binary variable), and the predictors are the Excitement score (ordinal), Soundness score (ordinal), Confidence score (ordinal), Reproducibility score (ordinal), length of Reasons to Reject (interval), length of Reasons to Accept (interval), the Contribution Match (binary), Area Match (binary), Language Match (three-layer factor), Reviewer’s Experience (three-layer factor), and Reviewer’s Activity (binary). The link32This finding is important for the model reported in Table 4: the acceptance decisions are indeed based on both factors, and they are meant to capture different information, but the high correlation between these two variables suggests that the estimates obtained in Table 4 should be interpreted with caution.

# Reviewer scores vs the amount of issues reported with reviews

| |TRUE|FALSE|
|---|---|---|
|Soundness|9%|2.9%|
| |43%|29.9%|
| |42.3%|41.2%|
| |5.5%|23%|
| |1|4|

Review Issues

0% 25% 50% 75% 100%

(a) Soundness

| |TRUE|FALSE|
|---|---|---|
|Excitement|3.8%|2.6%|
| |12.9%|18.3%|
| |25.5%|23.5%|
| |27.7%|21.6%|
| |19.8%|18.2%|
| |7.8%|10.8%|
| |2.2%|3.8%|
| |1|3.5|

Review Issues

0% 25% 50% 75% 100%

(b) Excitement

| |TRUE|FALSE|
|---|---|---|
|Confidence|11.8%|10.5%|
| |53.9%|53.7%|
| |28.8%|29.5%|
| |4.5%|5.8%|
| |1|4|

Review Issues

0% 25% 50% 75% 100%

(c) Reviewer confidence

| |TRUE|FALSE|
|---|---|---|
|Lowest Score|3%|16%|
| |29%|10%|
| |13%|33%|
| |43%|10%|
| |5%|24%|
| |6%|2%|
| |1|3.5|

Review Issues

0% 25% 50% 75% 100%

(d) Min(Soundness, Excitement)

function was logit, corresponding to a binomial distribution of the response variable (logistic regression).

The coefficients of the fitted model are presented in Table 12.

We further employ the type III Anova using the ANOVA() function in R in order to obtain significance levels for each factor which are presented in Table 13. While McFadden’s pseudo-R2 of the fitted model is only 0.067, several variables of this model are significant predictors of the review issues.

The most significant factors are Soundness, Excitement, and the length of Reasons to Accept. All of these variables have a negative relationship with the reviewer issues, perhaps unsurprisingly, with higher scores the review is less likely to be reported. Similarly, longer text in the Reason to Accept field leads to less chance of the review being reported. Counter-intuitively, the positive coefficient associated with the reviewer being active suggests that when the reviewer is active (i.e. with at least one review revision or a forum message) the log-odds of the review issue increase by about 0.32, all else being equal. That is, the more active reviewers (putting in more effort) are actually receiving more complaints.

Other significant factors are Language Match and the reviewer’s confidence; both associated with negative coefficients. This suggests that when the reviewer is familiar with the non-English language investigated in the study, the log-odds of a review issue decrease by approximately 0.26 (i.e., the review is 1.29 times less likely to be flagged for issues). Similarly, the negative coefficient of the reviewer’s confidence suggests that...

We inspect the residuals plots and compute the variance inflation factor to assure that the assumptions of GLM are not violated.

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

|Estimate|Std. Error|z value|Pr(> |z|)| |
|---|---|---|---|---|
|(Intercept)|0.5999|0.2570|2.334|0.0196 *|
|Soundness|-0.3816|0.0479|-7.967|1.63e-15 ***|
|Excitement|-0.4584|0.0549|-8.349|&lt; 2e-16 ***|
|Confidence|-0.0855|0.0393|-2.176|0.0295 *|
|Reproducibility|0.0609|0.0335|1.816|0.0693 .|
|Reasons to Reject|0.0004|0.0002|1.508|0.1315|
|Reasons to Accept|-0.0052|0.0011|-4.748|2.06e-06 ***|
|Match Contribution (True)|0.0763|0.1148|0.664|0.5066|
|Match Area (True)|-0.1352|0.1030|-1.313|0.1892|
|Match Language (False)|0.0270|0.1476|0.183|0.8550|
|Match Language (True)|-0.2639|0.1275|-2.070|0.0384 *|
|Experience (Experienced)|-0.0744|0.0684|-1.087|0.2769|
|Experience (Zero)|-0.0274|0.1164|-0.235|0.8143|
|Reviewer Active (True)|0.3172|0.0737|4.303|1.69e-05 ***|

Table 12: Coefficients of the Generalized Linear Model predicting the review issues. The table includes the coefficient estimate, standard error, z-value, and p-value for each predictor.

| |LR Chisq|Df|Pr(>Chisq)|
|---|---|---|---|
|Soundness|64.65|1|0.0000 ***|
|Excitement|70.45|1|0.0000 ***|
|Confidence|4.71|1|0.0300 *|
|Reproducibility|3.31|1|0.0688 .|
|Reasons to Reject|2.23|1|0.1353|
|Reasons to Accept|24.17|1|0.0000 ***|
|Match Contribution|0.45|1|0.5035|
|Match Area|1.69|1|0.1940|
|Match Language|4.61|2|0.0998 .|
|Experience|1.24|2|0.5386|
|Reviewer Active|19.35|1|0.0000 ***|

Table 13: Type III Analysis of Deviance for the variables in the Generalized Linear Model predicting whether issues were reported for the given review.

Confidence suggests that with an increased Confidence score the likelihood of the review to be reported decreases though by a small margin.

# 7.5.6 Do we have bad actors?

To explore the possibility that many reported review issues are due to individual unprofessional reviewers, let us consider the fact that 1,620 reviews with reported issues were authored by 1311 reviewers, i.e. about a third of our total pool. But most of these reviewers had more than three reviews, and 1060 of them were only reported once. Of the remaining reviewers, 201 were flagged twice, and 50 reviewers had more than 3 complaints. We conclude that while there are indeed some unprofessional reviewers, and conferences need to systematically share such information and develop a system to address this problem, there are few such cases (6.2% if we consider all reviewers with more than 2 flags, and 1.2% with more than 3 flags). An interesting takeaway from Figure 11c is that the reviews that are problematic according to the authors, do not have lower confidence scores, so these are unlikely to be the new reviewers or the reviewers unfamiliar with the area.

According to folk wisdom, the bad reviewer is usually Reviewer2 (sometimes Reviewer3). We clear their good name: at ACL’23, the most issues were reported for Reviewer1, as shown in Figure 12.

# 7.5.7 Can the reviewers tell who the authors are?

In 567/12606 (4.5%) reviews the reviewers indicated that they have seen the paper, either by seeing a preprint (533) or by other means (34). Additionally, 513 (4.1%) reviewers indicated that they had a good guess of the author identity based on the paper content. 11460 (90.9%) ACL’23 reviews were reported as fully anonymous.

The community “recall” on the preprinted submissions is as follows: we had 628 submissions (13.8%

# 7.5.8 Do preprints affect the peer review process?

Having established that reviewers do have a high recall for preprints (§7.5.7), we investigate the possible connection between the reviewer’s awareness of the author identity on their Soundness, Excitement, and Confidence scores by fitting Cumulative Link Mixed Effect models with the Laplace approximation using the clmm() function for the ordinal package in R (Christensen, 2022). The response variable is the given score and the predictor is the Anonymity answer (fixed effects). We also employ random intercepts for the paper (SubmissionID) and reviewer (ReviewerID) to account for this variability (random effects).

# Soundness

The results of the model fitted for the effect of Anonymity on the Soundness scores are present in Table 14. The Anonymity has five possible values: (1) the reviewer does not know the authors (reference level), (2) the reviewer may know the authors, (3) the reviewer knows the authors via means other than online posting, (4) the reviewer knows the authors via online posting prior to the anonymity period, and (5) the reviewer knows the authors via online posting post to the anonymity period. Estimates for different answers to the anonymity question presented in Table 14 suggest that the reviewers were 1.59 times more likely to assign higher Soundness scores when they thought they may know the authors, and 1.75 times more likely to assign higher Soundness scores when they have seen the preprint online.

# Excitement

The results of the model fitted for the effect of Anonymity on Excitement are present in Table 15. Estimates for different answers to the anonymity question presented in Table 15 suggest that the reviewers were 1.49 times more likely to assign higher Excitement scores when they thought they may know the authors, and 1.73 times more likely to assign higher Excitement scores when they have seen the preprint online.

# Confidence

The results of the model fitted for the effect of Anonymity on reviewer’s Confidence are present in Table 16. Estimates for different answers to the anonymity question presented in the table suggest that the reviewers were 1.29 times more likely to report higher Confidence scores when they have seen the preprint online.

We validate the model fit by examining residual plots and convergence criteria. The residual plots showed no clear patterns or extreme outliers, and the satisfactory convergence indicates a reasonable model fit. We further observe that, perhaps unsurprisingly, both SubmissionID and ReviewerID account for a substantial portion of the variability in each of the response variables.

# Table 14: Cumulative Link Mixed Model Results for the effect of Anonymity on the Soundness scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

|Estimate|Std. Error|z-value|Pr(>|z|)| | |
|---|---|---|---|---|---|
|Random effects:| | | | | |
|SubmissionID (Intercept)|2.2427|1.4976| | | |
|ReviewerID (Intercept)|0.7806|0.8835| | | |
|Fixed effects:| | | | | |
|Anonymity (2)|0.46037|0.11744|3.920|8.85e-05|***|
|Anonymity (3)|0.02567|0.41291|0.062|0.9500| |
|Anonymity (4)|0.55947|0.13081|4.277|1.90e-05|***|
|Anonymity (5)|0.36749|0.27565|1.333|0.1820| |

# Table 15: Cumulative Link Mixed Model Results for the effect of Anonymity on the Excitement scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

| |Estimate|Std. Error|z-value|Pr(>|z|)| |
|---|---|---|---|---|---|
|Random effects:| | | | | |
|SubmissionID (Intercept)|1.6675|1.2913| | | |
|ReviewerID (Intercept)|0.5163|0.7185| | | |
|Fixed effects:| | | | | |
|Anonymity (2)|0.39828|0.10629|3.747|0.000179|***|
|Anonymity (3)|0.13179|0.37724|0.349|0.726816| |
|Anonymity (4)|0.54498|0.11816|4.612|3.98e-06|***|
|Anonymity (5)|0.08329|0.24708|0.337|0.736049| |

thought they may know the authors, and 1.80 times more likely to assign higher Confidence scores when they saw the preprinted online.

We thus conclude that submissions with preprints, as well as submissions where the reviewers believe they could guess the authors, systematically receive higher ratings for both Soundness and Excitement, as well as higher Confidence scores. We further note that preprinted papers are disproportionately recommended for consideration for best paper awards (and without such a recommendation from at least one reviewer the submissions are not considered by the best paper committee). In total, only 1.6% papers received any reviewer nominations at all, and for 30% of those papers, the authors had disclosed preprints.

While our data shows the pattern of higher scores, acceptance chances, and best paper nominations for preprinted submissions, the causal mechanism remains a question: is it because such papers are inherently higher quality, or because of the benefits of community feedback they may receive, or because of the well-documented reviewer biases towards famous names and institutions (Peters and Ceci, 1982; Tomkins et al., 2017, among many others)? Since these possibilities necessitate different actions on the part of the chairs who strive for higher-quality program, the causal question needs to be answered for informed policy decisions. Since we observe an increase in likelihood of higher scores both for real preprints and for submissions where the reviewers only thought that they might know the authors (although the effect is smaller in that case), we can conclude that the social factor is definitely present—but more research is needed to establish its exact contribution. But the fact that we only had 13.8% preprints suggests that the current 1-month embargo policy is effective in at least reducing the volume of the problem.

# 8 Special Review Processes

# 8.1 Ethics review

Following the practice started at NAACL 2021, we formed an Ethics Committee (EC) dedicated to ethical issues. The review process was based on work in prior conferences and further developed by ARR and recommendations from the ACL ethics committee. Initially there were 235 technical reviews flagging 218 papers for ethics concerns, and the SACs narrowed down the list based on the guidelines developed by the ethics chairs) to 75 papers, 6 of which did not make it to the ethics review (either withdrawn or cleared).

| |Estimate|Std. Error|z-value|Pr(>|z|)|
|---|---|---|---|---|
|Random effects:| | | | |
| |SubmissionID (Intercept)|0.416|0.645| |
| |ReviewerID (Intercept)|3.413|1.847| |
|Fixed effects:| | | | |
|Anonymity (2)|0.2576|0.1227|2.099|0.0358 *|
|Anonymity (3)|0.4210|0.4194|1.004|0.3155|
|Anonymity (4)|0.5874|0.1342|4.376|1.21e-05 ***|
|Anonymity (5)|0.3413|0.2864|1.192|0.2334|

Table 16: Cumulative Link Mixed Model Results for the effect of Anonymity on the Confidence scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

20 papers under ethics review were labeled accept as-is, 43 received conditional accepts, and 6 were recommend for rejection. Of those recommended for rejection, 1 was accepted nonetheless, 1 was rejected as a result, and 4 were rejected on technical grounds. Of the conditionally accepted ones, 26 were rejected on technical grounds, and 1 was withdrawn. 16 passed the technical review and were conditionally accepted, meaning the ethics issues had to be addressed in the camera-ready version, to be verified by the SAC (based on EC guidance) prior to final acceptance.

The authors of all conditionally accepted papers submitted the camera-ready version and a short response that explained how they had made the changes requested. The SAC double-checked these revised submissions and responses, and confirmed that the ethical concerns had been addressed. As a result, all conditionally accepted papers were accepted to the main conference or Findings.

# 8.2 Best paper selection

ACL’23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding. In total, only 73 papers, i.e. 1.6% of all direct submissions were nominated by the reviewers or ACs for consideration for awards. These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 4 special awards (social impact, resource, reproduction, theme paper), and 39 outstanding papers. The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023.

We encountered several issues with implementing the best paper policy as described in the wiki. With 73 nominated papers, to keep it down to 10 papers per judge and have 2 reviews per paper, we had to recruit 15 judges. At this scale, the workload is compatible with organizing a separate track: recruitment, paper assignments, chasing late reviews – only this time recruiting exclusively very senior and busy people, and it is very important to uphold diversity considerations (which we weren’t able to do full justice). For the future, we recommend that a separate chair role is created for managing this process, similar in scope to the role of the ethics review chairs.

Furthermore, since the diversity considerations in the committee selection entail incompatible time zones, we found it impractical to require the judges to meet and jointly decide on the cases where they disagree (as recommended in the policy). Hence, after the judges cast their votes, the PCs made the final decisions on the basis of their recommendations (in particular, in the cases where one judge recommended outstanding paper and the other recommended not considering it further), we upheld the objections to flaws in the papers, shallowness of analysis, and ethical issues, which left us with 39 papers (a little short of the 1-1.5% total submissions policy target for the outstanding papers).

Finally, the ACL award policy described an Area Chair Award: the award that the SACs of a given track can give to one paper in their track, fully on their own authority. This was part of the guidelines for the final SAC recommendations, but we did not require them to be made at the same time. We sent out reminders after that, but received such nominations from only 12/26 tracks (with the theme).

Footnote: This is only for the direct submissions to ACL. Due to the difficulty of seeing ARR nominations in START, we did not notice the 2 nominations out of 305 ARR submissions until it was too late.

We found the agreement on the best paper committee votes to also be not very high: only 24/73 nominated papers received a unanimous vote to either consider for (any) award or not consider further.

# 9 Improving the Incentives

# 9.1 Improving Reporting Incentives for the Authors: Responsible NLP checklist

Following the effort started by NAACL 2022 and continued at ACL Rolling Review (Carpuat et al., 2021), we used the Responsible NLP Checklist as a way to ensure that all submissions conform to a certain minimum standard of reporting on their reproducibility efforts, data collection principles, and consideration of broader impacts. However, at NAACL 2022 and ACL Rolling Review, these checklists are only used internally during peer review.

To improve the transparency of NLP research and create a stronger incentive to invest effort in this work, we made the Responsible NLP Checklists an official part of all published papers. The authors filled out the checklist information in a special form, and we later used that form to generate pdf versions of the checklist, which was appended to every paper pdf for the ACL Anthology.

This change was announced in our Call for Papers, and we additionally communicated it to the authors. The authors had the opportunity to update the checklist form during the preparation of the camera-ready version of their papers.

One modification to the checklist was introducing a mandatory question about AI writing assistance. This was motivated by the introduction of OpenAI’s ChatGPT (OpenAI, 2022), the precedent of AI-assisted scientific paper writing of Meta’s Galactica (Taylor et al., 2022), and, more importantly, a massive wave of promotion for AI “writing assistants” shortly before our direct submission deadline. We did not aim to completely ban AI-assisted writing (which does have legitimate use cases such as assistance to non-native English speakers), but to improve transparency: just like with the other ethics-related questions in the checklists, our posted policy required authors to explicitly state what they did. Our question and policy were subsequently adopted by ACL Rolling Review.

Magnusson et al. (2023) have reported that the higher rate of “yes” responses to the Reproducibility checklist at 4 NLP conferences. Given that our checklist includes reproducibility questions, and reproducibility positively correlates with both Soundness and Excitement, we would expect the Responsible NLP checklist to perform the same role. The reviewers themselves were predominantly positive about it: 66.99% rated it as “somewhat useful”, 18.13% as “very useful”, and only 14.35% — as “not useful”.

Table 17 shows the ratios of submissions answering ‘yes’ to the questions of the checklist, and the acceptance rates for the submissions that answered ‘yes’ vs those that didn’t. For most questions of the checklist, there is a small increase in acceptance rate for submissions that answer ‘yes’. The most significant increases are for reporting limitations (so we recommend that the conferences keep mandating this section), reporting hyperparameters and computation budget (in line with the high correlation between reproducibility ratings and reviewer scores §7.5), citing relevant work, contributing scientific artifacts such as models and software (in line with our finding of a significant effect for this contribution type discussed in §7.3).

An interesting case is the “catch question” A3 (does your abstract accurately summarize your work?). It drew some criticism as “meaningless bureaucracy”, since all submissions should respond “yes” to it. It was actually intended to see that the responders were not just clicking through the checklist. Most authors did respond ‘yes’, but those 2.24% that didn’t saw a -25.4 decrease in acceptance rate. We interpret this as suggesting that the sloppiness in filling out the checklist correlates with sloppiness elsewhere in the work. Finally, our new question about the use of writing assistants is the only one where the response ‘Yes’ is associated with a decrease in acceptance rate, although not very large.

# 9.2 Improving Incentives for Reviewers: Reviewer Awards

Arguably the biggest source of issues with peer review quality is the lack of incentives to invest more work in invisible service labor. One direction is reputational awards, eg via creating reviewer profiles, as in Publons. Another is material awards, such as monetary prizes similar to the best paper awards.

# Checklist question

|Checklist question|% submissions|Yes|Not Yes*|Yes-Not_yes|
|---|---|---|---|---|
|A1 (limitations)|46.92|47.62|17.05|30.57|
|A2 (risks)|56.23|49.28|43.88|5.4|
|A3 (catch question)|97.76|47.49|22.09|25.4|
|A4 (AI-assisted writing)|7.3|41.28|47.36|-6.08|
|B (artifacts)|72.45|50.09|38.58|11.51|
|B1 (cite)|71.02|49.96|39.46|10.5|
|B2 (license)|37.8|52.48|43.54|8.94|
|B3 (intended use)|45.28|49.48|44.8|4.68|
|B4 (PII)|22.02|49|46.33|2.67|
|B5 (documentation)|48.95|50.93|43.08|7.85|
|B6 (statistics)|70.47|49.76|40.14|9.62|
|C (computation)|92.31|47.76|36.82|10.94|
|C1 (parameters)|78.58|48.96|39.44|9.52|
|C2 (hyperparams)|85.5|48.49|37.63|10.86|
|C3 (stats)|81.02|48.19|41.51|6.68|
|C4 (packages)|76.01|47.16|46.15|1.01|
|D (humans)|28.98|52.11|44.8|7.31|
|D1 (instructions)|20.95|53.85|45.08|8.77|
|D2 (payment)|21.19|53.5|45.15|8.35|
|D3 (consent)|17.31|51.2|46.02|5.18|
|D4 (IRB)|9.62|53.24|46.25|6.99|
|D5 (demographics)|14.61|54.27|45.66|8.61|

Table 17: The ratio of ‘Yes’ responses to checklist questions vs the responses other than ‘yes’ (i.e. both ‘no’ and ‘no response’). The average acceptance rate in this pool is 46.92%.

Another is punitive incentives, such as penalizing the late reviewers by delaying the reviews for their own submissions (Hauser and Fehr, 2007), or even blocking them from reviewing at future conferences. All of these approaches are not without issues. Punitive incentives generally shift the focus to not getting penalized, rather than delivering high-quality reviews. Material awards may introduce the wrong incentives (Squazzoni et al., 2013), and, depending on the institution and the country, the prize may be taxed or not even make it to the recipient. Conference fee waivers also may reward the reviewer’s institution rather than the reviewer, since the institutions usually bear the registration costs. While a survey found that reviewers generally prefer reputational awards over material (Warne, 2016), their value also depends on whether the reviewer’s institution rewards such work.

We proposed to the ACL exec (and received their approval for) an initiative to match the new ACL best paper award policy with recognizing about 1-1.5% of outstanding reviewers and chairs. This combines reputational and material incentives. Instead of monetary prizes, we proposed awarding vouchers for virtual attendance of any ACL (ACL, NAACL, EACL, AACL, EMNLP) conference of the awardee’s choice, to be used within a year of the award date. Since many institutions do not support the attendance of conferences without accepted papers (or even with papers accepted to workshops and Findings), we hope that this measure will increase the overall number of conferences that the awardees can attend.

We asked the area chairs to nominate the reviewers in their pool who provided extra helpful reviews, high-quality emergency reviews, “champion” reviews, reviewers who were particularly active in the discussion phase, or demonstrated exceptional open-mindedness or expertise. We received 51 such nominations. We also asked the Senior Area chairs to nominate exceptional area chairs, receiving 13 nominations. Finally, we as the program chairs also nominated the (3) SACs of the track who were the most on-time, provided the most helpful feedback, and followed our instructions the most closely. Excluding the duplicates, this resulted in 67 total nominations. All awards will be announced on the conference website.

Since the total number of nominations was within our target number of awards (1-1.5% of total reviewers and chairs), we were able to award all 66 nominations (out of 4998) without creating a selection committee. In the future, we recommend that an extra volunteer role is created for managing the selection of awardees and managing the awards.

40https://2023.aclweb.org/program/best_reviewers

Caveats: despite our calls to nominate reviewers and chairs, relatively few ACs and SACs did that: only 7/70 SACs and 28/438 ACs. We recommend that the AC/SAC guidelines are expanded with a section about these awards, and that ACs are asked to start keeping track of potential outstanding reviewers at the (a) review quality check stage, (b) discussion stage, rather than only during meta-reviews (as we did). The SACs could be asked to start keeping track of outstanding ACs at the (a) assignment checks, if that is the process used by the venue, (b) meta-reviews, (c) nominating on the basis of quantitative analysis of the activity in the discussion forum and the number of author-reported review issues that the AC addressed.

# 9.3 Improving Incentives for Chairs: Peer Review Reports

Our final proposal for improving the incentives for peer review work was to increase its visibility by placing the program chair reports and any findings from their analysis of the internal conference data as an official part of the proceedings for the respective conference. This report is aiming to create a precedent for that. In the past, there have been two options for publishing such work: standalone research papers that undergo their own peer review, and miscellaneous blog posts and reports published in ACL wiki. But the former is not appropriate for reporting on incidental findings (since most of the program chairs work is not executed as a research project targeting a specific research question). The latter is unfortunately too difficult to discover, especially for the people outside of our field or new organizers who may not know which blog posts and wikis to search.

This initiative aims to improve the transparency of the overall process, and lets the younger members of the community have more insight into how the ACL conferences work. Moreover, given the increasing attention to peer review in NLP community (Gao et al., 2019; Caragea et al., 2019) and more broadly in ML conferences (Price, 2014; Stelmakh, 2020; Beygelzimer et al., 2021), it would be useful to make the incidental findings from the conferences more easily discoverable, incl. to the researchers in the ML community and other fields.

The main difficulty for the program chairs and the publication chairs with implementing this proposal is that the full report needs to be prepared before the conference, when there is a lot of other work. To implement this, the set of volunteer roles would need to be expanded (see section 10). We also recommend that to the extent possible, the future chairs start documenting their workflow for the report early on (perhaps during the main review cycle).

# 10 Recommendations

Improving logistics. There are several sources of papers to the ACL main conference that the program chairs have no control over: TACL, CL, Industry Track Papers, SRW papers. This means that the PCs need to ingest four different sources of information with potentially little means of interacting with the relevant authors (in contrast to direct submissions). ARR is in a liminal space between direct submissions and these other papers. The timing and format of how the papers enter ACL should be standardized.

Desk rejections. Desk reject requirements should be clearly stated in the call for papers or in the ACL Paper formatting guidelines. The guidelines omit rules or lack clear thresholds for rejection. For example, there is no minimum separation between captions and tables/figures nor between section titles and the text above and below. Nor are there minimum text sizes for text within tables or figures. Adding clear rules would make the first pass reviewing more efficient and fair. ACL also needs to communicate more clearly about the role of the aclpubcheck script: it’s a necessary but not sufficient check. Many authors assume that if they pass the aclpubcheck script, then they have followed all formatting guidelines.

Soundness/Excitement scores. With predominantly positive feedback in the exit survey (§6.4), and evidence of significant improvement in raw agreement (§7.4), we believe this experiment was successful and should be continued. The formulation of the scores and the review form should be improved, and care should be taken to reduce the overall complexity of the form.

Review issue flagging. This feature received overwhelming support from the authors, and should be continued and standardized (i.e., cleanly incorporated into author response form)—especially since it

is likely to improve after several iterations, when everybody is more familiar with it and the reviewer guidelines. More AC training is needed to address the flagged issues.

Continued reviewer policy publications. 12.9% of all ACL’23 reviews were flagged by the authors for various issues, with the most frequent problem being reviewer heuristics such as “too simple” and “not SOTA”. It is reassuring to know that the ratio of bad reviews is already not very high, but of course we should strive to further decrease it. The reviewer guidelines, in combination with the review issue flagging mechanism, serve a double purpose: even if the reviewers do not read them, the authors will (since they have the incentive to call out problematic reviews), and then the area chairs also will (to handle the author-flagged issues). Hence, eventually, these policies will become widely known across the community, and enforced by it. We urge the future chairs to continue publishing their reviewer policy or simply re-use ours, and explicitly point to it in review, author response, and meta-review forms.

Reviewer assignment check support. There is currently no convenient interface for the ACs to look up the assigned reviewers and browse the alternatives with up-to-date availability information. Its lack is a major hurdle for the chairs, and it may cause either delays in the process or skipping the checks.

Reviewer match explanations. Our area chairs were very positive about this feature. For venues not using an interpretable assignment algorithm such as our keywords-based process, at the very least, the reviewer profiles and relevant papers should be provided directly with the review, without any extra search.

Post-acceptance decision litigation. Having increased the acceptance rate for Findings, we were surprised to still receive a large volume of emails from the authors who, considering their scores and meta-review, argued that either their paper should have been accepted to the main track, or that it shouldn’t have been rejected. It appears that some subcommunities share their scores with each other, under the mistaken impression that if one paper with certain scores was accepted, others with similar scores should be too. We had no capacity for anything beyond checking for clerical errors. The peer review process is by no means perfect, and there was certainly some noise in the decisions—but it is also certain that many authors who disagree with their decisions would try to argue their case if given the chance. If such litigation is not an announced an official part of the conference process—doing so for the select few would not be fair to all the other authors who also disagree with their decisions. We recommend that the future chairs either build this into their process and dedicate time and resources to it, or pre-announce that decisions are final and will not be reconsidered, beyond the cases of clerical errors.

Area-Contribution-Language matching. The results of our experiment with exactly matching the reviewers with submissions by these areas allowed us to establish that it is possible to ensure a fair acceptance rate for most “non-mainstream” contribution types, and for the 63.8% of the submissions that had target languages other than English, we were able to provide a reviewer competent in that language. These results are by no means perfect, and it is important that the future venues improve on them, perhaps with other methods. But Area-Contribution-Language matching could be considered a fair baseline for the future conferences, when considering the success rates for different types of submissions and languages. All that is needed from the chairs is to include in submission forms the checkboxes for different types of contributions, and input fields for the target languages other than English. At the very minimum, the chairs would then be able to analyze the acceptance rates of different types of submissions, and compare it with ours (Table 7). One step further would be to also solicit this information from the reviewers, and estimate the quality of automated matches by the explicit keyword matches (see Table 2).

One more practical takeaway for future work is that if we used a solution relying purely on publication history from Semantic Scholar—25% of our matches would have been made on unreliable information. For embeddings-based solutions to work better, we would first need to provide them with better data, and this will take a bigger Semantic Scholar cleaning campaign than what we were able to elicit.

Reconsidering the acceptance rate for Findings. The initial iterations of Findings starting with EMNLP 2020 had the Findings acceptance rate at about 35%. This is the target rate we gave to our SACs, and then we tried to accommodate as many of their ranked preferences as we could. Although

we had over 40% rate with Findings, still, in many SAC comments we saw that they were overriding acceptance recommendations of ACs only to meet the quotas. While the quota for the Main track will stay at 20-25% for venue ranking reasons, we do not see why Findings could not be further extended to have room for most sufficiently sound work. About 60% of our direct submissions had at least two positive (above-borderline) reviews for Soundness and at least one for Excitement. Assuming some noise in the negative reviews for Soundness, it would be only reasonable to expect that at least 45%-50% submissions are Findings-worthy. Of course, the track SACs would not have to accept that many (the ratio of high-quality papers may vary between tracks and years), but when they do not see good reasons to reject — they should not be constrained by the Findings quota. This step would presumably also further decrease the burden of re-reviewing for resubmissions. We also recommend developing a standard process for Findings authors to apply for presentation at topically matching workshops, and for at least virtual poster presentation slots at the main conference.

# Further research on the effect of preprinting on peer review.

We find that the preprinted papers have consistently higher ratings (for both Soundness, Excitement, and reviewer confidence), get more recommendations for awards, and a higher acceptance rate. There are several possible underlying causes (from reviewer biases to higher initial paper quality and benefits of community feedback), which likely all contribute to this effect. Since these factors necessitate different actions if they were the major contributor to the observed effect, for informed policy decisions it is necessary to establish how they intermix. We observe however that although the present 1-month embargo policy does not solve this problem, it is effective at mitigating it, since we only had 13.8% such papers.

# Consistently working to improve peer review consistency.

Our analysis shows that the inconsistency in numerical reviewer score ratings is remarkably consistent across *ACL conferences (at about α 0.3 across EMNLP’22, EACL’23, and ACL’23). Among the likely culprits are miscalibrated scales, different interpretations of scales, at least some reviewers not even reading the guidelines, and reviewer biases. That said, we do see almost twice the raw agreement for our Soundness score (that is supposed to be more objective) over Excitement (more subjective), when the scores are mapped to the sound/unsound vs exciting/unexciting categorical variables. This suggests that asking more concrete questions does help (as long as the reviewer form does not become too complicated), and we can continue improving peer review on the basis of the general NLP methodology for iterating on guidelines and measuring agreement.

# Ethics review.

The innovation of the ethics review is useful and necessary, but it should be explicitly built into the timeline. We particularly struggled with the conditional accepts.

# Responsible NLP Checklist.

With predominantly positive reviewer feedback and evidence of improved acceptance rates for submissions that follow the best reporting practices, we believe that this is an important instrument for creating the right incentives for better science. We also recommend continuing to make it public, to strengthen these incentives.

# AI-assisted reviews.

We did not expect this happen so soon, but already at ACL’23 some chairs reached out to us with questions about reviews that they suspected to be at least partly generated. The reviewer guidelines will need to be updated with respect to that as well, including how sending papers to cloud-based language models may violate confidentiality.

# Review policy updates.

The rise of popular commercial systems such as ChatGPT that are claimed to be general-purpose, made an unfortunate match with our field’s tendency to expect the popular systems in all papers as universal baselines. We did not consider this at ACL’23, since ChatGPT fell out of scope of 3-month policy for considering contemporaneous work, but we did already have at least one precedent of a reviewer asking for a comparison with ChatGPT. We recommend that future chairs develop a clear policy in the reviewer guidelines about requests for comparisons with “closed” systems, to avoid numerous issues with evaluation methodology and benchmark data contamination (Rogers, 2023).

# Expanding the set of volunteer roles.

Our experience suggests that PC-ing a conference of ACL’23 size is a job that can no longer be realistically done by 3 volunteers. Early on, we introduced a visa

support team to start early with issuing the letters of invitation for Canada. We also had crucial help from two PC assistants: Youmi Ma, an administrative assistant who handled much of the conference email, and Marzena Karpinska, who helped with analysis of peer review data in this report. In the future, we recommend that a dedicated role of a peer review chair is created, whose responsibility will be to supplement PC report with analysis of the data of the respective conference and comparing it with any records from previous conferences (so as to establish the effect of any new policies), and to coordinate the peer review awards selection and logistics (see §9.2). The growing volume of nominations for best papers requires a best paper chair, handling in effect the organization of a separate track and review process. Finally, we could have used a lot of help in the conference schedule: ideally there would be a dedicated schedule chair, ideally serving at several conferences so as to reduce friction and reuse the skill set as much as possible, as well as incorporate feedback from several events. Given that ACL had papers from SRW, Industry, ARR, TACL, CL, Findings, and the Main Conference, it’s not necessarily feasible that the main track PCs can effectively coordinate scheduling all of these papers.

Another option would be for each conference to have two sets of PC chairs, one remaining from the previous year and one new. This would lighten the workload and ensure a smoother process (since people do not learn how to do everything from scratch each time). The first-year PCs would do the bulk of the work after the paper notifications are sent, and the second-year PCs would concentrate on the review process, analysis and the report. The first-year PCs would observe that and have better knowledge for designing the review process (CFP, SAC nominations, review criteria, etc). The second-year PCs would observe the COI requirements.

# 11 Acknowledgements

ACL’23 was the result of an incredible effort of 70 SACs, 438 ACs, 4490 reviewers, and 13,658 authors. We also thank our 2 ethics chairs and their 21 reviewers, as well as 15 judges on the best paper committee.

We thank the ARR team, and particularly Jonathan K. Kummerfeld, Thamar Solorio and Mausam, for their help with integrating ARR submissions and analyzing them.

We had a chance to learn from the past chairs Smaranda Muresan, Preslav Nakov and Aline Villavicencio (ACL 2022), Yoav Goldberg, Zornitsa Kozareva, Yue Zhang (EMNLP 2022), and Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur (NAACL 2021). We also thank EMNLP 2022 and EACL 2023 (Isabelle Augenstein, Andreas Vlachos) for sharing their score distribution data for our analysis.

Our work is built on many iterations of previous ACL conferences, including the AC and SAC guidelines developed at ACL 2021, and peer review tutorials developed by Anna Rogers and Isabelle Augenstein for ACL Rolling Review.

Our paper-reviewer matching relied on Semantic Scholar data, kindly provided by Kyle Lo (AI2). The Semantic Scholar team also provided extra support to numerous authors working to clean up their profiles.

Emma Strubell, Ian Magnusson, and Jesse Dodge helped us to prepare publishable versions of Responsible NLP checklist.

We were only able to devote that much effort to peer review and its analysis thanks to the help of our brilliant assistants Youmi Ma and Marzena Karpinska.

Richard Gerber (START) responded to numerous issues and implemented several changes at our request, including the possibility to include “explanations" for the paper-reviewer matching.

We deeply thank the ACL Executive (especially Iryna Gurevych, Tim Baldwin, David Yarowsky, Yusuke Miyao, and Emily M. Bender) for their support of many of our crazy ideas, including the reviewer awards and the publication of this report.

Last but not least, we thank our publication chairs and ACL Anthology team, in particular, Ryan Cotterell and Matt Post — for their infinite patience with this last-minute publication.

41 https://2023.aclweb.org/blog/visa-info/

# References

Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névéol, Fanny Ducel, Saif M. Mohammad, and Karën Fort. 2023. The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research.

Omer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei Hwu, and JinJun Xiong. 2019. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 518–528, Hong Kong, China. Association for Computational Linguistics.

Ron Artstein and Massimo Poesio. 2008. Survey article: Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.

Rachel Bawden. 2019. One paper, nine reviews.

Emily M. Bender. 2019. The #BenderRule: On Naming the Languages We Study and Why It Matters.

Emily M. Bender and Leon Derczynski. 2018. Paper Types.

Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan. 2021. The NeurIPS 2021 Consistency Experiment.

Cornelia Caragea, Ana Uban, and Liviu P. Dinu. 2019. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2317–2327, Hong Kong, China. Association for Computational Linguistics.

Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz. 2021. Responsible NLP research Checklist.

Rune Haubo Bojesen Christensen. 2022. ordinal—Regression Models for Ordinal Data. R package version 2022.11-16.

Kenneth Ward Church. 2020. Emerging trends: Reviewing the reviewers (again). Natural Language Engineering, 26(2):245–257.

Trevor Cohn, Yulan He, Yang Liu, and Bonnie Webber. 2020. Advice on Reviewing for EMNLP.

Corinna Cortes and Neil D. Lawrence. 2021. Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment. arXiv:2109.09774 [cs].

D.R. Cox and E.J. Snell. 1989. Analysis of Binary Data, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis.

Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond.

Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, and Yusuke Miyao. 2019. Does My Rebuttal Matter? Insights from a Major NLP Conference. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1274–1290, Minneapolis, Minnesota. Association for Computational Linguistics.

Marc Hauser and Ernst Fehr. 2007. An Incentive Solution to the Peer Review Problem. PLOS Biology, 5(4):e107.

Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. 2019. Argument Mining for Understanding Peer Reviews. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2131–2137, Minneapolis, Minnesota. Association for Computational Linguistics.

Letizia Jaccheri, Cristina Pereira, and Swetlana Fast. 2020. Gender Issues in Computer Science: Lessons Learnt and Reflections for the Future. In 2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), pages 9–16.

Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B. Shah, and Fei Fang. 2022. A Dataset on Malicious Paper Bidding in Peer Review.

# References

Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1647–1661, New Orleans, Louisiana. Association for Computational Linguistics.

Klaus Krippendorff. 2011. Computing Krippendorff’s Alpha-Reliability.

Andy Liaw and Matthew Wiener. 2002. Classification and Regression by randomForest. R News, 2(3):18–22.

Michael L. Littman. 2021. Collusion Rings Threaten the Integrity of Computer Science Research. Communications of the ACM, 64(6):43–44.

Ian Magnusson, Noah A. Smith, and Jesse Dodge. 2023. Reproducibility in NLP: What Have We Learned from the Checklist?

Daniel McFadden. 1973. Conditional Logit Analysis of Qualitative Choice Behaviour. In P. Zarembka, editor, Frontiers in Econometrics, pages 105–142. Academic Press New York, New York, NY, USA.

Nico Nagelkerke. 1991. A note on a general definition of the coefficient of determination. Biometrika, 78(3):691–692.

OpenAI. 2022. Introducing ChatGPT.

Katarina Pantic and Jody Clarke-Midura. 2019. Factors That Influence Retention of Women in the Computer Science Major: A Systematic Literature Review. Journal of Women and Minorities in Science and Engineering, 25(2).

Silviu Paun, Ron Artstein, and Massimo Poesio. 2022. Statistical Methods for Annotation Analysis. Springer International Publishing.

Douglas P. Peters and Stephen J. Ceci. 1982. The Fate of Published Articles, Submitted Again. Behavioral and Brain Sciences, 5(2):199–199.

Eric Price. 2014. The NIPS experiment.

Anna Rogers. 2023. Closed AI Models Make Bad Baselines.

Anna Rogers and Isabelle Augenstein. 2020. What Can We Do to Improve Peer Review in NLP? In Findings of EMNLP, pages 1256–1262, Online. Association for Computational Linguistics.

Richard Smith. 2010. Classical Peer Review: An Empty Gun. Breast Cancer Research, 12(4):S13.

Charles Spearman. 1987. The Proof and Measurement of Association between Two Things. The American Journal of Psychology, 100(3/4):441.

Flaminio Squazzoni, Giangiacomo Bravo, and Károly Takács. 2013. Does Incentive Provision Increase the Quality of Peer Review? An Experimental Study. Research Policy, 42(1):287–294.

Ivan Stelmakh. 2020. Experiments with the ICML 2020 Peer-Review Process.

Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. 2019. PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review. In Proceedings of the 30th International Conference on Algorithmic Learning Theory, pages 828–856. PMLR.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science.

Terne Thorn Jakobsen and Anna Rogers. 2022. What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4810–4823, Seattle, United States. Association for Computational Linguistics.

Andrew Tomkins, Min Zhang, and William D. Heavlin. 2017. Reviewer Bias in Single- versus Double-Blind Peer Review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.

William N. Venables and Brian D. Ripley. 2002. Modern Applied Statistics with S, fourth edition. Springer, New York. ISBN 0-387-95457-0.

Verity Warne. 2016. Rewarding reviewers – Sense or Sensibility? A Wiley Study Explained. Learned Publishing, 29(1):41–50.

# Table of Contents

- Should you marginalize over possible tokenizations?
Nadezhda Chirkova, Germán Kruszewski, Jos Rozen and Marc Dymetman . . . . . . . . . . . . . . . . . . . 1
- Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie
Naoki Yoshinaga . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
- Transformed Protoform Reconstruction
Young Min Kim, Kalvin Chang, Chenxuan Cui and David R. Mortensen . . . . . . . . . . . . . . . . . . . . 24
- Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models
Daniel Hardt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
- Bootstrapping Neural Relation and Explanation Classifiers
Zheng Tang and Mihai Surdeanu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
- A Fast Algorithm for Computing Prefix Probabilities
Franz Nowak and Ryan Cotterell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
- Analyzing Text Representations by Measuring Task Alignment
Cesar Gonzalez-Gutierrez, Audi Primadhanty, Francesco Cazzaro and Ariadna Julieta Quattoni 70
- Tracing Linguistic Markers of Influence in a Large Online Organisation
Prashant Khare, Ravi Shekhar, Mladen Karan, Stephen McQuistin, Colin Perkins, Ignacio Castro, Gareth Tyson, Patrick G.T. Healey and Matthew Purver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
- Metaphor Detection via Explicit Basic Meanings Modelling
Yucheng Li, Shun Wang, Chenghua Lin and Frank Guerin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
- xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages
Mingda Chen, Kevin Heffernan, Onur C ¸ elebi, Alexandre Mourachko and Holger Schwenk . . . . . . . . . . . . . . . . 101
- Graph Propagation based Data Augmentation for Named Entity Recognition
Jiong Cai, Shen Huang, Yong Jiang, Zeqi Tan, Pengjun Xie and Kewei Tu . . . . . . . . . . . . . . . . . 110
- Dataset Distillation with Attention Labels for Fine-tuning BERT
Aru Maekawa, Naoki Kobayashi, Kotaro Funakoshi and Manabu Okumura . . . . . . . . . . . . . . . . 119
- Multi-Document Summarization with Centroid-Based Pretraining
Ratish Surendran Puduppully, Parag Jain, Nancy Chen and Mark Steedman . . . . . . . . . . . . . . . . 128
- Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times
Andrea Gregor de Varda and Marco Marelli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
- Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques
Daking Rai, Bailin Wang, Yilun Zhou and Ziyu Yao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
- HiPool: Modeling Long Documents Using Graph Neural Networks
Irene Li, Aosong Feng, Dragomir Radev and Rex Ying . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
- A Weakly Supervised Classifier and Dataset of White Supremacist Language
Michael Miller Yoder, Ahmad Diab, David West Brown and Kathleen M Carley . . . . . . . . . . . . 172

# BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases

Xin Liu, Muhammad Khalifa and Lu Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186

# mOKB6: A Multilingual Open Knowledge Base Completion Benchmark

Shubham Mittal, Keshav Kolluru, Soumen Chakrabarti and Mausam - . . . . . . . . . . . . . . . . . . . . . 201

# Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment

Roni Rabin, Alexandre Djerbetian, Roee Engelberg, Lidan Hackmon, Gal Elidan, Reut Tsarfaty and Amir Globerson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

# Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts

Skyler R Hallinan, Alisa Liu, Yejin Choi and Maarten Sap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

# A Natural Bias for Language Generation Models

Clara Meister, Wojciech Jan Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell and Adhiguna Kuncoro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243

# Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion

Ananjan Nandi, Navdeep Kaur, Parag Singla and Mausam - . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256

# Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer

Xingtai Lv, Ning Ding, Yujia Qin, Zhiyuan Liu and Maosong Sun . . . . . . . . . . . . . . . . . . . . . . . . 270

# Faithfulness Tests for Natural Language Explanations

Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen and Isabelle Augenstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283

# COGEN: Abductive Commonsense Language Generation

rohola zandie, Diwanshu Shekhar and Mohammad Mahoor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295

# Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis

Xuming Hu, Zhijiang Guo, ZHIYANG TENG, Irwin King and Philip S. Yu . . . . . . . . . . . . . . . . 303

# Characterization of Stigmatizing Language in Medical Records

Keith Harrigian, Ayah Zirikly, Brant Chee, Alya Ahmad, Anne R Links, Somnath Saha, Mary Catherine Beach and Mark Dredze . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312

# Abstractive Summarizers are Excellent Extractive Summarizers

Daniel Varab and Yumo Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

# Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions

Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang and Louis-Philippe Morency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340

# PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English

Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian and Kai-Wei Chang . . . . . . . . . . . . . . . . . . . . . . . . . 352

# Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages

Yasmine Karoui, Rémi Lebret, Negar Foroutan Eghlidi and Karl Aberer . . . . . . . . . . . . . . . . . . . 366

# BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering

Jie He, Simon Chi Lok U, Victor Gutierrez-Basulto and Jeff Z. Pan . . . . . . . . . . . . . . . . . . . . . . . 376

# Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases

Haozhe An and Rachel Rudinger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

# Improving Syntactic Probing Correctness and Robustness with Control Tasks

Weicheng Ma, Brian C Wang, Hefan Zhang, Lili Wang, Rolando Coto-Solano, Saeed Hassanpour and Soroush Vosoughi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402

# Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications

Jatin Arora and Youngja Park . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416

# Credible without Credit: Domain Experts Assess Generative Language Models

Denis Peskoff and Brandon Stewart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427

# Grokking of Hierarchical Structure in Vanilla Transformers

Shikhar Murty, Pratyusha Sharma, Jacob Andreas and Christopher D. Manning . . . . . . . . . . . . 439

# Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data

Ujan Deb, Ridayesh Ramesh Parab and Preethi Jyothi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449

# Context-Aware Transformer Pre-Training for Answer Sentence Selection

Luca Di Liello, Siddhant Garg and Alessandro Moschitti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458

# Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities

Zhihong Chen, Maya Varma, Xiang Wan, Curtis Langlotz and Jean-Benoit Delbrouck . . . . . . 469

# Efficient Diagnosis Assignment Using Unstructured Clinical Notes

Louis Blankemeier, Jason Fries, Robert Tinn, Joseph S Preston, Nigam Shah and Akshay Chaudhari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485

# MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models

Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang and Kai-Wei Chang 495

# On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach

Francisco Valentini, Germán Federico Rosati, Damián Blasi, Diego Fernandez Slezak and Edgar Altszyler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509

# Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models

Ehsan Doostmohammadi, Tobias Norlund, Marco Kuhlmann and Richard Johansson . . . . . . . . 521

# MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents

Anastasiia Razdaibiedina and Aleksandr V. Brechalov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530

# KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations

Myeongjun Jang, Bodhisattwa Prasad Majumder, Julian McAuley, Thomas Lukasiewicz and Oana-Maria Camburu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540

# Measuring the Effect of Influential Messages on Varying Personas

Chenkai Sun, Jinning Li, Hou Pong Chan, ChengXiang Zhai and Heng Ji . . . . . . . . . . . . . . . . . . 554

# Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity

Hongwei Wang and Dong Yu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563

# Robust Learning for Multi-party Addressee Recognition with Discrete Addressee Codebook

Pengcheng Zhu, Wei Zhou, Kuncai Zhang, Yuankai Ma and Haiqing Chen . . . . . . . . . . . . . . . . . 571

# TwistList: Resources and Baselines for Tongue Twister Generation

Tyler Loakman, Chen Tang and Chenghua Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579

# Substitution-based Semantic Change Detection using Contextual Embeddings

Dallas Card

# Probing Physical Reasoning with Counter-Commonsense Context

Kazushi Kondo, Saku Sugawara and Akiko Aizawa

# Morphological Inflection with Phonological Features

David Guriel, Omer Goldman and Reut Tsarfaty

# A Holistic Approach to Reference-Free Evaluation of Machine Translation

Hanming Wu, Wenjuan Han, Hui Di, Yufeng Chen and Jinan Xu

# Balancing Lexical and Semantic Quality in Abstractive Summarization

Jeewoo Sul and Yong Suk Choi

# Learning Neuro-Symbolic World Models with Conversational Proprioception

Don Joven Agravante, Daiki Kimura, Michiaki Tatsubori, Asim Munawar and Alexander Gray

# In and Out-of-Domain Text Adversarial Robustness via Label Smoothing

Yahan Yang, Soham Dan, Dan Roth and Insup Lee

# LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning

Amirhossein Abaskohi, Sascha Rothe and Yadollah Yaghoobzadeh

# Considerations for meaningful sign language machine translation based on glosses

Mathias Müller, Zifan Jiang, Amit Moryossef, Annette Rios and Sarah Ebling

# Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature

Daniel N Sosa, Malavika Suresh, Christopher Potts and Russ B Altman

# The Role of Global and Local Context in Named Entity Recognition

Arthur Amalvy, Vincent Labatut and Richard Dufour

# Joint End-to-end Semantic Proto-role Labeling

Elizabeth Spaulding, Gary Kazantsev and Mark Dredze

# Improving Automatic Quotation Attribution in Literary Novels

Krishnapriya Vishnubhotla, Frank Rudzicz, Graeme Hirst and Adam Hammond

# Modular Visual Question Answering via Code Generation

Sanjay Subramanian, Medhini Narasimhan, Kushal M Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell and Dan Klein

# Target-Based Offensive Language Identification

Marcos Zampieri, Skye Morgan, Kai North, Tharindu Ranasinghe, Austin Simmmons, Paridhi Khandelwal, Sara Rosenthal and Preslav Nakov

# Unsupervised Subtitle Segmentation with Masked Language Models

David Ponce, Thierry Etchegoyhen and Victor Ruiz

# Exploring Continual Learning for Code Generation Models

Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Parminder Bhatia, Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal and Bing Xiang

# Deep Active Learning for Morphophonological Processing

Seyed Morteza Mirbostani, Yasaman Boreshban, Salam Khalifa, Seyed Abolghasem Mirroshandel and Owen Rambow

# Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios

Jiaxuan Li, Lang Yu and Allyson Ettinger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804

# Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages

Yash H. Madhani, Mitesh M. Khapra and Anoop Kunchukuttan . . . . . . . . . . . . . . . . . . . . . . . . . . . 816

# Using contradictions improves question answering systems

Etienne Fortier-Dubois and Domenic Anthony Rosati . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 827

# Token-Level Self-Evolution Training for Sequence-to-Sequence Learning

Keqin Peng, Liang Ding, Qihuang Zhong, Yuanxin Ouyang, Wenge Rong, Zhang Xiong and Dacheng Tao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 841

# Gradient Ascent Post-training Enhances Language Model Generalization

Dongkeun Yoon, Joel Jang, Sungdong Kim and Minjoon Seo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851

# An Open Dataset and Model for Language Identification

Laurie V Burchell, Alexandra Birch, Nikolay Bogoychev and Kenneth Heafield . . . . . . . . . . . . 865

# Evaluating Paraphrastic Robustness in Textual Entailment Models

Dhruv Verma, Yash Kumar Lal, Shreyashee Sinha, Benjamin Van Durme and Adam Poliak . 880

# Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?

Chenming Tang, Xiuyu Wu and Yunfang Wu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893

# Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality

Tanay Dixit, Fei Wang and Muhao Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 902

# With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness

Julius Steen, Juri Opitz, Anette Frank and Katja Markert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 914

# A Better Way to Do Masked Language Model Scoring

Carina Kauf and Anna Ivanova . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925

# ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?

Michael Heck, Nurul Lubis, Benjamin Matthias Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, Hsien-chin Lin, Carel van Niekerk and Milica Gasic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 936

# Controllable Mixed-Initiative Dialogue Generation through Prompting

Maximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi and Zhou Yu . . . . . . . . . . . . . . . . . . . . . . 951

# Enhancing Event Causality Identification with Counterfactual Reasoning

Feiteng Mu and Wenjie Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967

# Contrastive Bootstrapping for Label Refinement

Shudi Hou, Yu Xia, Muhao Chen and Sujian Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 976

# NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification

Iyanuoluwa Adeola Shode, David Ifeoluwa Adelani, JIng Peng and Anna Feldman . . . . . . . . . . 986

# Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement

Samuel Mensah, Kai Sun and Nikolaos Aletras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 999

# An (unhelpful) guide to selecting the best ASR architecture for your under-resourced language

Robert Jimerson, Zoey Liu and Emily Prud’hommeaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1008

# The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics

Matthias Orlikowski, Paul Röttger, Philipp Cimiano and Dirk Hovy . . . . . . . . . . . . . . . . . . . . . . 1017

# Decomposed scoring of CCG dependencies

Aditya Bhargava and Gerald Penn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1030

# Do GPTs Produce Less Literal Translations?

Vikas Raunak, Arul Menezes, Matt Post and Hany Hassan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1041

# Environmental Claim Detection

Dominik Stammbach, Nicolas Webersinke, Julia Anna Bingler, Mathias Kraus and Markus Leopold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1051

# Black-box language model explanation by context length probing

Ondřej Cífka and Antoine Liutkus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1067

# Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation

Sirui Wang, Kaiwen Wei, Hongzhi Zhang, Yuntao Li and Wei Wu . . . . . . . . . . . . . . . . . . . . . . . 1080

# The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics

Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa Coheur, Alon Lavie and André Martins 1089

# Typo-Robust Representation Learning for Dense Retrieval

Panuthep Tasawong, Wuttikorn Ponwitayarat, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich and Sarana Nutanong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1106

# Focused Prefix Tuning for Controllable Text Generation

Congda Ma, Tianyu Zhao, Makoto Shing, Kei Sawada and Manabu Okumura . . . . . . . . . . . . . 1116

# ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models

Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi and Yiran Chen . . . . . . . . . . . . . . . . . . . 1128

# Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood

Yu Xia, Yongwei Zhao, Wenhao Wu and Sujian Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1137

# Deriving Language Models from Masked Language Models

Lucas Torroba Hennigen and Yoon Kim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1149

# UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation

Zhiming Mao, Huimin Wang, Yiming Du and Kam-Fai Wong . . . . . . . . . . . . . . . . . . . . . . . . . . . 1160

# Reasoning Implicit Sentiment with Chain-of-Thought Prompting

Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li and Tat-Seng Chua . . . . . . . . . . . . . . . . . . . . . 1171

# Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings

Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, alexander rudnicky and Peter J Ramadge . . . . . . 1183

# Is Anisotropy Truly Harmful? A Case Study on Text Clustering

Mira Ait-Saada and Mohamed Nadif . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1194

# Class based Influence Functions for Error Detection

Thang Nguyen-Duc, Hoang Thanh-Tung, Quan Hung Tran, Dang Huu-Tien, Hieu Ngoc Nguyen, Anh T. V. Dau and Nghi D. Q. Bui . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1204

# Leveraging Prefix Transfer for Multi-Intent Text Revision

Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye Jin, Liner Yang, Yange Fan, Hanghang Fan and Erhong Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1219

# Learning Multi-Step Reasoning by Solving Arithmetic Tasks

Tianduo Wang and Wei Lu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1229

# Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning

Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, jun huang and Songfang Huang1239

# Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting

Zahra Fatemi, Chen Xing, Wenhao Liu and Caimming Xiong. . . . . . . . . . . . . . . . . . . . . . . . . . . .1249

# Class-Incremental Learning based on Label Generation

Yijia Shao, Yiduo Guo, Dongyan Zhao and Bing Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1263

# Evaluating pragmatic abilities of image captioners on A3DS

Polina Tsvilodub and Michael Franke . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1277

# The Art of Prompting: Event Detection based on Type Specific Prompts

Sijia Wang, Mo Yu and Lifu Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1286

# Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation

Zhuoyuan Mao, Raj Dabre, Qianying Liu, Haiyue Song, Chenhui Chu and Sadao Kurohashi1300

# Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning

Po-Nien Kung and Nanyun Peng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1317

# Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models

James O’Neill and Sourav Dutta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1329

# Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation

Yuchen Han, Chen Xu, Tong Xiao and Jingbo Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1340

# Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data

Yufei Li, Xiao Yu, Yanchi Liu, Haifeng Chen and Cong Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1349

# Text-to-SQL Error Correction with Language Models of Code

Ziru Chen, Shijie Chen, Michael White, Raymond Mooney, Ali Payani, Jayanth Srinivasa, Yu Su and Huan Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1359

# The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks

Nikil Roashan Selvam, Sunipa Dev, Daniel Khashabi, Tushar Khot and Kai-Wei Chang . . . . 1373

# Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)

Chantal Shaib, Millicent L Li, Sebastian A Joseph, Iain Marshall, Junyi Jessy Li and Byron C. Wallace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1387

# Prefix Propagation: Parameter-Efficient Tuning for Long Sequences

Jonathan X Li, Will Aitken, Rohan Bhambhoria and Xiaodan Zhu . . . . . . . . . . . . . . . . . . . . . . . 1408

# Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain

Shih-Lun Wu, Yi-Hui Chou and Liangze Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1420

# Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations

Baikjin Jung, Myungji Lee, Jong-Hyeok Lee and Yunsu Kim . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1433

# An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition

Hang Yan, Yu Sun, Xiaonan Li and Xipeng Qiu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1442

# Hexatagging: Projective Dependency Parsing as Tagging

Afra Amini, Tianyu Liu and Ryan Cotterell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1453

# Understanding Demonstration-based Learning from a Causal Perspective

Ruiyi Zhang and Tong Yu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1465

# RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation

Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu and Maria Nadejde . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1476

# Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation

Haoyang Wen and Alexander Hauptmann. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1491

# Discourse-Level Representations can Improve Prediction of Degree of Anxiety

Swanie Juhng, Matthew Matero, Vasudha Varadarajan, Johannes C Eichstaedt, Adithya V Ganesan and H. Andrew Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1500

# Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning

Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh and Rahul Gupta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1512

# MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting

Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng and Sadao Kurohashi . . . . . . . . . . . . . . . . . . . . . 1522

# mPMR: A Multilingual Pre-trained Machine Reader at Scale

Weiwen Xu, Xin Li, Wai Lam and Lidong Bing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1533

# MOSPC: MOS Prediction Based on Pairwise Comparison

Kexin Wang, Yunlong Zhao, Qianqian Dong, Tom Ko and Mingxuan Wang . . . . . . . . . . . . . . . 1547

# LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering

Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria de Gispert and Gonzalo Iglesias . . . . . . . . . 1557

# How Well Apply Simple MLP to Incomplete Utterance Rewriting?

Jiang Li, Xiangdong Su, Xinlan Ma and Guanglai Gao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1567

# XL-LEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE

Pierluigi Cassotti, Lucia Siciliani, Marco DeGemmis, Giovanni Semeraro and Pierpaolo Basile . . . . 1577

# Theory-Grounded Computational Text Analysis

Arya D. McCarthy and Giovanna Maria Dora Dore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1586

# AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing

Abelardo Carlos Martínez Lorenzo, Pere Lluís Huguet Cabot and Roberto Navigli . . . . . . . . . 1595

# MolXPT: Wrapping Molecules with Text for Generative Pre-training

Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang and Tie-Yan Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1606

# A Study on the Efficiency and Generalization of Light Hybrid Retrievers

Man Luo, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz, Debojeet Chatterjee, Xilun Chen, Chitta Baral and Peyman Heidari. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1617

# The Mechanical Bard: An Interpretable Machine Learning Approach to Shakespearean Sonnet Generation

Edwin Agnew, Michelle Qiu, Lily Zhu, Sam Wiseman and Cynthia Rudin . . . . . . . . . . . . . . . . 1627

# When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants

Anuj Diwan, Eunsol Choi and David Harwath . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1639

# Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction (ACE) Annotations

Erica Cai and Brendan O’Connor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1651

# Event Extraction as Question Generation and Answering

Di Lu, Shihao Ran, Joel Tetreault and Alejandro Jaimes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1666

# Are Sample-Efficient NLP Models More Robust?

Nelson F. Liu, Ananya Kumar, Percy Liang and Robin Jia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1689

# Diversity-Aware Coherence Loss for Improving Neural Topic Models

Raymond Li, Felipe Gonzalez-Pizarro, Linzi Xing, Gabriel Murray and Giuseppe Carenini . 1710

# NarrowBERT: Accelerating Masked Language Model Pretraining and Inference

Haoxin Li, Phillip Keung, Daniel Cheng, Jungo Kasai and Noah A. Smith . . . . . . . . . . . . . . . . 1723

# S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering

Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao and Kang Liu . . . . . 1731

# Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue

Bin Sun, Yitong Li, Fei Mi, fanhu bie, Yiwei Li and Kan Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1741

# AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models

Siheng Li, Cheng Yang, Yichun Yin, Xinyu Zhu, Zesen Cheng, Lifeng Shang, Xin Jiang, Qun Liu and Yujiu Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1751

# STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions

Michel Plüss, Jan Deriu, Yanick Schraner, Claudio Paonessa, Julia Hartmann, Larissa Schmidt, Christian Scheller, Manuela Hürlimann, Tanja Samardžić, Manfred Vogel and Mark Cieliebak . . . 1763

# Teaching Small Language Models to Reason

Lucie Charlotte Magister, Jonathan Mallinson, Jakub Dominik Adamek, Eric Malmi and Aliaksei Severyn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1773

# A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification

Rohan Bhambhoria, Lei Chen and Xiaodan Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1782

# A Simple Concatenation can Effectively Improve Speech Translation

Linlin Zhang, Kai Fan, Boxing Chen and Luo Si . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1793

# ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning

Jingyuan S. She, Christopher Potts, Samuel R. Bowman and Atticus Geiger . . . . . . . . . . . . . . . 1803

# Revisiting Automated Prompting: Are We Actually Doing Better?

Yulin Zhou, Yiren Zhao, Ilia Shumailov, Robert Mullins and Yarin Gal . . . . . . . . . . . . . . . . . . . 1822

# Mind the Gap between the Application Track and the Real World

Ananya Ganesh, Jie Cao, E. Margaret Perkoff, Rosy Southwell, Martha Palmer and Katharina Kann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1833

# How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation

# Objectives

Xinpeng Wang, Leonie Weissweiler, Hinrich Schütze and Barbara Plank . . . . . . . . . . . . . . . . . 1843

# ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion

Anastasiia Sedova and Benjamin Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1853

# Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering

Hao Cheng, Hao Fang, Xiaodong Liu and Jianfeng Gao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1864

# Linear Classifier: An Often-Forgotten Baseline for Text Classification

Yu-Chen Lin, Si-An Chen, Jie-Jyun Liu and Chih-Jen Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1876

# Randomized Positional Encodings Boost Length Generalization of Transformers

Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Ben- nani, Shane Legg and Joel Veness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1889

# Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models

Hidetaka Kamigaito, Katsuhiko Hayashi and Taro Watanabe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1904

# Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints

Chao Lou and Kewei Tu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1918

# TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation

Yiming Ai, Zhiwei He, Kai Yu and Rui Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1930

