# ACL 2023

# The 61st Conference of the Association for Computational Linguistics

# Proceedings of the Conference

# Volume 1: Long Papers

# July 9-14, 2023

# The ACL organizers gratefully acknowledge the support from the following sponsors.

|Diamond-Level Welcome Event|Diamond|Google Research|
|---|---|---|
|Platinum|Baid EE|ByteDance|

# DATAOCEAN AI

# YOUR GLOBAL DATA PARTNER

|Gold|ANT|JPMorgan|
|---|---|---|
|Silver| | |
|Bronze| | |

# Diversity & Inclusion Champions

©2023 Association for Computational Linguistics

Order copies of this and other ACL proceedings from:

Association for Computational Linguistics (ACL)

209 N. Eighth Street

Stroudsburg, PA 18360

USA

Tel: +1-570-476-8006

Fax: +1-570-476-0860

acl@aclweb.org

ISBN 978-1-959429-72-2

v

# Message from the General Chair

Welcome to ACL 2023, the 61st Annual Meeting of the Association for Computational Linguistics! The conference will be held in Toronto, Canada, July 9-14, 2023. Following the succession of the recent conferences in our field, ACL 2023 will adopt a hybrid format. While the impact of Covid has considerably diminished in terms of traveling, obtaining visas to Canada entails a very long process. Moreover, the global economic conditions pose challenges for many individuals to travel to conferences. Recognizing these circumstances, we know many participants may not be able to attend the conference in person. Therefore, we are committed to providing a great virtual platform so everyone has the opportunity to interact with other participants and enjoy the conference. Based on the current registered participants, approximately 30% have chosen to attend the conference virtually. Whether you join us in person or virtually, we sincerely hope everyone has a remarkable conference experience.

This General Chair’s message is where I express my gratitude to the many individuals who have made enormous contributions to the conference over the past year.

First and foremost, I am grateful for the tremendous efforts by the program chairs: Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki. The rapid growth of our field is challenging from the perspective of organizing a conference. Program chairs have admirably handled a huge number of submissions and implemented novel review criteria to improve the quality of reviews and the paper decision process. They responded promptly after ChatGPT was launched and provided guidelines for using it in paper writing. Beyond their responsibilities as program chairs, they have assisted me with various other decisions. Their efforts have truly shaped the conference. Also, thanks to all the senior area chairs, area chairs, reviewers, and the best paper committee, whose commitment and dedication made paper review and selection possible.

Next, I would like to thank the entire organizing committee for their service. It has been an honor for me to collaborate with such a dedicated team. This includes:

- Industry track chairs: Beata Beigman Klebanov, Jason Williams, and Sunayana Sitaram. An addition to this year’s ACL is the introduction of a separate industry track. This is motivated by two factors. First, ACL is held in North America this year (and thus no NAACL), and NAACL has an established tradition of hosting an industry track. Second there was an increasing number of industry track submissions at EMNLP last year from previous years. We hope that a separate industry track can foster the dissemination of research on real-world applications in industry settings. Thanks to the industry track chairs for their efforts in coordinating all the logistics associated with this track.
- Demo chairs: Alan Ritter, Danushka Bollegala, and Ruihong Huang, who managed demo submissions and accepted 58 demos that will be presented in the main conference.
- Student research workshop (SRW) chairs: Gisela Vallejo, Vishakh Padmakumar, and Yao Fu, who showed remarkable enthusiasm and dedication in organizing the workshop. They selected 45 papers to be presented in the main conference program. Also thanks to the faculty advisors: Ivan Vulic and Lu Wang, for providing guidance to the SRW chairs and obtaining NSF support for the workshop.
- Workshop chairs: Annie Louis, Eduardo Blanco, and Yang Feng, who collaborated with EACL workshop chairs to select 22 workshops, and served as the vital link between the conference and individual workshop organizers.
- Tutorials chairs: Margot Mieskes, Siva Reddy, and Vivian Chen, who also worked with EACL chairs to select 6 high quality tutorials that cater to the interest and needs of our conference.

# Conference Acknowledgments

- Ethics chairs: Dirk Hovy and Yonatan Bisk, who checked papers flagged with ethics issues. Thanks for their meticulous work to ensure our papers uphold the ethical standards.
- Publication chairs: Ryan Cotterell, Chenghua Lin, Jesse Thomason, Lei Shu, and Lifu Huang, who prepared the conference handbook, ensured proper formatting of papers, and produced the conference proceedings.
- Virtual infrastructure chairs: Jiacheng Xu, Martín Villalba, and Pedro Rodriguez, who worked hard to develop a virtual platform to ensure an engaging conference experience for both in-person and remote participants. They also made various innovations and enhancements on top of the Underline platform, which the conference utilizes.
- Publicity and social media chairs: Devamanyu Hazarika, Eva Vanmassenhove, and Tong Xu, who communicated and publicized the conference through various social media channels, enhancing the visibility and reach of the conference.
- Website chairs: Jinho Choi and Zhongyu Wei, who updated and maintained the conference website to keep participants informed.
- Diversity and inclusion (D&I) chairs: Daniel Beck, Maryam Fazel-Zarandi, and Nedjma Djouhra Ousidhoum, who arranged support to participants facing financial hardships, and organized a diverse array of activities aimed at promoting diversity and inclusion in our community.
- Student volunteer chairs: Ayah Zirikly and Tao Yu, who reviewed applications and selected student volunteers for the conference.
- Sponsorship chairs: Alla Rozovskaya and Lei Li. Thanks to them and Chris Callison-Burch, the ACL sponsorship Director for their efforts in securing sponsorships and managing the relationship between sponsors and the conference. The generous support from our sponsors has played a crucial role in enabling us to maintain a reasonable registration cost for attendees, and the additional sponsorship for D&I initiatives helps our commitment to fostering a diverse and inclusive environment.
- Visa assistance team: Ayana Niwa, Qingwen Liu, Renxiang Zhang, Samridhi Choudhary, and Tao You. Many participants require visas to attend the conference, and we fully understand this lengthy process. This team has been diligently handling visa requests by sending out numerous invitation letters to facilitate visa applications.
- Infrastructure support from Softconf (Richard Gerber) and Underline (Damira Mrsic, Sol Rosenberg). Both platforms kindly accommodated our many, many requests and implemented several new features.

I also want to specially thank Jennifer Rachford, the ACL event director, who handled all the local arrangements for this conference. Though she was relatively new to the role, and oftentimes needed to juggle multiple ACL conferences, she remained well organized, and consistently provided all the necessary information to all members of the organizer committee. Her contributions ensure the success of this conference.

Thanks to previous ACL/EMNLP conference chairs for sharing their knowledge, tips, and best practices on organizing this conference, and ACL Exec for the support they provided throughout the entire planning and execution of this conference.

Lastly, I extend my appreciation to every participant. Regardless of your role, whether as authors or presenters, workshop organizers, tutorial speakers, student volunteers, session chairs, or simply attendees, your involvement is essential in creating a memorable conference.

Welcome everyone to the conference!

# ACL 2023 General Chair

Yang Liu

Alexa, Amazon

# Message from the Program Chairs

It’s hard to believe that we’re actually going to be seeing the program come together in Toronto. We’re really looking forward to it and to seeing you all there!

Most of the work of a program chair is behind the scenes: herding reviewers and chairs, wrangling data from various sources, and answering lots and lots of email. This is a volunteer position, so the only reward we get for this is our chance to make the process of submitting and reviewing papers to our conference better. This letter will outline some of those experiments.

First, we asked reviewers for two scores: soundness and excitement. Our goal was that any sound paper would be accepted to some ACL affiliated venue, but that the “main conference” distinction (limited by space) would be focused on the most exciting papers. Our hope was that soundness would be less noisy than a single “overall recommendation” score, which would help reduce the randomness of decisions. Judging by the exit surveys, this change was well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change.

Next, we developed a new process for matching papers to reviewers based on keywords for not only the subject matter of the paper, but also its type of contribution and target language(s). This allowed more fine-grained control over the paper-reviewer matches, and we were also able to provide the chairs with context for the paper-reviewer matches.

To improve review quality, we also updated the reviewer guidelines, and developed a system for the authors to flag specific types of issues with reviews. Finally, we have also proposed a new initiative for recognizing outstanding reviewers and chairs (73 awards at ACL’23).

Finally, we have tried to give more options for presentations. Findings papers now have an in-person presentation spotlight slot and virtual posters in addition to recording videos. Virtual posters have portals to link in-person attendees to virtual posters. We have also brought back Miniconf and RocketChat to allow for better virtual communication between papers (regardless of where the authors are).

This conference is a result of the joint efforts of over ten thousand people. We deeply thank them all, and apologize for the many nagging emails we had to send out. In particular:

- the general chair Yang Liu, who led the whole process;
- the incredible team of 70 SACs, 438 ACs, and 4490 reviewers, who were able to handle our record number of submissions;
- the 13,658 authors for their phenomenal scientific contributions, which we were honored to shepherd through the reviewing process;
- the ACL Executive (esp. Iryna Gurevych, Tim Baldwin, David Yarowsky, Yusuke Miyao, Emily M. Bender) for their support of many of our crazy ideas;
- 21 ethics committee reviewers, chaired by Dirk Hovy and Yonatan Bisk, for their hard work to uphold the ACL code of ethics;
- Our Best Paper Award committee (Jonathan Berant, Jose Camacho-Collados, Danqi Chen, Benjamin Van Durme, David Jurgens, Desmond Elliott, Sasha Luccioni, Jonathan May, Tom McCoy, Yusuke Miyao, Ekaterina Shutova, Emma Strubell, Jun Suzuki, Xiaojun Wan, Luke Zettlemoyer), who reviewed a record number of nominated papers under tight schedule;
- Our assistant Youmi Ma, for reducing our email and Softconf workload significantly and suggesting ideas to make the job run smoothly;
- Past ACL PCs, including Smaranda Muresan, Preslav Nakov and Aline Villavicencio (ACL 2022), Yoav Goldberg, Zornitsa Kozareva, Yue Zhang (EMNLP 2022), Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur (NAACL 2021), for their advice and suggestions;

# Acknowledgments

- Publication chairs Ryan Cotterell, Chenghua Lin, Jesse Thomason, Lei Shu, and Lifu Huang, who ensured the proper formatting of camera-ready papers;
- Emma Strubell, Ian Magnusson, and Jesse Dodge for their help in preparing publishable versions of Responsible NLP checklist;
- ACL Anthology director Matt Post;
- TACL editors-in-chief (Asli Celikyilmaz, Roi Reichart, Ani Nenkova) and CL Editor-in-Chief Hwee Tou Ng for coordinating TACL and CL presentations with us;
- Workshop chairs Annie Louis, Eduardo Blanco, and Yang Feng, for helping us to connect the Findings papers to possible presentation slots at workshops;
- Rich Gerber at Softconf, who answered countless emails and implemented several new features on our request;
- Kyle Lo and Semantic Scholar team, who kindly assisted us with data for paper-reviewer matching;
- Our virtual infrastructure chairs (Pedro Rodriguez, Jiacheng Xu, Martín Villalba) and Underline team (Damira Mrsic, Sol Rosenberg) for enabling a new kind of hybrid experience, combining miniconf and Underline;
- the ACL event director Jennifer Rachford and our visa support team (Ayana Niwa, Qingwen Liu, Renxiang Zhang, Samridhi Choudhary, and Tao You), who did everything possible to facilitate the Canada visa situation for ACL attendees.

# Submission and Acceptance

We had two routes to submit papers to ACL 2023: directly to the conference or through ACL Rolling Review (ARR). We received a record number of direct submissions (3601 long papers and 958 short papers) in January 2023. In addition, we received 305 commitments from ARR (271 long papers and 34 short papers) in March 2023. In total, we considered 4864 (3872 long and 992 short) papers with 70 senior area chairs, 438 area chairs, 4024 reviewers, 445 secondary reviewers, and 21 ethics reviewers in 27 tracks. We accepted 910 (23.50%) long and 164 (16.53%) short papers for the main conference, and 712 (41.89% including the long papers for the main conference) long and 189 (35.58% including the short papers for the main conference) short papers for Findings. To sum long and short papers, ACL 2023 accepted 1074 (22.08%) papers for the conference and 901 (40.60% including the papers for the main conference) papers for Findings. The ACL 2023 program also features 46 papers from the Transactions of the Association for Computational Linguistics (TACL) journal, and 7 from the Computational Linguistics (CL) journal.

# Limitations Section and Responsible NLP Checklist

Following EMNLP 2022 and EACL 2023, we required that each submitted paper must include an explicitly named Limitations section, discussing the limitations of the work. This was to counterbalance the practice of over-hyping the take-away messages of papers, and to encourage more rigorous and honest scientific practice. This discussion did not count towards the page limit, and we asked reviewers to not use the mentioned limitations as reasons to reject the paper, unless there was a really good reason to. In addition to the mandatory discussion of limitations, a new element at ACL 2023 is that the Responsible NLP Checklist for the accepted papers is not only considered by the reviewers, but also published together with the accepted papers as a special appendix, in an effort to improve transparency and accountability in the field.

# Areas

To ensure a smooth process, the submissions to ACL 2023 were divided into 26 areas. The areas mostly followed these of previous ACL, and more broadly ACL conferences, reflecting the typical divisions in the field. Following EMNLP 2022, we split the “Large Language Models” track away from “Machine learning in NLP”, reflecting the growth of submissions in the area. We also offered two new tracks (“Linguistic diversity” and “Multilingualism and Cross-Lingual NLP”). For the papers authored by SACs, the final recommendation decisions were made by a separate SAC team. The most popular areas (with over 250 submissions) were “Dialogue and Interactive Systems”, “Information Extraction”, “Large Language Models”, “Machine Learning for NLP”, and “NLP Applications”.

# Best Paper Awards

ACL’23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding. In total, 73 papers were nominated by the reviewers or area chairs for consideration for awards. These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 3 special awards (social impact, resource, reproduction), and several dozen outstanding papers. The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023.

# Presentation Mode

In ACL 2023, there is no meaningful distinction between oral and poster presentations in terms of paper quality. The composition of the oral sessions were proposed by the SACs of their respective tracks, so as to compose a thematically coherent set of papers on a shared topic or method, which would allow for an engaging discussion. The decisions were not based on the authors’ virtual or on-site attendance.

We hope you enjoy the program and the new elements we introduced (but let us know either way). We are looking forward to a great ACL 2023!

Anna Rogers (IT University of Copenhagen, Denmark)

Jordan Boyd-Graber (University of Maryland, USA)

Naoaki Okazaki (Tokyo Institute of Technology, Japan)

ACL 2023 Programme Committee Co-Chairs

# Organizing Committee

# General Chair

Yang Liu, Amazon

# Program Chairs

Anna Rogers, IT University of Copenhagen

Jordan Boyd-Graber, University of Maryland

Naoaki Okazaki, Tokyo Institute of Technology

# Workshop Chairs

Annie Louis, Google

Eduardo Blanco, Arizona University

Yang Feng, Chinese Academy of Science

# Tutorials Chairs

Margot Mieskes, University of Applied Sciences, Darmstadt

Siva Reddy, McGill University; Mila

Vivian Chen, National Taiwan University

# Demonstrations Chairs

Alan Ritter, Georgia Institute of Technology

Danushka Bollegala, University of Liverpool

Ruihong Huang, Texas A&M University

# Industry Track Chairs

Beata Beigman Klebanov, ETS

Jason Williams, Apple

Sunayana Sitaram, Microsoft Research India

# Student Research Workshop Chairs

Gisela Vallejo, University of Melbourne

Vishakh Padmakumar, New York University

Yao Fu, University of Edinburgh

# Faculty Advisors to SRW

Ivan Vulić, University of Cambridge

Lu Wang, University of Michigan

# Ethics Chairs

Dirk Hovy, Bocconi University

Yonatan Bisk, Carnegie Mellon University

# Publication Chairs

Ryan Cotterell, ETH Zürich

Chenghua Lin, University of Sheffield

Jesse Thomason, University of Southern California

Lei Shu, Google

Lifu Huang, Virginia Tech

# Publicity and Social Media Chairs

Devamanyu Hazarika, Amazon

Eva Vanmassenhove, Tilburg University

Tong Xu, University of Science and Technology of China

# Website Chairs

Jinho Choi, Emory University

Zhongyu Wei, Fudan University

# Student Volunteer Chairs

Ayah Zirikly, Johns Hopkins University

Tao Yu, University of Hong Kong

# Virtual Infrastructure Chairs

Jiacheng Xu, Salesforce

Martín Villalba, Saarland University

Pedro Rodriguez, Meta

# Diversity and Inclusion Chairs

Daniel Beck, The University of Melbourne

Maryam Fazel-Zarandi, Meta

Nedjma Djouhra Ousidhoum, University of Cambridge

# Sponsorship Chairs

Alla Rozovskaya, The City University of New York

Lei Li, University of California at Santa Barbara

# Program Chair Assistant

Youmi Ma, Tokyo Institute of Technology

# Visa Assistance Team

- Ayana Niwa, Tokyo Institute of Technology
- Qingwen Liu, Fudan University
- Renxiang Zhang, Amazon
- Samridhi Choudhary, Amazon
- Tao You, Fudan University

# ACL Event Director

Jennifer Rachford, Association for Computational Linguistics

xiv

# Program Committee

# Computational Social Science and Cultural Analytics

Walid Magdy, Daniel Preotiuc-Pietro, Md. Shad Akhtar, Nikolaos Aletras, Kalina Bontcheva, Kareem Darwish, Mai Elsherief, Kiran Garimella, Marco Guerini, Kokil Jaidka, Barbara Mcgillivray, Yelena Mejova, Usman Naseem, Bjorn Ross, James Thorne, Marco Viviani, Soroush Vosoughi, Ingmar Weber

# Dialogue and Interactive Systems

Y-Lan Boureau, Mary Ellen Foster, Minlie Huang, João Sedoc, Luciana Benotti, Paul Crook, Maryam Fazel-Zarandi, Michel Galley, Kallirroi Georgila, Alborz Geramifard, Devamanyu Hazarika, Baotian Hu, Wenqiang Lei, Gina-Anne Levow, Piji Li, Andrea Madotto, Fei Mi, Seungwhan Moon, Lili Mou, Natalie Parde, Baolin Peng, Oleg Rokhlenko, Samira Shaikh, Lei Shu, Kurt Shuster, Ruihua Song, Yiping Song, Shabnam Tafreshi, Ryuichi Takanobu, David Traum, Stefan Ultes, Charles Welch, Min Yang, Zhou Yu, Wei-Nan Zhang, Hao Zhou

# Discourse and Pragmatics

Christian Hardmeier, Jey Han Lau, Jacob Andreas, Chloé Braud, Luis Fernando D’haro, Junyi Jessy Li, Sharid Loaiciga, Nafise Sadat Moosavi, Anna Nedoluzhko, Juntao Yu, Amir Zeldes

# Ethics and NLP

Vinodkumar Prabhakaran, Diyi Yang, Kai-Wei Chang, Sunipa Dev, Karen Fort, Jack Hessel, Debora Nozza, Zeerak Talat, Yulia Tsvetkov

# Generation

Sebastian Gehrmann, Mohit Iyyer, Nina Dethlefs, Nan Duan, Greg Durrett, Angela Fan, Claire Gardent, Albert Gatt, Yeyun Gong, Srinivasan Iyer, Meng Jiang, Sujian Li, Ankur Parikh, Nanyun Peng, Lianhui Qin, Sudha Rao, Hannah Rashkin, Jinsong Su, Hiroya Takamura, John Wieting, Rui Yan, Jiajun Zhang

# Information Extraction

Lifu Huang, Chin-Yew Lin, Aaron White, Yixin Cao, Shiyu Chang, Muhao Chen, Brian Davis, Antoine Doucet, Xinya Du, Radu Florian, Xianpei Han, Filip Ilievski, Diana Inkpen, Reno Kriz, Lane Lawley, Manling Li, Kang Liu, Zhiyuan Liu, Bonan Min, Thien Nguyen, Qiang Ning, Alan Ritter, Benjamin Roth, Lei Sha, Jingbo Shang, Ge Shi, Xianzhi Wang, Wenpeng Yin, Mo Yu, Dongyan Zhao, Jun Zhao, Christos Christodoulopoulos

# Information Retrieval and Text Mining

Benjamin Piwowarski, Qifan Wang, Yi Fang, Fuli Feng, Yiqun Liu, Jian-Yun Nie, Xiaojun Quan, Yi Tay, Hongning Wang, Jingang Wang, Zenglin Xu, Grace Hui Yang

# Interpretability and Analysis of Models for NLP

Carolin Lawrence, Ana Marasovic, Chenhao Tan, Jasmijn Bastings, Dallas Card, Samuel Carton, Oana Cocarascu, Nadir Durrani, Jacob Eisenstein, Mor Geva, Ivan Habernal, Peter Hase, Alon Jacovi, Yangfeng Ji, Divyansh Kaushik, Piyawat Lertvittayakumjorn, Zaiqiao Meng, Pasquale Minervini, Isar Nejadgholi, Danish Pruthi, Abhilasha Ravichander, Roi Reichart, Swabha Swayamdipta, Martin Tutek, Elena Voita, Sarah Wiegreffe, Tongshuang Wu

# Language Grounding to Vision, Robotics, and Beyond

Zhongyu Wei, Mark Yatskar, Yoav Artzi, Yi Cai, Jingjing Chen, Zhihao Fan, Daniel Fried, Jiasen Lu, Lin Ma, Aishwarya Padmakumar, Zhaochun Ren, Freda Shi, Carina Silberer, Alessandro Suglia, Alane Suhr, Chen Sun, Hao Tan, Meng Wang, Tong Xu

# Large Language Models

Dipanjan Das, Bhuwan Dhingra, Mike Lewis, Xuezhe Ma, Miguel Ballesteros, Kenneth Church, Kumar Dubey, Orhan Firat, Marjan Ghazvininejad, Hila Gonen, Junxian He, Harsh Jhamtani, Mandar Joshi, Xiang Kong, Ni Lao, Moontae Lee, Bing Liu, Peter Liu, Eric Malmi, Huan Sun, Lijun Wu, Chunting Zhou

# Linguistic Diversity

Constantine Lignos, Emily Prud’hommeaux, Rebecca Knowles, Zoey Liu, Teresa Lynn, Lane Schwartz, Francis Tyers, Marcos Zampieri

# Linguistic Theories, Cognitive Modeling, and Psycholinguistics

Afra Alishahi, Najoung Kim, Lisa Beinborn, Abdellah Fourtassi, Nan-Jiang Jiang, R. Thomas McCoy, Aida Nematzadeh, Grusha Prasad

# Machine Learning for NLP

Marie-Francine Moens, Anna Rumshisky, Kevin Small, Heike Adel, Mikhail Burtsev, Giuseppe Castellucci, Trevor Cohn, Danilo Croce, Julian Eisenschlos, Francis Ferraro, Matthias Galle, Dan Goldwasser, Hannaneh Hajishirzi, Ricardo Henao, Estevam Hruschka, Pei Ke, Parisa Kordjamshidi, Omer Levy, Zemin Liu, André Martins, Ashutosh Modi, Ndapa Nakashole, Thanh Tam Nguyen, Giannis Nikolentzos, Barbara Plank, Steven Schockaert, Freda Shi, Vivek Srikumar, Jun Suzuki, Hao Tang, Lu Wang, Taro Watanabe, Ningyu Zhang

# Machine Translation

Markus Freitag, Tom Kocmi, Lei Li, Boxing Chen, Colin Cherry, George Foster, Roman Grundkiewicz, Francisco Guzman, Shujian Huang, Philipp Koehn, Qun Liu, Chi-Kiu Lo, Haitao Mi, Jan Niehues, Stephan Peitz, Maja Popović, Ricardo Rei, Felix Stahlberg, Zhaopeng Tu, David Vilar, Mingxuan Wang, Joern Wuebker, Tong Xiao, Jingjing Xu, François Yvon, Yue Zhang, Hao Zhou

# Multilingualism and Cross-Lingual NLP

A. Seza Doğruöz, Sunayana Sitaram, Muhammad Abdul-Mageed, David Ifeoluwa Adelani, Alham Fikri Aji, Antonios Anastasopoulos, Mikel Artetxe, Yoshinari Fujinuma, Dan Garrette, Shruti Rijhwani, Sebastian Ruder, Xinyi Wang

# NLP Applications

Sophia Ananiadou, Mark Dras, Jing Jiang, Makoto Miwa, Vincent Ng, Hadi Amiri, Riza Batista-Navarro, Jose Camacho-Collados, Fenia Christopoulou, Giovanni Da San Martino, Dina Demner-Fushman, Luigi Di Caro, Haibo Ding, Mariano Felice, Wei Gao, Sanda Harabagiu, Seung-Won Hwang, Naoya Inoue, Shafiq Joty, Ekaterina Kochmar, Mamoru Komachi, Wei Lu, Shervin Malmasi, David Mimno, Preslav Nakov, Maria Leonor Pacheco, Marek Rei, Kirk Roberts, Sara Rosenthal, Alla Rozovskaya, Tulika Saha, Hiroki Sakaji, Matthew Shardlow, Shuohang Wang, Jason Wei, Qianqian Xie, Jianfei Yu, Chrysoula Zerva, Aston Zhang, Arkaitz Zubiaga

# Phonology, Morphology, and Word Segmentation

Miikka Silfverberg, Ekaterina Vylomova, Ryan Cotterell, Xuanjing Huang, David R. Mortensen

# Question Answering

Eunsol Choi, Mrinmaya Sachan, Rishiraj Saha Roy, Priyanka Agrawal, Chitta Baral, Gianni Baralacchi, Hao Cheng, Danish Contractor, Pradeep Dasigi, Tushar Khot, Rik Koncel-Kedziorski, Bill Yuchen Lin, Bang Liu, Ismini Lourentzou, Sewon Min, Liangming Pan, Panupong Pasupat, Peng Qi, Ashish Sabharwal, Xiaoyu Shen, Veselin Stoyanov, Yu Su, Kai Sun, Mihai Surdeanu, Di Wang, Ziyu Yao, Yuhao Zhang

# Resources and Evaluation

Sarvnaz Karimi, Nathan Schneider, Karin Verspoor, Rachel Bawden, Asma Ben Abacha, Doina Caragea, Jennifer D’souza, Rotem Dror, Ondrej Dusek, Steffen Eger, Jorge Gracia, Udo Hahn, Lifeng Han, Radu Tudor Ionescu, David Janiszek, Sudipta Kar, Jin-Dong Kim, Jonathan Kummerfeld, John P. Lalor, Fabrice Lefèvre, Jochen Leidner, Roser Morante, Gabriella Pasi, Maja Popović, German Rigau, Yves Scherrer, Manish Shrivastava, Sowmya Vajjala, Lucy Lu Wang

# Semantics: Lexical

Marianna Apidianaki, Gabriella Lapesa, Chris Biemann, Guy Emerson, Allyson Ettinger, Goran Glavaš, Dieuwke Hupkes, Nancy Ide, Andrey Kutuzov, Alessandro Lenci, Mohammad Taher Pilehvar, Yuval Pinter, Edoardo Maria Ponti, Vered Shwartz, Lonneke Van Der Plas, Ivan Vulić

# Semantics: Sentence-level Semantics, Textual Inference, and Other Areas

Yuki Arase, Roberto Navigli, Roy Schwartz, Tommaso Caselli, Simone Conia, Lei Cui, Li Dong, Lea Frermann, Atsushi Fujita, Christophe Gravier, Luheng He, Germán Kruszewski, Tommaso Pasini, Adam Poliak, Jakob Prange, Michael Roth, Keisuke Sakaguchi, Abulhair Saparov, Ji-Rong Wen, Wei Xu, Sho Yokoi, Chen Zhao

# Sentiment Analysis, Stylistic Analysis, and Argument Mining

Lun-Wei Ku, Henning Wachsmuth, Khalid Al Khatib, Elena Cabrio, Hao Fei, Anette Frank, Lin Gui, Yufang Hou, Ting-Hao Huang, Kentaro Inui, Anne Lauscher, John Lawrence, Saif Mohammad, Joonsuk Park, Shabnam Tafreshi, Orith Toledo-Ronen, Serena Villata, Shuai Wang

# Speech and Multimodality

Grzegorz Chrupała, Frank Rudzicz, Laurent Besacier, Manaal Faruqui, Sharon Goldwater, Florian Metze, Okko Rasanen, Andrew Rosenberg, Hao Tang, Wenwu Wang, Xin Wang, Shinji Watanabe

# Summarization

Chenghua Lin, Shashi Narayan, Reinald Kim Amplayo, Avi Caciularu, Chung-Chi Chen, Gong Cheng, Markus Dreyer, Xiaocheng Feng, Kathleen Mckeown, Stuart Middleton, Richard Yuanzhe Pang, Xiaojun Wan, Xingxing Zhang, Yao Zhao

# Syntax: Tagging, Chunking, and Parsing

Wanxiang Che, Djamé Seddah, Xinchi Chen, Leyang Cui, Lifeng Jin, Zhenghua Li, Joakim Nivre, Kenji Sagae, Meishan Zhang

# Theme: Reality Check

Ehud Reiter, Xiang Ren, Malihe Alikhani, Jan Buys, Jesse Dodge, Antske Fokkens, Robin Jia, Daniel Khashabi, Emiel Krahmer, Saad Mahamood, Margaret Mitchell, Richard Sproat, Byron Wallace, Adina Williams

# COI

Shay B. Cohen, Daisuke Kawahara

# Ethics

Yonatan Bisk, Dirk Hovy, Jin-Dong Kim, Zeerak Talat

# Best Paper Selection Committee

Jonathan Berant, Jose Camacho-Collados, Danqi Chen, Benjamin Van Durme, David Jurgens, Desmond Elliott, Sasha Luccioni, Jonathan May, Tom McCoy, Yusuke Miyao, Ekaterina Shutova, Emma Strubell

# Primary Reviewers

Amirhossein Abaskohi, Harika Abburi, Asad Abdi, Sadaf Abdul Rauf, Muhammad Abdul-Mageed, Kaori Abe, Omri Abend, Gavin Abercrombie, Sallam Abualhaija, Abdalghani Abujabal, Alafate Abulimiti, Lars Ackermann, Griffin Adams, Ife Adebara, David Ifeoluwa Adelani, Benedikt Adelmann, Tosin Adewumi, Jiban Adhikary, Suman Adhya, Yossi Adi, Somak Aditya, Vaibhav Adlakha, Noemi Aepli, Stergos Afantenos, Haithem Afli, Ankur Agarwal, Sanchit Agarwal, Shivam Agarwal, Rodrigo Agerri, Arshiya Aggarwal, Karan Aggarwal, Piush Aggarwal, Manex Agirre-zabal, Guy Aglionby, Aishwarya Agrawal, Ameeta Agrawal, Sweta Agrawal, Roee Aharoni, Wasi Uddin Ahmad, Sina Ahmadi, Natalie Ahn, Aman Ahuja, Chaitanya Ahuja, Kabir Ahuja, Lin Ai, Xi Ai, Ankit Aich, Annalena Aicher, Laura Aina, Salah Aït-Mokhtar, Akiko Aizawa, Alham Fikri Aji, Aswathy Ajith, Reina Akama, Pritom Saha Akash, Alan Akbik, Adewale Akinfaderin, Nader Akoury, Burak Aksar, Ibrahim Taha Aksu, Mousumi Akter, Arjun Akula, Ekin Akyurek, Hend Al-Khalifa, Hadeel Al-Negheimish, Hussein Al-Olimat, Rami Al-Rfou, Nora Al-Twairesh, Firoj Alam, Mehwish Alam, Alon Albalak, Abdullah Albanyan, Chris Alberti, Hanan Aldarmaki, Vasiliy Alekseev, Jan Alexandersson, Georgios Alexandridis, Mark Alfano, David Alfter, Robin Algayres, Raquel G. Alhama, Abdulaziz Alhamadani, Tariq Alhindi, Hamed Alhoori, Hassan Alhuzali, Badr Alkhamissi, Maxime Allard, Emily Allaway, Liesbeth Allein, Tiago Almeida, Khalid Alnajjar, Omar Alonso, Abdullah Alrajeh, Milad Alshomary, Maha Jarallah Althobaiti, Duygu Altinok, Fernando Alva-Manchego, Rami Aly, Chiara Alzetta, Bharat Ram Ambati, Maxime Amblard, Iqra Ameer, Saadullah Amin, Afra Amini, Silvio Amir, Maaz Amjad, Haozhe An, Jie An, Jisun An, Ashish Anand, Sophia Ananiadou, Raviteja Anantha, Rafael Anchiêta, Mark Anderson, Nicholas Andrews, Raghuram Annasamy, Diego Antognini, Jean-Yves Antoine, Maria Antoniak, Wissam Antoun, Rishita Anubhai, Xiang Ao, Emilia Apostolova, Mario Aragon, Erik Arakelyan, Jun Araki, Rahul Aralikatte, Ayme Arango Monnar, Oscar Araque, Matheus Araujo, John Arevalo, Arturo Argueta, Mozhdeh Ariannezhad, Hiba Arnaout, Akhil Arora, Piyush Arora, Siddhant Arora, Leila Arras, Ekaterina Artemova, Philip Arthur, Ron Artstein, Anjana Arunkumar, Saurav Aryal, Akari Asai, Ehsaneddin Asgari, Elliott Ash, Nicholas Asher, Md.sadek Hossain Asif, Arian Askari, Matthias Assenmacher, Zhenisbek Assylbekov, Berk Atil, Giuseppe Attanasio, Mohammed Attia, Aitziber Atutxa Salazar, Lauriane Aufrant, Tal August, Hayastan Avetisyan, Eleftherios Avramidis, Vera Axelrod, Hammad Ayyubi, Hosein Azarbonyad, Gorka Azkune, Aslan B. Wong, Bogdan Babych, Luca Bacco, Nguyen Bach, Sarkhan Badirli, Ebrahim Bagheri, Petra Bago, Parnia Bahar, Ashutosh Baheti, Vikas Bahirwani, Bing Bai, Fan Bai, He Bai, Jiaxin Bai, Long Bai, Xuefeng Bai, Yinhao Bai, Yu Bai, Yushi Bai, Jinyeong Bak, Amir Bakarov, Collin Baker, Vidhisha Balachandran, Mithun Balakrishna, Oana Balalau, Vevake Balaraman, Ananth Balashankar, Ramya Balasubramaniam, Gunjan Balde, Ioana Baldini, Timothy Baldwin, Simone Balloccu, Mohammadreza Banaei, Dibyanayan Bandyopadhyay, Debayan Banerjee, Pratyay Banerjee, Seojin Bang, Yejin Bang, Vinayshekhar Bannihatti Kumar, Hritik Bansal, Forrest Sheng Bao, Guangsheng Bao, Junwei Bao, Yu Bao, Yuwei Bao, Ankur Bapna, Kfir Bar, Roy Bar-Haim, Claire Barale, Mohamad Hardyman Barawi, Edoardo Barba, Adrien Barbaresi, Verginica Barbu Mititelu, M Saiful Bari, Loic Barrault, Alberto Barrón-Cedeño, Sabine Bartsch, Sabyasachee Baruah, Marco Basaldella, Pierpaolo Basile, Valerio Basile, Ali Basirat, Elisa Bassignana, Mohaddeseh Bastan, Kinjal Basu, Somnath Basu Roy Chowdhury, Tatiana Batura, Daniel Bauer, Timo Baumann, Ian Beaver, Björn Bebensee, Daniel Beck, Lee Becker, Maria Becker, Barend Beekhui.

zen, Dorothee Beermann, Gasper Begus, Melika Behjati, Shabnam Behzad, Andrei Stefan Bejgu, Nazar Beknazarov, Nuria Bel, Yonatan Belinkov, Eric Bell, Meriem Beloucif, Luca Benedetto, Martin Benjamin, Lauren Benson, Gábor Berend, Benjamin Bergen, Leon Bergen, Maria Berger, Nathaniel Berger, Rafael Berlanga, Gabriel Bernier-Colborne, Dario Bertero, Laurent Besacier, Chandra Bhagavatula, Rasika Bhalerao, Rohan Bhambhoria, Avanti Bhandarkar, Rishabh Bhardwaj, Aditya Bhargava, Pushpak Bhattacharyya, Satwik Bhattamishra, Bimal Bhattarai, Shohini Bhattasali, Anahita Bhiwandiwalla, Plaban Bhowmick, Rajarshi Bhowmik, Mukul Bhutani, Nikita Bhutani, Bin Bi, Guanqun Bi, Wei Bi, Giovanni Biancofiore, Adrien Bibal, Ann Bies, Laura Biester, Geetanjali Bihani, Yi Bin, Arne Binder, Jennifer Bishop, Debmalya Biswas, Yonatan Bitton, Johannes Bjerva, Henrik Björklund, Johanna Bjorklund, Philippe Blache, Nate Blaylock, Avi Bleiweiss, Terra Blevins, Rexhina Blloshmi, Su Lin Blodgett, Jelke Bloem, Michael Bloodgood, Carlos Bobed Lisbona, Victoria Bobicev, Ben Bogin, Bernd Bohnet, Ondřej Bojar, Huang Bojun, Valeriia Bolotova-Baranova, Necva Bülbül, Rishi Bommasani, Daniele Bonadiman, Alessandro Bondielli, Francesca Bonin, Logan Born, Mihaela Bornea, Emanuela Boros, Johan Bos, Digbalay Bose, Robert Bossy, Kaj Bostrom, Florian Boudin, Mohand Boughanem, Gerlof Bouma, Gosse Bouma, Zied Bouraoui, Andrey Bout, Johan Boye, Faeze Brahman, António Branco, Stephanie Brandl, Kiante Brantley, Pavel Braslavski, Adrian Brasoveanu, Daniel Braun, Jacob Bremerman, Jonathan Brennan, Chris Brew, Shaked Brody, Thomas Brovelli (meyer), Hannah Brown, Caroline Brun, Dominique Brunato, Yi Bu, Emanuele Bugliarello, Trung Bui, Paul Buitelaar, Razvan Bunescu, Laurie Burchell, Susanne Burger, Jill Burstein, Victor Bursztyn, Davide Buscaldi, Hendrik Buschmeier, Miriam Butt, Joan Byamugisha, Bill Byrne, Donna Byron, José G. C. De Souza, Michele Cafagna, Aoife Cahill, Samuel Cahyawijaya, Deng Cai, Han Cai, Hengyi Cai, Hongjie Cai, Pengshan Cai, Xiangrui Cai, Ruken Cakici, Iacer Calixto, Zoraida Callejas, Jesus Calvillo, Giovanni Campagna, Leonardo Campillos-Llanos, Niccolò Campolungo, Daniel Campos, Jon Ander Campos, Ricardo Campos, Burcu Can, M Abdullah Canbaz, Nicola Cancedda, Marie Candito, Ed Cannon, Erion Çano, Boxi Cao, Hailong Cao, Hejing Cao, Jiangxia Cao, Jie Cao, Kris Cao, Mengyun Cao, Pengfei Cao, Qingqing Cao, Qingxing Cao, Ruisheng Cao, Shuyang Cao, Yixuan Cao, Yu Cao, Yu Cao, Yuan Cao, Yuwei Cao, Ziqiang Cao, Cristian Cardellino, Rémi Cardon, Boaz Carmeli, Xavier Carreras, Paula Carvalho, Francisco Casacuberta, Fabio Casati, Helena Caseli, Pierluigi Cassotti, Sheila Castilho, Arie Cattan, Andrew Cattle, Paulo Cavalin, Roberto Centeno, Dumitru-Clementin Cercel, Christophe Cerisara, Mauro Cettolo, Sky Ch-Wang, Haixia Chai, Heyan Chai, Joyce Chai, Junyi Chai, Yekun Chai, Tuhin Chakrabarty, Megha Chakraborty, Tanmoy Chakraborty, Bharathi Raja Chakravarthi, Yllias Chali, Ilias Chalkidis, Nathanael Chambers, Hou Pong Chan, Zhangming Chan, Anshuma Chandak, Chandrahas, Raman Chandrasekar, Baobao Chang, Buru Chang, Ernie Chang, Haw-Shiuan Chang, Heng Chang, Kent Chang, Serina Chang, Shuaichen Chang, Tyler Chang, Yapei Chang, Yung-Chun Chang, Tai Chang-You, Guan-Lin Chao, Rajen Chatterjee, Akshay Chaturvedi, Iti Chaturvedi, Aditi Chaudhary, Vishrav Chaudhary, Subhajit Chaudhury, Geeticka Chauhan, Kushal Chawla, Chao Che, Ciprian Chelba, Emmanuel Chemla, Beiduo Chen, Berlin Chen, Bo Chen, Boli Chen, Canyu Chen, Catherine Chen, Chacha Chen, Chen Chen, Deli Chen, Derek Chen, Dongsheng Chen, Francine Chen, Fuxiang Chen, Guanhua Chen, Guanliang Chen, Guanyi Chen, Hanjie Chen, Howard Chen, Huiyuan Chen, Hung-Ting Chen, Jia Chen, Jiaao Chen, Jiangjie Chen, Jiaze Chen, Jifan Chen, Jingye Chen, John Chen, Junfan Chen, Junyang Chen, Kehai Chen, Kezhen Chen, Lei Chen, Lichang Chen, Lihu Chen, Lin Chen, Linqing Chen, Long Chen, Lu Chen, Luoxin Chen, Maximillian Chen, Mei-Hua Chen, Meiqi Chen, Meng Chen, Mingda Chen, Nuo Chen, Pei Chen, Qian Chen, Qiang Chen, Qianglong Chen, Qin Chen, Qipin Chen, Qiyuan Chen, Ruey-Cheng Chen, Sanxing Chen, Shijie Chen, Shizhe Chen, Sihao Chen, Tao Chen, Tongfei Chen, Xiaojun Chen, Xiaoli Chen, Xiaoyin Chen, Xilun Chen, Xingran Chen, Xinhong Chen, Xiuyi Chen, Xiuying Chen, Yang Chen, Yangbin Chen, Yangi Chen, Yanping Chen, Yen-Chun Chen, Yiming Chen, Ying Chen, Yongjun Chen, Yu Chen, Yubo Chen, Yubo Chen, Yue Chen, Yue Chen, Yulong Chen, Yun Chen, Yunmo Chen, Zeming Chen, Zhibin Chen, Zhihong Chen, Zhijun Chen, Zhiyu Chen, Zhiyu Chen, Zhuang Chen, Fei

Cheng, Liying Cheng, Lu Cheng, Myra Cheng, Pengxiang Cheng, Qinyuan Cheng, Shanbo Cheng, Sijie Cheng, Weiwei Cheng, Yong Cheng, Yu Cheng, Zhi-Qi Cheng, Vijil Chenthamarakshan, Joe Cheri, Artem Chernodub, Emmanuele Chersoni, Jackie Chi Kit Cheung, Jianfeng Chi, Zewen Chi, Cheng-Han Chiang, David Chiang, Patricia Chiril, Nadezhda Chirkova, Luis Chiruzzo, Billy Chiu, Javier Chiyah-Garcia, Hyunchang Cho, Hyundong Cho, Hyunsoo Cho, Sangwoo Cho, Seunghyuk Cho, Sungjun Cho, Sungzoon Cho, Won Ik Cho, Young Min Cho, Daejin Choi, Jihun Choi, Jinho D. Choi, Seungtaek Choi, Yejin Choi, Yunseok Choi, Shamil Chollampatt, Jaegul Choo, Shubham Chopra, Leshem Choshen, Prafulla Kumar Choubey, Monojit Choudhury, Md Faisal Mahbub Chowdhury, Shammur Absar Chowdhury, Lukas Christ, Chenhui Chu, Yun-Wei Chu, Zewei Chu, Zhendong Chu, Yung-Sung Chuang, Jayeol Chun, Jin-Woo Chung, Abu Nowshed Chy, Alessandra Teresa Cignarella, Philipp Cimiano, Manuel Ciosici, Jorge Civera Saiz, Christopher Clark, Elizabeth Clark, Vincent Claveau, Ann Clifton, Maximin Coavoux, Anne Cocos, Daniel Cohen, Raphael Cohen, Mariona Coll Ardanuy, Davide Colla, Marcus Collins, Pedro Colon-Hernandez, Andrei Coman, Mathieu Constant, Paul Cook, Asa Cooper Stickland, Anna Corazza, Francesco Corcoglioniti, João Cordeiro, Nathan Cornille, Gonçalo Correia, Erin Crabb, Benoit Crabbé, Mathias Creutz, Liam Cripwell, Fabien Cromieres, Maxwell Crouse, Heriberto Cuayahuitl, Ganqu Cui, Haotian Cui, Peng Cui, Shaobo Cui, Shiyao Cui, Wanyun Cui, Xia Cui, Yiming Cui, Rossana Cunha, Washington Cunha, Jeff Da, Iria Da Cunha, Raj Dabre, Gautier Dagan, Deborah Dahl, Leonard Dahlmann, Daniel Dahlmeier, Damai Dai, Hongliang Dai, Qin Dai, Wenliang Dai, Xiang Dai, Yi Dai, Yinpei Dai, Yong Dai, Daniel Dakota, Fahim Dalvi, Marco Damonte, Sandipan Dandapat, Rumen Dangovski, Verna Dankers, Aswarth Abhilash Dara, Amitava Das, Anubrata Das, Ayan Das, Debopam Das, Dipankar Das, Mithun Das, Sarkar Snigdha Sarathi Das, Souvik Das, Mithun Das Gupta, Sarthak Dash, Debajyoti Datta, Vidas Daudaravicius, Sam Davidson, Forrest Davis, Joe Davison, Luna De Bruyne, Gael De Chalendar, Orphee De Clercq, Adria De Gispert, Michiel De Jong, Kordula De Kuthy, Éric De La Clergerie, Cyprien De Lichy, Ernesto William De Luca, Renato De Mori, Andrea De Varda, Alok Debnath, Mathieu Dehouck, Maksym Del, Luciano Del Corro, Jean-Benoit Delbrouck, Marc Delcroix, Sebastien Delecraz, Louise Deleger, Felice Dell’orletta, Pieter Delobelle, Vera Demberg, Daryna Dementieva, David Demeter, Seniz Demir, Dorottya Demszky, Steve Deneefe, Haolin Deng, Mingkai Deng, Shumin Deng, Xiang Deng, Xun Deng, Yang Deng, Yuntian Deng, Zhi-Hong Deng, Pascal Denis, Michael Denkowski, Leon Derczynski, Jan Deriu, Daniel Deutsch, Premkumar Devanbu, Murthy Devarakonda, Chris Develder, Hannah Devinney, Suvodip Dey, Jay Deyoung, Prajit Dhar, Zonglin Di, Barbara Di Eugenio, Mattia Di Gangi, Luca Di Liello, Giorgio Maria Di Nunzio, Shizhe Diao, Gaël Dias, Alberto Diaz, Dimitar Dimitrov, Emily Dinan, Bosheng Ding, Chenchen Ding, Jie Ding, Kaize Ding, Keyang Ding, Liang Ding, Ning Ding, Shuoyang Ding, Wenjian Ding, Wentao Ding, Yangruibo Ding, Yuning Ding, Zeyuan Ding, Zixiang Ding, Anca Dinu, Liviu P. Dinu, Peter Dirix, Ajay Divakaran, Kalpit Dixit, Tanay Dixit, Nemanja Djuric, Dmitriy Dligach, Sumanth Doddapani, Pavel Dolin, Miguel Domingo, Chenhe Dong, Haoyu Dong, Meixing Dong, Mengxing Dong, Ming Dong, Minghui Dong, Qianqian Dong, Qingxiu Dong, Xiangjue Dong, Xin Dong, Christine Doran, Bonaventure F. P. Dossou, Longxu Dou, Zhicheng Dou, Zi-Yi Dou, Jad Doughman, Eduard Dragut, Aleksandr Drozd, Jinhua Du, Li Du, Li Du, Mengnan Du, Pan Du, Tianyu Du, Wanyu Du, Yangkai Du, Yulun Du, Yupei Du, Dheeru Dua, Hanyu Duan, Jiali Duan, Jiaxin Duan, Junwen Duan, Pengfei Duan, Sufeng Duan, Xiangyu Duan, Pablo Duboue, Philipp Dufter, Liam Dugan, Nicolas Dugue, Kevin Duh, Jonathan Dunn, Tejas Duseja, Brian Dusell, Sourav Dutta, Tomasz Dwojak, William Dyer, Haihong E, Kurt Eberle, Sebastian Ebert, Hiroshi Echizen’ya, Lukas Edman, Daniel Edmiston, Aleksandra Edwards, Santiago Egea Gómez, Markus Egg, Koji Eguchi, Yo Ehara, Maud Ehrmann, Roald Eiselen, Jason Eisner, Asif Ekbal, Ismail El Maarouf, Samhaa R. El-Beltagy, Aparna Elangovan, Yanai Elazar, Maha Elbayad, Heba Elfardy, Mohamed Elgaar, Michael Elhadad, Basil Ell, Desmond Elliott, Fatma Elsafoury, Micha Elsner, Chris Chinenye Emezue, Saman Enayati, Joseph Enguehard, Sugyeong Eo, Liana Ermakova, Ori Ernst, Patrick Ernst, Engin Erzin, Carlos Escolano, Arash Eshghi, Cristina España-Bonet, Luis Espinosa

Anke, Dominique Estival, Kawin Ethayarajh, Kilian Evang, Kenneth Ezukwoke, Saad Ezzini, Alex Fabbri, Marzieh Fadaee, Michael Faerber, Guglielmo Faggioli, Fahim Faisal, Agnieszka Falenska, Neele Falk, Tobias Falke, James Fan, Jungwei Fan, Yao-Chung Fan, Yimin Fan, Yue Fan, Zhihao Fan, Hui Fang, Qingkai Fang, Tianqing Fang, Yihao Fang, Yimai Fang, Yuwei Fang, Hossein Fani, Ana C Farinha, Nawshad Farruque, Amany Fashwan, Mehwish Fatima, Adam Faulkner, Benoit Favre, Amir Feder, Marc Feger, Zichu Fei, Guy Feigenblat, Nils Feldhus, Sergey Feldman, Virginia Felkner, Jianzhou Feng, Jiazhan Feng, Shangbin Feng, Shi Feng, Shutong Feng, Steven Y. Feng, Weixi Feng, Xiachong Feng, Yang Feng, Yansong Feng, Yu Feng, Yunhe Feng, Zhangyin Feng, Paulo Fernandes, Nigel Fernandez, Ramon Fernandez Astudillo, Javier Fernandez-Cruz, Daniel Fernández-González, Elisa Ferracane, Javier Ferrando, Rafael Ferreira, Besnik Fetahu, Alejandro Figueroa, Matthew Finlayson, Mauajama Firdaus, Mark Fishel, Margaret Fleck, Michael Flor, Jose Fonollosa, Marco Aurelio Fonseca, Tommaso Fornaciari, Karen Fort, Jennifer Foster, Abdellah Fourtassi, Robert Frank, Kathleen C. Fraser, Flavius Frasincar, Diego Frassinelli, Dayne Freitag, André Freitas, Simona Frenda, Victor Fresno, Dan Friedman, Annemarie Friedrich, Jason Fries, Francesca Frontini, Guohong Fu, Jie Fu, Lisheng Fu, Liye Fu, Peng Fu, Xiyan Fu, Yao Fu, Nancy Fulda, Kotaro Funakoshi, Pascale Fung, Yi Fung, Martin Funkquist, Hagen Fürstenau, Richard Futrell, Matteo Gabburo, Kata Gábor, Marco Gaido, Amit Gajbhiye, Dimitris Galanis, Olivier Galibert, Lukas Galke, Ramiro H. Gálvez, Mihaela Gaman, Leilei Gan, Yujian Gan, Sudeep Gandhe, Ashwinkumar Ganesan, Balaji Ganesan, Ananya Ganesh, Varun Gangal, Debasis Ganguly, William Gantt, Chang Gao, Chongyang Gao, Cuiyun Gao, Ge Gao, Hongyang Gao, Jiahui Gao, Jinhua Gao, Jun Gao, Lingyu Gao, Pengzhi Gao, Qiaozi Gao, Shen Gao, Tianyu Gao, Wei Gao, Xin Gao, Yifan Gao, Yingbo Gao, Cristina Garbacea, Marcos Garcia, Aitor García Pablos, Leibny Paola Garcia Perera, Iker García-Ferrero, Diego Garcia-Olano, Krishna Garg, Muskan Garg, Sarthak Garg, Siddhant Garg, Aina Garí Soler, Ekaterina Garmash, Łukasz Garncarek, Nicolas Garneau, Federico Gaspari, Judith Gaspers, Itai Gat, Susan Gauch, Eric Gaussier, Tanja Gaustad, Dipesh Gautam, Mengshi Ge, Suyu Ge, Xiou Ge, Yixiao Ge, Zhaocheng Ge, Michaela Geierhos, Christian Geishauser, Ruiying Geng, Ariel Gera, Felix Gervits, Luke Gessler, Hamidreza Ghader, Sahar Ghannay, Sarik Ghazarian, Mozhdeh Gheini, Deepanway Ghosal, Amur Ghose, Sayan Ghosh, Sayontan Ghosh, Soumitra Ghosh, Sourav Ghosh, Sreyan Ghosh, Sucheta Ghosh, Filip Ginter, John Giorgi, Salvatore Giorgi, Voula Giouli, Mario Giulianelli, Ameya Godbole, Nathan Godey, Pranav Goel, Rahul Goel, Vaibhava Goel, Anne Göhring, Koldo Gojenola, Tejas Gokhale, Yoav Goldberg, Seraphina Goldfarb-Tarrant, Sujatha Das Gollapalli, Olga Golovneva, Luís Gomes, Jose Manuel Gomez-Perez, Carlos Gómez-Rodríguez, Hugo Goncalo Oliveira, Marcos Goncalves, Teresa Goncalves, Lovedeep Gondara, Hongyu Gong, Jiaying Gong, Linyuan Gong, Shansan Gong, Zhuocheng Gong, Jeff Good, Michael Goodman, Senthilkumar Gopal, Karthik Gopalakrishnan, Jonathan Gordon, Philip John Gorinski, Isao Goto, Yanjie Gou, Antoine Gourru, Cyril Goutte, Venkata Subrahmanyan Govindarajan, Edward Gow-Smith, Thamme Gowda, Kartik Goyal, Navita Goyal, Palash Goyal, Prasoon Goyal, Natalia Grabar, Mario Graff, Damien Graux, David Griol, Milan Gritta, Loïc Grobol, Stig-Arne Grönroos, David Gros, Adam Grycner, Jia-Chen Gu, Jiasheng Gu, Shuhao Gu, Yue Gu, Yuxian Gu, Saiping Guan, Yong Guan, Nuno M. Guerreiro, Liangke Gui, Vincent Guigue, Bruno Guillaume, Adrien Guille, Kalpa Gunaratna, James Gung, Tunga Gungor, Sharath Chandra Guntuku, Biyang Guo, Fengyu Guo, Han Guo, Jiang Guo, Jiaqi Guo, Jinyang Guo, Junliang Guo, Lin Guo, Meiqi Guo, Qipeng Guo, Quan Guo, Ruocheng Guo, Shaoru Guo, Shu Guo, Wangzhen Guo, Xin Guo, Xinnan Guo, Yanzhu Guo, Yinpeng Guo, Zhijiang Guo, Abhirut Gupta, Akshat Gupta, Amulya Gupta, Anchit Gupta, Ankit Gupta, Ankita Gupta, Ashim Gupta, Jai Gupta, Nitish Gupta, Prakhar Gupta, Raghav Gupta, Rishabh Gupta, Sonu Gupta, Sparsh Gupta, Umang Gupta, Vivek Gupta, Ximena Gutierrez-Vasques, Jeremy Gwinnup, Loitongbam Gyanendro Singh, Le An Ha, Nizar Habash, Kais Haddar, Katharina Haemmerl, Christopher Hahn, Joonghyuk Hahn, Michael Hahn, Zhen Hai, Jan Hajič, Eva Hajicova, Hossein Hajipour, Sherzod Hakimov, Kishaloy Halder, Anaïs Halftermeyer, Harald Hammarström, Michael Hammond, Thierry Hamon, Chengcheng Han, Chi Han, Hojae Han, Kelvin Han, Ridong Han, Rujun Han.

Ting Han, Xiaochuang Han, Xiaohui Han, Xu Han, Xudong Han, Yo-Sub Han, Yu Han, Zhen Han, Zhongyuan Han, Chung-Wei Hang, Viktor Hangya, Jie Hao, Junheng Hao, Tianyong Hao, Rejwanul Haque, Syed Haque, Tatsuya Harada, David Harbecke, Momchil Hardalov, Daniel Hardt, Hardy Hardy, Keith Harrigian, William Hartmann, John Harvill, Sadid A. Hasan, Maram Hasanain, Taku Hasegawa, Chikara Hashimoto, Sabit Hassan, Bradley Hauer, Claudia Hauff, Shreya Havaldar, William Havard, Adi Haviv, Hiroaki Hayashi, Yoshihiko Hayashi, Amir Hazem, Ben He, Guoxiu He, Jacqueline He, Jianfeng He, Jiangen He, Jiayuan He, Jinzheng He, Kai He, Keqing He, Ru He, Shizhu He, Tianxing He, Wanwei He, Wei He, Xiaodong He, Xingwei He, Xuanli He, Yifan He, Yunjie He, Zexue He, Zhongjun He, Michael Heck, Behnam Hedayatnia, Michael Hedderich, Stefan Heindorf, Johannes Heinecke, Jindřich Helcl, William Held, Oliver Hellwig, Chadi Helwe, Christian Hempelmann, Lisa Anne Hendricks, Iris Hendrickx, Cui Hengbin, Leonhard Hennig, Yu-Jung Heo, David Herel, Delia Irazu Hernandez Farias, Christian Herold, Daniel Hershcovich, Jonathan Herzig, Christian Heumann, John Hewitt, Gerhard Heyer, Christopher Hidey, Derrick Higgins, Stefan Hillmann, Tsutomu Hirao, Tatsuya Hiraoka, Namgyu Ho, Cong Duy Vu Hoang, Cuong Hoang, Julia Hockenmaier, Chris Hokamp, Samuel Hollands, Nora Hollenstein, Pavan Holur, Christopher Homan, Takeshi Homma, Ukyo Honda, Giwon Hong, Pengyu Hong, Zhi Hong, Mark Hopkins, Ales Horak, Andrea Horbach, Sho Hoshino, Tom Hosking, Md Mosharaf Hossain, Mohammad Javad Hosseini, Pedram Hosseini, Rasa Hosseinzadeh, Lei Hou, Yifan Hou, Phillip Howard, David M. Howcroft, Cheng-Yu Hsieh, Chao-Chun Hsu, Chun-Nan Hsu, I-Hung Hsu, Yi-Li Hsu, Phu Mon Htut, Chi Hu, Dou Hu, Guangneng Hu, Guimin Hu, Hai Hu, Hailin Hu, Han Hu, Hexiang Hu, Jinyi Hu, Linmei Hu, Mengting Hu, Minda Hu, Songbo Hu, Xiang Hu, Xiaodan Hu, Xuming Hu, Yibo Hu, Yuchen Hu, Yushi Hu, Zhe Hu, Zhiwei Hu, Zhiyuan Hu, Zikun Hu, Ziniu Hu, Hang Hua, Wenyue Hua, Xinyu Hua, Chao-Wei Huang, Chen Huang, Chieh-Yang Huang, Fei Huang, Hen-Hsen Huang, Hui Huang, Jen-Tse Huang, Jiaxin Huang, Jie Huang, Jimin Huang, Jimmy Huang, Jin-Xia Huang, Junjie Huang, Kuan-Hao Huang, Kung-Hsiang Huang, Luyang Huang, Qingbao Huang, Quzhe Huang, Rongjie Huang, Shaohan Huang, Tenghao Huang, Xinting Huang, Yinya Huang, Yongjie Huang, Youcheng Huang, Zhiqi Huang, Zhongqiang Huang, Luwen (vivian) Huangfu, Patrick Huber, John Hudzina, Pere-Lluís Huguet Cabot, Mans Hulden, Chia-Chien Hung, Fantine Huot, Ali Hürriyetoğlu, Tin Huynh, Rebecca Hwa, Dae Yon Hwang, Jena D. Hwang, Dongmin Hyun, Ignacio Iacobacci, Muhammad Okky Ibrohim, Adrian Iftene, Ryu Iida, Gabriel Ilharco, Nikolai Ilinykh, Kenji Imamura, Ayyoob Imanigooghari, Joseph Marvin Imperial, Hirofumi Inaguma, Mert Inan, Svanhvít Lilja Ingólfsdóttir, Koji Inoue, Takashi Inui, Hitoshi Isahara, Tatsuya Ishigaki, Etsuko Ishii, Aminul Islam, Tunazzina Islam, Masaru Isonuma, Takumi Ito, Abe Ittycheriah, Hamish Ivison, Tomoya Iwakura, Ran Iwamoto, Kenichi Iwatsuki, Vivek Iyer, Peter Izsak, Bassam Jabaian, Aashi Jain, Alankar Jain, Parag Jain, Rishabh Jain, Milos Jakubicek, Masoud Jalili Sabet, Shoaib Jameel, Richard James, Abhik Jana, Eugene Jang, Hyeju Jang, Myeongjun Jang, Youngsoo Jang, Sepehr Janghorbani, Peter Jansen, Maarten Janssen, Sujay Kumar Jauhar, Tommi Jauhiainen, Inigo Jauregi Unanue, Ganesh Jawahar, Sébastien Jean, Fran Jelenić, Sungho Jeon, Minwoo Jeong, Myeongho Jeong, Young-Seob Jeong, Kevin Jesse, Elisabetta Jezek, Akshita Jha, Prince Jha, Sneha Jha, Bin Ji, Haozhe Ji, Seunghyun Ji, Shaoxiong Ji, Ziwei Ji, Chen Jia, Qi Jia, Zixia Jia, Yiren Jian, Aiqi Jiang, Chao Jiang, Feng Jiang, Hang Jiang, Hao Jiang, Jie Jiang, Jiyue Jiang, Junfeng Jiang, Jyun-Yu Jiang, Lan Jiang, Lavender Jiang, Ming Jiang, Ridong Jiang, Tianwen Jiang, Tianyu Jiang, Wenbin Jiang, Xiaotong Jiang, Xuhui Jiang, Yichen Jiang, Yong Jiang, Yuxin Jiang, Zhengbao Jiang, Zhiwei Jiang, Zhiying Jiang, Zhuoren Jiang, Zhuoxuan Jiang, Cathy Jiao, Wenxiang Jiao, Yizhu Jiao, Zhanming Jie, Bernal Jimenez Gutierrez, Di Jin, Li Jin, Lisa Jin, Mali Jin, Qiao Jin, Shuning Jin, Woojeong Jin, Xiaomeng Jin, Yiping Jin, Zhi Jin, Zhijing Jin, Zijian Jin, Hwiyeol Jo, Richard Johansson, Kristen Johnson, Michael Johnston, Erik Jones, Kenneth Joseph, Abhinav Joshi, Aditya Joshi, Brihi Joshi, Nitish Joshi, Rishabh Joshi, Xincheng Ju, Yiming Ju, Zeqian Ju, Jaap Jumelet, Kyomin Jung, Myong Chol Jung, Taehee Jung, Juraj Juraska, David Jurgens, Raquel Justo, Prathyusha Jwalapuram, Preethi Jyothi, Vimal Kumar K, Kishan K C, Besim Kabashi, Srikanth Doss Kadarundalagi Raghuram Doss, Kxxii

zuma Kadowaki, Andrea Kahn, Magdalena Kaiser, Ivana Kajic, Tomoyuki Kajiwara, Mihir Kale, Oren Kalinsky, Laura Kallmeyer, Aikaterini-Lida Kalouli, Katikapalli Subramanyam Kalyan, Abu Raihan Kamal, Ehsan Kamalloo, Nishant Kambhatla, Hidetaka Kamigaito, Jaap Kamps, Hiroshi Kanayama, Kamil Kanclerz, Masahiro Kaneko, Gi-Cheon Kang, Jaewook Kang, Minki Kang, Yoshinobu Kano, Diptesh Kanojia, Pinar Karagoz, Giannis Karamanolakis, Siddharth Karamcheti, Mladen Karan, Akbar Karimi, Younes Karimi, Payam Karisani, B¨orje Karlsson, Shubhra Kanti Karmaker Santu, Sanjeev Kumar Karn, Constantinos Karouzos, Marzena Karpinska, Omid Kashefi, Zdenˇek Kasner, Aly Kassem, Anisia Katinskaia, Yoav Katz, David Kauchak, Pride Kavumba, Noriaki Kawamae, Hideto Kazawa, Ashkan Kazemi, Ghazaleh Kazeminejad, Amirhossein Kazemnejad, Zixuan Ke, Akhil Kedia, Sedrick Scott Keh, Katherine Keith, Amr Keleg, Frank Keller, Casey Kennington, Tom Kenter, Roman Kern, Santosh Kesiraju, Lee Kezar, Shahram Khadivi, Muhammad Khalifa, Salam Khalifa, Anant Khandelwal, Dinesh Khandelwal, Shima Khanehzar, Simran Khanuja, Kyung Seo Ki, Mert Kilickaya, Halil Kilicoglu, Bugeun Kim, Gangwoo Kim, Gene Kim, Geonmin Kim, Gunhee Kim, Gyuhak Kim, Harksoo Kim, Hong Kook Kim, Hyoung-hun Kim, Hyunjae Kim, Hyunwoo Kim, Jaeyoung Kim, Jihyuk Kim, Jongwon Kim, Joo-Kyung Kim, Joshua Y. Kim, Jung-Jae Kim, Kangil Kim, Kyungho Kim, Minsoo Kim, Sungdong Kim, Taeuk Kim, Yekyung Kim, Young Jin Kim, Youngwoo Kim, Yu Jin Kim, Yasutomo Kimura, Milton King, Tracy Holloway King, Svetlana Kiritchenko, Christo Kirov, Denis Kiselev, Hirokazu Kiyomaru, Shun Kiyono, Christopher Klamm, Ayal Klein, Tassilo Klein, Jan-Christoph Klie, Roman Klinger, Julien Kloetzer, Miyoung Ko, Goro Kobayashi, Hayato Kobayashi, Thomas Kober, Elena Kochkina, Jan Kocon, Prashant Kodali, Jordan Kodner, Arne Koehn, Rob Koeling, Svetla Koeva, Jing Yu Koh, Mare Koit, Noriyuki Kojima, Stanley Kok, Daan Kolkman, Anton Kolonin, Kazunori Komatani, Kanako Komiya, Grzegorz Kondrak, Cunliang Kong, Lingkai Kong, Miloslav Konopík, Ioannis Konstas, Selcuk Kopru, Michalis Korakakis, Katerina Korre, Ana Kotarcic, Suraj Kothawade, Fajri Koto, Neema Kotonya, Alexander Kotov, Manolis Koubarakis, Anna Koufakou, Vasiliki Kougia, Punit Singh Koura, Venelin Kovatchev, Ivan Koychev, Michael Kranzlein, Matthias Kraus, Simon Krek, Brigitte Krenn, Amrith Krishna, Kalpesh Krishna, Kundan Krishna, Adit Krishnan, Nikhil Krishnaswamy, Canasai Kruengkrai, Udo Kruschwitz, Anna Kruspe, Da Kuang, Andrei Kucharavy, Ilia Kulikov, Aditya Prakash Kulkarni, Ashish Kulkarni, Atharva Kulkarni, Vivek Kulkarni, Ashutosh Kumar, Puneet Kumar, Ritesh Kumar, Sachin Kumar, Sawan Kumar, Shankar Kumar, Shanu Kumar, Sumeet Kumar, Varun Kumar, Sadhana Kumaravel, Anoop Kunchukuttan, Adhiguna Kuncoro, Tsung-Ting Kuo, Yuri Kuratov, Murathan Kurfalı, Tatsuki Kuribayashi, Mikko Kurimo, Shuhei Kurita, Ugur Kursuncu, Guy Kushilevitz, Mucahid Kutlu, Ilia Kuznetsov, Haewoon Kwak, Sunjun Kweon, Yeonsu Kwon, Moreno La Quatra, Philippe Laban, Sofie Labat, Matthieu Labeau, Yanis Labrak, Faisal Ladhak, Katrien Laenen, Allison Lahnala, Huiyuan Lai, Kenneth Lai, Viet Lai, Yi-An Lai, Yuxuan Lai, Veronika Laippala, Surafel M. Lakew, Kushal Lakhotia, Yash Kumar Lal, Tsz Kin Lam, Wai Lam, Hemank Lamba, Vasileios Lampos, Gerasimos Lampouras, Nur Lan, Yunshi Lan, Lukas Lange, Maurice Langner, Mateusz Lango, Mirella Lapata, Issam Laradji, Samuel Larkin, Mikel Larra˜naga, Stefan Larson, Samuel L¨aubli, Frances Adriana Laureano De Leon, Alberto Lavelli, Alexandra Lavrentovich, Dawn Lawrie, Phong Le, Joseph Le Roux, Kevin Leach, Gianluca Lebani, Lynda Lechani, Andrew Lee, Bruce W. Lee, Deokjae Lee, Dong-Ho Lee, Donghun Lee, Dongkyu Lee, Dongyub Lee, Fei-Tzin Lee, Gibbeum Lee, Grandee Lee, Hung-Yi Lee, Hwaran Lee, I-Ta Lee, Jackson Lee, Jae Hee Lee, Jae Sung Lee, Jay Yoon Lee, Jeong Min Lee, Ji-Ung Lee, Jihwan Lee, Jinhyuk Lee, John Lee, Jongwuk Lee, Jun-Min Lee, Koanho Lee, Kyumin Lee, Lung-Hao Lee, Mina Lee, Minho Lee, Minwoo Lee, Mong Li Lee, Nayeon Lee, Roy Ka-Wei Lee, Sang-Woo Lee, Seolhwa Lee, Wonkee Lee, Yongjae Lee, Yoonjoo Lee, Young-Suk Lee, Younghun Lee, Els Lefever, Jo¨el Legrand, Jens Lemmens, Yves Lepage, Leo Lepp¨anen, Pietro Lesci, Chun Wa Leung, Gregor Leusch, Ran Levy, Sharon Levy, Alexander Hanbo Li, Baoli Li, Bei Li, Belinda Z. Li, Bin Li, Bo Li, Bobo Li, Boyang Li, Changmao Li, Cheng Li, Cheng-Te Li, Chengming Li, Chenliang Li, Chong Li, Dianqi Li, Fangtao Li, Fei Li, Guanlin Li, Haizhou Li, Haochen Li, Haonan Li, Haoqi Li, Haoran

Li, Haoran

Li, Irene

Li, Jiacheng

Li, Jialu

Li, Jiangnan

Li, Jiangtong

Li, Jiaqi

Li, Jiaxuan

Li, Jieyu

Li, Jing

Li, Jinpeng

Li, Jiyi

Li, Juanhui

Li, Juncheng

Li, Junyi

Li, Junyi

Li, Keyi

Li, Lei

Li, Li

Li, Erran

Li, Liangyou

Li, Linjie

Li, Linyang

Li, Liunian Harold

Li, Maoxi

Li, Margaret

Li, Miao

Li, Miaoran

Li, Mingda

Li, Mingjie

Li, Mukai

Li, Peifeng

Li, Peiguang

Li, Peng

Li, Qian

Li, Qintong

Li, Ru

Li, Rui

Li, Ruifan

Li, Ruizhe

Li, Sha

Li, Shaobo

Li, Sheng

Li, Shengjie

Li, Shimin

Li, Shiyang

Li, Shuangyin

Li, Shujun

Li, Shuyang

Li, Si

Li, Siyan

Li, Tao

Li, Tianjian

Li, Wei

Li, Wei

Li, Wenyan

Li, Xia

Li, Xiang

Li, Xiang Lisa

Li, Xiao

Li, Xiaonan

Li, Ximing

Li, Xin

Li, Xintong

Li, Xinxin

Li, Xue

Li, Yafu

Li, Yanran

Li, Yanyang

Li, Yanzeng

Li, Yanzhou

Li, Yaoyiran

Li, Yinghui

Li, Yingjie

Li, Yingya

Li, Yitong

Li, Yiyuan

Li, Yu

Li, Yuan-Fang

Li, Yucheng

Li, Yuliang

Li, Yuncong

Li, Yunji

Li, Zekun

Li, Zhenhao

Li, Zhi

Li, Zhongli

Li, Zongxi

Li, Zuchao

Li, Vladislav Lialin

Li, Yixin Lian

Liang, Bin

Liang, Chao-Chun

Liang, Davis

Liang, Di

Liang, Hongru

Liang, Junjie

Liang, Miya

Liang, Paul Pu

Liang, Ping

Liang, Sheng

Liang, Yaobo

Liang, Zheng-zhong

Liang, Zhenwen

Liang, Zhicheng

Liao, Baohao

Liao, Lizi

Liao, Peiyuan

Liao, Siyu

Libovický, Jindˇ

Liesaputra, Veronica

Likhobaba, Daniil

Lim, Gilbert

Lim, Heuiseok

Lim, Jungwoo

Lim, Kwan Hui

Lim, Tomasz

Limisiewicz, Nut

Limsopatham, Bingqian

Lin, Bo

Lin, Chuan-Jie

Lin, Hongyu

Lin, Huan

Lin, Kevin

Lin, King Ip

Lin, Li

Lin, Lucy

Lin, Nankai

Lin, Peiqin

Lin, Qika

Lin, Sheng-Chieh

Lin, Ting-En

Lin, Victoria

Lin, Wei

Lin, Weizhe

Lin, Xiang

Lin, Xinshi

Lin, Yankai

Lin, Ying-Jia

Lin, Yu-Hsiang

Lin, Zeqi

Lin, Zhaojiang

Lin, Zhenxi

Lin, Zhouhan

Lin, Zi

Lindemann, Matthias

Ling, Jeffrey

Ling, Zhenhua

Linzen, Tal

Lippi, Marco

Lison, Pierre

Litman, Diane

Litschko, Robert

Litvak, Marina

Liu, Alisa

Liu, Ao

Liu, Bing

Liu, Boyang

Liu, Chen

Liu, Chi-Liang

Liu, Dayiheng

Liu, Dexi

Liu, Emmy

Liu, Fangyu

Liu, Fenglin

Liu, Guangliang

Liu, Guisheng

Liu, Han

Liu, Haokun

Liu, Hui

Liu, Hui

Liu, Jiacheng

Liu, Jiangming

Liu, Jiawei

Liu, Jiduan

Liu, Jie-Jyun

Liu, Jinglin

Liu, Jingzhou

Liu, Junhao

Liu, Lei

Liu, Linlin

Liu, Linqing

Liu, Luyang

Liu, Ming

Liu, Minqian

Liu, Nayu

Liu, Nelson F.

Liu, Peng

Liu, Qian

Liu, Qian

Liu, Qianying

Liu, Shuaiqi

Liu, Siyang

Liu, Song

Liu, Tianyuan

Liu, Wenqiang

Liu, Xianggen

Liu, Xiangyang

Liu, Xiao

Liu, Xiao

Liu, Xiaoyuan

Liu, Xingxian

Liu, Xuebo

Liu, Xuye

Liu, Yang

Liu, Yang Janet

Liu, Ye

Liu, Ye

Liu, Yijia

Liu, Yiren

Liu, Yixin

Liu, Yizhu

Liu, Yong

Liu, Yongbin

Liu, Yongfei

Liu, Yonghao

Liu, Yongkang

Liu, Yuanxin

Liu, Zechun

Liu, Zeming

Liu, Zequn

Liu, Zeyu

Liu, Zhe

Liu, Zhenghao

Liu, Zhengyuan

Liu, Zhengzhong

Liu, Zhijian

Liu, Zihan

Liu, Zitao

Liu, Zuozhu

Ljubeˇ si´ c, Nikola

Lo, Kuan-Chieh

Lo, Kyle

Logan Iv, Robert L

Logeswaran, Lajanugen

Kashyap, Abhay Lokesh

Lolive, Damien

Long, Guodong

Long, Shangbang

Long, Yunfei

Lopes, Lucelene

Lopes, Marcos

Lopes Cardoso, Henrique

Lopez De Lacalle, Oier

Lopez Monroy, Adrian Pastor

Lorge, Isabelle

Lou, Chao

Lou, Jian-Guang

Lou, Renze

Louka-chevitch, Natalia

Loukina, Anastassia

Loureiro, Daniel

Lourie, Nicholas

Loyola, Pablo

Lu, Di

Lu, Hongyuan

Lu, Jianqiao

Lu, Jinghui

Lu, Jinliang

Lu, Junru

Lu, Pan

Lu, Peng

Lu, Weiming

Lu, Wenpeng

Lu, Xiaolei

Lu, Yao

Lu, Yaojie

Lu, Yu

Lu, Yu

Lu, Yujie

Lubis, Nurul

Lukin, Stephanie M.

Luu, Gunnar

Luo, Cheng

Luo, Haoran

Luo, Haozheng

Luo, Hongyin

Luo, Jiaming

Luo, Jiebo

Luo, Junyu

Luo, Ling

Luo, Man

Luo, Renqian

Luo, Ruipu

Luo, Wencan

Luo, Zhunchen

Luo, Ziyang

Luu, Kelvin

Lv, Qi

Lyu, Chenyang

Lyu, Weimin

Lyu, Yajuan

Lyu, Yougang

M’hamdi, Meryem

Ma, Chenkai

Ma, Chunpeng

Ma, Congbo

Ma, Danni

Ma, Huifang

Ma, Kaixin

Ma, Longxuan

Ma, Mingyu Derek

Ma, Qianli

Ma, Ruotian

Ma, Tengfei

Ma, Wei-Yun

Ma, Weizhi

Ma, Xinyin

Ma, Yubo

Ma, Yukun

Ma, Zhanyu

Ma, Ziqiao

Macháˇ cek, Dominik

Macherey, Wolfgang

Macina, Jakub

Madaan, Aman

Madasu, Avinash

Maddela, Mounica

Madureira, Brielen

Mager, Manuel

Magnini, Bernardo

Mahendra, Rahmad

Maheshwari, Ayush

Mahowald, Kyle

Maier, Wolfgang

Maillard, Jean

Majumder, Bodhisattwa Prasad

Majumder, Navonil

Mak-rai, Márton

Malakasiotis, Prodromos

Mali, Ankur

Malkiel, Itzik

Malko, Anton

Malykh, Valentin

Mamou, Jonathan

Mandal, Arpan

Maneriker, Pranav

Manning, Emma

Manotas, Irene

Mansimov, Elman

Mansour, Saab

Manuvinakurike, Ramesh

Manzoor, Emaad

Mao, Jiaxin

Mao, Kelong

Mao, Rui

Mao, Wenji

Mao, Yuning

Mao, Zhendong

Mao, Zhiming

Mao, Zhuoyuan

Mardziel, Piotr

Margatina, Katerina

Marin, Alex

Marras, Mirko

Marrese-Taylor, Edison

Marro, Santiago

Martelli, Federico

Martínez Cámara, Eugenio

Martínez Garcia, Eva

Martínez Lorenzo, Abelardo Carlos

Fernando Martínez-Plumed, Juan Martinez-Romo, Bruno Martins, Pedro Henrique Martins, David Martins De Matos, Luisa M¨ arz, Laura Mascarell, Lambert Mathias, Sandeep Mathias, Sergio Matos, Yuichiroh Matsubayashi, Yuji Matsumoto, Takuya Matsuzaki, Evgeny Matusov, Borislav Mavrin, Jonathan May, Tobias Mayer, Joshua Maynez, Amir Mazaheri, Sahisnu Mazumder, Alessandro Mazzei, R. Thomas McCoy, Nick Mckenna, Paul Mcnamee, Quentin Meeus, Alexander Mehler, Ninareh Mehrabi, Nikhil Mehta, Sanket Vaibhav Mehta, Clara Meister, Dheeraj Mekala, Julia Mendelsohn, Erick Mendez Guzman, Arul Menezes, Telmo Menezes, Chuan Meng, Rui Meng, Yu Meng, Yuanliang Meng, Zhao Meng, Samuel Mensah, William Merrill, Mohsen Mesgar, Kourosh Meshgi, Eleni Metheniti, Lars Meyer, Adam Meyers, Ivan Vladimir Meza Ruiz, Yisong Miao, Alessio Miaschi, Antonio Valerio Miceli Barone, Timothee Mickus, Lesly Miculicich, Margot Mieskes, Todor Mihaylov, Nandana Mihindukulasooriya, Simon Mille, Timothy Miller, Hye-Jin Min, Koji Mineshima, Gosse Minnema, Andrei Mircea, Seyedabolghasem Mirroshandel, Paramita Mirza, Maryam Sadat Mirzaei, Abhijit Mishra, Pushkar Mishra, Shubhanshu Mishra, Siddhartha Mishra, Kanishka Misra, Masato Mita, Mitch Mithun, Ashish Mittal, Sarthak Mittal, Vibhu Mittal, Yasuhide Miura, Tong Mo, Yijun Mo, Daichi Mochihashi, Daniela Moctezuma, Ali Modarressi, Sandip Modha, Hans Moen, Aditya Mogadala, Nikita Moghe, Hosein Mohebbi, Behrang Mohit, Mrinal Mohit, Afroz Mohiuddin, Tasnim Mohiuddin, Michael Mohler, Luis Mojica De La Vega, Negar Mokhberian, Diego Molla, Nicholas Monath, Sneha Mondal, Helena Moniz, Ali Montazeralghaem, Manuel Montes, Johanna Monti, Hyeonseok Moon, Jihyung Moon, Lori Moon, Raymond Mooney, Jared Moore, Richard Moot, Mehrad Moradshahi, Goncalo Mordido, Erwan Moreau, Antonio Moreno-Ortiz, Antonio Moreno-Sandoval, Mathieu Morey, Yusuke Mori, Véronique Moriceau, Emmanuel Morin, Gaku Morio, Makoto Morishita, John Morris, Marius Mosbach, Larry Moss, Xiangyang Mou, Maximilian Mozes, Frank Mtumbuka, Jesse Mu, Aaron Mueller, David Mueller, Aldrian Obaja Muis, Shashank Mujumdar, Animesh Mukherjee, Rajdeep Mukherjee, Matthew Mulholland, Benjamin Muller, Mathias M¨ uller, Philippe Muller, Max M¨ uller-Eberstein, Emir Munoz, Rafael Mu˜ noz Guillena, Saliha Muradoglu, Koji Murakami, Deepak Muralidharan, Yugo Murawaki, Kenton Murray, Rudra Murthy, Shikhar Murty, Karthik Murugadoss, Skatje Myers, Agnieszka Mykowiecka, Sheshera Mysore, Anandhavelu N, Seung-Hoon Na, Nona Naderi, Seema Nagar, Masaaki Nagata, Aakanksha Naik, Saeed Najafi, Tetsuji Nakagawa, Yukiko Nakano, Yuta Nakashima, Hideki Nakayama, Christoforos Nalmpantis, Sung-jin Nam, Marcin Namysl, Subhrangshu Nandi, Abhilash Nandy, Tarek Naous, Diane Napolitano, Jason Naradowsky, Sharan Narasimhan, Tahira Naseem, Sudip Naskar, Alexis Nasr, Vivi Nastase, Borja Navarro-Colorado, Tapas Nayak, Mojtaba Nayyeri, Claire Nedellec, Carina Negreanu, Preksha Nema, Joshua Nemecek, Graham Neubig, Guenter Neumann, Aurélie Névéol, Mariana Neves, Hwee Tou Ng, Axel-Cyrille Ngonga Ngomo, Cam Tu Nguyen, Dang Tuan Nguyen, Dat Quoc Nguyen, Dong Nguyen, Duc-Vu Nguyen, Huy Nguyen, Huyen Nguyen, Kiet Nguyen, Nhung Nguyen, Thanh Nguyen, Thanh-Tung Nguyen, Trang Nguyen, Truc-Vien T. Nguyen, Trung Hieu Nguyen, Vincent Nguyen, Hoang-Quoc Nguyen-Son, Ansong Ni, Jianmo Ni, Jingwei Ni, Minheng Ni, Zhaoheng Ni, Eric Nichols, Garrett Nicolai, Massimo Nicosia, Feng Nie, Ping Nie, Shaoliang Nie, Zhijie Nie, Sofia Nikiforova, Dmitry Nikolaev, Nikola I. Nikolov, Vassilina Nikoulina, Iftitahu Nimah, Lasguido Nio, Noriki Nishida, Masaaki Nishino, Sergiu Nisioi, Malvina Nissim, Tong Niu, Xing Niu, Yulei Niu, Zheng-Yu Niu, Bill Noble, Mariana Noguti, Tadashi Nomoto, Armineh Nourbakhsh, Jekaterina Novikova, Pierre Nugues, Diarmuid Ó Séaghdha, Alexander O’connor, Brendan O’connor, Tim O’gorman, Stephen Obadinma, Jose Ochoa-Luna, Kemal Oflazer, Maciej Ogrodniczuk, Kelechi Ogueji, Tolulope Ogunremi, Alice Oh, Shin Ah Oh, Mayumi Ohta, Kiyonori Ohtake, Atul Kr. Ojha, Oleg Okun, Eda Okur, Amy Olex, Anais Ollagnier, Ali Omrani, Byung-Won On, Donovan Ong, Ethel Ong, Yasumasa Onoe, Juri Opitz, Abigail Oppong, Matan Orbach, Hadas Orgad, Riccardo Orlando, John E. Ortega, Pedro Ortiz Suarez, Yohei Oseki, Naoki Otani, Zhijian Ou, Hiroki Ouchi, Nedjma Ousidhoum, Nedjma Ousidhoum, Jessica Ouyang, Iris Oved, Lilja Øvrelid, Kehinde Owoeye, Deepak P, Trilok Padhi, Ankur Padia, Vishakh Padmakumar, Gustavo Paetzold, Artidoro Pagnoni, Vardaan Pahuja, Santanu Pal, Vaishali

Pal, Shriphani Palakodety, Chester Palen-Michel, Alexis Palmer, Alessio Palmero Aprosio, Shramay Palta, Junshu Pan, Xiang Pan, Xiaoman Pan, Yi-Cheng Pan, Youcheng Pan, Yu Pan, Yudai Pan, Artemis Panagopoulou, Alexander Panchenko, Mugdha Pandya, Liang Pang, Sheena Panthaplackel, Alessandro Panunzi, Isabel Papadimitriou, Pinelopi Papalampidi, Alexandros Papangelis, Nikos Papasarantopoulos, Paolo Papotti, Nikolaos Pappas, Emerson Paraiso, Bhargavi Paranjape, Letitia Parcalabescu, Antonio Pareja-Lora, Tanmay Parekh, Shantipriya Parida, Pierre-Henri Paris, Chaehun Park, Chan Young Park, Jun-Hyung Park, Jungsoo Park, Kunwoo Park, Seong-Bae Park, Seongmin Park, Seongsik Park, Shinwoo Park, Sunghyun Park, Sungjoon Park, Yannick Parmentier, Patrick Paroubek, Ankita Pasad, Lucia Passaro, Rebecca Passonneau, Ramakanth Pasunuru, Arkil Patel, Raj Patel, Roma Patel, Sapan Patel, Braja Gopal Patra, Jasabanta Patro, Parth Patwa, Manasi Patwardhan, Siddharth Patwardhan, Debjit Paul, Indraneil Paul, Shounak Paul, Adam Pauls, Nikita Pavlichenko, Ellie Pavlick, John Pavlopoulos, Siddhesh Pawar, Justin Payan, Pavel Pecina, Jiahuan Pei, Jiaxin Pei, Weiping Pei, Hao Peng, Hao Peng, Qianqian Peng, Qiwei Peng, Siyao Peng, Tao Peng, Wei Peng, Wei Peng, Wenjun Peng, Xutan Peng, Yifan Peng, Gerald Penn, Oren Pereg, Ethan Perez, Juan Antonio Perez-Ortiz, Gabriele Pergola, Charith Peris, Stanislav Peshterliev, Denis Peskoff, Ben Peters, Slav Petrov, Miriam R. L. Petruck, Pavel Petrushkov, Maxime Peyrard, Sandro Pezzelle, Jonas Pfeiffer, Quang Nhat Minh Pham, Thang Pham, Jason Phang, Maciej Piasecki, Massimo Piccardi, Matúš Pikuliak, Nisha Pillai, Tiago Pimentel, Juan Pino, Leticia Pinto-Alva, Irina Piontkovskaya, Telmo Pires, Flammie Pirinen, Jakub Piskorski, Lidia Pivovarova, Daniel Platt, Laura Plaza, Flor Miriam Plaza-Del-Arco, Lahari Poddar, Massimo Poesio, Thierry Poibeau, Lucie Polakova, Marco Polignano, Senja Pollak, Maria Pontiki, Simone Paolo Ponzetto, Andrei Popescu-Belis, Maja Popović, Beatrice Portelli, Rafał Poświatek, Martin Potthast, Christopher Potts, Amir Pouran Ben Veyseh, Rohit Prabhavalkar, Shrimai Prabhumoye, Aniket Pramanick, Soumajit Pramanik, Animesh Prasad, Radityo Eko Prasojo, Adithya Pratapa, Pavel Příbáň, Prokopis Prokopidis, Piotr Przybyła, Michal Ptaszynski, Dongqi Pu, Ratish Surendran Puduppully, Rajkumar Pujari, Stephen Pulman, Hemant Purohit, Alberto Purpura, Matthew Purver, James Pustejovsky, Valentina Pyatkin, Ehsan Qasemi, Fanchao Qi, Ji Qi, Jianzhong Qi, Jingyuan Qi, Shuhan Qi, Siya Qi, Wang Qi, Weizhen Qi, Chen Qian, Hongjin Qian, Jing Qian, Yujie Qian, Zhong Qian, Yaqiong Qiao, Bosheng Qin, Bowen Qin, Chuan Qin, Jinghui Qin, Kechen Qin, Libo Qin, Yujia Qin, Jielin Qiu, Liang Qiu, Long Qiu, Xinying Qiu, Zhaopeng Qiu, Zimeng Qiu, Chen Qu, Tingyu Qu, Rakesh R. Menon, Ella Rabinovich, Alexandre Rademaker, Daniele Radicioni, Alessandro Raganato, Preethi Raghavan, Dinesh Raghu, Afshin Rahimi, Sunny Rai, Vyas Raina, Nishant Raj, Navid Rajabi, Hossein Rajaby Faghihi, Dheeraj Rajagopal, Kanagasabai Rajaraman, Taraka Rama, Heri Ramampiaro, Naveen Raman, Giulia Rambelli, Owen Rambow, Abhinav Ramesh Kashyap, Sahana Ramnath, Rita Ramos, Alan Ramponi, Tharindu Ranasinghe, Surangika Ranathunga, Priya Rani, Yanghui Rao, Okko Rasanen, Mohammad Sadegh Rasooli, Fedor Ratnikov, Vikas Raunak, Andrea Amelio Ravelli, Shauli Ravfogel, Manikandan Ravikiran, Srinivas Ravishankar, Bhanu Pratap Singh Rawat, Vipula Rawte, Soumya Ray, Jishnu Ray Chowdhury, Manny Rayner, Anastasiia Razdaibiedina, Yasaman Razeghi, Evgeniia Razumovskaia, Livy Real, Traian Rebedea, Gabor Recski, Hanumant Redkar, Michael Regan, Ines Rehbein, Georg Rehm, Machel Reid, Markus Reiter-Haas, Navid Rekabsaz, Da Ren, Feiliang Ren, Haopeng Ren, Liliang Ren, Pengjie Ren, Ruiyang Ren, Shuhuai Ren, Steven Rennie, Christian Retoré, Kiamehr Rezaee, Mehdi Rezagholizadeh, Ryokan Ri, Eugénio Ribeiro, Leonardo F. R. Ribeiro, Giuseppe Riccardi, Kyle Richardson, Caitlin Richter, Martin Riedl, Stefan Riezler, Davide Rigoni, Mattia Rigotti, Shruti Rijhwani, Matı̄ss Rikters, Fabio Rinaldi, Ruty Rinott, Annette Rios, Anthony Rios, Elijah Rippeth, Andrey Risukhin, Yara Rizk, Brian Roark, Alvaro Rodrigo, Melissa Roemmele, Morteza Rohanian, Mukesh Kumar Rohil, Mahdin Rohmatillah, Paul Roit, Lina M. Rojas Barahona, Roland Roller, Julia Romberg, Salvatore Romeo, Julien Romero, Srikanth Ronanki, Md Rashad Al Hasan Rony, Tanya Roosta, Rudolf Rosa, Domenic Rosati, Guy Rosin, Alexis Ross, Robert Ross, Sophie Rosset, Paolo Rosso, Guy Rotman, Hossein Rouhizadeh, Dmitri Roussinov, Rachel Edita Roxas, Aurko Roy, Shamik Roy, Soumyadeep Roy, Sumegh Roychowdhury, Jos Rozen, Antoine Roze

nknop, Yu-Ping Ruan, Susanna R¨ ucker, Koustav Rudra, Amina Rufai, Federico Ruggeri, Ramon Ruiz-Dolz, Mukund Rungta, Josef Ruppenhofer, Benjamin Ruppik, Thomas Ruprecht, Alexander Rush, Irene Russo, Piotr Rybak, Maciej Rybinski, Maria Ryskina, Hadeel Saadany, Arkadiy Saakyan, Caroline Sabty, Devendra Sachan, Fatiha Sadat, Farig Sadeque, Arka Sadhu, Philipp Sadler, Sahar Sadrizadeh, Mehrnoosh Sadrzadeh, Niloofar Safi Samghabadi, Sylvie Saget, Alsu Sagirova, Amrita Saha, Punyajoy Saha, Sougata Saha, Swarnadeep Saha, Tanay Kumar Saha, Tulika Saha, Saurav Sahay, G¨ ozde S ¸ ahin, Nihar Sahoo, Sovan Kumar Sahoo, Sunil Kumar Sahu, Surya Kant Sahu, Ananya Sai B, Oscar Sainz, Tarek Sakakini, Sakriani Sakti, Ander Salaberria, Julian Salazar, Elizabeth Salesky, Jonne Saleva, Avneesh Saluja, Tanja Samardˇ zi´ c, Rajhans Samdani, Younes Samih, I˜ naki San Vicente, Abhilasha Sancheti, Vicente Ivan Sanchez Carmona, Danae Sánchez Villegas, Víctor M. Sánchez-Cartagena, German Sanchis-Trilles, Mario S¨ anger, Ananth Sankar, Chinnadhurai Sankar, Scott Sanner, Sashank Santhanam, Andrea Santilli, Diana Santos, Rodrigo Santos, Bishal Santra, Sebastin Santy, Soumya Sanyal, Maarten Sap, Naomi Saphra, Ruhi Sarikaya, Efsun Sarioglu Kayi, Anoop Sarkar, Kamal Sarkar, Ritesh Sarkhel, Prathusha K Sarma, Prof. Shikhar Kumar Sarma, Gabriele Sarti, Kengatharaiyer Sarveswaran, Sheikh Sarwar, Felix Sasaki, Minoru Sasaki, Shota Sasaki, Ryohei Sasano, Giorgio Satta, Danielle Saunders, Ketki Savle, Guergana Savova, Apoorv Saxena, Michael Saxon, Asad Sayeed, Shigehiko Schamoni, Wout Schellaert, Frank Schilder, David Schlangen, Viktor Schlegel, Michael Sejr Schlichtkrull, J¨ org Schl¨ otterer, Helmut Schmid, Robin Schmidt, Patricia Schmidtova, Martin Schmitt, Tyler Schnoebelen, Stephanie Schoch, Annika Marie Schoene, Mirco Schoenfeld, Lenhart Schubert, Hendrik Schuff, William Schuler, Sabine Schulte Im Walde, Claudia Schulz, Hannes Schulz, Elliot Schumacher, Raphael Schumann, Sebastian Schuster, Ineke Schuurman, Jackson Scott, Kyle Seelman, Ethan Selfridge, Thibault Sellam, David Semedo, Nasredine Semmar, Cansu Sen, Srinivasan Sengamedu Hanumantha Rao, Ayan Sengupta, Shubhashis Sengupta, Rico Sennrich, Jaehyung Seo, Ronald Seoh, Yeon Seonwoo, Royal Sequiera, Sofia Serrano, Mahsa Shafaei, Stephen Shaffran, Simra Shahid, Omar Shaikh, Igor Shalyminov, Chao Shang, Mingyue Shang, Chenze Shao, Wei Shao, Yijia Shao, Yutong Shao, Ori Shapira, Aditya Sharma, Ashish Sharma, Piyush Sharma, Ser- ge Sharoff, Tatiana Shavrina, Shuaijie She, Artem Shelmanov, Aili Shen, Hua Shen, Jiaming Shen, Jianhao Shen, Sheng Shen, Shiqi Shen, Siqi Shen, Tianhao Shen, Xudong Shen, Yatian Shen, Ying Shen, Yongliang Shen, Yuming Shen, Zejiang Shen, Zhengyuan Shen, Emily Sheng, Qiang Sheng, Ashish Shenoy, Tom Sherborne, Botian Shi, Bowen Shi, Chen Shi, Jihao Shi, Kaize Shi, Ning Shi, Peng Shi, Tian Shi, Tianze Shi, Weijia Shi, Xiao Shi, Yangyang Shi, Zhan Shi, Zhouxing Shi, Tomohide Shibata, Hidetoshi Shimodaira, Jamin Shin, Seungjae Shin, Kazutoshi Shinoda, Takahiro Shinozaki, Keiji Shinzato, Prashant Shiralkar, Yow-Ting Shiue, Harry Shomer, Ziyi Shou, Mohit Shridhar, Ritvik Shrivastava, Dimitar Shterionov, Kai Shu, Raphael Shu, Kai Shuang, Zeren Shui, Alexander Shvets, Chenglei Si, Suzanna Sia, Anthony Sicilia, A.b. Siddique, Melanie Siegel, Ingo Siegert, Alejandro Sierra-Múnera, Ankur Sikarwar, Sandipan Sikdar, Andrew Silva, João Ricardo Silva, Danilo Silva De Carvalho, Fabrizio Silvestri, Stefano Silvestri, Robert Sim, Michel Simard, Patrick Simianer, Dharani Simma, Dan Simonson, Edwin Simpson, Jyotika Singh, Mayank Singh, Pranaydeep Singh, Thoudam Doren Singh, Sneha Singhania, Priyanka Sinha, Olivier Siohan, Amy Siu, Inguna Skadina, Gabriel Skantze, Victor Skobov, Aviv Slobodkin, Alisa Smirnova, David Smith, Noah A. Smith, Vésteinn Snæbjarnarson, Felipe Soares, Marco Antonio Sobrevilla Cabe- zudo, Artem Sokolov, Luca Soldaini, Amir Soleimani, Ilia Sominsky, Pia Sommerauer, Junyoung Son, Seonil (simon) Son, Youngseo Son, Haiyue Song, Haoyu Song, Hyeonho Song, Hyun-Je Song, Kai Song, Kaiqiang Song, Kaitao Song, Linfeng Song, Ran Song, Wei Song, Xiaohui Song, Yan Song, Yangqiu Song, Yifan Song, Zhenqiao Song, Sarvesh Soni, Shashank Sonkar, Taylor Sorensen, Ionut-Teodor Sorodoc, Alexey Sorokin, Daniil Sorokin, Anna Sotnikova, Xabier Soto, Sajad Sotudeh, Gerasimos Spanakis, Manuela Speranza, Andreas Spitz, Richard Sproat, Rachele Sprugnoli, Makesh Narsimhan Sreedhar, Mukund Srinath, Kavya Srinet, Balaji Vasan Srinivasan, Tejas Srinivasan, Vijay Srinivasan, Ankit Srivastava, Saurabh Srivastava, Efstathios Stamatatos, Dominik Stammbach, Karolina Stanczak, Marija Stanojevic, Gabriel Stanovsky, Katherine Sta- xxvii

saski, Manfred Stede, Julius Steen, Michal ˇ Stefánik, Shane Steinert-Threlkeld, Georg Stemmer, Evgeny Stepanov, Zachary Stine, Regina Stodden, Niklas Stoehr, Alessandro Stolfo, Matthew Sto-ne, Shane Storks, Kevin Stowe, Marco Antonio Stranisci, Karl Stratos, Kristina Striegnitz, Phillip Str¨ obel, David Strohmaier, Jannik Str¨ otgen, Tomek Strzalkowski, Sara Stymne, Dan Su, Hsuan Su, Qi Su, Qinliang Su, Ruolin Su, Xin Su, Ying Su, Yixuan Su, Yusheng Su, Nishant Subramani, Katsuhito Sudoh, Saku Sugawara, Hiroaki Sugiyama, Kazunari Sugiyama, Yoshi Suhara, Zhifang Sui, Octavia S ¸ ulea, Elior Sulem, Md Arafat Sultan, Aixin Sun, Changzhi Sun, Chengjie Sun, Chenkai Sun, Guangzhi Sun, Haipeng Sun, Hao Sun, Hao Sun, Haohai Sun, Jian Sun, Jiao Sun, Ming Sun, Mingwei Sun, Qingfeng Sun, Renliang Sun, Shichao Sun, Simeng Sun, Tianxiang Sun, Weiwei Sun, Zewei Sun, Zhaoyue Sun, Zhiqing Sun, Dhanasekar Sundararaman, Mujeen Sung, Yi-Lin Sung, Yoo Yeon Sung, Hanna Suominen, Marek Suppa, Benjamin Suter, Mirac Suzgun, Sandesh Swamy, Stan Szpakowicz, Piotr Szyma´ nski, Ana¨ ıs Tack, Oyvind Tafjord, Shabnam Tafreshi, Dima Taji, Sho Takase, Ece Takmaz, George Tambouratzis, Aleˇ s Tamchyna, Anirudd-ha Tammewar, Akihiro Tamura, Chao-Hong Tan, Haochen Tan, Hongye Tan, Samson Tan, Wei Tan, Xiao Tan, Ryota Tanaka, Karan Taneja, Buzhou Tang, Chengguang Tang, Gongbo Tang, Hao Tang, Jialong Tang, Raphael Tang, Shuai Tang, Tianyi Tang, Wei Tang, Xiangyun Tang, Xuemei Tang, Xunzhu Tang, Yun Tang, Yun Tang, Zheng Tang, Simon Tannert, Chaofan Tao, Wei Tao, Allahsera Auguste Tapo, Shiva Taslimipoor, Sandeep Tata, Michiaki Tatsubori, Marta Tatu, Simone Tedeschi, Selma Tekir, Serra Sinem Tekiro˘ glu, Zhiyang Teng, Ian Tenney, Alberto Testoni, Joel Tetreault, Martin Teuffenbach, Kapil Thadani, Katherine Thai, Urmish Thakker, Surendrabikram Thapa, Avijit Thawani, Anton Thielmann, Krishnaprasad Thirunarayan, Brian Thompson, Jana Thompson, Craig Thomson, Sam Thomson, David Thulke, Chang Tian, Yuanhe Tian, Zhiliang Tian, Zuoyu Tian, J¨ org Tiedemann, Christoph Tillmann, Tiago Timponi Torrent, Prayag Tiwari, Amalia Todirascu, Nadi Tomeh, Nicholas Tomlin, Antonio Toral, Cagri Toraman, Manabu Torii, Kentaro Torisawa, Juan-Manuel Torres-Moreno, Lucas Torroba Hennigen, Shubham Toshniwal, Samia Touileb, Yannick Toussaint, Benjamin Towle, Amine Trabelsi, Khanh Tran, Trang Tran, Marcos Treviso, Jan Trienes, Bayu Distiawan Trisedya, Harsh Trivedi, Enrica Troiano, Chen-Tse Tsai, Adam Tsakalidis, Bo-Hsiang Tseng, Ioannis Tsiamas, Masaaki Tsuchida, Oren Tsur, Satoshi Tsutsui, Jingxuan Tu, Kewei Tu, Lifu Tu, Yunbin Tu, Yi-Lin Tuan, Marco Turchi, Ferhan Ture, Elena Tutubalina, Rutuja Ubale, Ana Sabina Uban, Adrian Ulges, Eddie Ungless, Bhargav Upadhyay, Kartikeya Upasani, Olga Uryupina, Asahi Ushio, Dmitry Ustalov, Ahmet ¨ Ust¨ un, Masao Utiyama, Venktesh V, Saujas Vaduguru, Ashwini Vaidya, Marco Valentino, Gisela Vallejo, Jannis Vamvas, Tim Van De Cruys, Antal Van Den Bosch, Rob Van Der Goot, Daan Van Esch, Josef Van Genabith, Emiel Van Miltenburg, Rik Van Noord, Vincent Vandeghinste, Keith Vanderlinden, David Vandyke, Natalia Vanetik, Eva Vanmassenhove, Daniel Varab, Francielle Vargas, Siddharth Varia, Neeraj Varshney, Rossella Varvara, Siddharth Vashishtha, Jake Vasilakes, Eva Maria Vecchi, Nikhita Vedula, Aswathy Velutharambath, Giulia Venturi, Gaurav Verma, Rakesh Verma, Yannick Versley, Anvesh Rao Vijjini, David Vilares, Jesús Vilares, Manuel Vilares Ferro, Martina Vilas, Veronika Vincze, Lucas Vinh Tran, Sami Virpioja, Juraj Vladika, Nikolai Vogler, Rob Voigt, Pius Von D¨ aniken, Spencer Von Der Ohe, Nikos Voskarides, Ali Vosoughi, Pavlos Vougiouklis, Thuy Vu, Thuy-Trang Vu, Yogarshi Vyas, Akifumi Wachi, Takashi Wada, Joachim Wagner, Jan Philip Wahle, Hiromi Wakaki, David Wan, Stephen Wan, Xingchen Wan, Yao Wan, Yu Wan, Ante Wang, Bailin Wang, Bang Wang, Baoxin Wang, Baoxun Wang, Beilun Wang, Benyou Wang, Bin Wang, Bin Wang, Bingqing Wang, Bingyu Wang, Bo Wang, Bo Wang, Boxin Wang, Chao Wang, Chengyi Wang, Chengyu Wang, Chuan-Ju Wang, Chunliu Wang, Cunxiang Wang, Dingquan Wang, Fei Wang, Guangrun Wang, Guoyin Wang, Hai Wang, Han Wang, Han Wang, Han Wang, Hanrui Wang, Hao Wang, Haobo Wang, Haoyu Wang, Haoyu Wang, Haoyu Wang, Heyuan Wang, Hong Wang, Hongfei Wang, Hsin-Min Wang, Huimin Wang, Jiaan Wang, Jian Wang, Jianing Wang, Jianyu Wang, Jianzong Wang, Jiayi Wang, Jie Wang, Jin Wang, Jin Wang, Jinpeng Wang, Jue Wang, Jun Wang, Lei Wang, Liang Wang, Lidan Wang, Lingzhi Wang, Longshaokan Wang, Longyue Wang, Meiqi Wang, Peifeng Wang, Pidong Wang, Ping Wang, Qiang Wang, Qingyun Wang, Qiqi

# Wang

Rui Wang, Runze Wang, Ryan Wang, Shufan Wang, Shuhe Wang, Shuo Wang, Sijia Wang, Sirui Wang, Tao Wang, Tianduo Wang, Tianlu Wang, Wei Wang, Wen Wang, Wenping Wang, Wenxuan Wang, Wenya Wang, Xiangdong Wang, Xiao Wang, Xiaojie Wang, Xiaolin Wang, Xiaozhi Wang, Xin Wang, Xindi Wang, Xing Wang, Xingjin Wang, Xintong Wang, Xinyi Wang, Xinyu Wang, Xuewei Wang, Xun Wang, Yan Wang, Yanlin Wang, Yanshan Wang, Ye Wang, Ye Wang, Yibo Wang, Yifan Wang, Yigong Wang, Yihan Wang, Yiwei Wang, Yizhong Wang, Yue Wang, Yun Cheng Wang, Yuxuan Wang, Zekun Wang, Zhaowei Wang, Zhen Wang, Zheng Wang, Zheng Wang, Zhenhailong Wang, Zhenyi Wang, Zhichun Wang, Zhiguang Wang, Zhilin Wang, Zhiqiang Wang, Zhiwei Wang, Zhuoer Wang, Zhuoyi Wang, Zifeng Wang, Zihan Wang, Zihan Wang, Zihao Wang, Zijian Wang, Zijie Wang, Zilong Wang, Zirui Wang, Prashan Wanigasekara, Leo Wanner, Alex Warstadt, Cedric Waterschoot, Julia Watson, Bonnie Webber, Leon Weber, Albert Webson, Kellie Webster, Tharindu Cyril Weerasooriya, Chengkun Wei, Jerry Wei, Lingwei Wei, Penghui Wei, Tianxin Wei, Xiangpeng Wei, Xiaochi Wei, Shira Wein, Nathaniel Weir, Henry Weld, Orion Weller, Marion Weller-Di Marco, Simon Wells, Bingyang Wen, Haoyang Wen, Jiaxin Wen, Liang Wen, Lijie Wen, Rongxiang Weng, Lukas Wertz, Peter West, Matthijs Westera, Jennifer C. White, Richard Wicentowski, Michael Wiegand, Ethan Wilcox, Rodrigo Wilkens, Bram Willemsen, Ronald Wilson, Shomir Wilson, Steven Wilson, Grégoire Winterstein, Shuly Wintner, Sam Wiseman, Guillaume Wisniewski, Emilia Wisnios, Tomer Wolfson, Marcin Woliński, Diedrich Wolter, Derek F. Wong, Ka Ho Wong, Raymond Wong, Tak-Lam Wong, Alina Wróblewska, Anna Wroblewska, Anne Wu, Bowen Wu, Changxing Wu, Chen Wu, Chen Henry Wu, Chien-Sheng Wu, Chuhan Wu, Di Wu, Di Wu, Fangzhao Wu, Hua Wu, Hui Wu, Junda Wu, Ledell Wu, Lianwei Wu, Linzhi Wu, Shengqiong Wu, Shih-Hung Wu, Sixing Wu, Stephen Wu, Te-Lin Wu, Tianxing Wu, Ting-Wei Wu, Weibin Wu, Wenhao Wu, Winston Wu, Xian Wu, Xianchao Wu, Xin Wu, Xixin Wu, Yang Wu, Yangjun Wu, Yaoyao Wu, Yike Wu, Yimeng Wu, Youzheng Wu, Yu Wu, Yuanbin Wu, Yuexin Wu, Yunfang Wu, Yuting Wu, Yuxiang Wu, Zeqiu Wu, Zhaofeng Wu, Zhen Wu, Zhijing Wu, Zhiyong Wu, Zhiyong Wu, Zhizheng Wu, Zhuofeng Wu, Zihao Wu, Zixiu Wu, Jian Xi, Zhaohan Xi, Fei Xia, Menglin Xia, Mengzhou Xia, Patrick Xia, Qingrong Xia, Yingce Xia, Anhao Xiang, Jiannan Xiang, Suncheng Xiang, Changrong Xiao, Chaojun Xiao, Chunyang Xiao, Jinfeng Xiao, Jing Xiao, Jinghui Xiao, Min Xiao, Yanghua Xiao, Zhaomin Xiao, Jun Xie, Kaige Xie, Ning Xie, Ruobing Xie, Shangyu Xie, Yiqing Xie, Yuqing Xie, Yuxi Xie, Zhiwen Xie, Zhouhang Xie, Ji Xin, Chen Xing, Linzi Xing, Zhenchang Xing, Bo Xiong, Chao Xiong, Jing Xiong, Kai Xiong, Wenhan Xiong, Binfeng Xu, Boyan Xu, Canwen Xu, Chen Xu, Chenchen Xu, Chunpu Xu, Dongfang Xu, Fan Xu, Fangyuan Xu, Frank F. Xu, Guandong Xu, Guangyue Xu, Hanzi Xu, Hongfei Xu, Hongzhi Xu, Jiacheng Xu, Jiashu Xu, Jin Xu, Jinan Xu, Jitao Xu, Jun Xu, Kang Xu, Keyang Xu, Kun Xu, Lei Xu, Lu Xu, Mingbin Xu, Mingzhou Xu, Nan Xu, Peng Xu, Peng Xu, Qiongkai Xu, Ruifeng Xu, Ruochen Xu, Shicheng Xu, Wang Xu, Weiran Xu, Weiwen Xu, Wenda Xu, Wenduan Xu, Xiao Xu, Xinnuo Xu, Yan Xu, Yang Xu, Yang Xu, Yi Xu, Yige Xu, Yiheng Xu, Yumo Xu, Zhen Xu, Zhenhui Xu, Zhichao Xu, Zhiyang Xu, Fuzhao Xue, Nianwen Xue, Shan Xue, Deshraj Yadav, Prateek Yadav, Yadollah Yaghoobzadeh, Bryce Yahn, Ikuya Yamada, Ivan Yamshchikov, An Yan, Hang Yan, Hanqi Yan, Jianhao Yan, Jun Yan, Lingyong Yan, Xifeng Yan, Xu Yan, Zhao Yan, Hitomi Yanaka, An Yang, Cheng Yang, Dejie Yang, Eugene Yang, Fan Yang, Guanqun Yang, Haoran Yang, Jian Yang, Jian Yang, Jianing Yang, Jie Yang, Jingfeng Yang, Jun Yang, Kexin Yang, Li Yang, Liner Yang, Linyi Yang, Liu Yang, Longfei Yang, Nan Yang, Sen Yang, Songlin Yang, Tsung-Yen Yang, Wei Yang, Wenmian Yang, Xianjun Yang, Xiaocong Yang, Yaqin Yang, Yazheng Yang, Yiben Yang, Yinfei Yang, Yuanhang Yang, Yue Yang, Zhao Yang, Zixiaofan Yang, Zonglin Yang, Ken Yano, Tae Yano, Barry Yao, Bingsheng Yao, Liang Yao, Peiran Yao, Zijun Yao, Mahsa Yarmohammadi, Bingyang Ye, Fanghua Ye, Hai Ye, Jiacheng Ye, Jiasheng Ye, Junjie Ye, Muchao Ye, Qinyuan Ye, Rong Ye, Seonghyeon Ye, Wei Ye, Wenting Ye, Xi Ye, An-Zi Yen, Jinyoung Yeo, Yu Ting Yeung, Jingwei Yi, Xiaoyuan Yi, Wen-Wai Yim, Seid Muhie Yimam, Chuantao Yin, Congchi Yin, Fan Yin, Kayo Yin, Qingyu Yin, Wenjie Yin, Xuwang Yin, Yu Yin, Yuwei Yin, Jiahao Ying, Anssi Yli-Jyra, Michael Yoder, Hikaru Yokono, Zheng Xin

# Secondary Reviewers

Sharon Adar, Sneha Agarwal, Utkarsh Agarwal, Akiko Aizawa, Christopher Akiki, Ilseyar Ali-

mova, Falah Amro, Miriam Anschütz, William Armstrong, Yuya Asano, Md Rabiul Awal, Ansar Aynetdinov, Andrea Bacciu, Yinhao Bai, Oliver Baumann, Alessandro De Bellis, Guillaume Le Berre, Marie Bexte, Hanoz Bhathena, Abari Bhattacharya, Mukul Bhutani, Verena Blaschke, Moritz Blum, Marc Brinner, Reynier Ortega Bueno, Kishan K C, Mingchen Cai, Yucheng Cai, Paul Caillon, Eduardo Calò, Marco Casavantes, Giulia Cassara, Roman Castagné, Brittany Cates, Amanda Chan, Ayon Chattopadhyay, Huiyao Chen, Liang Chen, Pei Chen, Tianyu Chen, Tongfei Chen, Weidong Chen, Xi Chen, Xingyu Chen, Yuan Chen, Yue Chen, Zhenghan Chen, Zhi Chen, Zhijia Chen, Zhikai Chen, Zifeng Cheng, Jae Sook Cheong, Lin Lee Cheong, Yan Kin Chi, Hanjun Cho, Eunsenog Choi, Sahil Chopra, Rennan Cordeiro, Matthias Cosler, Adrian Cosma, Liam Cripwell, Yudivián Almeida Cruz, Israel Cuevas, Shih-Chieh Dai, Yinpei Dai, Parag Dakle, Niklas Deckers, Zhongfen Deng, Sourabh Deoghare, Simma Dharani, Harshita Diddee, Qiuyu Ding, Yuning Ding, Zixiang Ding, Mingwen Dong, Kefei Duan, Fanny Ducel, Tobias Eder, Pavel Efimov, Suilan Estevez-Velarde, Saad Ezzini, Maurice Falk, Meng Fan, Ziwei Fan, Qingkai Fang, Mohsen Fayyaz, James Finch, Sarah Finch, Sheema Firdous, Martina Forster, Cady Gansen, Alberto Gasparin, Qiming Ge, Shiping Ge, Kinga Gémes, Lei Geng, Yaroslav Getman, Sadaf Ghaffari, Sarvjeet Singh Ghotra, Lukas Gienapp, Jonas Golde, Mahsa Goodarzi, Shuhao Gu, Gael Guibon, Mika Hämäläinen, Kelvin Han, Shiyi Han, Yu Han, Sami Ul Haq, Bradley Hauer, Hui He, Junyi He, Yunjie He, Zhiwei He, Julien Heitmann, Alexander Henlein, Ondřej Herman, Xanh Ho, Julian Hoellig, Chun-Cheng Hsieh, Echo Hu, Langlin Huang, Shih-Cheng Huang, Shuyan Huang, Yerin Hwang, Radu Cristian Alexandru Iacob, Etsuko Ishii, Itay Itzhak, Adam Ivankay, Nazanin Jafari, Anubhav Jangra, Seongjun Jeong, Tianbo Ji, Qi Jia, Yiren Jian, Chengyue Jiang, Junfeng Jiang, Yiwei Jiang, Hailong Jin, Omisa Jinsi, Richard Jonker, Minjoon Jung, Danial Kamali, Jeongwoo Kang, Beatrice Kanyi, Abhinav Ramesh Kashyap, Prachuryya Kaushik, Joschka Kersting, Shamir Khandaker, Aditi Khandelwal, Niama El Khbir, Sopan Khosla, Mohammad Khosravani, Hajung Kim, Hyunjong Kim, Jeonghwan Kim, Jiwoo Kim, Seungone Kim, Yongil Kim, Youngbin Kim, Chaitanya Kirti, Xenia Klinge, Erik Körner, Ádám Kovács, Vojtěch Kovář, Shachi H Kumar, Vivek Kumar, Gitanjali Kumari, Maddalen López De Lacalle, Jack Lanchantin, Loic De Langhe, Anna Laskina, Chaeeun Lee, Dongryeol Lee, Kang-Il Lee, Sunkyung Lee, Yongjae Lee, Els Lefever, Zhihong Lei, Elisa Leonardelli, Hang Li, Jiazhao Li, Junlong Li, Mengyu Li, Minghan Li, Senyu Li, Shiyang Li, Shuqin Li, Wenyan Li, Xinhang Li, Yan Li, Yichen Li, Yichuan Li, Yunshui Li, Zekun Li, Zhaoqun Li, Zhuoqun Li, Zitong Li, Zhenwen Liang, Ruotong Liao, Boda Lin, Jiuheng Lin, Hali Lindsay, Alisa Liu, Andy T. Liu, Hong Liu, Hongyi Liu, Huijun Liu, Mengying Liu, Zhexiong Liu, Alessandro Locaputo, Roberto López, Sebastian Lopez-Cot, Yuze Lou, Xuantao Lu, Kamile Lukosiute, Gunnar Lund, Chu Fei Luo, Haoran Luo, Xin Lv, Congbo Ma, Da Ma, Andrew Mackey, Hiren Madhu, Daniele Malitesta, Oscar Mañas, Fabienne Marco, Salima Mdhaffar, Marek Medveď, Nikhil Mehta, Di Mei, Althis Mendes, Augusto Mendes, Stefano Menini, Elena Merdjanovska, Hossein Mohammadi, Samraj Moorjani, Yusuke Mori, Durgesh Nandini, Gaurav Negi, Hoang Nguyen, Vincent Nguyen, Feng Nie, Anna Nikiforovskaya, Jingcheng Niu, Gibson Nkhata, Rik Van Noord, Michael Ogezi, Olubusayo Olabisi, Katrina Olsen, Talgat Omarov, Andreas Opedal, Junshu Pan, Suehyun Park, Daraksha Parveen, Maya Pavlova, Diogo Pernes, Jan Pfister, Alejandro Piad-Morffis, Max Ploner, Alexander Podolskiy, Dejan Porjazovski, Pradyot Prakash, Adrien Pupier, Maarten De Raedt, Pétur Orri Ragnarsson, Sai Krishna Rallabandi, Leonardo Ranaldi, Abhinav Rao, Anton Razzhigaev, Sebastian Reimann, Raphael Reinauer, François Remy, Jiaqian Ren, Siyu Ren, Akseli Reunamo, Valentin Richard, Ruty Rinott, Elsa Rizk, Giulia Rizzi, Sean Robertson, Cristian Rodriguez, Sudipta Singha Roy, Susanna Rücker, Elena Sofia Ruzzetti, Tasnim Kabir Sadik, Joy Sain, Jose Ignacio Abreu Salas, Hossein Salemi, Mufan Sang, Twisampati Sarkar, Simone Scaboro, Felix Schmidt, Frederik Schmitt, Christopher Schröder, Simeon Schüz, Nina Seemann, Vincent Segonne, Yasas Senarath, Ashish Seth, Silvio Severino, Lele Sha, Stephen Shaffran, Anastassia Shaitarova, Hee Ming Shan, Kai Shen, Xingyu Shen, Shuqian Sheng, Kaize Shi, Ke Shi, Yuanjun Shi, Yuxuan Shu, Lucas Dos Santos Silva, Harmanpreet Singh, Pranaydeep Singh, Salam Michael Singh, Iustin Sirbu, Sonish Sivarajkumar, Mohamed Soliman.

# xxxii

Chenyang Song, Kunzhe Song, William Soto, Florian Steuber, Manuel Stoeckel, Vit Suchomel, Bin Sun, Changzhi Sun, Cong Sun, Jingdong Sun, Qiujie Sun, Xiaohui Sun, Xueyao Sun, Shahbaz Syed, Zhaoxuan Tan, Shaowen Tang, Ziming Tang, Kumar Tanmay, Jingxuan Tu, Sichang Tu, Xiao Chi Tu, Mehmet Deniz Turkmen, Sagar Uprety, Hannah Vanderhoeven, Julien Velcin, Elad Venezian, Radhakrishnan Venkatakrishnan, Ivo Vigan, Fedor Vitiugin, Nikolas Vitsakis, Xiangpeng Wan, An Wang, Bingyu Wang, Cong Wang, Haoran Wang, Hu Wang, Junlin Wang, Junting Wang, Ke Wang, Lei Wang, Lingzhi Wang, Qianli Wang, Ruofan Wang, Shih-Heng Wang, Teng Wang, Weizhi Wang, Xinyou Wang, Yigong Wang, Yiming Wang, Yueguan Wang, Zihao Wang, Haitian Wei, Martyna Wiacek, Ronald Wilson, Moritz Wolf, Haibin Wu, Jay Zhangjie Wu, Jian Wu, Yexin Wu, Yuan-Kuei Wu, Siyuan Xiang, Yang Xiao, Yao Xiao, Zhouhang Xie, Benfeng Xu, Chenwei Xu, Kaishuai Xu, Yuzhuang Xu, Zhichao Xu, Zhiyang Xu, Bo Xue, Siyuan Xue, Xiaojun Xue, Baosong Yang, Kaiqi Yang, Shiping Yang, Yanjie Yang, Yinguan Yang, Jiarui Yao, Bingyang Ye, Yongjing Yin, Yuwei Yin, Tarik Yousef, Guoxin Yu, Nan Yu, Tiezheng Yu, Zhengqing Yuan, Klim Zaporojets, Urchade Zaratiana, Omnia Zayed, Weihao Zeng, Ge Zhang, Hanlei Zhang, Jingyu Zhang, Le Zhang, Mian Zhang, Qi Zhang, Ruike Zhang, Songyang Zhang, Tao Zhang, Weijia Zhang, Yidan Zhang, Yunan Zhang, Zhiling Zhang, Ziheng Zhang, Ziqiing Zhang, Ziqing Zhang, Honghong Zhao, Jiahao Zhao, Jinman Zhao, Siyang Zhao, Wei Zhao, Xingmeng Zhao, Yingxiu Zhao, Yu Zhao, Gui Zhen, Kangjie Zhen, Kai Zheng, Kaiwen Zhou, Terry Zhou, Zhengping Zhou, Zhijie Zhou, Ming Zhu, Zhihong Zhu, Haojie Zhuang, Anni Zou

# Keynote Talk: Two Paths to Intelligence

# Geoffrey Hinton

# University of Toronto (emeritus)

Monday, July 10 – Time: 9:30 - 10:30 EDT – Room: Metropolitan

# Abstract

I will briefly describe the forty year history of neural net language models with particular attention to whether they understand what they are saying. I will then discuss some of the main differences between digital and biological intelligences and speculate on how the brain could implement something like transformers. I will conclude by addressing the contentious issue of whether current multimodal LLMs have subjective experience.

# Bio

Geoffrey Hinton received his PhD in Artificial Intelligence from Edinburgh in 1978. After five years as a faculty member at Carnegie-Mellon he became a fellow of the Canadian Institute for Advanced Research and moved to the University of Toronto where he is now an emeritus professor. He is also the Chief Scientific Adviser at the Vector Institute.

He was one of the researchers who introduced the backpropagation algorithm and the first to use backpropagation for learning word embeddings. His other contributions to neural network research include Boltzmann machines, distributed representations, time-delay neural nets, mixtures of experts, variational learning and deep learning. His research group in Toronto made major breakthroughs in deep learning that revolutionized speech recognition and object classification.

He is a fellow of the UK Royal Society and a foreign member of the US National Academy of Engineering, the US National Academy of Sciences and the American Academy of Arts and Sciences. His awards include the David E. Rumelhart prize, the IJCAI award for research excellence, the Killam prize for Engineering, the Royal Society Royal Medal, the NSERC Herzberg Gold Medal, the IEEE James Clerk Maxwell Gold medal, the NEC C&C award, the BBVA award, the Honda Prize and the Turing Award.

# Keynote Talk: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models

# Alison Gopnik

# University of California at Berkeley

Wednesday, July 12 – Time: 14:00 - 15:00 EDT – Room: Metropolitan

# Abstract

Its natural to ask whether large language models like LaMDA or GPT-3 are intelligent agents. But I argue that this is the wrong question. Intelligence and agency are the wrong categories for understanding them. Instead, these Al systems are what we might call cultural technologies, like writing, print, libraries, internet search engines or even language itself. They are new techniques for passing on information from one group of people to another. Cultural technologies arent like intelligent humans, but they are essential for human intelligence. Many animals can transmit some information from one individual or one generation to another, but no animal does it as much as we do or accumulates as much information over time. New technologies that make cultural transmission easier and more effective have been among the greatest engines of human progress, but they have also led to negative as well as positive social consequences. Moreover, while cultural technologies allow transmission of existing information cultural evolution, which is central to human success, also depends on innovation, exploration and causal learning. Comparing LLM’s responses in prompts based on developmental psychology experiments to the responses of children may provide insight into which capacities can be learned through language and cultural transmission, and which require innovation and exploration in the physical world. I will present results from several studies making such comparisons.

# Bio

Alison Gopnik is a professor of psychology and affiliate professor of philosophy at the University of California at Berkeley, and a member of the Berkeley AI Research Group. She received her BA from McGill University and her PhD. from Oxford University. She is a leader in the study of cognitive science and of children’s learning and development and was one of the founders of the field of “theory of mind”, an originator of the “theory of cognitive development”, and the first to apply Bayesian probabilistic models to children’s learning. She has received both the APS Lifetime Achievement Cattell and William James Awards, the Bradford Washburn Award for Science Communication, and the SRCD Lifetime Achievement Award for Basic Science in Child Development. She is an elected member of the Society of Experimental Psychologists and the American Academy of Arts and Sciences and a Cognitive Science Society, American Association for the Advancement of Science, and Guggenheim Fellow. She was 2022-23 President of the Association for Psychological Science.

She is the author or coauthor of over 140 journal articles and several books including “Words, thoughts and theories” MIT Press, 1997, and the bestselling and critically acclaimed popular books “The Scientist in the Crib” William Morrow, 1999, “The Philosophical Baby; What children’s minds tell us about love,

truth and the meaning of life” 2009, and “The Gardener and the Carpenter” 2016, Farrar, Strauss and Giroux, the latter two won the Cognitive Development Society Best Book Prize in 2009 and 2016. Since 2013 she has written the Mind and Matter column for the Wall Street Journal and she has also written widely about cognitive science and psychology for The New York Times, The Economist, The Atlantic, The New Yorker, Scientific American, The Times Literary Supplement, The New York Review of Books, New Scientist and Slate, among others. Her TED talk on her work has been viewed more than 5.2 million times. She has frequently appeared on TV, radio and podcasts including “The Charlie Rose Show”, “The Colbert Report”, “Radio Lab” and “The Ezra Klein Show”. She lives in Berkeley with her husband Alvy Ray Smith and has three children and five grandchildren.

xxxv

# The Future of Computational Linguistics in the LLM Age

# Panel Discussion

Chair: Iryna Gurevych

Technische Universität at Darmstadt

Tuesday, July 11 - Time: 14:45-15:45

This is a panel discussion with:

- Dan Klein (UC Berkeley)
- Meg Mitchell (Hugging Face)
- Roy Schwartz (the Hebrew University of Jerusalem)

They will present short statements (5 to 7 min.) related to the main topic of the panel:

- New opportunities (e.g., artificial general intelligence, responsible NLP);
- Technical challenges (e.g., multimodality, instruction-tuning, etc.);
- Real life problems & societal implications (e.g., hallucinations, biases, future job market);
- LLMs and the future of NLP; and
- Open-science vs. commercial LLMs.

Followed by discussion with the panel and audience.

# Memorial: Dragomir Radev

Tuesday, July 11, 2023 - Room: Metropolitan - Time: 13:00–13:30

Dragomir Radev, the A. Bartlett Giamatti Professor of Computer Science at Yale University, passed away this year on Wed, March 29th. Drago contributed in substantial ways to research in NLP, to the organization of the ACL and to mentoring the next generation of computational linguists. Drago’s role in our ACL community spans four decades. He was recognized for his work over this period through his selection as an ACL Fellow in 2018 for his significant contributions to text summarization and question answering, and through his receipt of the Distinguished ACL Service Award in 2022. In this session, speakers from different time periods of his life will discuss his contributions to the field and the impact his life had on so many of us.

xxxvii

# Ethics Panel

Kar¨ en Fort, Min-Yen Kan and Yulia Tsvetkov, Luciana Benotti, Mark Dredze, Pascale Fung, Dirk Hovy, Jin-Dong Kim, Malvina Nissim

Tuesday, July 11, 2023 - Room: Pier 4&5 - Time: 16:15–17:45

We present our ACL Ethics Committee’s progress over the last few years. Of core interest, we will present the results of the ACL stakeholder survey about the role of ethics and ethics training exposure. Results from the survey respondents indicate that ethics is of primary interest to the community and that there is a mandate for the further creation and dissemination of ethics related training for authors, reviewers and event organisers. We will briefly review the survey results and feature a lengthed question and answer session in support of extended dialogue with our community. Our session will culminate through a dialogue with our session’s participants in a moderated panel that includes participation from the entire ethics committee.

xxxviii

# Transitioning to Rolling Review Discussion

Mausam, Professor, IIT Delhi (ARR EIC), Jonathan K. Kummerfeld, Assistant Professor, University of Sydney (ARR CTO)

Tuesday, July 11, 2023 - Room: Metropolitan - Time: 14:15–14:45

This session will contain a presentation on progress in ARR over the past year and provide an opportunity for community questions and discussion.

xxxix

# Program Chairs’ Report on Peer Review at ACL 2023

Anna Rogers♢ &nbsp; &nbsp; &nbsp; Marzena Karpinska♡ &nbsp; &nbsp; &nbsp; Jordan Boyd-Graber♠ &nbsp; &nbsp; &nbsp; Naoaki Okazaki♣

♢IT University of Copenhagen &nbsp; &nbsp; &nbsp; ♡University of Massachusetts Amherst

♠University of Maryland &nbsp; &nbsp; &nbsp; ♣Tokyo Institute of Technology

arog@itu.dk &nbsp; &nbsp; &nbsp; mkarpinska@cs.umass.edu

jbg@umiacs.umd.edu &nbsp; &nbsp; &nbsp; okazaki@c.titech.ac.jp

# Abstract

We present a summary of the efforts to improve conference peer review that were implemented at ACL’23. This includes work with the goal of improving review quality, clearer workflow and decision support for the area chairs, as well as our efforts to improve paper-reviewer matching for various kinds of non-mainstream NLP work, and improve the overall incentives for all participants of the peer review process. We present analysis of the factors affecting peer review, identify the most problematic issues that the authors complained about, and provide suggestions for the future chairs. We hope that publishing such reports would (a) improve transparency in decision-making, (b) help the people new to the field to understand how the *ACL conferences work, (c) provide useful data for the future chairs and workshop organizers, and also academic work on peer review, and (d) provide useful context for the final program, as a source of information for meta-research on the structure and trajectory of the field of NLP.

# 1 Introduction

With the continued growth of our field and the rising number of conference submissions, peer review draws more and more attention from the community—as an application area (Hua et al., 2019; Anjum et al., 2019; Stelmakh et al., 2019, inter alia), in meta-research (Rogers and Augenstein, 2020; Church, 2020, inter alia), in initiatives to organize and release peer review data (Kang et al., 2018; Jecmen et al., 2022; Dycke et al., 2022, inter alia), and, of course, in the regular heated social media discussions during submission deadlines, review release dates, and acceptance notifications. It is unlikely that peer review will ever be perfect – it remains ‘the least bad system’ we have for ensuring the quality of scientific publications (Smith, 2010). Still, with each iteration we should learn a little more about what works better for organizing peer review at such scale, and in a community so diverse in expertise and experience.

As a step in that direction, ACL’23 makes its peer review report public and an official part of the conference proceedings, complementing the introduction and other administrative materials. The goal is to increase the visibility of the results of the conference process, as well as any incidental findings from conference organizations and the lessons learned the hard way that may be useful to the future chairs and workshop organizers. Such publications also provide extra incentives for the future program chairs to invest more effort in the analysis of their process, and they provide a useful background to the composition of the final program that may be useful for meta-science research (since they essentially document the selection process for that program). Last but not least, such publications will improve the transparency of the *ACL conference process, which may be useful to the researchers who are new to the field.

We present the core statistics per track (§2), analysis of resubmissions (§3) and core demographics (§4), our efforts for improving peer review quality (§5), improving decision support for the chairs (§6), our analysis of various factors contributing to review scores and final decisions (§7), ethics review and best paper selection (§8), and our efforts towards improving incentives for the authors, reviewers and chairs (§9). We conclude with overall recommendations for future conference organizers (§10). The materials we developed will be available at a dedicated repository 1.

The results presented here are based on the analysis of internal data of ACL’23, as well as exit surveys that we sent to the chairs, authors and reviewers. We received responses from 25 senior area chairs (SACs).

# Tracks and Acceptance Statistics

|Track| |Direct submissions| | |ARR submissions|Submitted Main Findings|Submitted Main Findings|
|---|---|---|---|---|---|---|---|
|Computational Social Science and Cultural Analytics|113|22.12|19.47|10|90.00|10.00| |
|Dialogue and Interactive Systems|269|24.54|15.24|19|21.05|42.11| |
|Discourse and Pragmatics|52|21.15|34.62|1|100.00|0.00| |
|Ethics and NLP|54|22.22|31.48|7|42.86|42.86| |
|Generation|175|25.71|20.57|6|66.67|16.67| |
|Information Extraction|279|25.45|16.13|33|24.24|36.36| |
|Information Retrieval and Text Mining|94|14.89|21.28|9|44.44|0.00| |
|Interpretability and Analysis of Models for NLP|189|24.34|28.04|20|35.00|55.00| |
|Language Grounding to Vision, Robotics, and Beyond|147|24.49|21.77|5|40.00|40.00| |
|Large Language Models|252|28.17|21.03|10|50.00|30.00| |
|Linguistic Diversity|18|27.78|22.22|1|0.00|100.00| |
|Linguistic Theories, Cog. Modeling & Psycholinguistics|38|23.68|23.68|8|50.00|37.50| |
|Machine Learning for NLP|313|21.09|23.32|37|56.76|2.70| |
|Machine Translation|198|25.25|18.18|7|0.00|57.14| |
|Multilingualism and Cross-Lingual NLP|85|20.00|30.59|12|25.00|16.67| |
|NLP Applications|354|22.88|19.77|25|52.00|8.00| |
|Phonology, Morphology, and Word Segmentation|21|28.57|19.05|0| | | |
|Question Answering|197|18.78|18.78|22|45.45|18.18| |
|Resources and Evaluation|213|28.17|19.72|23|56.52|0.00| |
|Semantics: Lexical|54|25.93|25.93|3|66.67|33.33| |
|Semantics: Sentence-level Semantics|81|27.16|11.11|9|22.22|22.22| |
|Sentiment Analysis, Stylistic Analysis, Arg. Mining|107|17.76|30.84|10|30.00|0.00| |
|Speech and Multimodality|72|27.78|36.11|7|57.14|14.29| |
|Summarization|139|23.02|21.58|12|33.33|8.33| |
|Syntax: Tagging, Chunking, and Parsing|69|23.19|21.74|5|20.00|20.00| |
|Theme: Reality Check|110|26.36|30.91|1|100.00|0.00| |
|Total|4559|20.73|18.36|305|42.30|20.98| |

Table 1: Number of submissions and acceptance rates per track for direct and ARR submissions to ACL’23.

(35.7% response rate), 134 area chairs (ACs) (30.5% response rate), 510 reviewers (11.4% response rate), and 556 authors (4.07% response rate of all authors2).

2 Tracks and Acceptance Statistics ACL’23 had 26 tracks, most of which have also been offered at other recent NLP conferences. At the suggestion of EMNLP 2022 chairs, we kept their separation of “Large Language Models”3 track from “Machine Learning for NLP” track. At community requests we added the following tracks: “Linguistic Diversity” and “Multilingualism and Cross-lingual NLP”. Each track had at least two Senior Area Chairs (SACs), who then recruited area chairs (ACs) for that track. The full list of senior chairs per track is available at the conference website.4

3 The EMNLP original name was Language Modeling and Analysis of Language Models. In our version it was simply Large Language Models, as they are the most frequent topic currently, but in retrospect the original version is preferable as it is more inclusive.

4 https://2023.aclweb.org/committees/program/

# Findings

| | |EACL|Direct-New|Withdrawn| | |
|---|---|---|---|---|---|---|
| | |33%|16%| | | |
| | | | |28%|78%| |
| |EMNLP|29%|0%| | | |
| |Arr-Resubmissions| |17%|15%| | |
| | | |Main|13%|39%| |
| |Direct-Resubmissions|4%|6%| | | |
| | |11%|3%| | | |

(a) Submissions vs resubmissions (b) Prior venues of resubmissions (c) The fate of resubmitted papers

Figure 1: Resubmissions at ACL’23

ACL Rolling Review (ARR). Table 1 shows that in most tracks, ARR submissions had a much higher acceptance rate, sometimes twice higher. This is to be expected because ARR submissions self-select for high scores and positive reviews before committing to ACL.

Since in the hybrid process ARR submissions and direct submissions directly compete for acceptance, a question arises to what extent this is a fair competition. We asked that question to our SACs. 58.3% believe that this process is fair enough, 12.5% - that it is unfair to the direct submissions, and 29.6%—that it is unfair to the ARR submissions. Of 17 SACs who believed that this situation is unfair in some way, 23.5% suggested that they should have separate acceptance rate, 41.2%—that they should have a separate process and acceptance criteria, and 47.1%—that there should be some other solution (many comments pointing to the confusion, the apples-to-oranges comparisons of reviews performed with different evaluation, the less-than-ideal import of openreview data into START (browsing attachments takes more time). Many expressed a preference for a non-hybrid process.

As program chairs, our biggest challenge with ARR was that by design it provides reviews and meta-reviews, but the acceptance decisions are then made by our SACs—who generally do not provide extra feedback to either direct submissions or ARR submissions (nor can they be expected to: some tracks had over 300 papers per 3 SACs). For direct submissions, nobody expects SAC-level feedback. But to ARR authors, who likely self-selected for high scores and positive reviews, to be rejected without explanation is more frustrating, and we received a lot of angry emails demanding extra feedback (even though neither we nor ARR promised that). It seems that by design, a process where there are acceptance quotas, and decisions are fully decoupled from feedback, will necessarily leave the majority of authors rejected without explanation—and hence disappointed and unsure what they could do to improve their work (and we agree that this would indeed be frustrating to the authors).

The above factors could transform into a bigger problem in the future. We only had 305 ARR submissions, but if a majority of our submissions came with high scores and positive reviews—this just would not be a useful signal anymore. The acceptance odds of direct submissions would decrease (as compared to a process where everyone starts at the same stage of peer review). The SAC-ing would become harder (since selecting among high-quality papers is less easy than among papers of varying quality), and the authors would be disappointed because many would be rejected with high scores and no idea what they could do differently.

# 3 Resubmissions

Among the 4559 direct submissions to ACL’23, 754 indicated that they were resubmissions (see fig. 1a). The biggest “donors” were EACL5 (296), EMNLP (258), ICLR (103), AAAI (52), and ACL Rolling Review6 (39). Although the selectivity of top-tier conferences means that the majority of papers are

5 Because our submission deadline was shortly before EACL and ICLR notification deadlines, we made an exception to no-cross-submission policy and allowed their submissions to be also submitted to ACL. After their respective notifications many such papers withdrew from our pool, which explains the high withdrawal rate in Figure 1c.

6 There were 11 resubmissions from October 2022, 6 from September, and 1-3 from many other months of 2022.

# 4 Authors and Reviewers at ACL’23

We received a record 4864 submissions (4559 direct, 305 from ARR) from the total of 13,658 authors, reviewed by 4490 reviewers. This section reviews our recruitment process and the three demographic variables (country, affiliation type, and gender) to which we had access in the global START profiles of all participants of ACL peer review process.

# Reviewer recruitment.

We initially sent review invitations to the reviewer list which we had received from the organizers of previous conferences. We also required the authors of all submissions to nominate at least one experienced reviewer, whom we also sent invitations.

As we elicited reviewer data, we found that for a quarter of our reviewers7 there is no reliable Semantic Scholar publication history data that can be used for paper-reviewer matching. For conferences that fully rely on automated paper–reviewer matching based on publication history, this factor obviously sets a bound on their possible performance. Often the author pages exist because Semantic Scholar automatically created them, but the authors did not claim them and did not clean them up, which

Out of the reviewers who filled in our sign-up forms, only 75.4% confirmed that their Semantic Scholar profile is accurate and can actually be used to estimate their areas of interest and expertise. In addition to that, 8.9% reviewers listed in START did not specify their Semantic Scholar IDs in their profiles.

# Figure 2: Author and reviewer pool at ACL’23*

| | |Role|Count|Ratio|Gender| | |
|---|---|---|---|---|---|---|---|
| | | |Author|4000|0.8|Female| |
|3000| |0.6|Male| | | | |
|2000| |0.4| | |Not specified| | |
|Reviewer|1000|0.2|Other| | | | |
|0| |0.0| | | | | |

# Affiliation type counts

|Faculty|Industry|Postdoc|PhD Student|Masters Student|Government|
|---|---|---|---|---|---|
|4000|0.8|0.6|0.4|0.2|0.0|

# Gender per role

|Role|Author|Reviewer|Chair|
|---|---|---|---|
|Gender|Female|Male|Not specified|

# Country

China
USA
UK
Other
Korea
India
Canada
Japan
France
Italy
Israel
Spain
Taiwan
Russia
Brazil
Poland
Ireland
Belgium
Sweden
Iran
Germany
Singapore
Australia
Hong Kong
Denmark
Portugal
Viet Nam
Switzerland
Netherlands
Bangladesh
Czech Republic

* All information is self-reported, not independently verified, and does not correspond to any specific definition of affiliation, gender, or country (e.g., some authors from Edinburgh may elect to list their country as “Scotland” rather than “UK”.)

may result in the addition of publications by namesake authors (e.g. the automatically created profile for “Anna Rogers” originally had contributions from at least three researchers with that name.) This is particularly worrying because at this point many venues have used this information for paper-reviewer matching, and urged the NLP community to maintain their Semantic Scholar profiles. We also specifically reminded about this, but still a quarter of our sign-up pool stated that their publication history is not accurate. In addition to this problem, matching based on publication history has the issue with establishing expertise of different authors on multi-author publications. Hence, we developed an alternative matching approach described in §5.2.

# 5 Efforts towards improving review quality

This section describes the following steps that ACL’23 proposed and implemented within its peer review process to improve review quality: review tutorials (§5.1), Area-Contribution-Language paper-reviewer matching (§5.2), flagging of review issues by the authors (§5.3). The efforts to improve the overall incentives are described in §9.2 and §9.3.

# Affiliation types

Figure 2a presents the overall distribution of the affiliations of our authors and reviewers (as stated in START profiles). The biggest group of authors, reviewers, and chairs are academic faculty. The second biggest group (by absolute numbers) in all three categories is industry, which is relevant to the recent concerns about the influence of industry on academic NLP research (Abdalla et al., 2023). Furthermore, students form at least 26% of reviewer pool (Ph.D. 22.7%, M.Sc. 3.3%). This was also our experience as area chairs at other recent conferences, and it highlights the need to continue the reviewer training efforts.

# Gender distribution

Based on the information in softconf profile, about 20% of ACL peer review participants in all roles did not answer the question about their gender (Figure 2b). For a part of this population this is likely a deliberate choice, but judging by how many other fields in the START profiles were not accurately filled in or updated, in many cases this likely signals simply the lack of desire to fill in forms, especially for the new authors who had to register in START last minute in order to make a submission. Considering only those profiles that responded to this question, we see a heavy imbalance for “male”, in agreement with the reports on under-representation of women in Computer Science (Jaccheri et al., 2020; Pantic and Clarke-Midura, 2019), where a lot of NLP research is currently happening. This underscores the need to continue the Diversity and Inclusion efforts.

# Top contributing countries

The analysis of the countries of all authors and reviewers suggests that the balance between reviewing and submitting papers is considerably off for many locations, and particularly China. We believe that this is at least partly due to the fact that our recruitment efforts started with the pool of the previous conferences. That pool needs to be deliberately expanded by more active and targeted reviewer recruitment efforts among Chinese institutions.

Church (2020) estimates that at 20% acceptance rate the authors of published papers “owe” the community at least 15 reviews per each publication (3 for their own paper, and 4x3 for the papers that didn’t get in). While some dis-balance between the author and reviewer list is to be expected (e.g., since many junior authors are not yet qualified to review, and many senior authors perform other organization roles)—we clearly need to decrease it in order to decrease the reviewer load. Our default quota was six papers per reviewer, in line with most recent conferences. This is a significant workload, and it can hardly be expected to improve the quality of reviews. Moreover, the more reviewers are in the pool, the smaller the trade-off between optimizing for best matches or smaller workload per reviewer.

In absolute numbers: 3881 authors vs 1271 reviewers for China (ratio 3.05, absolute difference 2610). For the US: 2608 authors, 1809 reviewers (ratio 1.4, absolute difference 799). While the reviewer:author ratios are also high for India (2.6) and Korea (2.64), from the point of view of a conference organizer China stands out due to the sheer volume of submissions.

We gave the reviewers a chance to request a lighter load at sign-up, and respected those quotas in our automated assignments, but there were still some over-assignments due to manual corrections of assignments by the chairs.

# 5.1 Reviewer training

As part of reviewer training, we prepared the following public materials (as a revision of an earlier tutorial10, developed by Anna Rogers and Isabelle Augenstein for ARR):

- ACL’23 Peer Review Process: the general tutorial about review process for novice reviewers, that covers the basic structure of ACL peer review process, author response, and discussion period, as well as tips for planning the time, reporting conflicts of interest and assessing whether to ask for reassignment. These materials were optional for experienced reviewers, and could be used across different ACL venues as is.
- ACL’23 Peer Review Policies: the tutorial explaining our review form and responsible NLP checklist (§9.1), as well as our peer review policy: specific, professional reviews with scores supported by the text. Our list of reviewer heuristics such as “reject if not SOTA” currently contains 14 heuristics (continued from the original eight heuristics pioneered at EMNLP 2020 (Cohn et al., 2020)). We asked even experienced reviewers to read this tutorial. The future chairs could reuse parts of this tutorial, with necessary updates to the review form description and review policies.

Feedback. The exit survey indicates that the reviewers found the materials clear (43% respondents rated them as at 4 out of 4 and 40.5% - as 3 out of 4 on 4-point scale). One avenue of improvement suggested in many free comments was adding examples of good reviews.

We also asked the reviewers about their preferences for alternative formats, and the self-paced text-based tutorial was the majority choice (62.5% vs 13% preferring video tutorials and 9.6% preferring interactive tutorial with quizzes). But 13.4% respondents said that they would probably never be able to spend time on reviewer training, no matter what format it is offered in. This suggests that reviewer training, while valuable, will not help in all cases, and could perhaps be interpreted as an upper bound on the effect of any reviewer training.

# 5.2 ACL paper-reviewer matching: Area-Contribution-Language

One of the peer review issues that authors (and chairs) often complain about is “meh” reviews: the reviewer does not really find any significant problems with methodology or execution of the paper, but the overall recommendation is middling. This could be a symptom of paper-reviewer mismatch: the reviewer just is not sufficiently interested in the overall topic or approach, and hence no matter how good the paper is, it would not elicit much enthusiasm. In a recent survey (Thorn Jakobsen and Rogers, 2022) of authors, reviewers and ACs about their prior experience at NLP venues, many reviewers stated that “the area match was right, but... the subject of the paper was not interesting to me (e.g. I would prefer another NLP task, model, or data)” (54%), or the paper was not asking a research question that would be interesting for me” (45%). At the same time, over 27% of the author respondents in that survey reported that they had experience of reviews where the reviewer was not interested in the subject of the paper.

Most recent ACL conferences and ARR work with some version of an automated paper-reviewer matching system that computes affinity scores between the abstract and title of the submission and the candidate reviewer, based on their publication history. Interestingly, the same survey by Thorn Jakobsen and Rogers (2022) found that both authors, reviewers, and ACs generally considered these scores to be the least important factor for paper-reviewer matching. Besides the limitations of the current systems, one factor here is probably the noise in the reviewer publication history data (only 75% of our reviewers indicated that their Semantic Scholar profiles were accurate enough to use for review assignments, see §4). Then there is also the inherent difficulty with establishing level of expertise on a particular topic in multi-author papers.

A traditional alternative to affinity scores, that also addresses the issue with reviewer interest, is bidding: the reviewers explicitly say which papers they would be interested in. But this process is rather laborious: for a big track, a reviewer would need to indicate their interest for hundreds of papers. It also opens up the possibility of collusion rings (Littman, 2021). In our experience, many reviewers do not even respond to bidding calls on time, which once again leads to some part of assignments being essentially random.

10https://aclrollingreview.org/reviewertutorial

|Match by area|Match by contribution|Match by language|Review count|Review %|
|---|---|---|---|---|
|✓|✓|English|8996|71.36|
|n/a*|n/a|n/a|1052|8.35|
|✗|✓|English|691|5.48|
|✓|✗|English|558|4.43|
|✓|✓|✓|476|3.78|
|✓|✓|✗|345|2.74|
|✗|✓|✓|164|1.3|
|✗|✗|English|142|1.13|
|✗|✗|✓|52|0.41|
|✓|✗|✓|50|0.40|

Table 2: The number of reviews matched to submission by different combinations of ACL (Area-Contribution-Language) criteria. The ’n/a’ row corresponds to manual assignments by ACs, for which we do not have the match information.

Thus, we experimented with a new workflow that we dub ACL (Area-Contribution-Language) paper-reviewer-matching. It is a keywords-based matching process that explicitly targets three dimensions of submissions: track sub-areas (topical match), contribution types (match by focus/methodology), and target language (for submissions not focusing on English). To the extent possible, the paper-reviewer matching aimed to provide matches across all these dimensions. This approach further enabled us to provide the ACs with explanations for the specific matches (see §6.3).

# Track sub-areas.

Each track at ACL 2023 had an associated set of keywords describing its potential sub-areas. The goal was to describe the biggest expected sub-areas, and hopefully provide the authors with a better idea of the kind of work that the track was inviting. The full list of our keywords is publicly available in our blog post.11 Our keywords were provided by the SACs of all tracks independently, but the future chairs may wish to take a more top-down approach to editing this list, and to ask their SACs to check that the list still describes the sub-areas for which the most submissions are expected, and the individual keywords are sufficiently clear for the authors.

# Language(s).

Due to the “default” status of English (Bender, 2019), submissions targeting other languages may be perceived as “niche” by reviewers. Additionally, the lack of expertise in a language may make it harder for reviewers to spot potential issues. Hence, for papers on languages other than English, we endeavoured to also maximize reviewer matches along this dimension.

# Contribution types.

The contribution types cross-cut tracks, and we hope they would help to decrease the amount of cases where the reviewer just fundamentally does not recognize a certain type of work (Bawden, 2019) and hence scores it down, or has unreasonable expectations (e.g. experimental results in a position paper). For example, the category of compute/data-efficiency creates a de-facto equivalent of efficiency track spread across all tracks.

Our contribution types are based on COLING 2018 classification (Bender and Derczynski, 2018), which we extended as follows: (1) NLP engineering experiment (most papers proposing methods to improve state-of-the-art), (2) approaches for low-compute settings, efficiency, (3) approaches for low-resource settings, (4) data resources, (5) data analysis (6) model analysis & interpretability, (7) reproduction studies, (8) position papers, (9) surveys, (10) theory, (11) publicly available software and pre-trained models.

# Implementation.

To collect the information for this kind of matching, we asked the authors at submission time to specify their preferred track (up to two), the best-matching keywords in that track (multiple selection possible, or “other” option with free text entry), the best matching contribution type(s) and target language(s). Correspondingly, at reviewer recruitment stage we asked the reviewers to fill in a form specifying their preferences for the tracks, keywords, contribution types, and the language(s) the work on which they could review. The matching itself was based on Integer Linear Programming, aiming to maximize matches across the three keyword types (with more types of matching being more valuable than

11https://2023.aclweb.org/blog/reviewer-assignment/

e.g. more matches only by area). As a fallback, we also retrieved Semantic Scholar profile data for the reviewers and computed the similarity between submission abstracts to the abstracts in the publication history of candidate reviewers, but this factor was given the lowest priority in the assignment strategy. The Area-Contribution-Language matches, as well as the most similar paper of the reviewer, then also became the basis for the rationales for the match (see §6.3). The SACs were given the opportunity to selectively check and adjust the matches as described in §6.2 (although few of them did), and the ACs and SACs were able to see the rationales for the matches when considering the reviews.

From the analysis of the final 12606 reviews in START, 1052 (8.3%) did not have the match information (due to manual reviewer reassignment by the chairs, most likely emergency reviewers). Of the remaining 93.7% reviews made by our criteria, only 1.13% reviews with automated assignment were assigned based on the similarity scores from publication history, after exhausting the possible keywords-based matches in the reviewer pool. 82.9% reviews had at least one match by the type of area, 84.97% - by contribution type. Importantly for DEI efforts and development of NLP for languages other than English, we had 1167 reviews for submissions that specified at least one target language other than English – and we were able to provide a reviewer matching by (at least one) language in 63.58% such reviews.

# Feedback

When asked to rate on 4-point scale how well the paper-reviewer matching worked for them, 85.5% ACL’23 reviewers rated it positively (35.7% at 4/4, 49.8% at 3/4). When asked for the kinds of mismatch, if any, 28.4% pointed at the topic, 13.7% at the methods, 10.4% at the type of contribution, 4.5% at languages, and 5.7% at other kinds of mismatch.

We conclude that Area-Contribution-Language assignments are overall a promising direction that can contribute to DEI efforts in the field and diversity of its contributions (see also §7). The matches could be further refined by (a) revising the area keywords12, and (b) more targeted reviewer recruitment to include speakers of various languages. One of our SACs suggested providing a glossary together with the list of keywords. We also recommend investing effort into a dedicated interface for checking reviewer assignments that would enable ACs to help with reviewer assignment checks while seeing the up-to-date reviewer availability information, and highlighting the possible problems with the current assignments (such as imperfect matches, rare types of contributions or languages that may need extra attention, insufficient pool for a area or a contribution that turns out to be more popular this year).

# 5.3 Review issue flagging

Even with all the above efforts, we anticipated that there would still be problematic and mismatched reviews. Given that the only people with the incentive to read the reviewer guidelines and enforce them are the authors, we developed a way for them to flag reviews for specific issues, which the ACs could be given specific instructions about, and be able to address more systematically.

Unfortunately, the START system does not have an editor for the author response form or meta-review form. Hence we had to provide the authors and ACs with the list of possible issues, and ask them to specify their type and rationale in plain text form, as shown in Figure 3. As could be expected, even with a template there were many format errors. We recommend that the future conferences use a form with a multi-selector, per each reviewer.

The authors actively used this feature at ACL’23, flagging 12.9% of all reviews. This is reassuring: judging by the intensity of online discussions of peer review at each review release day, most reviews are bad). The frequency of various reported issues is shown in Table 3. The biggest reported problem is the heuristics such as “not novel”, “not surprising”, “too simple”, and “not SOTA”. Particularly concerning are the rude/unprofessional reviews: even though there are only 1.69%, they have the most potential to impact the mental health of the authors, and we should strive for that number to be 0.

The author-reported issues should be interpreted as a lower bound on the number of review issues, because of 100 papers were reviewed but withdrew before the final decisions. It is possible that they did because they (a) agreed with the criticism and wished to revise the paper, or (b) that they disagreed but did not see a chance to persuade the reviewers. Assuming the latter, and that all their reviews were problematic, this would raise the upper bound of problematic reviews to 15.3%. But it is unlikely that all

12 In particular, our Language Grounding SACs indicated that their keywords should be revised and clarified.

# Response to Chairs

In rare cases reviews may be of unacceptably low quality, which violates the conference peer review policy: If this happened to You, you can use the box below to report the type of the issue and explain your rationale to the chairs. This mechanism should only be used for serious issues. It is not in the authors' interest to make their meta-reviewers investigate cases where the authors disagree with the reviewers but the reviewers have done due diligence and provide their arguments, evidence, references.

The following types of issues are known from past conferences:

- A. The review is not specific enough, e.g. missing references are not specified.
- B. The review exhibits one of the heuristics discussed in the ACL23 review policy post, such as "not novel", "not surprising", "too simple", "not SOTA". Note that these criticisms may be legitimate, if the reviewer explains their reasoning, and backs up the criticism with arguments, evidence, references. Please flag only the cases where you believe that the reviewer has not done due diligence.
- C. The scores do not match the review text. Note that in ACL23, the "soundness" score is meant to reflect the technical merit of the submission, and low soundness should be backed up with serious objections to the work: The "excitement" score is more subjective and its justification may not be reflected in the text.
- D. The review is rude/unprofessional.
- E. The review does not evince expertise (incl. texts that seem to be synthetic and not based on a deep understanding of the submission).
- F. The review does not match the paper type (e.g. short paper expected to produce more experiments than is necessary to support the stated claim).
- G. The review does not match the type of contribution (e.g. experimental work expected of a paper of a different kind of contribution).
- H. The review is missing or too short and uninformative.
- I. The review was late and could not be addressed in the author response.
- J. Other (please explain).

If you feel that you have such a problem, please use the following format to report it in the text box below (without the #comment lines, 250 words max):

In this example Reviewer 1 had issue A (unspecific review) and Reviewer 2 had issues C and D (rude review; scores don't match the text):

RI: A

# explanation

R1 states [reviewer statement]; which we believe corresponds to the review issue type A. It is unreasonable in this case because [rationale]:

R2: C,D

R2 states [reviewer statement]:

# Submit

Figure 3: Review issue flagging: minimal plain-text implementation in START withdrawn papers were of the (b) type, and the comments from ACs also suggest that many issues were not fully justified.

# Feedback

When asked to rate the utility of this system at ACL’23 on a 4-point scale, with 4 being the highest score, 42.1% of the authors in our exit survey rated it at 4/4, and 40.3% - at 3/4. We interpret it as overwhelming support, and recommend that this feature is maintained in future conferences. However, the qualitative analysis of the authors’ comments suggests that in some cases the ACs did not respond to the flagged issues properly, which entails the need for further training and monitoring by the SACs.

Our follow-up analysis suggests that ACs reported addressing the author-flagged issues in at least 30.59% submissions (judging by their using a similar template to Figure 3 in the “confidential notes to chairs” in the meta-review). This should be interpreted as a lower bound: since the interface was very clunky, it is possible that some ACs did consider the flagged issues, but did not report their actions. But, clearly, many issues were not properly addressed, and there is much room for improvement and further training of ACs. Still, given that this is the first implementation of this system, this is a promising approach and it should improve in the future.

# 5.4 Reviewer discussion

Similarly to most of the recent ACL conferences, we implemented the author response period: a week during which the authors have the opportunity to read the reviews and send their response. The goal of this process is improving the quality of the reviews, and we supplemented that goal with the above new option for the authors to flag specific types of review issues (§5.3). The authors could (but didn’t have to) provide a response and flag review issues; this was done for 88.3% of reviewed submissions. In 57.3% review forms the reviewers indicated that they read the response (it is possible that more did read the response but did not fill in the form).

Those comments were seen by the ACs, not the reviewers. The ACs had the option to initiate reviewer discussions for the cases where they saw significant disagreements, quality issues, or misunderstandings. Each paper had an associated “forum” on START, where the reviewers could communicate in an.

|Type of issue|Number of reviews|% of reviews|
|---|---|---|
|A: The review is not specific enough|272|2.16|
|B: Review heuristics such as “not novel”, “not surprising”, “too simple”, “not SOTA”|678|5.38|
|C: The scores do not match the review text|448|3.55|
|D: The review is rude/unprofessional|213|1.69|
|E: The review does not evince expertise|542|4.3|
|F: The review does not match the paper type|98|0.78|
|G: The review does not match the type of contribution|152|1.21|
|H: The review is missing or too short|205|1.63|
|I: The review was late|12|0.1|
|J: Other|162|1.29|

Table 3: Review issue statistics

Anonymized fashion (as R1, R2, R3). The ACs were provided with instructions and suggested starter message template.

In total, out of 4559 direct submissions to ACL, 4069 had received reviews, and for 2901 out of those the ACs initiated discussions. In total, ACL review process generated 8553 messages (3879 by the ACs). However, only 2107 discussions (72.63%) had at least one response from at least one reviewer. Somewhat consistently, the discussions were overall initiated by 77.4% of all ACs. We conclude that both AC and reviewer involvement have room for improvement.

We reviewed one case of a strong paper that ended up being rejected. The AC could have been persuaded by a “champion” reviewer, and there was one such expert in the set who was surprised by the final outcome—but they did not engage in the forum discussion. We followed up with the reviewer, and they explained that since their review was already positive, they did not feel that they needed to be “on the case” anymore. We cannot establish how common this misconception is, but we would urge all reviewers to always read all reviews and author response, and when certain of the merit of a paper—to try to make sure that the AC is convinced.

# 6 Improving decision support for the chairs

In addition to the efforts for improving the quality of peer review (§5), we implemented the following steps for facilitating the decision support by ACs and SACs: revised SAC and AC guidelines (§6.1), guidance for assignment checking (§6.2), match rationales (§6.3), Soundness/Excitement scores (§6.4).

# 6.1 Updated SAC and AC guidelines

We updated the SAC/AC guidelines that we received from the program chairs of ACL’21 in following ways. We reformatted it to Markdown to utilize the ecosystem of GitHub (e.g., version control, asynchronous collaboration among PCs, automated deployment). The guides were built by Sphinx 13 with MyST extension 14, which enables to use Markdown and variables (making it easy to keep the consistency of dates and external URLs between SAC and AC guides and for the future chairs to adapt to their timeline). We also adjusted the existing instructions and created new instructions to incorporate everything we developed, from the new reviewer guidelines to guidelines for making recommendations. We shared the guides before the review process so that SACs and ACs can be prepared for the tasks and workloads.

Feedback. 83.3% SACS and 90.3% ACs rated the clarity of instructions at 3/4 or 4/4. Some of the free-text comments indicated a preference for shorter guidelines, but since the process is complex, and the guidelines need to serve both new and experienced chairs, there are limits to how much they can be shortened.

# 6.2 Support for checking assignments

As mentioned above, the usual workflow in large conferences is that the assignments are made automatically based on affinity scores between candidate reviewers’ publication history and submissions. Usually, the automated assignments are then shown to the ACs and SACs to check manually, but this is very difficult in practice: SACs cannot process such a large volume on their own, so they need to rely on ACs. But ACs, at least on START, do not have access to the list of possible reviewers together with their current number of assignments and all their COIs, which means that even if they spot an error—it is difficult for them to identify and recommend an available alternative. Providing the up-to-date quota and COI information on all reviewers in track to the ACs is not possible in the current START platform. There are also no detailed guidelines for this step, which means that even if ACs had the reviewer information, everybody would be suggesting alternatives based on different criteria.

In our experience as SACs in previous conferences, although the automated assignments are not perfect, very few ACs actually report the problems or propose alternatives. To see whether this was widespread, we asked our SACs in the exit about whether, in their experience, the ACs asked to check the automated assignments usually recommend many changes. Only 9 of our respondents previously served as SACs in this set-up, but most of them (6/9) concurred with our experience, reporting that ACs adjust very few assignments. When asked why the ACs do not recommend more changes, 33.3% SACs stated that there are no adjustments because the ACs don’t really check, 29.9%—that it happens because the automated assignments are already good enough, 29.2%—because of the difficulty with sharing up-to-date reviewer availability information with them, and 20.8%—that there are no better candidates even if the ACs check. 37.5% indicated that there are also other issues contributing to the ACs not recommending more changes.

We interpret these results as pointing to the fundamental issue of systematically sharing up-to-date reviewer availability information together with their preferences, experience, and profile information, in a way that would make it easy for the ACs to perform such checks and recommend alternatives.

Given that the above factors make it unrealistic to adjust assignments with help of ACs, and that the volume of assignments to check was too large for SACs, we experimented with an alternative approach: since we had the “explanations” for the matches and also the quantitative information about different types of contributions, languages and area keywords, this information would make it possible for SACs to identify the types of submissions most in need of extra checks, and to focus on those. This way the workload would remain manageable, and the SACs would be able to do that while having full access to the latest reviewer availability data. To assist in this process, we developed Jupyter notebooks with quantitative analysis per track (identifying which keywords, types of contributions and languages were rare and could need extra attention)—as well as reviewer lookup functionality by preferred keywords, languages or types of contribution (or any combination thereof). This solution was better than nothing, but admittedly clunky and could be much improved.

Feedback. 66.7% of SACs stated that they believed selective checking to be overall sufficient given sufficiently strict reviewer pool criteria (although in our specific case not all reviewers in our pool were up to all SAC’s standards).

Caveat: we encountered difficulty with uploading the final automated assignments due to dynamic computation of conflicts-of-interest in START. Because of that, several hundred automated assignments had to be redone manually at the last minute. For the conferences based on START, we strongly recommend that this computation is frozen after the main part of reviewers and chairs are added to the tracks.

# 6.3 Paper-reviewer match rationales

Given the information for the paper-reviewer matches that we had collected (§5.2), we were able to provide the ACs with a list of rationales for each match (except for those reviewers who were added manually by the chairs, and for whom we did not have this information.) A sample “explanation” for a match is shown in Figure 4a. The idea was to provide the AC with not only the general information about the reviewer, but also what are their interests that match this submission. Importantly, we highlighted the cases where the author-stated type of contribution or language was not among the reviewer’s stated.

# Basis for this assignment

Match by track subarea: corpus creation, reproducibility

No match by contribution types. The authors specified: approaches for low-compute settings, efficiency

Match by target language (non-English): French

Most similar paper score: 0.744

Most similar paper:

Reviewer highest degree: PhD

Reviewer affiliation type: Academia

Reviewer publication history: scholar profile

(a) Example of paper-reviewer match rationales. The most similar paper titles directly link to the papers (based on Semantic Scholar). For contributions and languages, the rationales either show the match, or alert to the lack of the match, so that the AC could take that into account.

| |50|60|Area|
|---|---|---|---|
| |50|Contribution| |
| |40| |Language|
| |40|Most similar paper| |
| |30| |Reviewer profile|
| |30| | |
|Percentage|20| | |
| |20|Percentage| |
|10| |10| |
|0| |0| |
|Very helpful|Not helpful|Very helpful|Not helpful|
|Somewhat helpful|Somewhat helpful|Somewhat unhelpful|Somewhat unhelpful|
|Rating (AC)|Rating (SAC)| | |

(b) Chair feedback on which features of the match explanation they found the most useful.

Figure 4: Example explanation for paper-reviewer matches, and AC utility ratings for individual features displayed.

Feedback. This feature received overwhelming support from the chairs: 87.5% SACs and 73.9% ACs rated its utility at 3 or 4 out of 4 (Figure 4b). Among the suggestions for the future improvement, the SACs suggested indicating whether the reviewer was an emergency reviewer, and how late the review was, as well as some elements of reviewer history (e.g. whether they were late for other conferences). The numerical similarity scores were less useful than the titles of the most similar papers. While predominantly the ACs were very positive about easily accessible links to reviewer profiles (Figure 4b), some ACs raised fair concerns about the effect of this feature on reviewer deanonymization: the reviewers are already visible to ACs since they need this information for chasing late reviews, but providing links to reviewer profiles increases the saliency of the reviewers’ identities, and hence may by itself increase bias against, for instance, student reviewers.

# 6.4 Soundness/Excitement scores

While most of the experimental aspects of the ACL 2023 process was focused on matching reviewers to papers more effectively, a larger change visible to authors and reviewers was the introduction of two new scores on the review form to replace the Overall Recommendation that was previously the centerpiece of *CL review forms.

We asked reviewers for two scores: Soundness and Excitement. Our goal was that any sound paper would be accepted to some ACL affiliated venue (i.e., Findings), but that the “main conference” distinction (limited by space) would be focused on the most exciting papers. Our hope was that Soundness, as a more specific rubric with more objective criteria, would be less noisy than a single Overall Recommendation score, which would help reduce the randomness of decisions. The AC guidelines had explicit instructions for how these scores should map to their recommended status.

One more factor motivating our proposal was that the Soundness/Excitement distinction could help with the author-reviewer communication during the author response. When a reviewer points out issues with

15 See our definitions and rubrics for the review form and extra explanation here.

Soundness, the authors generally have a fair chance to clear any misunderstandings or issues with review quality, and the chairs are interested in this kind of discussion. The Excitement, however, is subjective, and the authors do not have a fair chance to convince reviewers that their general views or research agenda are wrong. The Soundness/Excitement distinction helps to focus the response on the Soundness issues, and hence have a more productive discussion.

Feedback. Judging by the exit surveys, this change was overall well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change. 38.1% authors, 35.1% reviewers and 29.9% ACs indicated that while the idea was good, it could be better executed. Among the named issues was the clarity of communication about what these scores meant, the difference in granularity (our scale for Excitement had 9 points, and Soundness only 5), and the wording could be adjusted to remove the semblance to Overall recommendation score. We made these recommendations to the program chairs of EMNLP 2023, who decided to keep this system.

From the communication with the authors who expressed dislike for this system, our impression is that one of the factors here is the mistaken impression that the final decisions are overall based on scores, and the papers with similar scores should be guaranteed the same outcome—whereas in reality the chairs know that scores can be noisy and miscalibrated, and hence the final decisions are made on case-by-case basis, with the full view of the reviews and meta-review, and also taking into account the acceptance quotas and their editorial priorities.16 The Soundness/Excitement scores were rather intended to make it harder for the chairs to just sort by the scores.

# 7 What Factors Contribute to ACL Peer Review Outcome?

Here we present the results of statistical analysis of ACL’23 data, with the goal of explicating what factors contributed to the final decisions and to the quality of individual reviews. We hope that this process both improves the transparency around chair decision-making, and highlights the potential biases and points of improvement for future conferences.

For the new authors, we should explain the general process for the acceptance decisions at ACL’23. First, the reviewers contribute their reviews. At the author response the authors see the reviews and have an opportunity to respond: a process mostly intended to clarify any misunderstandings (we disallowed submitting new results). Then the ACs initiate the reviewer discussion, with the goal to clarify misunderstandings and improve the quality of the reviews. Based on the final reviews and their own expertise, they write the meta-reviews and make recommendations for acceptance (Main track or Findings) or rejection. They are not concerned with the acceptance quotas. Their recommendations and meta-reviews (as well as reviews and author response if necessary) are then considered by the SACs, who have the constraint of the target acceptance quota (which we set at about 22% for the main track and 35% for Findings). Their decisions are based on three main factors: meta-reviews, quotas, and editorial priorities (with case-by-case consideration as needed). If they run out of their quota, they may additionally rank more papers by priority that may be accepted to main/track Findings if there is space (e.g., because some tracks did not use their quota fully). The final step is that the program chairs confirm the SAC decisions, and try to fit in as many papers of the ranked “maybes” as possible. In our case, that resulted in accepting more Findings papers than we originally planned based on prior conferences.

# 7.1 Review Scores: Overall Distribution

We start by exploring the overall distribution of the new Excitement and Soundness scores (described in §6.4) and how they mapped to the three possible decision outcomes (Rejection, acceptance to the Main track, or Findings). Both Excitement and Soundness are ordinal variables, and we use the mean as a rough estimate of the central tendency. Figure 5a shows that for both scores the means are higher for main track than for Findings, and for Findings they are higher than for rejections. For Excitement this is fully in line with our instructions to the chairs. For the main track, this suggests that higher (above 3) Soundness scores16 This is a general problem, and we imagine this would have also happened in the case of an Overall recommendation score. The drawback of the Soundness plus Excitement system is that less noisy decision cutoffs make outliers more salient.

# Figure 5: Soundness and Excitement scores per acceptance status

|Accept−Main|Accept−Findings|Reject|Accept−Main|Accept−Findings|Reject|
|---|---|---|---|---|---|
|5|5| |1500| | |
|4|4| |2000| | |
|3|3| |1000| | |
|Rating|Rating|Count| | | |
|2|2| |1000| | |
|1|1| | | | |
|Main|Findings|Reject|Main|Findings|Reject|
|0|1|2|3|4|5|
|0|1|2|3|4|5|

# Table 4: Coefficients and Standard Errors (SE) for the Multinomial Logistic Regression Model predicting the final acceptance decisions given the mean scores and AC/SAC recommendations.

| |Findings Coeff|Main Coeff|Findings SE|Main SE|
|---|---|---|---|---|
|(Intercept)|-1.48|3.77|0.79|1.43|
|Soundness Mean|0.71|0.76|0.22|0.37|
|Excitement Mean|0.61|0.03|0.23|0.42|
|AC Recommendation (L)|2.66|4.50|0.50|0.94|
|AC Recommendation (Q)|-1.16|-0.05|0.43|0.81|
|AC Recommendation (C)|-0.04|0.10|0.31|0.58|
|AC Recommendation (^4)|0.04|-0.27|0.19|0.37|
|SAC Recommendation (L)|5.84|28.26|0.47|0.71|
|SAC Recommendation (Q)|-1.06|13.59|0.34|0.77|
|SAC Recommendation (C)|1.18|7.82|0.60|0.82|
|SAC Recommendation (^4)|1.52|4.48|0.64|0.74|

# 7.2 Factors Impacting the Final Acceptance Decisions

# 7.2.1 Reviewer Scores and Chair Recommendations

To establish the odds of a paper being accepted into Findings or the Main track vs it being Rejected, based only on reviewer and chair recommendations, we fit a multinomial log-linear model with multinom() function from the NNET package in R (Venables and Ripley, 2002). The dependent variable (DV) is the Outcome coded as a three-layer categorical variable (Main track, Findings, or Reject) with Reject being set as the reference level. The independent variables (IVs) are AC Recommendation (ordinal), SAC Recommendation (ordinal), mean Soundness score (interval), and mean Excitement score (interval). The analysis is performed on the papers submitted directly to the conference as the ARR submissions were reviewed through a different process and had different scores. The model coefficients are shown in Table 4. The model is a good fit for the data with McFadden’s pseudo-R2 of 0.777 (McFadden, 1973).

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

While ordinal regression would be more fit to represent the ordinal order of the possible outcome (Main track > Findings > Reject) we use the multinomial model as it does not have the proportional odds assumption.

Note both, the Excitement and Soundness are ordinal variables. Here, we employ the mean to obtain a rough estimate of the central tendency.

Please note the pseudo-R2 for logistic models cannot be directly interpreted as the proportion of variance explained as in linear models. Nevertheless, the high value observed here signifies a good fit to the data. We also report Cox and Snell.

| | |LR Chisq|Df|Pr(>Chisq)| |
|---|---|---|---|---|---|
| |Soundness Mean|10.88|2|0.0043|**|
| |Excitement Mean|9.67|2|0.0080|**|
| |AC Recommendation|209.71|8|0.0000|***|
| |SAC Recommendation|1438.12|8|0.0000|***|

# Table 5: Type III Analysis of Deviance for Multinomial Logistic Regression in Table 4.17

To obtain the significance values for each IV (Table 5), we use the ANOVA() function in R on the fitted model (Type III Anova). As expected, all four IVs are significant (p&lt;0.05) but at different levels. The SAC Recommendation (χ2(8) = 1438.12, p&lt; 0.001)21 and AC Recommendation (χ2(8) = 209.71, p&lt; 0.001) significantly predict the Outcome with the SAC Recommendation appearing to be a better predictor (as expected, since AC recommendation are made without regards to the acceptance quotas). The mean Soundness score (χ2(2) = 10.88, p= 0.0043) and mean Excitement score (χ2(2) = 9.67, p = 0.0080) are also significant at p&lt;.05.

To establish the exact contributions of mean Soundness and Excitement scores to acceptance decisions for the Main track and Findings, we can look at Table 4 again. Note that since it is a multinomial regression model, the coefficients indicate an increase in log odds rather than directly interpretable odds (for which the coefficients need to be exponentiated). The “Findings Coeff” and “Main Coeff” correspond to the log-odds of being accepted into the Findings and Main track as opposed to being rejected.

# Soundness

In the case of the mean Soundness score the coefficient is positive for both Findings (0.71) and the Main track (0.76). This means that for one unit increase in the mean Soundness score the log-odds of being accepted as opposed to being rejected increase by 0.71 for Findings and 0.76 for the Main track. By taking the exponential of these values, we see that for one unit increase in the mean Soundness score the odds to be accepted increase 2.03 times for Findings and 2.14 times for the Main track.

# Excitement

Similarly, both coefficients are positive for the mean Excitement score for both Findings (0.61) and the Main track (0.03). This means that for one unit increase in the mean Excitement score the log-odds of being accepted vs rejected increase by 0.61 for Findings and 0.03 for the Main track. By taking the exponential of these values we see that for one unit increase in the mean Excitement score the odds of being accepted increase 1.84 times for Findings and 1.03 times for the Main track. While the values are still positive, this increase is much lower22 than for the mean Soundness scores, especially for the Main track. The overall distribution of these scores per acceptance status is shown in Figure 5b.

# AC Recommendations

Since AC Recommendation is an ordinal variable, it is coded using polynomial contrast, so the L indicates linear effect, Q a quadratic effect, C a cubic effect, and so on. Here we look mostly at the linear effect since it has a direct (linear) effect on the outcome. We see that both coefficients are positive, indicating that with an increase of one unit, the log-odds of being accepted vs being rejected increase by 2.66 units for Findings and 4.50 units for the Main track. By taking the exponential of these values we see that one unit increase in AC Recommendation corresponds to a 14.30-fold increase in the odds of being accepted into Findings (vs being rejected) and 90.02-fold increase in the odds of being accepted into the Main track (vs being rejected).

# SAC Recommendations

SAC Recommendation is also an ordinal variable, hence we see the same types of coefficients. However, the magnitude of the SAC’s decision appears to be much greater with a greater effect on the final outcome. With one unit increase in SAC Recommendation the log-odds of being accepted vs being rejected increase by 5.84 units for Findings, and 28.26 units for the Main track.

pseudo-R²=0.794 (Cox and Snell, 1989) and Nagelkerke pseudo-R²=0.913 (Nagelkerke, 1991).

21χ2 denotes likelihood ratio chi-square statistic.

22This latter finding seems counter-intuitive, given that our AC guidelines stressed that Findings is a venue for all sound work, while “sound & exciting” would be the basis for recommendations to the main track—but even among the papers accepted to the main track 39% have at least one “negative” Excitement score (Figure 7b). At the same time, even among the Findings papers, only 49% have predominantly negative Excitement ratings, so there is a preference for at least some Excitement. This could be related to the confusion about the meaning of the scores in the initial iteration (see subsection 6.4).

|Paper Type|LR Chisq|Df|Pr(>Chisq)| |
|---|---|---|---|---|
|12.47|2|0.0020|**| |
|Review Issues|43.61|2|0.0000|***|
|Preprinted|47.96|2|0.0000|***|
|Previous Submissions|4.38|2|0.1120| |
|Languages Number|0.57|2|0.7528| |
|Languages not only English|3.53|2|0.1711| |
|Contribution: Efficiency|1.18|2|0.5540| |
|Contribution: Resource|4.34|2|0.1139| |
|Contribution: Reproduction|16.59|2|0.0002|***|
|Contribution: Theory|7.70|2|0.0213|*|
|Contribution: Software|19.62|2|0.0001|***|

Table 6: Type III Analysis of Deviance for Multinomial Logistic Regression, predicting submission Outcome (Main, Findings, Reject) conditioned on the variables listed in the table.24

Converting these values to their exponentials, we see that one unit increase in SAC Recommendation corresponds to a 343.78-fold increase in the odds of being accepted into the Findings (vs being rejected) and a massive increase of 1.88 × 1012 for the odds of acceptance into the Main track (vs being rejected). The model hence shows that the SAC recommendation is a much stronger predictor than the AC recommendation, which helps to explain why it is possible for a paper to be rejected even with a positive meta-review. AC recommendations are made without regards to the acceptance quotas, and SACs necessarily have to override them in many cases.

# 7.3 The Impact of Other Submission Properties

There are many properties of submissions that could systematically make a difference to their final outcome. In this section we investigate the possible effect of the type of contribution, the target languages, whether the reviews were problematic (as reported by the authors), and whether the paper was available as a preprint. To establish the importance of these factors, we fit another multinom() model, similarly to what we did in Table 4, and obtain the significance levels for each variable using Type III Anova. While the ordinal model would potentially better preserve the natural order of the final outcome (rejection being the worst and acceptance to the main track being the best outcome), the fitted model violated the assumptions of the ordinal model.

Since this model does not include strong predictors such as reviewer scores and chair recommendations, the fit of this model is relatively poor23 compared to the model in Table 4, which has a McFadden’s pseudo-R2 of approximately 0.80 (indicating a substantial improvement over the null model). In contrast, this model has a McFadden’s pseudo-R2 of approximately 0.01, suggesting that it barely improves upon the null model. Nevertheless, this model can still be used to establish the individual contributions of the submission-level properties, which likely interact in complex ways in the scores and recommendations. Statistically significant factors are also not necessarily strong predictors by themselves.

The results of this experiment are shown in Table 6. According to this analysis, the following factors have a statistically significant impact on submission outcome: low-quality reviews, preprinting, short/long paper type, and three types of contributions (software, reproduction, and theory).

To also assess the relative importance of our predictors in forecasting the final outcome, we employed a Random Forest algorithm (Liaw and Wiener, 2002). The results are shown in Figure 6. The most crucial predictor was Review Issues (i.e., author complaints about reviews25) with a Mean Decrease Gini value of 46.09. This suggests that this predictor played the most significant role in reducing the Gini impurity, and therefore, in improving the precision of our model. The second factor with the biggest Mean Decrease Gini is Preprinting (22.84). This analysis does not state the absolute importance of any factor (e.g., that

23 Its 3-class accuracy is 52%, vs 90% for the model shown in Table 4. This is the accuracy of the model on the withheld test set when the model is fitted with 70% of the data. The accuracy of the model on all data is about 1% higher.

24 Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

25 The number of author complaints likely reflects (at least) two factors: the reviews that were truly problematic, and simply negative reviews since the authors are more likely to complain about those. In the latter case the leading cause for rejection is the negative review.

# Variable Importance

# Review issues

# Preprinted

# Contribution software

# Languages number

# Contribution resource

# Significance

# Paper Type

Not Significant

# Previous Submissions

Significant

# Contribution reproduction

# Contribution efficiency

# Contribution theory

# Languages not only english

|0|0|0|0|0|10|20|30|40|
|---|---|---|---|---|
|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|Mean Decrease Gini|

Figure 6: The importance of predictors in predicting the Outcome, ranked by mean decrease in Gini impurity. Predictor significance is indicated by color, with dark purple for not significant and dark green for significant predictors as per levels of significance indicated in Table 6.

# Contribution type

|Contribution type|% submissions|Match|Mismatch|Match-Mismatch|
|---|---|---|---|---|
|Efficiency|9.62|50.27|46.56|3.71|
|NLP engineering experiment|61.5|46.66|47.33|-0.67|
|Software and pre-trained models|12.14|56.75|45.56|11.19|
|Data resources|19|49.25|46.37|2.88|
|Data analysis|10.48|48.14|46.78|1.36|
|Reproduction studies|2.08|66.25|46.51|19.74|
|Approaches for low-resource settings|18.22|49.79|46.28|3.51|
|Surveys|1.64|44.44|46.96|-2.52|
|Interpretability|25.29|51.8|45.27|6.52|
|Theory|3.8|56.85|46.53|10.32|
|Position papers|2.57|53.54|46.74|6.8|

Table 7: Acceptance rate among direct submissions that were reviewed and considered for acceptance, with (Match) and without (Mismatch) given contribution types. The average acceptance rate in this pool is 46.92%. Preprinting increases the chances of acceptance by X%), and we are not claiming that these effects are independently large—but they do appear to be statistically significant. We will discuss these factors further: short/long papers in §7.3.1, contribution types in §7.3.2, review issues in §7.5.5, preprints in §7.5.7.

# 7.3.1 Short/long papers

Short papers have had significantly lower acceptance rates at most recent *ACL conferences. To mitigate that, we highlighted the problem in the reviewer instructions, had a separate Soundness formulation for short papers, and asked the SACs to consider the short and long papers separately, with their own target acceptance quotas. Despite all that, the significant effect of paper type (Table 6) is obvious: the long papers had 23.50% acceptance rate to main track vs 16.53% for short, and for Findings, the rate was respectively 41.89% vs 35.58%. The core reason seems to be that the source reviewer scores are systematically lower, despite all calls to not expect 120% thoroughness of short papers.

# 7.3.2 Types of contribution

We were pleasantly surprised to find a significant positive effect for the contributions of theory, reproductions, and pre-trained models and software (Table 6). The two latter types are in line with the findings by (Magnusson et al., 2023) who report that reproducibility efforts are rewarded. This effect is also visible from simply considering the differences in acceptance rates for papers with and without these contribution types, shown in Table 7. In fact, the “average” acceptance rate of 46.92% is the closest to the most “mainstream” type of contribution (NLP engineering experiment, 61.5% submissions) – and all other contribution types except surveys have the acceptance rate at least slightly higher than that.

|Submissions subset|Contribution type|% submissions|Match|Mismatch|Match-Mismatch|
|---|---|---|---|---|---|
|Resources & Evaluation|Resource|5.48|48.39|48.21|0.18|
|All tracks without Resources & Evaluation|Resource|94.52|49.48|46.34|3.14|
|Interpretability and Analysis of Models|Interpretability|4.89|52.69|57.14|-4.45|
|All tracks without Interpretability|Interpretability|95.11|51.61|45.18|6.43|

Table 8: Acceptance rate among direct submissions inside and outside tracks that targeted a resources and interpretability contributions, with (Match) and without (Mismatch) given contribution types. The average acceptance rate in this pool is 46.92%.

| | |Accepted papers only|Rejected papers only|All papers| | | |
|---|---|---|---|---|---|---|---|
| |%|α[CI]|%|α[CI]|%|α[CI]| |
|Ordinal|Soundness|20.72|0.093[0.047,0.137]|17.68|0.116[0.076,0.156]|19.10|0.318[0.294,0.340]|
|Ordinal|Excitement|12.68|0.120[0.075,0.169]|10.65|0.134[0.094,0.173]|23.23|0.311[0.287,0.334]|
|Categorical|Soundness|77.28|0.032[−0.052,0.112]|37.39|0.092[0.064,0.119]|53.80|0.221[0.194,0.248]|
|Categorical|Excitement|37.11|0.087[0.055,0.120]|49.60|0.074[0.039,0.114]|43.74|0.233[0.212,0.255]|

Table 9: Inter-reviewer agreement on soundness and excitement scores, measured as raw % agreement (%) and Krippendorff’s alpha (α) with 95% confidence interval [CI]. We consider only direct submissions to ACL’23 that were fully reviewed, and for which the final decisions were made: 3847 in total, 1805 “accept” (to either Main track of Findings), and 2042 “reject”.

The lack of a visible disadvantage in acceptance rates for non-mainstream types of contributions is a very positive finding. Consider the case of efficiency-oriented papers: they did not have a dedicated track, but their acceptance rate was not lower (and even a bit higher) than for the average in the pool (where the majority of engineering-oriented submissions focuses on performance). In effect, every track was an efficiency track, allowing both access to the area expertise and reviewers with interest in this type of contribution. We cannot establish to what extent this is due to Area-Contribution-Language matching or an overall increased interest in the need for efficient NLP solutions. But as long as such contributions are in the minority, we would recommend ensuring the matches by this criterion.

A complication for our analysis arises for two contribution types that also had large associated tracks: resources and interpretability. In this case, it is possible that the lack of difference in acceptance rate is due to the extra effort of ensuring the reviewers with matching interests through the track mechanism. To check for that, we compare the acceptance rates for these types of contributions inside and outside of the dedicated tracks (Table 8). We find that in all cases the match between tracks and contribution types yields a 3-6% increase above the average acceptance rate of 46.92%. An interesting case is interpretability and model analysis, which has a 4.45% higher acceptance rate outside of its dedicated track (probably indicating an appreciation for papers that perform analysis in addition to some other type of contribution).

# 7.4 How Much do ACL Reviewers Agree?

The issues with consistency of peer review were recently highlighted in the ML community by the two NeurIPS experiments (Price, 2014; Cortes and Lawrence, 2021; Beygelzimer et al., 2021). By treating peer review as an annotation problem (Rogers and Augenstein, 2020), we can apply the existing methodology for analyzing inter-annotator agreement (IAA). We consider three reviewers (annotators) per paper, discarding the rare cases of 4 reviews (from emergency assignments). We compute Krippendoff’s α (Krippendorff, 2011) on the Soundness and Excitement scores (Table 9). We treat these scores as ordinal data. We also experiment with mapping both scores to binary “positive/negative” categories (3–5 > “sound" for Soundness and 3.5–5 > “exciting” for Excitement, since the borderline scores were 2 for Soundness was 2 and 3 for Excitement).

“Ordinal” refers to the α coefficient computed using raw scores treated as ordinal variables. The percentage agreement for Soundness was computed using the raw scores (5-point scale). In order to match the scale length the percentage agreement for Excitement was computed on the rounded scores (i.e., 3.5 was treated as 4.0, etc.). “Categorical” denotes scores converted into either positive or negative decisions based on the given threshold (3.0 for Soundness and 3.5 for Excitement).

# Findings

| |Excitement| |Soundness| | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|
|Reject|7%|17%|24%|27%|18%|6%|11%|44%|39%|5%|
|Findings|18%|29%|27%|16%|6%|35%|49%|13%| | |
|Main|6%|39%|31%|15%|5%|8%|59%|30%|4%| |

0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00

# (a) Ratio of review scores per acceptance status

| |Excitement| |Soundness| | | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|---|
|Reject|3%|13%|37%|47%|22%|38%|29%|11%| | | |
|Findings|11%|40%|37%|13%|65%|30%|5%|− − −|− − +|+ + −|+ + +|
|Main|48%|39%|12%| |88%|11%| | | | | |

0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00

# (b) Ratio of positive (+) and negative (-) score combinations per acceptance status

|Soundness|+ + + + + - - + - - -|556|33|7|0|600| |
|---|---|---|---|---|---|---|---|
| |+ + +|500| |+ + +|400| | |
| |457|461|246|23|400| | |
| |+ + -| |300| | | | |
| |631|296|36|7| | | |
|Excitement|200|181|339|354|205|100| |
| |- - -| |65|46|11|2|100|
| | |0| | | | | |

# (c) Total number of submissions with different combinations of positive (+) and negative (-) scores

# (d) Number of accepted submissions with different combinations of positive (+) and negative (-) scores

Figure 7: Review scores vs acceptance outcome. “Positive” scores (+) refer to the above-borderline scores (Soundness >=3, Excitement >=3.5), and “negative” (-) - to the number of scores below borderline.

Consistent with the general perception of inconsistency in peer review, α shows a level of IAA that seems far too low (the rule of thumb is that “substantial” agreement is in the range of 0.6-0.8 (Artstein and Poesio, 2008; Paun et al., 2022)). However, the raw agreement for the accepted papers (in the categorical view, i.e. as sound/unsound, exciting/unexciting) is almost twice higher for Soundness than for Excitement. We interpret this as an indication that although the scores are still noisy, it helps to ask more specific questions with more objective criteria. The much lower raw agreement on the Excitement is also in line with our point that this is overall a less relevant direction for the author response and reviewer discussion. Arguably we do not even want a high agreement on Excitement: everybody interested in the same thing could indicate that the field is ossifying and stagnating.

As a sanity check, we also analyzed IAA for the raw reviewer scores of EMNLP 2022 and EACL 2023. Both of these conferences used a single “overall recommendation” score, formulated differently for short and long papers. In EMNLP 2022, for 3092 observations for 3 reviewers (discarding R4 data), with scores treated as ordinal data, we got α 0.316 for the short papers, 0.31 for long, and 0.318 for the whole distribution – which is almost exactly the same as our α for both our scores (in the ordinal case). In EACL 2023, for 1121 subjects for 3 reviewers we got α 0.317 for the short papers, 0.34 for long, and 0.348 for the whole distribution.

A related question is “what kind of disagreements do we actually have?” Figure 7a shows the distribution of individual score values for all papers in a given acceptance status, which suggests that even papers accepted to the main conference had some very negative reviews. Figure 7b breaks down the scores into “positive” (Soundness >= 3, Excitement >= 3.5) and “negative”, and considers the combinations of three reviews as “all positive” (+ + +), “all negative” (- - -), “2 positive, 1 negative” (+ + -) and “2 negative, 1 positive” (- - +). We can see that despite disagreements on the exact scores, the papers accepted to the main track have a high ratio of “positive” review combinations for Soundness (88%, only 11% papers with one negative Soundness score). But for Excitement our SACs accepted to the main track 39% papers with one negative Excitement score, and 37% papers with a single “champion” reviewer. For Findings, they even accepted 37% papers which only 1 reviewer was excited about. Figure 7c shows the total number of submissions with various combinations of positive and negative Soundness and Excitement scores, and Figure 7d shows the same categories, but with the number of accepted papers with that score combination. Our data indicates that despite noisy scores and high disagreement, the mechanism of ACs and SACs does “rescue” many papers with one negative review, and at least the raw agreement does improve for the more specific Soundness score. Judging by the community feedback (§5), in this first implementation there was a lot of confusion about what the scores meant, and we expect that in future iterations the agreement could improve further.

# 7.5 Analysing Reviews and Review Scores

In this section, we take a step back from the final acceptance decisions and look only at the individual reviews and their scores, rather than the final outcome of the submission.

# 7.5.1 Do the Area-Contribution-Language matches impact reviewer scores?

To answer this question, Figure 8 shows the distributions of the individual reviewer scores for Soundness, Excitement, reviewer Confidence, and Reproducibility for all cases where the reviews were or weren’t matched by the area, contribution type, or language. The biggest visible impact is in reviewer Confidence, where the contributions are not matched by area: the ratio of reviews with high scores (4+) is decreased by about 14%. A worrying observation is that there is a 5% increase in high Confidence scores for the submissions where the reviewer is not matched by language and could be expected to feel less rather than more confident. We also observe an 11% increase in Soundness ratings 3+ from reviewers matched by language vs those mismatched, and 7% in Reproducibility.

# 7.5.2 Do the Area-Contribution-Language matches impact the reviewer activity?

To establish whether Area-Contribution-Language matching had any effect on reviewer activity, we counted the reviewers as “active” if they had at least one forum message or more than one review edit. The distributions of active/inactive reviewers that are/aren’t well-matched to submissions by Area-Contribution-Language criteria are shown in Figure 9. At a glance, there are a lot more matched & active reviewers, but since generally a lot more reviewers were matched than mismatched (see Table 2), we would generally expect that to be the case even by chance.

To establish whether there are any statistically significant effects, we first fit a generalized linear model (GLM) using the glm() function in R. The dependent variable was binary (the activity of the reviewer). The predictors were a contribution match (binary variable), a studied language match (three-layer categorical variable), and an area match (binary variable), all of which were treated as categorical variables (at least one matching keyword of the correct type). The link function was logit, corresponding to a binomial distribution of the response variable (logistic regression).

To validate the assumptions of the GLM, we examined the variance inflation factors (VIFs) using the vif() function in R to assess multicollinearity among predictors. The VIFs were all close to 1, suggesting that multicollinearity was not a concern. We also visually inspected residual plots to assess the model fit and did not find any obvious deviations from homoscedasticity or linearity.

For the language we consider three categories: (1) non-English language match, (2) non-English language mismatch, and (3) match only by English; under the assumption that all reviewers will be familiar with English.

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

# Match area

# Match contribution

# Match language

|True|2%|27%|42%|26%|3%|
|---|---|---|---|---|---|
|True|3%|27%|41%|25%|3%|
|English|2%|28%|42%|25%|3%|
|Soundness| |True| |1| |
| |3%|24%|44%|26%|3|
|False| |False| |3| |
| |3%|29%|40%|25%|3%|
| |2%|27%|41%|26%|4%|
|False|4%|22%|34%|33%|7%|
| |4| | | | |
| |0%|25%|50%|75%|100%|

# (a) Soundness

# Match area

# Match contribution

# Match language

|True| |True| |English|17%|22%|22%|19%|12%| |
|---|---|---|---|---|---|---|---|---|---|---|
| |16%|22%|22%|19%|12%| |1| | | |
|True| |2| |15%|21%|22%|21%|12%| | |
| | | |3| |False|19%|22%|19%|22%|10%|
|False|17%|21%|21%|21%|12%|False| |3.5| | |
| | |16%|18%|16%|18%|17%|8%| |4| |
| | | |4.5| |5| | | | | |
| |0%|25%|50%|75%|100%| | | | | |

# (b) Excitement

# Match area

# Match contribution

# Match language

|True|11%|55%|29%|5%| | | |
|---|---|---|---|---|---|---|---|
|True|11%|54%|29%|6%| | | |
| |English|11%|54%|29%|6%| | |
| | | |Rev. Conf.| |True| |1|
| | |8%|54%|32%|6%| | |
|False|7%|45%|35%|11%| | | |
|False|11%|53%|28%|7%| | | |
|False| |14%|53%|27%|5%| | |
| |0%|25%|50%|75%|100%| | |

# (c) Reviewer confidence

# Match area

# Match contribution

# Match language

|True|12%|46%|30%|10%| |
|---|---|---|---|---|---|
|True|12%|46%|31%|10%| |
|English|13%|46%|30%|9%| |
| |Reproducibility| |True| |1|
| |10%|47%|31%|10%| |
|False|13%|45%|30%|10%| |
|False|13%|49%|27%|9%| |
|False|11%|39%|30%|18%| |
| |0%|25%|50%|75%|100%|

# (d) Reproducibility

Figure 8: Area-Contribution-Language Matches impact on reviewer scores. In each plot, True/False refers to the reviews where the submissions were/weren’t matched by area, contribution or language.

The results of the GLM (see Table 10) suggest that contribution match is a significant predictor of the reviewer’s activity (β = 0.16, SE = 0.08, z = 1.97, p = 0.048). Since the estimates relate to log-odds we consider the exponential of the reported value (1.178) which suggests that the odds of the reviewer being active when the contribution type is well-matched are 1.178 times higher than when the contribution does not match the reviewer’s expertise. The remaining variables, that is language match and area match, are not significant predictors in this model (p > 0.05).30

Finally, we considered the language match as a binary variable, excluding English language papers. We conduct a Chi-square test (χ2) to examine the association between the language match (excluding English) and reviewer activity Table 11. The test reveals no significant association between the language match and reviewer activity (χ2(1)=0.73432, p = 0.3915). The chi-square test was performed using Pearson’s Chi-squared test with Yates’ continuity correction with the chisq.test() function in R.

We conclude that of the Area-Contribution-Language matching rubrics, only the contribution type contributes to improvement in reviewer activity. Although the effect is modest (1.178 times increase in likelihood of reviewer activity), given that reviewer activity post-submission is very important, and its level needs to be improved (§5.4), we would urge the future chairs to consider this criterion in the assignments. It also provides a quick and interpretable way to consider the variety of the types of work.

30McFadden‘s pseudo-R2 of the model is 0.0008231973, which is very low. This suggests that our model does not explain much of the variability in the data. However, it is important to note that in the context of generalized linear models, the interpretation of pseudo-R² is not as straightforward as it is in ordinary least squares regression. The pseudo-R² is not necessarily a measure of the proportion of variance explained by the model in the data. Instead, it is a measure of the likelihood improvement per observation relative to the null model. Despite the low pseudo-R², our model could still provide valuable insights into the relationships between the independent variables (match type) and the reviewer‘s activity.

# 7.5.3 Do reviewer confidence scores reflect their experience?

START profiles contain self-reported reviewer experience labels (“never”, “first time”, “3 or fewer events”, “4 events and more”. We explored the relationship between this data and reviewer Confidence scores but found no strong effect. We do observe a small (about 4%) increase in the volume of 4+ Confidence scores for the most experienced reviewers, and it’s significant according to the ordinal logistic regression model. But the effect is quite small, and judging by this data we don’t recommend relying on confidence as a proxy for reviewer experience. Moreover, we observe no relation between this reviewer experience data and the number of review issues reported by the authors. This is a rather depressing finding from the perspective of reviewer training, and we hope that it is rather due to START profiles not being updated by the reviewers.

# 7.5.4 Do the reviewer scores correlate with length of the reviews?

The ACL review form had the following text input fields: summary, reasons to accept, reasons to reject, questions to the authors, missing references, suggestions&typos, and confidential notes to the chairs. We roughly estimated the length of these inputs by splitting on the whitespace, and computed Spearmans correlation (Spearman, 1987) between these variables and reviewer scores for Soundness, Excitement.

# Figure 9: Area-Contribution-Language matches vs reviewer activity.

| |Reviewer Active|FALSE|TRUE| | | | | |
|---|---|---|---|---|---|---|---|---|
|8000|8009|8000|8243|8000|7978| | | |
|6000| |6000| |6000| | | | |
|4000| |4000| |4000| | | | |
|Count|2446| |2468|2409| | | | |
|2000|239|860|217|626|107|318|169|573|
|0| | | | | | | | |

# Table 10: Generalized linear model (GLM) estimates for predicting reviewer activity using match categories.

| |Estimate|Std. Error|z value|Pr(>|z|)|
|---|---|---|---|---|
|(Intercept)|1.1511|0.1012|11.38|0.0000 ***|
|Match Contribution (True)|0.1638|0.0830|1.97|0.0484 *|
|Match Language (False)|-0.1076|0.1142|-0.94|0.3461|
|Match Language (True)|0.0114|0.0921|0.12|0.9015|
|Match Area (True)|-0.1151|0.0786|-1.46|0.1432|

# Table 11: Results of Pearson’s Chi-squared test with Yates’ continuity correction for the effect of language match (excluding English) on the reviewer’s activity

|Test|Chisq|df|p-value|
|---|---|---|---|
|Pearson’s Chi-squared (Yates’ correction)|0.73432|1|0.3915|

# Confidence

|Confidence|1|Soundness| | | | | | | | |
|---|---|---|---|---|---|---|---|---|---|---|
|Soundness|-0.08|1|Excitement| | | | | | | |
|Excitement|-0.11|0.68|1|Reproducibility| | | | | | |
|Reproducibility|0.09|0.29|0.26|1|Reasons to Accept| | | | | |
|Reasons to Accept|-0.02|0.28|0.37|0.15|1|Reasons to Reject| | | | |
|Reasons to Reject|0.14|-0.36|-0.35|-0.11|0.17|1|Questions| | | |
|Questions|0.03|-0.01|0.04|-0.02|0.12|0.06|1|References| | |
|References|0.15|-0.08|-0.08|-0.01|0.03|0.15|0.14|1|Suggestions| |
|Suggestions|-0.03|-0.04|0.03|-0.03|0.14|0.14|0.27|0.16|1|Confidential|
|Confidential|0.01|-0.03|-0.03|-0.04|-0.01|0.04|0.04|0.05|0.09|1|

0.05).">

Confidence, and Reproducibility. The results are shown in Figure 10. As could be expected, we observe a significant negative correlation (-0.35-0.36) between the length of Reasons to Reject and both Soundness and Excitement scores, and the opposite trend for the Reasons to Accept (0.28-0.37). Interestingly, the length of Reasons to Accept also correlates positively with the Reproducibility score, indicating that the community appreciates this factor (0.15). Confidence has a similar correlation with the length of missing references. Finally, there is a high correlation between the length of “questions to the authors” and “suggestions”, indicating that the reviewers who engage with the submission deeply use both of these fields.

The highest positive correlation is between our Soundness and Excitement scores32 (0.68), which is in line with the intuition that unsound work would probably not be found exciting either.

# 7.5.5 What factors are associated with review issues?

As discussed in §5.3, we introduced a mechanism for the authors to flag specific types of issues with reviews, and we received such flags for 12.9% of the reviews. Figure 11 shows the ratio of reviews with complaints (True) and without (False). For both Soundness and Excitement there is a clear trend towards more complaints with lower scores, but there are also complaints for high scores (e.g., 43.1% of reviews which the authors complained about had Soundness 4). This makes more sense if we consider the figure Figure 11d, which shows that 95% complaints are made about reviews where at least one of the scores is 3 or less. This suggests that reported review issues are associated with negative reviews, even for Excitement (although we tried to make it clear that this score is subjective and does not need arguing).

To explore other possible factors that could make the reviews more likely to be reported we fit a GLM model using the glm() function in R. The dependent variable is the presence or absence of reported issues (binary variable), and the predictors are the Excitement score (ordinal), Soundness score (ordinal), Confidence score (ordinal), Reproducibility score (ordinal), length of Reasons to Reject (interval), length of Reasons to Accept (interval), the Contribution Match (binary), Area Match (binary), Language Match (three-layer factor), Reviewer’s Experience (three-layer factor), and Reviewer’s Activity (binary). The link

This finding is important for the model reported in Table 4: the acceptance decisions are indeed based on both factors, and they are meant to capture different information, but the high correlation between these two variables suggests that the estimates obtained in Table 4 should be interpreted with caution.

# Reviewer scores vs the amount of issues reported with reviews

| |TRUE|FALSE|
|---|---|---|
|Soundness|9%|2.9%|
| |43%|29.9%|
| |42.3%|41.2%|
| |5.5%|23%|
| |4|2.9%|

Review Issues

0% 25% 50% 75% 100%

(a) Soundness

| |TRUE|FALSE|
|---|---|---|
|Excitement|3.8%|2.6%|
| |12.9%|18.3%|
| |25.5%|23.5%|
| |27.7%|21.6%|
| |19.8%|18.2%|
| |7.8%|10.8%|
| |2.2%|3.8%|
| |1|3.5|

Review Issues

0% 25% 50% 75% 100%

(b) Excitement

| |TRUE|FALSE|
|---|---|---|
|Confidence|11.8%|10.5%|
| |53.9%|53.7%|
| |28.8%|29.5%|
| |4.5%|5.8%|
| |4|4|

Review Issues

0% 25% 50% 75% 100%

(c) Reviewer confidence

| |TRUE|FALSE|
|---|---|---|
|Lowest Score|3%|16%|
| |29%|10%|
| |13%|33%|
| |43%|10%|
| |5%|24%|
| |6%|2%|
| |1|3.5|

Review Issues

0% 25% 50% 75% 100%

(d) Min(Soundness, Excitement)

function was logit, corresponding to a binomial distribution of the response variable (logistic regression).

The coefficients of the fitted model are presented in Table 12.

We further employ the type III Anova using the ANOVA() function in R in order to obtain significance levels for each factor which are presented in Table 13. While McFadden’s pseudo-R2 of the fitted model is only 0.067, several variables of this model are significant predictors of the review issues.

The most significant factors are Soundness, Excitement, and the length of Reasons to Accept. All of these variables have a negative relationship with the reviewer issues, perhaps unsurprisingly, with higher scores the review is less likely to be reported. Similarly, longer text in the Reason to Accept field leads to less chance of the review being reported. Counter-intuitively, the positive coefficient associated with the reviewer being active suggests that when the reviewer is active (i.e. with at least one review revision or a forum message) the log-odds of the review issue increase by about 0.32, all else being equal. That is, the more active reviewers (putting in more effort) are actually receiving more complaints.

Other significant factors are Language Match and the reviewer’s confidence; both associated with negative coefficients. This suggests that when the reviewer is familiar with the non-English language investigated in the study, the log-odds of a review issue decrease by approximately 0.26 (i.e., the review is 1.29 times less likely to be flagged for issues). Similarly, the negative coefficient of the reviewer’s confidence suggests that...

We inspect the residuals plots and compute the variance inflation factor to assure that the assumptions of GLM are not violated.

Signif. codes: ‘p < 0.001’ ‘***’, ‘p < 0.01’ ‘**’, ‘p < 0.05’ ‘*’, ‘p < 0.1’ ‘.’, ‘p > 0.1’ ‘ ’.

|Estimate|Std. Error|z value|Pr(> |z|)| |
|---|---|---|---|---|
|(Intercept)|0.5999|0.2570|2.334|0.0196 *|
|Soundness|-0.3816|0.0479|-7.967|1.63e-15 ***|
|Excitement|-0.4584|0.0549|-8.349|&lt; 2e-16 ***|
|Confidence|-0.0855|0.0393|-2.176|0.0295 *|
|Reproducibility|0.0609|0.0335|1.816|0.0693 .|
|Reasons to Reject|0.0004|0.0002|1.508|0.1315|
|Reasons to Accept|-0.0052|0.0011|-4.748|2.06e-06 ***|
|Match Contribution (True)|0.0763|0.1148|0.664|0.5066|
|Match Area (True)|-0.1352|0.1030|-1.313|0.1892|
|Match Language (False)|0.0270|0.1476|0.183|0.8550|
|Match Language (True)|-0.2639|0.1275|-2.070|0.0384 *|
|Experience (Experienced)|-0.0744|0.0684|-1.087|0.2769|
|Experience (Zero)|-0.0274|0.1164|-0.235|0.8143|
|Reviewer Active (True)|0.3172|0.0737|4.303|1.69e-05 ***|

Table 12: Coefficients of the Generalized Linear Model predicting the review issues. The table includes the coefficient estimate, standard error, z-value, and p-value for each predictor.

|LR Chisq| |Df|Pr(>Chisq)|
|---|---|---|---|
|Soundness|64.65|1|0.0000 ***|
|Excitement|70.45|1|0.0000 ***|
|Confidence|4.71|1|0.0300 *|
|Reproducibility|3.31|1|0.0688 .|
|Reasons to Reject|2.23|1|0.1353|
|Reasons to Accept|24.17|1|0.0000 ***|
|Match Contribution|0.45|1|0.5035|
|Match Area|1.69|1|0.1940|
|Match Language|4.61|2|0.0998 .|
|Experience|1.24|2|0.5386|
|Reviewer Active|19.35|1|0.0000 ***|

Table 13: Type III Analysis of Deviance for the variables in the Generalized Linear Model predicting whether issues were reported for the given review.

Confidence suggests that with an increased Confidence score the likelihood of the review to be reported decreases though by a small margin.

# 7.5.6 Do we have bad actors?

To explore the possibility that many reported review issues are due to individual unprofessional reviewers, let us consider the fact that 1,620 reviews with reported issues were authored by 1311 reviewers, i.e. about a third of our total pool. But most of these reviewers had more than three reviews, and 1060 of them were only reported once. Of the remaining reviewers, 201 were flagged twice, and 50 reviewers had more than 3 complaints. We conclude that while there are indeed some unprofessional reviewers, and conferences need to systematically share such information and develop a system to address this problem, there are few such cases (6.2% if we consider all reviewers with more than 2 flags, and 1.2% with more than 3 flags). An interesting takeaway from Figure 11c is that the reviews that are problematic according to the authors, do not have lower confidence scores, so these are unlikely to be the new reviewers or the reviewers unfamiliar with the area.

According to folk wisdom, the bad reviewer is usually Reviewer2 (sometimes Reviewer3). We clear their good name: at ACL’23, the most issues were reported for Reviewer1, as shown in Figure 12.

# 7.5.7 Can the reviewers tell who the authors are?

In 567/12606 (4.5%) reviews the reviewers indicated that they have seen the paper, either by seeing a preprint (533) or by other means (34). Additionally, 513 (4.1%) reviewers indicated that they had a good guess of the author identity based on the paper content. 11460 (90.9%) ACL’23 reviews were reported as fully anonymous.

The community “recall” on the preprinted submissions is as follows: we had 628 submissions (13.8%

# 7.5.8 Do preprints affect the peer review process?

Having established that reviewers do have a high recall for preprints (§7.5.7), we investigate the possible connection between the reviewer’s awareness of the author identity on their Soundness, Excitement, and Confidence scores by fitting Cumulative Link Mixed Effect models with the Laplace approximation using the clmm() function for the ordinal package in R (Christensen, 2022). The response variable is the given score and the predictor is the Anonymity answer (fixed effects). We also employ random intercepts for the paper (SubmissionID) and reviewer (ReviewerID) to account for this variability (random effects).

# Soundness

The results of the model fitted for the effect of Anonymity on the Soundness scores are present in Table 14. The Anonymity has five possible values: (1) the reviewer does not know the authors (reference level), (2) the reviewer may know the authors, (3) the reviewer knows the authors via means other than online posting, (4) the reviewer knows the authors via online posting prior to the anonymity period, and (5) the reviewer knows the authors via online posting post to the anonymity period. Estimates for different answers to the anonymity question presented in Table 14 suggest that the reviewers were 1.59 times more likely to assign higher Soundness scores when they thought they may know the authors, and 1.75 times more likely to assign higher Soundness scores when they have seen the preprint online.

# Excitement

The results of the model fitted for the effect of Anonymity on Excitement are present in Table 15. Estimates for different answers to the anonymity question presented in Table 15 suggest that the reviewers were 1.49 times more likely to assign higher Excitement scores when they thought they may know the authors, and 1.73 times more likely to assign higher Excitement scores when they have seen the preprint online.

# Confidence

The results of the model fitted for the effect of Anonymity on reviewer’s Confidence are present in Table 16. Estimates for different answers to the anonymity question presented in the table suggest that the reviewers were 1.29 times more likely to report higher Confidence scores when they.

We validate the model fit by examining residual plots and convergence criteria. The residual plots showed no clear patterns or extreme outliers, and the satisfactory convergence indicates a reasonable model fit. We further observe that, perhaps unsurprisingly, both SubmissionID and ReviewerID account for a substantial portion of the variability in each of the response variables.

We take the exponential of each coefficient.

Figure 12: The number of review issues reported for R1, R2, and R3

|Reported review issues|800|
|---|---|
| |600|
| |400|
| |200|
|0| |
|R1| |
|R2| |
|R3| |

# Table 14: Cumulative Link Mixed Model Results for the effect of Anonymity on the Soundness scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

|Estimate|Std. Error|z-value|Pr(>|z|)| | |
|---|---|---|---|---|---|
|Random effects:| | | | | |
|SubmissionID (Intercept)|2.2427|1.4976| | | |
|ReviewerID (Intercept)|0.7806|0.8835| | | |
|Fixed effects:| | | | | |
|Anonymity (2)|0.46037|0.11744|3.920|8.85e-05|***|
|Anonymity (3)|0.02567|0.41291|0.062|0.9500| |
|Anonymity (4)|0.55947|0.13081|4.277|1.90e-05|***|
|Anonymity (5)|0.36749|0.27565|1.333|0.1820| |

# Table 15: Cumulative Link Mixed Model Results for the effect of Anonymity on the Excitement scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

| |Estimate|Std. Error|z-value|Pr(>|z|)| |
|---|---|---|---|---|---|
|Random effects:| | | | | |
|SubmissionID (Intercept)|1.6675|1.2913| | | |
|ReviewerID (Intercept)|0.5163|0.7185| | | |
|Fixed effects:| | | | | |
|Anonymity (2)|0.39828|0.10629|3.747|0.000179|***|
|Anonymity (3)|0.13179|0.37724|0.349|0.726816| |
|Anonymity (4)|0.54498|0.11816|4.612|3.98e-06|***|
|Anonymity (5)|0.08329|0.24708|0.337|0.736049| |

thought they may know the authors, and 1.80 times more likely to assign higher Confidence scores when they saw the preprinted online.

We thus conclude that submissions with preprints, as well as submissions where the reviewers believe they could guess the authors, systematically receive higher ratings for both Soundness and Excitement, as well as higher Confidence scores. We further note that preprinted papers are disproportionately recommended for consideration for best paper awards (and without such a recommendation from at least one reviewer the submissions are not considered by the best paper committee). In total, only 1.6% papers received any reviewer nominations at all, and for 30% of those papers, the authors had disclosed preprints.

While our data shows the pattern of higher scores, acceptance chances, and best paper nominations for preprinted submissions, the causal mechanism remains a question: is it because such papers are inherently higher quality, or because of the benefits of community feedback they may receive, or because of the well-documented reviewer biases towards famous names and institutions (Peters and Ceci, 1982; Tomkins et al., 2017, among many others)? Since these possibilities necessitate different actions on the part of the chairs who strive for higher-quality program, the causal question needs to be answered for informed policy decisions. Since we observe an increase in likelihood of higher scores both for real preprints and for submissions where the reviewers only thought that they might know the authors (although the effect is smaller in that case), we can conclude that the social factor is definitely present—but more research is needed to establish its exact contribution. But the fact that we only had 13.8% preprints suggests that the current 1-month embargo policy is effective in at least reducing the volume of the problem.

# 8 Special Review Processes

# 8.1 Ethics review

Following the practice started at NAACL 2021, we formed an Ethics Committee (EC) dedicated to ethical issues. The review process was based on work in prior conferences and further developed by ARR and recommendations from the ACL ethics committee. Initially there were 235 technical reviews flagging 218 papers for ethics concerns, and the SACs narrowed down the list based on the guidelines developed by the ethics chairs) to 75 papers, 6 of which did not make it to the ethics review (either withdrawn or cleared).

| |Estimate|Std. Error|z-value|Pr(>|z|)|
|---|---|---|---|---|
|Random effects:| | | | |
| |SubmissionID (Intercept)|0.416|0.645| |
| |ReviewerID (Intercept)|3.413|1.847| |
|Fixed effects:| | | | |
|Anonymity (2)|0.2576|0.1227|2.099|0.0358 *|
|Anonymity (3)|0.4210|0.4194|1.004|0.3155|
|Anonymity (4)|0.5874|0.1342|4.376|1.21e-05 ***|
|Anonymity (5)|0.3413|0.2864|1.192|0.2334|

Table 16: Cumulative Link Mixed Model Results for the effect of Anonymity on the Confidence scores. The reference level is Anonymity (1) (i.e., not knowing the authors).

20 papers under ethics review were labeled accept as-is, 43 received conditional accepts, and 6 were recommended for rejection. Of those recommended for rejection, 1 was accepted nonetheless, 1 was rejected as a result, and 4 were rejected on technical grounds. Of the conditionally accepted ones, 26 were rejected on technical grounds, and 1 was withdrawn. 16 passed the technical review and were conditionally accepted, meaning the ethics issues had to be addressed in the camera-ready version, to be verified by the SAC (based on EC guidance) prior to final acceptance.

The authors of all conditionally accepted papers submitted the camera-ready version and a short response that explained how they had made the changes requested. The SAC double-checked these revised submissions and responses, and confirmed that the ethical concerns had been addressed. As a result, all conditionally accepted papers were accepted to the main conference or Findings.

# 8.2 Best paper selection

ACL’23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding. In total, only 73 papers, i.e. 1.6% of all direct submissions were nominated by the reviewers or ACs for consideration for awards. These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 4 special awards (social impact, resource, reproduction, theme paper), and 39 outstanding papers. The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023.

We encountered several issues with implementing the best paper policy as described in the wiki. With 73 nominated papers, to keep it down to 10 papers per judge and have 2 reviews per paper, we had to recruit 15 judges. At this scale, the workload is compatible with organizing a separate track: recruitment, paper assignments, chasing late reviews – only this time recruiting exclusively very senior and busy people, and it is very important to uphold diversity considerations (which we weren’t able to do full justice). For the future, we recommend that a separate chair role is created for managing this process, similar in scope to the role of the ethics review chairs.

Furthermore, since the diversity considerations in the committee selection entail incompatible time zones, we found it impractical to require the judges to meet and jointly decide on the cases where they disagree (as recommended in the policy). Hence, after the judges cast their votes, the PCs made the final decisions on the basis of their recommendations (in particular, in the cases where one judge recommended outstanding paper and the other recommended not considering it further), we upheld the objections to flaws in the papers, shallowness of analysis, and ethical issues, which left us with 39 papers (a little short of the 1-1.5% total submissions policy target for the outstanding papers).

Finally, the ACL award policy described an Area Chair Award: the award that the SACs of a given track can give to one paper in their track, fully on their own authority. This was part of the guidelines for the final SAC recommendations, but we did not require them to be made at the same time. We sent out reminders after that, but received such nominations from only 12/26 tracks (with the theme.

38 This is only for the direct submissions to ACL. Due to the difficulty of seeing ARR nominations in START, we did not notice the 2 nominations out of 305 ARR submissions until it was too late.

39 We found the agreement on the best paper committee votes to also be not very high: only 24/73 nominated papers received a unanimous vote to either consider for (any) award or not consider further.

# 9 Improving the Incentives

# 9.1 Improving Reporting Incentives for the Authors: Responsible NLP checklist

Following the effort started by NAACL 2022 and continued at ACL Rolling Review (Carpuat et al., 2021), we used the Responsible NLP Checklist as a way to ensure that all submissions conform to a certain minimum standard of reporting on their reproducibility efforts, data collection principles, and consideration of broader impacts. However, at NAACL 2022 and ACL Rolling Review, these checklists are only used internally during peer review.

To improve the transparency of NLP research and create a stronger incentive to invest effort in this work, we made the Responsible NLP Checklists an official part of all published papers. The authors filled out the checklist information in a special form, and we later used that form to generate pdf versions of the checklist, which was appended to every paper pdf for the ACL Anthology.

This change was announced in our Call for Papers, and we additionally communicated it to the authors. The authors had the opportunity to update the checklist form during the preparation of the camera-ready version of their papers.

One modification to the checklist was introducing a mandatory question about AI writing assistance. This was motivated by the introduction of OpenAI’s ChatGPT (OpenAI, 2022), the precedent of AI-assisted scientific paper writing of Meta’s Galactica (Taylor et al., 2022), and, more importantly, a massive wave of promotion for AI “writing assistants” shortly before our direct submission deadline. We did not aim to completely ban AI-assisted writing (which does have legitimate use cases such as assistance to non-native English speakers), but to improve transparency: just like with the other ethics-related questions in the checklists, our posted policy required authors to explicitly state what they did. Our question and policy were subsequently adopted by ACL Rolling Review.

Magnusson et al. (2023) have reported that the higher rate of “yes” responses to the Reproducibility checklist at 4 NLP conferences. Given that our checklist includes reproducibility questions, and reproducibility positively correlates with both Soundness and Excitement, we would expect the Responsible NLP checklist to perform the same role. The reviewers themselves were predominantly positive about it: 66.99% rated it as “somewhat useful”, 18.13% as “very useful”, and only 14.35% — as “not useful”.

Table 17 shows the ratios of submissions answering ‘yes’ to the questions of the checklist, and the acceptance rates for the submissions that answered ‘yes’ vs those that didn’t. For most questions of the checklist, there is a small increase in acceptance rate for submissions that answer ‘yes’. The most significant increases are for reporting limitations (so we recommend that the conferences keep mandating this section), reporting hyperparameters and computation budget (in line with the high correlation between reproducibility ratings and reviewer scores §7.5), citing relevant work, contributing scientific artifacts such as models and software (in line with our finding of a significant effect for this contribution type discussed in §7.3).

An interesting case is the “catch question” A3 (does your abstract accurately summarize your work?). It drew some criticism as “meaningless bureaucracy”, since all submissions should respond “yes” to it. It was actually intended to see that the responders were not just clicking through the checklist. Most authors did respond ‘yes’, but those 2.24% that didn’t saw a -25.4 decrease in acceptance rate. We interpret this as suggesting that the sloppiness in filling out the checklist correlates with sloppiness elsewhere in the work. Finally, our new question about the use of writing assistants is the only one where the response ‘Yes’ is associated with a decrease in acceptance rate, although not very large.

# 9.2 Improving Incentives for Reviewers: Reviewer Awards

Arguably the biggest source of issues with peer review quality is the lack of incentives to invest more work in invisible service labor. One direction is reputational awards, eg via creating reviewer profiles, as in Publons. Another is material awards, such as monetary prizes similar to the best paper awards.

# Checklist question

|Checklist question|% submissions|Yes|Not Yes*|Yes-Not_yes|
|---|---|---|---|---|
|A1 (limitations)|46.92|47.62|17.05|30.57|
|A2 (risks)|56.23|49.28|43.88|5.4|
|A3 (catch question)|97.76|47.49|22.09|25.4|
|A4 (AI-assisted writing)|7.3|41.28|47.36|-6.08|
|B (artifacts)|72.45|50.09|38.58|11.51|
|B1 (cite)|71.02|49.96|39.46|10.5|
|B2 (license)|37.8|52.48|43.54|8.94|
|B3 (intended use)|45.28|49.48|44.8|4.68|
|B4 (PII)|22.02|49|46.33|2.67|
|B5 (documentation)|48.95|50.93|43.08|7.85|
|B6 (statistics)|70.47|49.76|40.14|9.62|
|C (computation)|92.31|47.76|36.82|10.94|
|C1 (parameters)|78.58|48.96|39.44|9.52|
|C2 (hyperparams)|85.5|48.49|37.63|10.86|
|C3 (stats)|81.02|48.19|41.51|6.68|
|C4 (packages)|76.01|47.16|46.15|1.01|
|D (humans)|28.98|52.11|44.8|7.31|
|D1 (instructions)|20.95|53.85|45.08|8.77|
|D2 (payment)|21.19|53.5|45.15|8.35|
|D3 (consent)|17.31|51.2|46.02|5.18|
|D4 (IRB)|9.62|53.24|46.25|6.99|
|D5 (demographics)|14.61|54.27|45.66|8.61|

Table 17: The ratio of ‘Yes’ responses to checklist questions vs the responses other than ‘yes’ (i.e. both ‘no’ and ‘no response’). The average acceptance rate in this pool is 46.92%.

Another is punitive incentives, such as penalizing the late reviewers by delaying the reviews for their own submissions (Hauser and Fehr, 2007), or even blocking them from reviewing at future conferences. All of these approaches are not without issues. Punitive incentives generally shift the focus to not getting penalized, rather than delivering high-quality reviews. Material awards may introduce the wrong incentives (Squazzoni et al., 2013), and, depending on the institution and the country, the prize may be taxed or not even make it to the recipient. Conference fee waivers also may reward the reviewer’s institution rather than the reviewer, since the institutions usually bear the registration costs. While a survey found that reviewers generally prefer reputational awards over material (Warne, 2016), their value also depends on whether the reviewer’s institution rewards such work.

We proposed to the ACL exec (and received their approval for) an initiative to match the new ACL best paper award policy with recognizing about 1-1.5% of outstanding reviewers and chairs. This combines reputational and material incentives. Instead of monetary prizes, we proposed awarding vouchers for virtual attendance of any ACL (ACL, NAACL, EACL, AACL, EMNLP) conference of the awardee’s choice, to be used within a year of the award date. Since many institutions do not support the attendance of conferences without accepted papers (or even with papers accepted to workshops and Findings), we hope that this measure will increase the overall number of conferences that the awardees can attend.

We asked the area chairs to nominate the reviewers in their pool who provided extra helpful reviews, high-quality emergency reviews, “champion” reviews, reviewers who were particularly active in the discussion phase, or demonstrated exceptional open-mindedness or expertise. We received 51 such nominations. We also asked the Senior Area chairs to nominate exceptional area chairs, receiving 13 nominations. Finally, we as the program chairs also nominated the (3) SACs of the track who were the most on-time, provided the most helpful feedback, and followed our instructions the most closely. Excluding the duplicates, this resulted in 67 total nominations. All awards will be announced on the conference website.

Since the total number of nominations was within our target number of awards (1-1.5% of total reviewers and chairs), we were able to award all 66 nominations (out of 4998) without creating a selection committee. In the future, we recommend that an extra volunteer role is created for managing the selection of awardees and managing the awards.

40 https://2023.aclweb.org/program/best_reviewers

Caveats: despite our calls to nominate reviewers and chairs, relatively few ACs and SACs did that: only 7/70 SACs and 28/438 ACs. We recommend that the AC/SAC guidelines are expanded with a section about these awards, and that ACs are asked to start keeping track of potential outstanding reviewers at the (a) review quality check stage, (b) discussion stage, rather than only during meta-reviews (as we did). The SACs could be asked to start keeping track of outstanding ACs at the (a) assignment checks, if that is the process used by the venue, (b) meta-reviews, (c) nominating on the basis of quantitative analysis of the activity in the discussion forum and the number of author-reported review issues that the AC addressed.

# 9.3 Improving Incentives for Chairs: Peer Review Reports

Our final proposal for improving the incentives for peer review work was to increase its visibility by placing the program chair reports and any findings from their analysis of the internal conference data as an official part of the proceedings for the respective conference. This report is aiming to create a precedent for that. In the past, there have been two options for publishing such work: standalone research papers that undergo their own peer review, and miscellaneous blog posts and reports published in ACL wiki. But the former is not appropriate for reporting on incidental findings (since most of the program chairs work is not executed as a research project targeting a specific research question). The latter is unfortunately too difficult to discover, especially for the people outside of our field or new organizers who may not know which blog posts and wikis to search.

This initiative aims to improve the transparency of the overall process, and lets the younger members of the community have more insight into how the ACL conferences work. Moreover, given the increasing attention to peer review in NLP community (Gao et al., 2019; Caragea et al., 2019) and more broadly in ML conferences (Price, 2014; Stelmakh, 2020; Beygelzimer et al., 2021), it would be useful to make the incidental findings from the conferences more easily discoverable, incl. to the researchers in the ML community and other fields.

The main difficulty for the program chairs and the publication chairs with implementing this proposal is that the full report needs to be prepared before the conference, when there is a lot of other work. To implement this, the set of volunteer roles would need to be expanded (see section 10). We also recommend that to the extent possible, the future chairs start documenting their workflow for the report early on (perhaps during the main review cycle).

# 10 Recommendations

# Improving logistics.

There are several sources of papers to the ACL main conference that the program chairs have no control over: TACL, CL, Industry Track Papers, SRW papers. This means that the PCs need to ingest four different sources of information with potentially little means of interacting with the relevant authors (in contrast to direct submissions). ARR is in a liminal space between direct submissions and these other papers. The timing and format of how the papers enter ACL should be standardized.

# Desk rejections.

Desk reject requirements should be clearly stated in the call for papers or in the ACL Paper formatting guidelines. The guidelines omit rules or lack clear thresholds for rejection. For example, there is no minimum separation between captions and tables/figures nor between section titles and the text above and below. Nor are there minimum text sizes for text within tables or figures. Adding clear rules would make the first pass reviewing more efficient and fair. ACL also needs to communicate more clearly about the role of the aclpubcheck script: it’s a necessary but not sufficient check. Many authors assume that if they pass the aclpubcheck script, then they have followed all formatting guidelines.

# Soundness/Excitement scores.

With predominantly positive feedback in the exit survey (§6.4), and evidence of significant improvement in raw agreement (§7.4), we believe this experiment was successful and should be continued. The formulation of the scores and the review form should be improved, and care should be taken to reduce the overall complexity of the form.

# Review issue flagging.

This feature received overwhelming support from the authors, and should be continued and standardized (i.e., cleanly incorporated into author response form)—especially since it.

is likely to improve after several iterations, when everybody is more familiar with it and the reviewer guidelines. More AC training is needed to address the flagged issues.

Continued reviewer policy publications. 12.9% of all ACL’23 reviews were flagged by the authors for various issues, with the most frequent problem being reviewer heuristics such as “too simple” and “not SOTA”. It is reassuring to know that the ratio of bad reviews is already not very high, but of course we should strive to further decrease it. The reviewer guidelines, in combination with the review issue flagging mechanism, serve a double purpose: even if the reviewers do not read them, the authors will (since they have the incentive to call out problematic reviews), and then the area chairs also will (to handle the author-flagged issues). Hence, eventually, these policies will become widely known across the community, and enforced by it. We urge the future chairs to continue publishing their reviewer policy or simply re-use ours, and explicitly point to it in review, author response, and meta-review forms.

Reviewer assignment check support. There is currently no convenient interface for the ACs to look up the assigned reviewers and browse the alternatives with up-to-date availability information. Its lack is a major hurdle for the chairs, and it may cause either delays in the process or skipping the checks.

Reviewer match explanations. Our area chairs were very positive about this feature. For venues not using an interpretable assignment algorithm such as our keywords-based process, at the very least, the reviewer profiles and relevant papers should be provided directly with the review, without any extra search.

Post-acceptance decision litigation. Having increased the acceptance rate for Findings, we were surprised to still receive a large volume of emails from the authors who, considering their scores and meta-review, argued that either their paper should have been accepted to the main track, or that it shouldn’t have been rejected. It appears that some subcommunities share their scores with each other, under the mistaken impression that if one paper with certain scores was accepted, others with similar scores should be too. We had no capacity for anything beyond checking for clerical errors. The peer review process is by no means perfect, and there was certainly some noise in the decisions—but it is also certain that many authors who disagree with their decisions would try to argue their case if given the chance. If such litigation is not an announced an official part of the conference process—doing so for the select few would not be fair to all the other authors who also disagree with their decisions. We recommend that the future chairs either build this into their process and dedicate time and resources to it, or pre-announce that decisions are final and will not be reconsidered, beyond the cases of clerical errors.

Area-Contribution-Language matching. The results of our experiment with exactly matching the reviewers with submissions by these areas allowed us to establish that it is possible to ensure a fair acceptance rate for most “non-mainstream” contribution types, and for the 63.8% of the submissions that had target languages other than English, we were able to provide a reviewer competent in that language. These results are by no means perfect, and it is important that the future venues improve on them, perhaps with other methods. But Area-Contribution-Language matching could be considered a fair baseline for the future conferences, when considering the success rates for different types of submissions and languages. All that is needed from the chairs is to include in submission forms the checkboxes for different types of contributions, and input fields for the target languages other than English. At the very minimum, the chairs would then be able to analyze the acceptance rates of different types of submissions, and compare it with ours (Table 7). One step further would be to also solicit this information from the reviewers, and estimate the quality of automated matches by the explicit keyword matches (see Table 2).

One more practical takeaway for future work is that if we used a solution relying purely on publication history from Semantic Scholar—25% of our matches would have been made on unreliable information. For embeddings-based solutions to work better, we would first need to provide them with better data, and this will take a bigger Semantic Scholar cleaning campaign than what we were able to elicit.

Reconsidering the acceptance rate for Findings. The initial iterations of Findings starting with EMNLP 2020 had the Findings acceptance rate at about 35%. This is the target rate we gave to our SACs, and then we tried to accommodate as many of their ranked preferences as we could. Although

we had over 40% rate with Findings, still, in many SAC comments we saw that they were overriding acceptance recommendations of ACs only to meet the quotas. While the quota for the Main track will stay at 20-25% for venue ranking reasons, we do not see why Findings could not be further extended to have room for most sufficiently sound work. About 60% of our direct submissions had at least two positive (above-borderline) reviews for Soundness and at least one for Excitement. Assuming some noise in the negative reviews for Soundness, it would be only reasonable to expect that at least 45%-50% submissions are Findings-worthy. Of course, the track SACs would not have to accept that many (the ratio of high-quality papers may vary between tracks and years), but when they do not see good reasons to reject — they should not be constrained by the Findings quota. This step would presumably also further decrease the burden of re-reviewing for resubmissions. We also recommend developing a standard process for Findings authors to apply for presentation at topically matching workshops, and for at least virtual poster presentation slots at the main conference.

# Further research on the effect of preprinting on peer review.

We find that the preprinted papers have consistently higher ratings (for both Soundness, Excitement, and reviewer confidence), get more recommendations for awards, and a higher acceptance rate. There are several possible underlying causes (from reviewer biases to higher initial paper quality and benefits of community feedback), which likely all contribute to this effect. Since these factors necessitate different actions if they were the major contributor to the observed effect, for informed policy decisions it is necessary to establish how they intermix. We observe however that although the present 1-month embargo policy does not solve this problem, it is effective at mitigating it, since we only had 13.8% such papers.

# Consistently working to improve peer review consistency.

Our analysis shows that the inconsistency in numerical reviewer score ratings is remarkably consistent across *ACL conferences (at about α 0.3 across EMNLP’22, EACL’23, and ACL’23). Among the likely culprits are miscalibrated scales, different interpretations of scales, at least some reviewers not even reading the guidelines, and reviewer biases. That said, we do see almost twice the raw agreement for our Soundness score (that is supposed to be more objective) over Excitement (more subjective), when the scores are mapped to the sound/unsound vs exciting/unexciting categorical variables. This suggests that asking more concrete questions does help (as long as the reviewer form does not become too complicated), and we can continue improving peer review on the basis of the general NLP methodology for iterating on guidelines and measuring agreement.

# Ethics review.

The innovation of the ethics review is useful and necessary, but it should be explicitly built into the timeline. We particularly struggled with the conditional accepts.

# Responsible NLP Checklist.

With predominantly positive reviewer feedback and evidence of improved acceptance rates for submissions that follow the best reporting practices, we believe that this is an important instrument for creating the right incentives for better science. We also recommend continuing to make it public, to strengthen these incentives.

# AI-assisted reviews.

We did not expect this happen so soon, but already at ACL’23 some chairs reached out to us with questions about reviews that they suspected to be at least partly generated. The reviewer guidelines will need to be updated with respect to that as well, including how sending papers to cloud-based language models may violate confidentiality.

# Review policy updates.

The rise of popular commercial systems such as ChatGPT that are claimed to be general-purpose, made an unfortunate match with our field’s tendency to expect the popular systems in all papers as universal baselines. We did not consider this at ACL’23, since ChatGPT fell out of scope of 3-month policy for considering contemporaneous work, but we did already have at least one precedent of a reviewer asking for a comparison with ChatGPT. We recommend that future chairs develop a clear policy in the reviewer guidelines about requests for comparisons with “closed” systems, to avoid numerous issues with evaluation methodology and benchmark data contamination (Rogers, 2023).

# Expanding the set of volunteer roles.

Our experience suggests that PC-ing a conference of ACL’23 size is a job that can no longer be realistically done by 3 volunteers. Early on, we introduced a visa

support team to start early with issuing the letters of invitation for Canada. We also had crucial help from two PC assistants: Youmi Ma, an administrative assistant who handled much of the conference email, and Marzena Karpinska, who helped with analysis of peer review data in this report. In the future, we recommend that a dedicated role of a peer review chair is created, whose responsibility will be to supplement PC report with analysis of the data of the respective conference and comparing it with any records from previous conferences (so as to establish the effect of any new policies), and to coordinate the peer review awards selection and logistics (see §9.2). The growing volume of nominations for best papers requires a best paper chair, handling in effect the organization of a separate track and review process. Finally, we could have used a lot of help in the conference schedule: ideally there would be a dedicated schedule chair, ideally serving at several conferences so as to reduce friction and reuse the skill set as much as possible, as well as incorporate feedback from several events. Given that ACL had papers from SRW, Industry, ARR, TACL, CL, Findings, and the Main Conference, it’s not necessarily feasible that the main track PCs can effectively coordinate scheduling all of these papers.

Another option would be for each conference to have two sets of PC chairs, one remaining from the previous year and one new. This would lighten the workload and ensure a smoother process (since people do not learn how to do everything from scratch each time). The first-year PCs would do the bulk of the work after the paper notifications are sent, and the second-year PCs would concentrate on the review process, analysis and the report. The first-year PCs would observe that and have better knowledge for designing the review process (CFP, SAC nominations, review criteria, etc). The second-year PCs would observe the COI requirements.

# 11 Acknowledgements

ACL’23 was the result of an incredible effort of 70 SACs, 438 ACs, 4490 reviewers, and 13,658 authors. We also thank our 2 ethics chairs and their 21 reviewers, as well as 15 judges on the best paper committee.

We thank the ARR team, and particularly Jonathan K. Kummerfeld, Thamar Solorio and Mausam, for their help with integrating ARR submissions and analyzing them. We had a chance to learn from the past chairs Smaranda Muresan, Preslav Nakov and Aline Villavicencio (ACL 2022), Yoav Goldberg, Zornitsa Kozareva, Yue Zhang (EMNLP 2022), and Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur (NAACL 2021). We also thank EMNLP 2022 and EACL 2023 (Isabelle Augenstein, Andreas Vlachos) for sharing their score distribution data for our analysis.

Our work is built on many iterations of previous ACL conferences, including the AC and SAC guidelines developed at ACL 2021, and peer review tutorials developed by Anna Rogers and Isabelle Augenstein for ACL Rolling Review.

Our paper-reviewer matching relied on Semantic Scholar data, kindly provided by Kyle Lo (AI2). The Semantic Scholar team also provided extra support to numerous authors working to clean up their profiles. Emma Strubell, Ian Magnusson, and Jesse Dodge helped us to prepare publishable versions of Responsible NLP checklist.

We were only able to devote that much effort to peer review and its analysis thanks to the help of our brilliant assistants Youmi Ma and Marzena Karpinska. Richard Gerber (START) responded to numerous issues and implemented several changes at our request, including the possibility to include “explanations" for the paper-reviewer matching.

We deeply thank the ACL Executive (especially Iryna Gurevych, Tim Baldwin, David Yarowsky, Yusuke Miyao, and Emily M. Bender) for their support of many of our crazy ideas, including the reviewer awards and the publication of this report. Last but not least, we thank our publication chairs and ACL Anthology team, in particular, Ryan Cotterell and Matt Post — for their infinite patience with this last-minute publication.

41 https://2023.aclweb.org/blog/visa-info/

# References

Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névéol, Fanny Ducel, Saif M. Mohammad, and Karën Fort. 2023. The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research.

Omer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei Hwu, and JinJun Xiong. 2019. PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 518–528, Hong Kong, China. Association for Computational Linguistics.

Ron Artstein and Massimo Poesio. 2008. Survey article: Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.

Rachel Bawden. 2019. One paper, nine reviews.

Emily M. Bender. 2019. The #BenderRule: On Naming the Languages We Study and Why It Matters.

Emily M. Bender and Leon Derczynski. 2018. Paper Types.

Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan. 2021. The NeurIPS 2021 Consistency Experiment.

Cornelia Caragea, Ana Uban, and Liviu P. Dinu. 2019. The Myth of Double-Blind Review Revisited: ACL vs. EMNLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2317–2327, Hong Kong, China. Association for Computational Linguistics.

Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz. 2021. Responsible NLP research Checklist.

Rune Haubo Bojesen Christensen. 2022. ordinal—Regression Models for Ordinal Data. R package version 2022.11-16.

Kenneth Ward Church. 2020. Emerging trends: Reviewing the reviewers (again). Natural Language Engineering, 26(2):245–257.

Trevor Cohn, Yulan He, Yang Liu, and Bonnie Webber. 2020. Advice on Reviewing for EMNLP.

Corinna Cortes and Neil D. Lawrence. 2021. Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment. arXiv:2109.09774 [cs].

D.R. Cox and E.J. Snell. 1989. Analysis of Binary Data, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis.

Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond.

Yang Gao, Steffen Eger, Ilia Kuznetsov, Iryna Gurevych, and Yusuke Miyao. 2019. Does My Rebuttal Matter? Insights from a Major NLP Conference. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1274–1290, Minneapolis, Minnesota. Association for Computational Linguistics.

Marc Hauser and Ernst Fehr. 2007. An Incentive Solution to the Peer Review Problem. PLOS Biology, 5(4):e107.

Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. 2019. Argument Mining for Understanding Peer Reviews. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2131–2137, Minneapolis, Minnesota. Association for Computational Linguistics.

Letizia Jaccheri, Cristina Pereira, and Swetlana Fast. 2020. Gender Issues in Computer Science: Lessons Learnt and Reflections for the Future. In 2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), pages 9–16.

Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B. Shah, and Fei Fang. 2022. A Dataset on Malicious Paper Bidding in Peer Review.

# References

Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1647–1661, New Orleans, Louisiana. Association for Computational Linguistics.

Klaus Krippendorff. 2011. Computing Krippendorff’s Alpha-Reliability.

Andy Liaw and Matthew Wiener. 2002. Classification and Regression by randomForest. R News, 2(3):18–22.

Michael L. Littman. 2021. Collusion Rings Threaten the Integrity of Computer Science Research. Communications of the ACM, 64(6):43–44.

Ian Magnusson, Noah A. Smith, and Jesse Dodge. 2023. Reproducibility in NLP: What Have We Learned from the Checklist?

Daniel McFadden. 1973. Conditional Logit Analysis of Qualitative Choice Behaviour. In P. Zarembka, editor, Frontiers in Econometrics, pages 105–142. Academic Press New York, New York, NY, USA.

Nico Nagelkerke. 1991. A note on a general definition of the coefficient of determination. Biometrika, 78(3):691–692.

OpenAI. 2022. Introducing ChatGPT.

Katarina Pantic and Jody Clarke-Midura. 2019. Factors That Influence Retention of Women in the Computer Science Major: A Systematic Literature Review. Journal of Women and Minorities in Science and Engineering, 25(2).

Silviu Paun, Ron Artstein, and Massimo Poesio. 2022. Statistical Methods for Annotation Analysis. Springer International Publishing.

Douglas P. Peters and Stephen J. Ceci. 1982. The Fate of Published Articles, Submitted Again. Behavioral and Brain Sciences, 5(2):199–199.

Eric Price. 2014. The NIPS experiment.

Anna Rogers. 2023. Closed AI Models Make Bad Baselines.

Anna Rogers and Isabelle Augenstein. 2020. What Can We Do to Improve Peer Review in NLP? In Findings of EMNLP, pages 1256–1262, Online. Association for Computational Linguistics.

Richard Smith. 2010. Classical Peer Review: An Empty Gun. Breast Cancer Research, 12(4):S13.

Charles Spearman. 1987. The Proof and Measurement of Association between Two Things. The American Journal of Psychology, 100(3/4):441.

Flaminio Squazzoni, Giangiacomo Bravo, and Károly Takács. 2013. Does Incentive Provision Increase the Quality of Peer Review? An Experimental Study. Research Policy, 42(1):287–294.

Ivan Stelmakh. 2020. Experiments with the ICML 2020 Peer-Review Process.

Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. 2019. PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review. In Proceedings of the 30th International Conference on Algorithmic Learning Theory, pages 828–856. PMLR.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science.

Terne Thorn Jakobsen and Anna Rogers. 2022. What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4810–4823, Seattle, United States. Association for Computational Linguistics.

Andrew Tomkins, Min Zhang, and William D. Heavlin. 2017. Reviewer Bias in Single- versus Double-Blind Peer Review. Proceedings of the National Academy of Sciences, 114(48):12708–12713.

William N. Venables and Brian D. Ripley. 2002. Modern Applied Statistics with S, fourth edition. Springer, New York. ISBN 0-387-95457-0.

Verity Warne. 2016. Rewarding reviewers – Sense or Sensibility? A Wiley Study Explained. Learned Publishing, 29(1):41–50.

# Table of Contents

# One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems

Yajiao LIU, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan and Benyou Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

# SafeConv: Explaining and Correcting Conversational Unsafe Behavior

Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen and Dong Yu . . . . . . . . . . . . 22

# Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better

David Dale, Elena Voita, Loic Barrault and Marta R. Costa-jussà . . . . . . . . . . . . . . . . . . . . . . . . . . 36

# Explainable Recommendation with Personalized Review Retrieval and Aspect Learning

Hao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu and Hao Liao . . . . 51

# Binary and Ternary Natural Language Generation

Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi and Raghuraman Krishnamoorthi . . . . 65

# Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking

Björn Bebensee and Haejun Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

# EM Pre-training for Multi-party Dialogue Response Generation

Yiyang Li and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

# ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER

Sreyan Ghosh, Utkarsh Tyagi, Manan Suri, Sonal Kumar, Ramaneswaran S and Dinesh Manocha . . . . 104

# Natural Language to Code Generation in Interactive Data Science Notebooks

Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek K Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov and Charles Sutton . . . . 126

# Subset Retrieval Nearest Neighbor Machine Translation

Hiroyuki Deguchi, Taro Watanabe, Yusuke Matsui, Masao Utiyama, Hideki Tanaka and Eiichiro Sumita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

# MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning

Xu Zhang and Xiaojun Wan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190

# Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies

Iria de-Dios-Flores, Juan Pablo Garcia Amboage and Marcos Garcia . . . . . . . . . . . . . . . . . . . . . . 203

# Open-ended Long Text Generation via Masked Language Modeling

Xiaobo Liang, Zecheng Tang, Juntao Li and Min Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

# A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces

Gabriella Chronis, Kyle Mahowald and Katrin Erk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

# Holographic CCG Parsing

Ryosuke Yamaki, Tadahiro Taniguchi and Daichi Mochihashi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262

# Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning

Zujie Liang, feng wei, Yin Jie, YUXI QIAN, Zhenghong Hao and Bing Han . . . . . . . . . . . . . . . 277

# Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation

Yubing Ren, Yanan Cao, Ping Guo, Fang Fang, Wei Ma and Zheng Lin . . . . . . . . . . . . . . . . . . . . 293

# WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning

Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li and Yajuan Lyu . . . . . . . . . . . . . . . . . 307

# AMR-based Network for Aspect-based Sentiment Analysis

Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang, Shuang Li, Philip S. Yu and Lijie Wen . . . . 322

# Text Adversarial Purification as Defense against Adversarial Attacks

Linyang Li, Demin Song and Xipeng Qiu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338

# SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres

Shumin Deng, Shengyu Mao, Ningyu Zhang and Bryan Hooi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351

# Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection

Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars and Mei Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364

# What about em"? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns

Anne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley and Dirk Hovy . . . . . . . . . . . . 377

# What Is Overlap Knowledge in Event Argument Extraction? APE: A Cross-datasets Transfer Learning Model for EAE

Kaihang Zhang, Kai Shuang, Xinyue Yang, Xuyang Yao and Jinyu Guo . . . . . . . . . . . . . . . . . . . 393

# Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation

Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen and Jun Xie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410

# Knowledge of cultural moral norms in large language models

Aida Ramezani and Yang Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428

# Songs Across Borders: Singable and Controllable Neural Lyric Translation

Longshen Ou, Xichu Ma, Min-Yen Kan and Ye Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447

# Fantastic Expressions and Where to Find Them: Chinese Simile Generation with Multiple Constraints

Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Xiangpeng Wei, Zhengyuan Liu and Jun Xie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468

# Revealing Single Frame Bias for Video-and-Language Learning

Jie Lei, Tamara Berg and Mohit Bansal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

# Learning with Partial Annotations for Event Detection

Jian Liu, Dianbo Sui, Kang Liu, Haoyan Liu and Zhe Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508

# World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models

Ziqiao Ma, Jiayi Pan and Joyce Chai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524

# A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models

Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf and Mrinmaya Sachan

545

# Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information

Kun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio and Xiaohui Cui

562

# Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions

John Joon Young Chung, Ece Kamar and Saleema Amershi

575

# Pruning Pre-trained Language Models Without Fine-Tuning

Ting Jiang, deqing wang, Fuzhen Zhuang, Ruobing Xie and Feng Xia

594

# When Does Translation Require Context? A Data-driven, Multilingual Exploration

Patrick Fernandes, Kayo Yin, Emmy Liu, André Martins and Graham Neubig

606

# Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection

Ziwei Chen, Linmei Hu, Weixin Li, Yingxia Shao and Liqiang Nie

627

# LexSym: Compositionality as Lexical Symmetry

Ekin Akyurek and Jacob Andreas

639

# Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition

Jun Sun, Shoukang Han, Yu-Ping Ruan, Xiaoning Zhang, Shu-Kai Zheng, Yulong Liu, Yuxin Huang and Taihao Li

658

# CASN: Class-Aware Score Network for Textual Adversarial Detection

Rong Bao, Rui Zheng, Liang Ding, Qi Zhang and Dacheng Tao

671

# Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from The New Yorker Caption Contest

Jack Hessel, Ana Marasovic, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff and Yejin Choi

688

# Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation

Martijn Bartelds, Nay San, Bradley McDonnell, Dan Jurafsky and Martijn Wieling

715

# CLCL: Non-compositional Expression Detection with Contrastive Learning and Curriculum Learning

Jianing Zhou, Ziheng Zeng and Suma Bhat

730

# Multi-VALUE: A Framework for Cross-Dialectal English NLP

Caleb Ziems, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta and Diyi Yang

744

# Self-Edit: Fault-Aware Code Editor for Code Generation

Kechi Zhang, Zhuo Li, Jia Li, Ge Li and Zhi Jin

769

# ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning

Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim and Leshem Choshen

788

# Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization

Runzhe Zhan, Xuebo Liu, Derek F. Wong, Cuilian Zhang, Lidia S. Chao and Min Zhang

807

# Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling

Haw-Shiuan Chang, Ruei-Yao Sun, Kathryn Ricci and Andrew McCallum

821

lxxviii

# On-the-fly Cross-lingual Masking for Multilingual Pre-training

Xi Ai and Bin Fang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855

# How About Kind of Generating Hedges using End-to-End Neural Models?

Alafate Abulimiti, Chloé Clavel and Justine Cassell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877

# DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models

Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover and Duen Horng Chau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893

# From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization

Arie Cattan, Lilach Eden, Yoav Kantor and Roy Bar-Haim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912

# When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications

Kevin Song Pei, Ishan Jindal, Kevin Chen-Chuan Chang, ChengXiang Zhai and Yunyao Li . . 929

# Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful Crowd Opinion with Population-level Learning

Tharindu Cyril Weerasooriya, Sarah Luger, Saloni Poddar, Ashiqur KhudaBukhsh and Christopher Homan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 950

# Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA

Neeraj Varshney and Chitta Baral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967

# UniLG: A Unified Structure-aware Framework for Lyrics Generation

Tao Qian, Fan Lou, Jiatong Shi, Yuning Wu, Shuai Guo, Xiang Yin and Qin Jin . . . . . . . . . . . . 983

# FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering

Lingxi Zhang, Jing Zhang, Yanling Wang, Shulin Cao, Xinmei Huang, Cuiping Li, Hong Chen and Juanzi Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1002

# Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models

Lennart Wachowiak and Dagmar Gromann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1018

# Being Right for Whose Right Reasons?

Terne Sasha Thorn Jakobsen, Laura Cabello and Anders Søgaard . . . . . . . . . . . . . . . . . . . . . . . . 1033

# ALERT: Adapt Language Models to Reasoning Tasks

Ping Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona Diab and Asli Celikyilmaz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1055

# Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages

Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon and Hinrich Schütze 1082

# Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction

Huawen Feng, Junlong Liu, Junhao Zheng, Haibin Chen, Xichen Shang and Qianli Ma . . . . 1118

# Pretrained Bidirectional Distillation for Machine Translation

Yimeng Zhuang and Mei Tu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1132

# Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning

Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyungmin Kim, Jung-Woo Ha and Sang-Woo Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1146

# Improving Continual Relation Extraction by Distinguishing Analogous Semantics

Wenzheng Zhao, Yuanning Cui and Wei Hu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1162

# Improving Pretraining Techniques for Code-Switched NLP

Richeek Das, Sahasra Ranjan, Shreya Pathak and Preethi Jyothi . . . . . . . . . . . . . . . . . . . . . . . . . . 1176

# A Theory of Unsupervised Speech Recognition

Liming Wang, Mark Hasegawa-Johnson and Chang D. Yoo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1192

# ThinkSum: Probabilistic reasoning over sets using large language models

Batu M Ozturkler, Nikolay Malkin, Zhen Wang and Nebojsa Jojic . . . . . . . . . . . . . . . . . . . . . . . 1216

# NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist

Iftitahu Nimah, Meng Fang, Vlado Menkovski and Mykola Pechenizkiy . . . . . . . . . . . . . . . . . . 1240

# DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations

Ang Lv, Jinpeng Li, yuhan chen, GAO XING, Ji Zhang and Rui Yan . . . . . . . . . . . . . . . . . . . . . 1267

# TECHS: Temporal Logical Graph Networks for Explainable Extrapolation Reasoning

Qika Lin, Jun Liu, Rui Mao, Fangzhi Xu and Erik Cambria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1281

# Consistency Regularization Training for Compositional Generalization

Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Jie Zhou and Yue Zhang . . . . . . . . . . . . . . 1294

# NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation

Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhen-gyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Ming Gong, Lijuan Wang, Zicheng Liu, Houqiang Li and Nan Duan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1309

# Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe

Xiang Yue, Huseyin A Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan and Robert Sim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1321

# A Close Look into the Calibration of Pre-trained Language Models

Yangyi Chen, Lifan Yuan, Ganqu CUI, Zhiyuan Liu and Heng Ji. . . . . . . . . . . . . . . . . . . . . . . . . 1343

# DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization

Yu Li, Baolin Peng, Pengcheng He, Michel Galley, Zhou Yu and Jianfeng Gao . . . . . . . . . . . . 1368

# MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction

Wang Jing, Aixin Sun, Hao Zhang and Xiaoli Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1387

# Diverse Demonstrations Improve In-context Compositional Generalization

Itay Levy, Ben Bogin and Jonathan Berant. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1401

# Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering

Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye and Lingpeng Kong . . . . . . . . . . . . . . . . . . . . . . . . . . 1423

# On the Efficacy of Sampling Adapters

Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan Wilcox and Ryan Cotterell . . . . . . . . . 1437

# Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis

Jianfei Yu, Qiankun Zhao and Rui Xia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1456

# Compositional Data Augmentation for Abstractive Conversation Summarization

Siru Ouyang, Jiaao Chen, Jiawei Han and Diyi Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1471

# PMAES: Prompt-mapping Contrastive Learning for Cross-prompt Automated Essay Scoring

Yuan Chen and Xia Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1489

# Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models

Myra Cheng, Esin Durmus and Dan Jurafsky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1504

# On Prefix-tuning for Lightweight Out-of-distribution Detection

Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang and Xinyu Dai . . . . . 1533

# GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding

Konstantin Yakovlev, Alexander Podolskiy, Andrey Bout, Sergey I Nikolenko and Irina Piontkovskaya . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1546

# Measuring Progress in Fine-grained Vision-and-Language Understanding

Emanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks and Aida Nematzadeh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1559

# Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information

Sunjae Kwon, Rishabh Garodia, Minhwa Lee, Zhichao Yang and hong yu . . . . . . . . . . . . . . . . 1583

# Chain-of-Skills: A Configurable Model for Open-Domain Question Answering

Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg and Jianfeng Gao . . . . . . . . . 1599

# Elaboration-Generating Commonsense Question Answering at Scale

Wenya Wang, Vivek Srikumar, Hannaneh Hajishirzi and Noah A. Smith . . . . . . . . . . . . . . . . . . 1619

# Neural Unsupervised Reconstruction of Protolanguage Word Forms

Andre W He, Nicholas Tomlin and Dan Klein. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1636

# DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation

Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu and Dongsheng Li . . . 1650

# On Evaluating Multilingual Compositional Generalization with Translated Datasets

Zi Wang and Daniel Hershcovich . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1669

# FAA: Fine-grained Attention Alignment for Cascade Document Ranking

Zhen Li, Chongyang Tao, Jiazhan Feng, Tao Shen, Dongyan Zhao, Xiubo Geng and Daxin Jiang 1688

# Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models

Zhong Zhang, Bang Liu and Junming Shao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1701

# Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach

Jinfeng Zhou, Zhuang Chen, Bo Wang and Minlie Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1714

# Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling

Mingzhu Cai, Siqi Bao, Xin Tian, Huang He, Fan Wang and Hua Wu . . . . . . . . . . . . . . . . . . . . 1730

# Why Aren’t We NER Yet? Artifacts of ASR Errors in Named Entity Recognition in Spontaneous Speech Transcripts

Piotr Szymański, Lukasz Augustyniak, Mikolaj Morzy, Adrian Szymczak, Krzysztof Surdyk and Piotr Zelasko

Page: 1746

# Precise Zero-Shot Dense Retrieval without Relevance Labels

Luyu Gao, Xueguang Ma, Jimmy Lin and Jamie Callan

Page: 1762

# White-Box Multi-Objective Adversarial Attack on Dialogue Generation

Yufei Li, Zexin Li, Yingfan Gao and Cong Liu

Page: 1778

# A Cautious Generalization Goes a Long Way: Learning Morphophonological Rules

Salam Khalifa, Sarah Payne, Jordan Kodner, Ellen Broselow and Owen Rambow

Page: 1793

# Few-shot Adaptation Works with UnpredicTable Data

Jun Shern Chan, Michael Pieler, Jonathan Jao, Jérémy Scheurer and Ethan Perez

Page: 1806

# Cross-lingual Science Journalism: Select, Simplify and Rewrite Summaries for Non-expert Readers

Mehwish Fatima and Michael Strube

Page: 1843

# HuCurl: Human-induced Curriculum Discovery

Mohamed Elgaar and Hadi Amiri

Page: 1862

# kNN-TL: k-Nearest-Neighbor Transfer Learning for Low-Resource Neural Machine Translation

Shudong Liu, Xuebo Liu, Derek F. Wong, Zhaocong Li, Wenxiang Jiao, Lidia S. Chao and Min Zhang

Page: 1878

# Do language models have coherent mental models of everyday things?

Yuling Gu, Bhavana Dalvi Mishra and Peter Clark

Page: 1892

# Rogue Scores

Max Grusky

Page: 1914

# Instruction Induction: From Few Examples to Natural Language Task Descriptions

Or Honovich, Uri Shaham, Samuel R. Bowman and Omer Levy

Page: 1935

# In-Context Analogical Reasoning with Pre-Trained Language Models

Xiaoyang Hu, Shane Storks, Richard L. Lewis and Joyce Chai

Page: 1953

# Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering

Avi Caciularu, Matthew Peters, Jacob Goldberger, Ido Dagan and Arman Cohan

Page: 1970

# Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation

Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan and Mu Li

Page: 1990

# REV: Information-Theoretic Evaluation of Free-Text Rationales

Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi and Swabha Swayamdipta

Page: 2007

# ELQA: A Corpus of Metalinguistic Questions and Answers about English

Shabnam Behzad, Keisuke Sakaguchi, Nathan Schneider and Amir Zeldes

Page: 2031

# Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking

Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng Lin, Shi Wang, Dacheng Tao and Li Guo

Page: 2048

# BIG-C: a Multimodal Multi-Purpose Dataset for Bemba

Claytone Sikasote, Eunice Mukonde, Md Mahfuz Ibn Alam and Antonios Anastasopoulos

Page: 2062

# Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues

Yue Feng, Yunlong Jiao, Animesh Prasad, Nikolaos Aletras, Emine Yilmaz and Gabriella Kazai

2079

# Robust Multi-bit Natural Language Watermarking through Invariant Features

KiYoon Yoo, Wonhyuk Ahn, Jiho Jang and Nojun Kwak

2092

# KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding

Shangbin Feng, Zhaoxuan Tan, Wenqian Zhang, Zhenyu Lei and Yulia Tsvetkov

2116

# AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction

Yanzeng Li, Bingcong Xue, Ruoyu Zhang and Lei Zou

2139

# Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization

Shiyue Zhang, David Wan and Mohit Bansal

2153

# Improving Translation Quality Estimation with Bias Mitigation

Hui Huang, Shuangzhi Wu, Kehai Chen, Hui Di, Muyun Yang and Tiejun Zhao

2175

# Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation

Josef Jon and Ondˇrej Bojar

2191

# MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions

Hao Sun, Zhexin Zhang, Fei Mi, Yasheng Wang, Wei Liu, Jianwei Cui, Bin Wang, Qun Liu and Minlie Huang

2213

# Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion

Shaoxiang Wu, Damai Dai, Ziwei Qin, Tianyu Liu, Binghuai Lin, Yunbo Cao and Zhifang Sui

2231

# SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder and Furu Wei

2244

# From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained

Hongliang Dai and Ziqian Zeng

2259

# Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers

Parikshit Bansal and Amit Sharma

2271

# What Makes Pre-trained Language Models Better Zero-shot Learners?

Jinghui Lu, dongsheng zhu, weidong Han, Rui Zhao, Brian Mac Namee and Fei Tan

2288

# Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations

Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer and Hannaneh Hajishirzi

2304

# Learning Optimal Policy for Simultaneous Machine Translation via Binary Search

Shoutao Guo, Shaolei Zhang and Yang Feng

2318

# Better Simultaneous Translation with Monotonic Knowledge Distillation

Shushu Wang, Jing Wu, Kai Fan, Wei Luo, Jun Xiao and Zhongqiang Huang

2334

# StoryARG: a corpus of narratives and personal experiences in argumentative texts

Neele Falk and Gabriella Lapesa

2350

# Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue

Maksim Eremeev, Ilya Valmianski, Xavier Amatriain and Anitha Kannan . . . . . . . . . . . . . . . . 2373

# Sequence Parallelism: Long Sequence Training from System Perspective

Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li and Yang You . . . . . . . . . . . . . . . . 2391

# MUSTIE: Multimodal Structural Transformer for Web Information Extraction

Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Zenglin Xu, Shaoliang Nie, Sinong Wang, Madian Khabsa, Hamed Firooz and Dongfang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2405

# Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In

Zichun Yu, Chenyan Xiong, Shi Yu and Zhiyuan Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2421

# TableVLM: Multi-modal Pre-training for Table Structure Recognition

Leiyuan Chen, Chengsong Huang, Xiaoqing Zheng, Jinshu Lin and Xuanjing Huang . . . . . . 2437

# Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?

Jiashu Xu, Mingyu Derek Ma and Muhao Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2450

# Dynamic Routing Transformer Network for Multimodal Sarcasm Detection

Yuan Tian, Nan Xu, Ruike Zhang and Wenji Mao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2468

# What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary

Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant and Amir Globerson2481

# Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach

Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen and Chao Zhang . . . . . . . . . . . . 2499

# Training-free Neural Architecture Search for RNNs and Transformers

Aaron Serianni and Jugal Kalita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2522

# CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs

Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Yuan-Fang Li, Yong-Bin Kang and Rifat Shahriyar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2541

# Improving Gradient Trade-offs between Tasks in Multi-task Text Classification

Heyan Chai, Jinhao Cui, Ye Wang, Min Zhang, Binxing Fang and Qing Liao . . . . . . . . . . . . . . 2565

# Bi-Phone: Modeling Inter Language Phonetic Influences in Text

Abhirut Gupta, Ananya B. Sai, Richard Sproat, Yuri Vasilevski, James S Ren, Ambarish Jash, Sukhdeep S Sodhi and Aravindan Raghuveer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2580

# Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment

Shengqiong Wu, Hao Fei, Wei Ji and Tat-Seng Chua . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2593

# Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee and Ee-Peng Lim 2609

# RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models

Zheng Liu, Shitao Xiao, Yingxia Shao and Zhao Cao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2635

# DecompX: Explaining Transformers Decisions by Propagating Token Decomposition

Ali Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh, Yadollah Yaghoobzadeh and Mohammad Taher Pilehvar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2649

# Symbolic Chain-of-Thought Distillation: Small Models Can Also Think Step-by-Step

Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang and Yejin Choi . . 2665

# Generating EDU Extracts for Plan-Guided Summary Re-Ranking

Griffin Adams, Alex Fabbri, Faisal Ladhak, Noémie Elhadad and Kathleen McKeown . . . . . 2680

# A Survey on Asking Clarification Questions Datasets in Conversational Systems

Hossein A. Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz and Aldo Lipani . . 2698

# Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters

Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer and Huan Sun 2717

# Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation

Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan and Francisco Guzman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2740

# RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks

Zhaoyang Wang, Zhiyue Liu, Xiaopeng Zheng, Qinliang Su and Jiahai Wang . . . . . . . . . . . . . 2757

# Gradient-based Intra-attention Pruning on Pre-trained Language Models

Ziqing Yang, Yiming Cui, Xin Yao and Shijin Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2775

# Learning to Substitute Spans towards Improving Compositional Generalization

Zhaoyi Li, Ying Wei and Defu Lian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2791

# DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation

Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin and Xiaodong He 2812

# BREAK: Breaking the Dialogue State Tracking Barrier with Beam Search and Re-ranking

Seungpil Won, Heeyoung Kwak, Joongbo Shin, Janghoon Han and Kyomin Jung . . . . . . . . . . 2832

# Faithful Low-Resource Data-to-Text Generation through Cycle Training

Zhuoer Wang, Marcus Collins, Nikhita Vedula, Simone Filice, Shervin Malmasi and Oleg Rokhlenko . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2847

# Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing

Li Du, Xiao Ding, Zhouhao Sun, Ting Liu, Bing Qin and Jingshuo Liu . . . . . . . . . . . . . . . . . . . 2868

# Dynamic and Efficient Inference for Text Generation via BERT Family

Xiaobo Liang, Juntao Li, Lijun Wu, Ziqiang Cao and Min Zhang . . . . . . . . . . . . . . . . . . . . . . . . 2883

# Learning to Generate Equitable Text in Dialogue from Biased Training Data

Anthony B Sicilia and Malihe Alikhani . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2898

# Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification

Ke Ji, Yixin Lian, Jingsheng Gao and Baoyuan Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2918

# Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization

Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen and Jie Zhou . . . . . . . . 2934

# Helping a Friend or Supporting a Cause? Disentangling Active and Passive Cosponsorship in the U.S. Congress

Giuseppe Russo, Christoph Gote, Laurence Brandenberger, Sophia Johanna Schlosser and Frank Schweitzer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2952

# TREA: Tree-Structure Reasoning Schema for Conversational Recommendation

Wendi Li, Wei Wei, Xiaoye Qu, Xian-Ling Mao, Ye Yuan, Wenfeng Xie and Dangyang Chen2970

# CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality

Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Rongyu Cao, Binhua Li, Fei Huang and Yongbin Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2983

# Multilingual Multifaceted Understanding of Online News in Terms of Genre, Framing, and Persuasion Techniques

Jakub Piskorski, Nicolas Stefanovitch, Nikolaos Nikolaidis, Giovanni Da San Martino and Preslav Nakov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3001

# Learning Action Conditions from Instructional Manuals for Instruction Understanding

Te-Lin Wu, Caiqi ZHANG, Qingyuan Hu, Alexander Spangher and Nanyun Peng . . . . . . . . . 3023

# StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation

Yulun Du and Lydia Chilton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3044

# Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning

Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong and Chien-Sheng Jason Wu3063

# Do PLMs Know and Understand Ontological Knowledge?

Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie and Kewei Tu . . . . . . . . . . . . . . . . . . . . . . 3080

# CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection

Chongyang Tao, Jiazhan Feng, Tao Shen, Chang Liu, Juntao Li, Xiubo Geng and Daxin Jiang 3102

# Exploring How Generative Adversarial Networks Learn Phonological Representations

Jingyi Chen and Micha Elsner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3115

# Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis

Mario Giulianelli, Iris Luden, Raquel Fernandez and Andrey Kutuzov . . . . . . . . . . . . . . . . . . . . 3130

# Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing

Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau Yih and Ziyu Yao . . . . . . . . 3149

# InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation

Anwen Hu, Shizhe Chen, Liang Zhang and Qin Jin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3171

# An Invariant Learning Characterization of Controlled Text Generation

Carolina Zheng, Claudia Shi, Keyon Vafa, Amir Feder and David Blei . . . . . . . . . . . . . . . . . . . . 3186

# HistRED: A Historical Document-Level Relation Extraction Dataset

Soyoung Yang, Minseok Choi, Youngwoo Cho and Jaegul Choo . . . . . . . . . . . . . . . . . . . . . . . . . 3207

# A Critical Evaluation of Evaluations for Long-form Question Answering

Fangyuan Xu, Yixiao Song, Mohit Iyyer and Eunsol Choi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3225

# HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation

Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang and Songfang Huang . . . . . . . . . . . . . . . . 3246

# Generating User-Engaging News Headlines

Pengshan Cai, Kaiqiang Song, Sangwoo Cho, Hongwei Wang, Xiaoyang Wang, hong yu, Fei Liu and Dong Yu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3265

# Word sense extension

Lei Yu and Yang Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3281

# PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism

Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang and Hinrich Schütze . . . . . . . . . . . . . . . . . 3295

# Decoding Symbolism in Language Models

Meiqi Guo, Rebecca Hwa and Adriana Kovashka. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3311

# A Survey on Zero Pronoun Translation

Longyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi and Zhaopeng Tu . . . 3325

# We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit

Davide Testa, Emmanuele Chersoni and Alessandro Lenci . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3340

# MPCHAT: Towards Multimodal Persona-Grounded Conversation

Jaewoo Ahn, Yeda Song, Sangdoo Yun and Gunhee Kim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3354

# DOC: Improving Long Story Coherence With Detailed Outline Control

Kevin Yang, Dan Klein, Nanyun Peng and Yuandong Tian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3378

# Dual-Alignment Pre-training for Cross-lingual Sentence Embedding

Ziheng Li, Shaohan Huang, Zihan Zhang, Zhi-Hong Deng, Qiang Lou, Haizhen Huang, Jian Jiao, Furu Wei, Weiwei Deng and Qi Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3466

# Exploring Better Text Image Translation with Multimodal Codebook

Zhibin Lan, Jiawei Yu, Xiang Li, Wen Zhang, Jian Luan, Bin Wang, Degen Huang and Jinsong Su . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3479

# FEDLEGAL: The First Real-World Federated Learning Benchmark for Legal NLP

Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu and Zenglin Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3492

# A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning

Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin and Weiping Wang . . . . . . . . . . . . . 3508

# History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling

Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang and Yongbin Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3521

# From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation

Zhibin Chen, Yansong Feng and Dongyan Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3534

# Alleviating Over-smoothing for Unsupervised Sentence Representation

Nuo Chen, Linjun Shou, Jian Pei, Ming Gong, Bowen Cao, Jianhui Chang, Jia Li and Daxin Jiang 3552

# Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model

Yeskendir Koishekenov, Alexandre Berard and Vassilina Nikoulina . . . . . . . . . . . . . . . . . . . . . . 3567

# DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue

William Held, Christopher Hidey, Fei Liu, Eric Y Zhu, Rahul Goel, Diyi Yang and Rushin Shah

3586

# From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding

Li Sun, Florian Luisier, Kayhan Batmanghelich, Dinei Florencio and Cha Zhang

3605

# MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling

Yu Song, Santiago Miret and Bang Liu

3621

# Code4Struct: Code Generation for Few-Shot Event Structure Prediction

Xingyao Wang, Sha Li and Heng Ji

3640

# GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles

Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang and Nanyun Peng

3664

# Efficient Semiring-Weighted Earley Parsing

Andreas Opedal, Ran Zmigrod, Tim Vieira, Ryan Cotterell and Jason Eisner

3687

# Tree-Based Representation and Generation of Natural and Mathematical Language

Alexander Scarlatos and Andrew Lan

3714

# ParaLS: Lexical Substitution via Pretrained Paraphraser

Jipeng Qiang, Kang Liu, Yun Li, Yunhao Yuan and Yi Zhu

3731

# Peer-Label Assisted Hierarchical Text Classification

Junru Song, Feifei Wang and Yang Yang

3747

# Free Lunch for Efficient Textual Commonsense Integration in Language Models

Wanyun Cui and Xingran Chen

3759

# A Probabilistic Framework for Discovering New Intents

Yunhua Zhou, Guofeng Quan and Xipeng Qiu

3771

# MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset

Leonhard Hennig, Philippe Thomas and Sebastian Möller

3785

# Towards Higher Pareto Frontier in Multilingual Machine Translation

yichong huang, Xiaocheng Feng, Xinwei Geng, Baohang Li and Bing Qin

3802

# Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization

Ze-Feng Gao, Kun Zhou, Peiyu Liu, Wayne Xin Zhao and Ji-Rong Wen

3819

# Entity Tracking in Language Models

Najoung Kim and Sebastian Schuster

3835

# A Textual Dataset for Situated Proactive Response Selection

Naoki Otani, Jun Araki, HyeongSik Kim and Eduard H Hovy

3856

# DiffusionNER: Boundary Diffusion for Named Entity Recognition

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang

3875

# WACO: Word-Aligned Contrastive Learning for Speech Translation

Siqi Ouyang, Rong Ye and Lei Li

3891

# Cross-lingual Continual Learning

Meryem M’hamdi, Xiang Ren and Jonathan May . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3908

# Faithful Question Answering with Monte-Carlo Planning

Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu and Changshui Zhang . . . . . . . . . . . . . 3944

# Unbalanced Optimal Transport for Unbalanced Word Alignment

Yuki Arase, Han Bao and Sho Yokoi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3966

# Guiding Computational Stance Detection with Expanded Stance Triangle Framework

Zhengyuan Liu, Yong Keong Yap, Hai Leong Chieu and Nancy Chen . . . . . . . . . . . . . . . . . . . . 3987

# Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast

Yiduo Guo, Yaobo Liang, Dongyan Zhao, Bing Liu and Nan Duan . . . . . . . . . . . . . . . . . . . . . . . 4002

# Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning

Ran Zhou, Xin Li, Lidong Bing, Erik Cambria and Chunyan Miao . . . . . . . . . . . . . . . . . . . . . . . 4018

# MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks

Letitia Parcalabescu and Anette Frank. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4032

# Towards Boosting the Open-Domain Chatbot with Human Feedback

Hua Lu, Siqi Bao, Huang He, Fan Wang, Hua Wu and Haifeng Wang . . . . . . . . . . . . . . . . . . . . 4060

# Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations

Yang Deng, Wenxuan Zhang, Yifei Yuan and Wai Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4079

# UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction

Hang Yan, Yu Sun, Xiaonan Li, Yunhua Zhou, Xuanjing Huang and Xipeng Qiu . . . . . . . . . . 4096

# Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model

Ali Omrani, Alireza Salkhordeh Ziabari, Charles Yu, Preni Golazizian, Brendan Kennedy, Mohammad Atari, Heng Ji and Morteza Dehghani . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4123

# Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation

Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Jason Wu, Caiming Xiong and Dragomir Radev . . . . . . . . . . . . . . . . . . . . . . . . . . . 4140

# FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information

Andrew Zhu, Karmanya Aggarwal, Alexander H Feng, Lara J. Martin and Chris Callison-Burch 4171

# A fine-grained comparison of pragmatic language understanding in humans and language models

Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko and Edward Gibson . . . . . 4194

# Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning

Wangzhen Guo, Qinkang Gong, Yanghui Rao and Hanjiang Lai . . . . . . . . . . . . . . . . . . . . . . . . . 4214

# Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning

Fan Zhou, Yuzhou Mao, Liu Yu, Yi Yang and Ting Zhong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4227

# Parameter-Efficient Fine-Tuning without Introducing New Latency

Baohao Liao, Yan Meng and Christof Monz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4242

# MANNER: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition

Jinyuan Fang, Xiaobin Wang, Zaiqiao Meng, Pengjun Xie, Fei Huang and Yong Jiang . . . . . 4261

# MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages

Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur and Prem Natarajan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4277

# Distilling Script Knowledge from Large Language Models for Constrained Language Planning

Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Pranav Shah, Charles Jankowski, Yanghua Xiao and Deqing Yang. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4303

# REDFM: a Filtered and Multilingual Relation Extraction Dataset

Pere Lluís Huguet Cabot, Simone Tedeschi, Axel-Cyrille Ngonga Ngomo and Roberto Navigli 4326

# Modeling Appropriate Language in Argumentation

Timon Ziegenbein, Shahbaz Syed, Felix Lange, Martin Potthast and Henning Wachsmuth . . 4344

# CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels

Hyunsoo Cho, Youna Kim and Sang-goo Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4364

# MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction

Zhibin Gou, qingyan guo and Yujiu Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4380

# ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems

Sarik Ghazarian, Yijia Shao, Rujun Han, Aram Galstyan and Nanyun Peng . . . . . . . . . . . . . . . 4398

# Explanation-based Finetuning Makes Models More Robust to Spurious Cues

Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki and Chris Callison-Burch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4420

# CAME: Confidence-guided Adaptive Memory Efficient Optimization

Yang Luo, Xiaozhe REN, Zangwei Zheng, ZHUO JIANG, Xin Jiang and Yang You . . . . . . . 4442

# On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning

Omar Shaikh, Hongxin Zhang, William Held, Michael S Bernstein and Diyi Yang . . . . . . . . . 4454

# Solving Math Word Problems via Cooperative Reasoning induced Language Models

Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, ruyi gan, Jiaxing Zhang and Yujiu Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4471

# Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model

Chantal Amrhein, Florian Schottmann, Rico Sennrich and Samuel L¨aubli. . . . . . . . . . . . . . . . .4486

# Early Discovery of Disappearing Entities in Microblogs

Satoshi Akasaki, Naoki Yoshinaga and Masashi Toyoda . . . . . . . . . . . . . . . . . . . . . . . . . 4507

# DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models

Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuanjing Huang and Xipeng Qiu4521

# Lifting the Curse of Capacity Gap in Distilling Language Models

Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang and Dawei Song 4535

# Towards Faithful Dialogues via Focus Learning

Yifan Deng, Xingsheng Zhang, Heyan Huang and Yue Hu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4554

# Back Translation for Speech-to-text Translation Without Transcripts

Qingkai Fang and Yang Feng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4567

# Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation

Ibrahim Taha Aksu, Min-Yen Kan and Nancy Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4588

# Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation

Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin and Frank Guerin . . . . . . . . . . . . . 4604

# Multi-modal Action Chain Abductive Reasoning

Mengze Li, Tianbao Wang, Jiahe Xu, Kairong Han, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, wenqiao zhang, Shiliang Pu and Fei Wu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4617

# Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change

Weinan He, Canming Huang, Zhanhao Xiao and Yongmei Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . 4629

# Unified Demonstration Retriever for In-Context Learning

Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, GUOTONG XIE, Xiaoling Wang and Xipeng Qiu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4644

# Movie101: A New Movie Understanding Benchmark

Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang and Qin Jin . . . . . . . . . . . . . . . . 4669

# Enhancing Language Representation with Constructional Information for Natural Language Understanding

Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai and Tianxiang Wang . . . . 4685

# Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs

Siyuan Wang, Zhongyu Wei, meng han, Zhihao Fan, Haijun Shan, Qi Zhang and Xuanjing Huang 4706

# DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships

Chenzhengyi Liu, Jie Huang, Kerui Zhu and Kevin Chen-Chuan Chang . . . . . . . . . . . . . . . . . . 4719

# Incorporating Attribution Importance for Improving Faithfulness Metrics

Zhixue Zhao and Nikolaos Aletras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4732

# Reward Gaming in Conditional Text Generation

Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur Parikh and He He . . 4746

# Hidden Schema Networks

Ramses J Sanchez, Lukas Alexander Conrads, Pascal Welke, Kostadin Cvejoski and Cesar Ali Ojeda Marin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4764

# Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations

Linlin Liu, Xingxuan Li, Megh Thakkar, Xin Li, Shafiq Joty, Luo Si and Lidong Bing . . 4799

# An Ordinal Latent Variable Model of Conflict Intensity

Niklas Stoehr, Lucas Torroba Hennigen, Josef Valvoda, Robert West, Ryan Cotterell and Aaron Schein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4817

# Multilingual Conceptual Coverage in Text-to-Image Models

Michael S Saxon and William Yang Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4831

# Pre-Training to Learn in Context

Yuxian Gu, Li Dong, Furu Wei and Minlie Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4849

# Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers

Manuel Mager, Elisabeth Albine Mager, Katharina Kann and Ngoc Thang Vu . . . . . . . . . . . . . 4871

# Revisiting non-English Text Simplification: A Unified Multilingual Benchmark

Michael Joseph Ryan, Tarek Naous and Wei Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4898

# Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments

Yu Gu, Xiang Deng and Yu Su . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4928

# Privacy-Preserving Domain Adaptation of Semantic Parsers

Fatemehsadat Mireshghallah, Yu Su, Tatsunori Hashimoto, Jason Eisner and Richard Shin . 4950

# Guide the Many-to-One Assignment: Open Information Extraction via IoU-aware Optimal Transport

Kaiwen Wei, Yiran Yang, li jin, Xian Sun, Zequn Zhang, Jingyuan Zhang, Xiao yu Li, Linhao Zhang, Jintao Liu and Guo Zhi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4971

# Actively Supervised Clustering for Open Relation Extraction

Jun Zhao, Yongxin Zhang, Qi Zhang, Tao Gui, Zhongyu Wei, Minlong Peng and Mingming Sun 4985

# ConvGQR: Generative Query Reformulation for Conversational Search

Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang and Jian-Yun Nie . . . . . . . . 4998

# KILM: Knowledge Injection into Encoder-Decoder Language Models

Yan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aishwarya Padmakumar, Yang Liu and Dilek Hakkani-Tur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5013

# VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions

Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang and Dongyan Zhao 5036

# NLPeer: A Unified Resource for the Computational Study of Peer Review

Nils Dycke, Ilia Kuznetsov and Iryna Gurevych . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5049

# IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures

Mingyu Zheng, Yang Hao, Wenbin Jiang, Zheng Lin, Yajuan Lyu, QiaoQiao She and Weiping Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5074

# Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization

Pengcheng He, Baolin Peng, Song Wang, Yang Liu, Ruochen Xu, Hany Hassan, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao and Xuedong Huang . . . . . . . . . . . . . . . . . . . 5095

# Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories

Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang and Tong Zhang . . . . . . . . . . . . . . . . . . . . . 5113

# Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model

Yi Xu, Shuqian Sheng, Jiexing Qi, Luoyi Fu, Zhouhan Lin, Xinbing Wang and Chenghu Zhou 5130

# Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications

Han Cheol Moon, Shafiq Joty, Ruochen Zhao, Megh Thakkar and Chi Xu . . . . . . . . . . . . . . . . 5145

SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes
                      Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li and William Yang Wang . . . . . . . . . . . . . . . . . 5166
                 Tokenization and the Noiseless Channel
                      Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan and Ryan Cotterell
                 5184
                 Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers
Jiaxi Li and Wei Lu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5208
                 MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning
                                                  Zhenrui Yue, Huimin Zeng, Yang Zhang, Lanyu Shang and Dong Wang . . . . . . . . . . . . . . . . . . 5223
                 Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment De-
                 tection
                                                                    yiwei wei, Shaozu Yuan, Ruosong Yang, Lei Shen, zhangmeizhi li, Longbiao Wang and Meng
                 Chen. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5240
                 COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective
                                                                       Zhaowei Wang, Quyet V. Do, Hongming Zhang, Jiayao Zhang, Weiqi Wang, Tianqing Fang,
                 Yangqiu Song, Ginny Y. Wong and Simon See . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5253
                 MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization
                      Shivam Sharma, Ramaneswaran S, Udit Arora, Md. Shad Akhtar and Tanmoy Chakraborty 5272
                 WikiHowQA: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering
                                                            Valeriia Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer and Mark Sanderson
                 5291
                 Making Language Models Better Reasoners with Step-Aware Verifier
                                                                 Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang LOU and Weizhu Chen5315
                 Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations
                                          Dongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang and Zheng Zhang . . . . . . . . . . . . . . . . . . . . . . . . . 5334
                 MISGENDERED: Limits of Large Language Models in Understanding Pronouns
                             Tamanna Hossain, Sunipa Dev and Sameer Singh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5352
                 Reasoning with Language Model Prompting: A Survey
                                                                   Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan,
                 Fei Huang and Huajun Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5368
                 Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evalua-
                 tion
                      Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoˆ
                                                                                                           ıt Sagot and Rachel Bawden . . . . . . . . 5394
                 Hybrid Knowledge Transfer for Improved Cross-Lingual Event Detection via Hierarchical Sample Se-
                 lection
                                                      Luis Fernando Guzman Nateras, Franck Dernoncourt and Thien Huu Nguyen . . . . . . . . . . . . . 5414
                 BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training
                                                                     Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun CHEN and Mingxuan Wang 5428
                 Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment
                      Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov and Louis-Philippe Morency
                 5444
                                                              xciii

# Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona

Yihong Tang, Bo Wang, Miao Fang, Dongming Zhao, Kun Huang, Ruifang He and Yuexian Hou

5456

# Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge

Yasumasa Onoe, Michael J.Q. Zhang, Shankar Padmanabhan, Greg Durrett and Eunsol Choi

5469

# Explaining How Transformers Use Context to Build Predictions

Javier Ferrando, Gerard I. Gállego, Ioannis Tsiamas and Marta R. Costa-jussà

5486

# DISCO: Distilling Counterfactuals with Large Language Models

Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal and Kyle Richardson

5514

# Non-Sequential Graph Script Induction via Multimedia Grounding

Yu Zhou, Sha Li, Manling Li, Xudong Lin, Shih-Fu Chang, Mohit Bansal and Heng Ji

5529

# SCOTT: Self-Consistent Chain-of-Thought Distillation

Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin and Xiang Ren

5546

# Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning

Nayeon Kim, Yinhua Piao and Sun Kim

5559

# Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization

Dongqi Pu, Yifan Wang and Vera Demberg

5574

# Evaluating Open-Domain Question Answering in the Era of Large Language Models

Ehsan Kamalloo, Nouha Dziri, Charles Clarke and Davood Rafiei

5591

# No clues good clues: out of context Lexical Relation Classification

Lucia Pitarch, Jordi Bernad, Lacramioara Dranca, Carlos Bobed Lisbona and Jorge Gracia

5607

# Won’t Get Fooled Again: Answering Questions with False Premises

Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu and Maosong Sun

5626

# What the DAAM: Interpreting Stable Diffusion Using Cross Attention

Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin and Ferhan Ture

5644

# Zero-shot Faithful Factual Error Correction

Kung-Hsiang Huang, Hou Pong Chan and Heng Ji

5660

# Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification

Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch and Jiawei Han

5677

# Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts

Mohna Chakraborty, Adithya Kulkarni and Qi Li

5698

# Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging

Fabian David Schmidt, Ivan Vulić and Goran Glavaš

5712

# Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training

Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng and Xinsong Zhang

5731

# Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars

Songlin Yang, Roger Levy and Yoon Kim

5747

# Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions

Satwik Bhattamishra, Arkil Patel, Varun Kanade and Phil Blunsom . . . . . . . . . . . . . . . . . . . . . . 5767

# Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation

Rishabh Gupta, Shaily Desai, Manvi Goel, Anil Bandhakavi, Tanmoy Chakraborty and Md. Shad Akhtar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5792

# DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation

Suraj N Kothawade, Anmol Reddy Mekala, D.Chandra Sekhara SS Hetha Havya, Mayank Kothyari, Rishabh K Iyer, Ganesh Ramakrishnan and Preethi Jyothi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5810

# Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework

Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin and Lidong Bing . . . . . . . . . . . . . . . . 5823

# Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation

Zhiwei Cao, Baosong Yang, Huan Lin, Suhang Wu, Xiangpeng Wei, Dayiheng Liu, Jun Xie, Min Zhang and Jinsong Su . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5841

# Node Placement in Argument Maps: Modeling Unidirectional Relations in High & Low-Resource Scenarios

Iman Jundi, Neele Falk, Eva Maria Vecchi and Gabriella Lapesa . . . . . . . . . . . . . . . . . . . . . . . . . 5854

# Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review

Fred Philippy, Siwen Guo and Shohreh Haddadan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5877

# Toward Human-Like Evaluation for Natural Language Generation with Error Analysis

Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F. Wong and Dacheng Tao . . . . . 5892

# Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation

Hongyi Wu, Hao Zhou, Man Lan, Yuanbin Wu and Yadong Zhang . . . . . . . . . . . . . . . . . . . . . . . 5908

# What is the best recipe for character-level encoder-only modelling?

Kris Cao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5924

# Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training

Zejun Li, Zhihao Fan, Jingjing Chen, Qi Zhang, Xuanjing Huang and Zhongyu Wei . . . . . . . 5939

# Learning OHelps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER

Ruotian Ma, xuanting chen, zhang lin, Xin Zhou, Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao and Yun Wen Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5959

# Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination

Hao Fei, Qian Liu, Meishan Zhang, Min Zhang and Tat-Seng Chua . . . . . . . . . . . . . . . . . . . . . . 5980

# CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition

Tingting Ma, Qianhui Wu, Huiqiang Jiang, B¨ orje F. Karlsson, Tiejun Zhao and Chin-Yew Lin 5995

# Dialect-robust Evaluation of Generated Text

Jiao Sun, Thibault Sellam, Elizabeth Clark, Tu Vu, Timothy Dozat, Dan Garrette, Aditya Siddhant, Jacob Eisenstein and Sebastian Gehrmann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6010

# Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation

Huaao Zhang, Qiang Wang, Bo - Qin, Zelin Shi, Haibo Wang and Ming Chen . . . . . . . . . . . 6029

# Language model acceptability judgements are not always robust to context

Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy and Adina Williams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6043

# RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations

Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi and Dragomir Radev . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6064

# Morphological Inflection: A Reality Check

Jordan Kodner, Sarah Payne, Salam Khalifa and Zoey Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6082

# TOME: A Two-stage Approach for Model-based Retrieval

Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen and Haifeng Wang . . . . . . . 6102

# Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner

Frank Palma Gomez, Subhadarshi Panda, Michael M Flor and Alla Rozovskaya . . . . . . . . . . . 6115

# Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks

Moritz Plenz, Juri Opitz, Philipp Heinisch, Philipp Cimiano and Anette Frank . . . . . . . . . . . . 6130

# miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings

Tassilo Klein and Moin Nabi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6159

# Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency

Mandar Sharma, Nikhil Muralidhar and Naren Ramakrishnan . . . . . . . . . . . . . . . . . . . . . . . . . . . 6178

# Forgotten Knowledge: Examining the Citational Amnesia in NLP

Janvijay Singh, Mukund Rungta, Diyi Yang and Saif M. Mohammad . . . . . . . . . . . . . . . . . . . . . 6192

# Measuring the Instability of Fine-Tuning

Yupei Du and Dong Nguyen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6209

# FairPrism: Evaluating Fairness-Related Harms in Text Generation

Eve Fleisig, Aubrie N Amstutz, Chad Atalla, Su Lin Blodgett, Hal Daumé III, Alexandra Olteanu, Emily Sheng, Dan Vann and Hanna Wallach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6231

# Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback

Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin and Idan Szpektor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6252

# SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams

Te-Lin Wu, Satwik Kottur, Andrea Madotto, Mahmoud Azab, Pedro Rodriguez, Babak Damavandi, Nanyun Peng and Seungwhan Moon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6273

# Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment

Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur and Tanmoy Chakraborty . . . . . . . . . . . 6292

# APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning

Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu and Xiang Ren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6308

MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering
                      Vaishali Pal, Andrew Yates, Evangelos Kanoulas and Maarten de Rijke . . . . . . . . . . . . . . . . . . . 6322
                 To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion
                                       Rui Li, Xu Chen, Chaozhuo Li, Yanming Shen, Jianan Zhao, Yujing Wang, Weihao Han, Hao
                 Sun, Weiwei Deng, Qi Zhang and Xing Xie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6335
                 CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation
                   Huimin Wang, Wai Chung Kwan, Kam-Fai Wong and Yefeng Zheng . . . . . . . . . . . . . . . . . . . . . 6348
                 Long-Tailed Question Answering in an Open World
      Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang and Yongbin Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6362
                 Parallel Context Windows for Large Language Models
                                       Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Dov
                 Karpas, Amnon Shashua, Kevin Leyton-Brown and Yoav Shoham . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6383
                 Efficient Transformers with Dynamic Token Pooling
                 Piotr Nawrot, Jan Chorowski, Adrian Lancucki and Edoardo Maria Ponti . . . . . . . . . . . . . . . . . 6403
                 Did the Models Understand Documents? Benchmarking Models for Language Understanding in Document-
                 Level Relation Extraction
    Haotian Chen, Bingsheng Chen and Xiangdong Zhou. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6418
                 ContraCLM: Contrastive Learning For Causal Language Model
                                   Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan,
                 Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma and Bing Xiang . . . . . . . . . . . . . . 6436
                 Advancing Multi-Criteria Chinese Word Segmentation Through Criterion Classification and Denoising
Tzu Hsuan Chou, Chun-Yi Lin and Hung-Yu Kao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6460
                 Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level
                 Implicit Discourse Relation Recognition
     Haodong Zhao, Ruifang He, Mengnan Xiao and Jing Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6477
                 Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model
                           Pengwei Zhan, Jing Yang, Xiao Huang, Chunlei Jing, Jingying Li and Liming Wang . . . . . . . 6493
                 Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children’s Fairy
                 Tales
                                        Paulina Toro Isaza, Guangxuan Xu, Toye Oloko, Yufang Hou, Nanyun Peng and Dakuo Wang
                 6509
                 FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue
                                      Weihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jingang Wang, Yunsen Xian and Weiran Xu
                 6532
                 LAMBADA: Backward Chaining for Automated Reasoning in Natural Language
                              Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu and Deepak Ramachandran . . . . . . . . 6547
                 PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives
                                  Silin Gao, Beatriz Borges, Soyoung Oh, Deniz Bayazit, Saya Kanno, Hiromi Wakaki, Yuki Mit-
                 sufuji and Antoine Bosselut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6569
                 OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment
                 Xize Cheng, Tao Jin, Linjun Li, Wang Lin, Xinyu Duan and Zhou Zhao . . . . . . . . . . . . . . . . . . 6592
                                                              xcvii

# Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models

Rui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang, Yasheng Wang, Yitong Li, Lifeng Shang, Kam-Fai Wong and Ruifeng Xu

6608

# BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval

Shicheng Xu, Liang Pang, Huawei Shen and Xueqi Cheng

6620

# Multiview Identifiers Enhanced Generative Retrieval

Yongqi Li, Nan Yang, Liang Wang, Furu Wei and Wenjie Li

6636

# Prompting Language Models for Linguistic Structure

Terra Blevins, Hila Gonen and Luke Zettlemoyer

6649

# Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis

Agam Shah, Suvan Satya Paturi and Sudheer Chava

6664

# RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction

Jun Zhao, WenYu Zhan, Xin Zhao, Qi Zhang, Tao Gui, Zhongyu Wei, Junzhe Wang, Minlong Peng and Mingming Sun

6680

# SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration

Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Meeyoung Cha, Yejin Choi, BYOUNGPIL KIM, Gunhee Kim, Eun-Ju Lee, Yong Lim, Alice Oh, Sangchul Park and Jung-Woo Ha

6692

# Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation

Soyoung Yoon, Sungjoon Park, Gyuwan Kim, Junhee Cho, Kihyo Park, Gyu Tae Kim, Minjoon Seo and Alice Oh

6713

# FLamE: Few-shot Learning from Natural Language Explanations

Yangqiaoyu Zhou, Yiming Zhang and Chenhao Tan

6743

# Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning

Subhajit Chaudhury, Sarathkrishna Swaminathan, Daiki Kimura, Prithviraj Sen, Keerthiram Murugesan, Rosario Uceda-Sosa, Michiaki Tatsubori, Achille Fokoue, Pavan Kapanipathi, Asim Munawar and Alexander Gray

6764

# Counterfactual Debiasing for Fact Verification

Weizhi Xu, Qiang Liu, Shu Wu and Liang Wang

6777

# What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics

Julia Watson, Barend Beekhuizen and Suzanne Stevenson

6790

# Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View

Changmeng Zheng, Junhao Feng, Yi Cai, Xiaoyong Wei and Qing Li

6810

# Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization

Rongxin Zhu, Jianzhong Qi and Jey Han Lau

6825

# Improving the Robustness of Summarization Systems with Dual Augmentation

Xiuying Chen, Guodong Long, Chongyang Tao, Mingzhe Li, Xin Gao, Chengqi Zhang and Xianliang Zhang

6846

# Interpretable Math Word Problem Solution Generation via Step-by-step Planning

mengxue zhang, Zichao Wang, Zhichao Yang, weiqi feng and Andrew Lan

6858

# TemplateGEC: Improving Grammatical Error Correction with Detection Template

Yinghao Li, Xuebo Liu, Shuo Wang, Peiyuan Gong, Derek F. Wong, Yang Gao, Heyan Huang and Min Zhang

6878

# Deep Model Compression Also Helps Models Capture Ambiguity

Hancheol Park and Jong Park

6893

# Are Experts Needed? On Human Evaluation of Counselling Reflection Generation

Zixiu Wu, Simone Balloccu, Ehud Reiter, Rim Helaoui, Diego Reforgiato Recupero and Daniele Riboni

6906

# PairSpanBERT: An Enhanced Language Model for Bridging Resolution

Hideo Kobayashi, Yufang Hou and Vincent Ng

6931

# Compounding Geometric Operations for Knowledge Graph Completion

Xiou Ge, Yun Cheng Wang, Bin Wang and C.-C. Jay Kuo

6947

# Few-shot In-context Learning on Knowledge Base Question Answering

Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su and Wenhu Chen

6966

# Fact-Checking Complex Claims with Program-Guided Reasoning

Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan and Preslav Nakov

6981

# Patton: Language Model Pretraining on Text-Rich Networks

Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu and Jiawei Han

7005

# Soft Language Clustering for Multilingual Model Pre-training

Jiali Zeng, Yufan Jiang, Yongjing Yin, Yi Jing, Fandong Meng, Binghuai Lin, Yunbo Cao and Jie Zhou

7021

# Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach

Nidhi Vakil and Hadi Amiri

7036

# When and how to paraphrase for named entity recognition?

Saket Sharma, Aviral Joshi, Yiyun Zhao, Namrata Mukhija, Hanoz Bhathena, Prateek Singh and Sashank Santhanam

7052

# UniEvent: Unified Generative Model with Multi-Dimensional Prefix for Zero-Shot Event-Relational Reasoning

Zhengwei Tao, Zhi Jin, Haiyan Zhao, Chengfeng Dou, yongqiang zhao, Tao Shen and Chongyang Tao

7088

# Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales

Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi and Xiang Ren

7103

# Automatic Annotation of Direct Speech in Written French Narratives

Noé Durandard, Viet Anh TRAN, Gaspard Michel and Elena V. Epure

7129

# Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations

Hyunjae Kim, jaehyo yoo, Seunghyun Yoon and Jaewoo Kang

7148

# Dynamic Transformers Provide a False Sense of Efficiency

Yiming Chen, Simin Chen, Zexin Li, Wei Yang, Cong Liu, Robby Tan and Haizhou Li

7164

# Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features

Ester Hlavnova and Sebastian Ruder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7181

# Local Byte Fusion for Neural Machine Translation

Makesh Narsimhan Sreedhar, Xiangpeng Wan, Yu Cheng and Junjie Hu . . . . . . . . . . . . . . . . . . 7199

# Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation

Benjamin Minixhofer, Jonas Pfeiffer and Ivan Vuli´c . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7215

# Multi-target Backdoor Attacks for Code Pre-trained Models

Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang and Yang Liu . . . . . 7236

# Learning Better Masking for Better Language Model Pre-training

Dongjie Yang, Zhuosheng Zhang and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7255

# VisText: A Benchmark for Semantically Rich Chart Captioning

Benny J. Tang, Angie Boggust and Arvind Satyanarayan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7268

# Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora

Svanhvít Lilja Ingólfsdóttir, Petur Orri Ragnarsson, Haukur Páll Jónsson, Haukur Barri Simonarson, Vilhjalmur Thorsteinsson and Vésteinn Snæbjarnarson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7299

# Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text

Qianhui Wu, Huiqiang Jiang, Haonan Yin, B¨orje F. Karlsson and Chin-Yew Lin . . . . . . . . . . . 7317

# Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection

Apoorva Singh, Raghav Jain, Prince Jha and Sriparna Saha . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7333

# MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation

Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao and Qingwei Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7348

# ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models

Jonas Belouadi and Steffen Eger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7364

# Envisioning Future from the Past: Hierarchical Duality Learning for Multi-Turn Dialogue Generation

Ang Lv, Jinpeng Li, Shufang Xie and Rui Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7382

# DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations

Duzhen Zhang, Feilong Chen and Xiuyi Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7395

# Consistent Prototype Learning for Few-Shot Continual Relation Extraction

Xiudi Chen, Hui Wu and xiaodong shi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7409

# Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models

Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco and Giulio Zizzo . 7423

# Large Language Models Meet NL2Code: A Survey

Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji and Jian-Guang LOU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7443

# When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP

Jingwei Ni, Zhijing Jin, QIAN WANG, Mrinmaya Sachan and Markus Leippold . . . . . . . . . . 7465

# Enhancing Grammatical Error Correction Systems with Explanations

Yuejiao Fei, Leyang Cui, Sen Yang, Wai Lam, Zhenzhong Lan and Shuming Shi . . . . . . . . . . 7489

# Linguistic representations for fewer-shot relation extraction across domains

Sireesh Gururaja, Ritam Dutt, Tinglong Liao and Carolyn Rosé . . . . . . . . . . . . . . . . . . . . . . . . . . 7502

# DarkBERT: A Language Model for the Dark Side of the Internet

Youngjin Jin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee and Seungwon Shin . . . . 7515

# MDACE: MIMIC Documents Annotated with Code Evidence

Hua Cheng, Rana Jafari, April D Russell, Russell Klopfer, Edmond Lu, Benjamin R Striner and Matthew R. Gormley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7534

# Towards Zero-Shot Multilingual Transfer for Code-Switched Responses

Ting-Wei Wu, Changsheng Zhao, Ernie Chang, Yangyang Shi, Pierce I-Jen Chuang, Vikas Chandra and Biing Juang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7551

# One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning

Guangtao Zeng, Peiyuan Zhang and Wei Lu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7564

# Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk

jianquan li, XiangBo Wu, Xiaokang Liu, Qianqian Xie, Prayag Tiwari and Benyou Wang . . . 7581

# Convergence and Diversity in the Control Hierarchy

Alexandra Cristina Butoi, Ryan Cotterell and David Chiang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7597

# ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis

Jiuding Yang, Yakun Yu, Di Niu, Weidong Guo and Yu Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7617

# Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic

Connor F Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss and Lise Getoor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7631

# Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Back-door Watermark

Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Benjamin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun and Xing Xie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7653

# Answering Ambiguous Questions via Iterative Prompting

Weiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren, Zhumin CHEN, Maarten de Rijke and Zhaochun Ren . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7669

# A Dataset of Argumentative Dialogues on Scientific Papers

Federico Ruggeri, Mohsen Mesgar and Iryna Gurevych . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7684

# Massively Multilingual Lexical Specialization of Multilingual Transformers

Tommaso Green, Simone Paolo Ponzetto and Goran Glavaˇ s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7700

# RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs

Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya and Niket Tandon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7716

# WebIE: Faithful and Robust Information Extraction on the Web

Chenxi Whitehouse, Clara Vania, Alham Fikri Aji, Christos Christodoulopoulos and Andrea Pierleoni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7734

# NormBank: A Knowledge Bank of Situational Social Norms

Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy and Diyi Yang . . . . . . . . . . . . . . 7756

# DIP: Dead code Insertion based Black-box Attack for Programming Language Model

CheolWon Na, YunSeok Choi and Jee-Hyong Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7777

# Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolu-

Wei Liu, Xiyan Fu and Michael Strube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7792

# HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification

He Zhu, Chong Zhang, Junjie Huang, Junran Wu and Ke Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7809

# Contextual Knowledge Learning for Dialogue Generation

Wen Zheng, Natasa Milic-Frayling and Ke Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7822

# Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation

Ke Wang, Xin Ge, Jiayi Wang, Yuqi Zhang and Yu Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7840

# Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated

Parallel Corpus of Novels Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Mrinmaya Sachan and Ryan Cotterell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7853

# CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation

Yan Zhou, Qingkai Fang and Yang Feng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7873

# On the Evaluation of Neural Selective Prediction Methods for Natural Language Processing

Zhengyao Gu and Mark Hopkins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7888

# Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment

Tianshu Yu, haoyu gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, chao wang, Fei Huang and Yongbin Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7900

# Text Style Transfer with Contrastive Transfer Pattern Mining

Jingxuan Han, Quan Wang, Licheng Zhang, Weidong Chen, Yan Song and Zhendong Mao . 7914

# Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning

Zhenrui Yue, Huimin Zeng, Mengfei Lan, Heng Ji and Dong Wang . . . . . . . . . . . . . . . . . . . . . . 7928

# Text Style Transfer Back-Translation

Daimeng Wei, Zhanglin Wu, Hengchao Shang, Zongyao Li, Minghan Wang, Jiaxin GUO, Xiaoyu Chen, Zhengzhe YU and Hao Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7944

# Generating Visual Spatial Description via Holistic 3D Scene Understanding

Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang and Tat-Seng Chua . . . 7960

# Continual Knowledge Distillation for Neural Machine Translation

Yuanchi Zhang, Peng Li, Maosong Sun and Yang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7978

# Query Refinement Prompts for Closed-Book Long-Form QA

Reinald Kim Amplayo, Kellie Webster, Michael Collins, Dipanjan Das and Shashi Narayan 7997

# CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding

Zhijian Hou, Wanjun Zhong, Lei Ji, DIFEI GAO, Kun Yan, W.K. Chan, Chong-Wah Ngo, Mike Zheng Shou and Nan Duan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8013

# Few-Shot Document-Level Event Argument Extraction

Xianjun Yang, Yujie Lu and Linda R Petzold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8029

# ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation

Kuan-Hao Huang, Varun Iyer, I-Hung Hsu, Anoop Kumar, Kai-Wei Chang and Aram Galstyan 8047

# Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation

Songming Zhang, Yunlong Liang, Shuaibo Wang, Yufeng Chen, Wenjuan Han, Jian Liu and Jinan Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8062

# Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering

Vishwajeet Kumar, Yash Gupta, Saneem Ahmed Chemmengath, Jaydeep Sen, Soumen Chakrabarti, Samarth Bharadwaj and Feifei Pan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8080

# HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level

Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song and Wei Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8095

# ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning

Wenjun Hou, Kaishuai Xu, Yi Cheng, Wenjie Li and Jiang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . 8108

# Data Curation Alone Can Stabilize In-context Learning

Ting-Yun Chang and Robin Jia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8123

# MidMed: Towards Mixed-Type Dialogues for Medical Consultation

Xiaoming Shi, Zeming Liu, Chuan Wang, Haitao Leng, Kui Xue, Xiaofan Zhang and Shaoting Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8145

# FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning

Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren and Hannaneh Hajishirzi . . . . . . . . . . . . . 8158

# S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction

Benfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yongdong Zhang and Zhendong Mao . . . . . 8186

# DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models

Xuxi Chen, Tianlong Chen, Weizhu Chen, Ahmed Hassan Awadallah, Zhangyang Wang and Yu Cheng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8208

# CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation

Jinfeng Zhou, Chujie Zheng, Bo Wang, Zheng Zhang and Minlie Huang . . . . . . . . . . . . . . . . . . 8223

# Comparative evaluation of boundary-relaxed annotation for Entity Linking performance

Gabriel Herman Bernardim Andrade, Shuntaro Yada and Eiji ARAMAKI . . . . . . . . . . . . . . . . 8238

# Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?

Shuheng Liu and Alan Ritter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8254

# READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises

Chenglei Si, Zhengyan Zhang, Yingfa Chen, Xiaozhi Wang, Zhiyuan Liu and Maosong Sun 8272

# MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification

Evan Dufraisse, Adrian Popescu, Julien Tourille, Armelle Brun and Jerome Deshayes . . . . . . 8286

# A New Dataset and Empirical Study for Sentence Simplification in Chinese

Shiping Yang, Renliang Sun and Xiaojun Wan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8306

# Factual or Contextual? Disentangling Error Types in Entity Description Generation

Navita Goyal, Ani Nenkova and Hal Daumé III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8322

# Weakly Supervised Vision-and-Language Pre-training with Relative Representations

Chi Chen, Peng Li, Maosong Sun and Yang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8341

# HermEs: Interactive Spreadsheet Formula Prediction via Hierarchical Formulet Expansion

Wanrong He, Haoyu Dong, Yihuai Gao, zhichao fan, Xingzhuo Guo, Zhitao Hou, Xiao Lv, Ran Jia, Shi Han and Dongmei Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8356

# ArgU: A Controllable Factual Argument Generator

Sougata Saha and Rohini K Srihari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8373

# Learning Answer Generation using Supervision from Automatic Question Answering Evaluators

Matteo Gabburo, Siddhant Garg, Rik Koncel-Kedziorski and Alessandro Moschitti . . . . . . . . 8389

# RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation

Shuai Liu, Hyundong J Cho, Marjorie Freedman, Xuezhe Ma and Jonathan May . . . . . . . . . . 8404

# Don’t Parse, Choose Spans! Continuous and Discontinuous Constituency Parsing via Autoregressive Span Selection

Songlin Yang and Kewei Tu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8420

# Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing

Maxwell Crouse, Pavan Kapanipathi, Subhajit Chaudhury, Tahira Naseem, Ramon Fernandez Astudillo, Achille Fokoue and Tim Klinger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8434

# AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression

Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang and Rui Wang . . . . . . . . . . . . . . . . . . . 8449

# (QA)2: Question Answering with Questionable Assumptions

Najoung Kim, Phu Mon Htut, Samuel R. Bowman and Jackson Petty . . . . . . . . . . . . . . . . . . . . . 8466

# Attributable and Scalable Opinion Summarization

Tom Hosking, Hao Tang and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8488

# Targeted Data Generation: Finding and Fixing Model Weaknesses

Zexue He, Marco Tulio Ribeiro and Fereshte Khani . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8506

# HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation

Anchun Gui and Han Xiao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8521

# CFSum Coarse-to-Fine Contribution Network for Multimodal Summarization

Min Xiao, Junnan Zhu, Haitao Lin, Yu Zhou and Chengqing Zong . . . . . . . . . . . . . . . . . . . . . . . 8538

# On Scientific Debtin NLP: A Case for More Rigour in Language Model Pre-Training Research

Made Nindyatama Nityasya, Haryo Akbarianto Wibowo, Alham Fikri Aji, Genta Indra Winata, Radityo Eko Prasojo, Phil Blunsom and Adhiguna Kuncoro . . . . . . . . . . . . . . . . . . . . . . . . . . 8554

# End-to-end Knowledge Retrieval with Multi-modal Queries

Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang and Chitta Baral . . . . . . . . . . . . . . . . . . 8573

# AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation

Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin and Zhou Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8590

Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection
                Feng Zhang, Wei Chen, Fei Ding and Tengjiao Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8605
                 VendorLink:                               An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on
                 Darknet Markets
                                       Vageesh Kumar Saxena, Nils Rethmeier, Gijs van Dijck and Gerasimos Spanakis . . . . . . . . . . 8619
                 Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-
                 of-Thought Method
               Yiming Wang, Zhuosheng Zhang and Rui Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8640
                 Efficient Shapley Values Estimation by Amortization for Text Classification
                                       Chenghao Yang, Fan Yin, He He, Kai-Wei Chang, Xiaofei Ma and Bing Xiang . . . . . . . . . . . . 8666
                 PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks
                       Weiwen Xu, Xin Li, Yang Deng, Wai Lam and Lidong Bing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8681
                 Dynamic Regularization in UDA for Transformers in Multimodal Classification
                                          Ivonne Monter-Aldana, Adrian Pastor Lopez Monroy and Fernando Sanchez-Vega . . . . . . . . . 8700
                 Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing
                             Lea Frermann, Jiatong Li, Shima Khanehzar and Gosia Mikolajczak . . . . . . . . . . . . . . . . . . . . . 8712
                 bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark
                                                Momchil Hardalov, Pepa Atanasova, Todor Mihaylov, Galia Angelova, Kiril Simov, Petya Oseno-
                 va, Veselin Stoyanov, Ivan K. Koychev, Preslav Nakov and Dragomir Radev . . . . . . . . . . . . . . . . . . . . 8733
                 DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation
                                     Yuxi Feng, Xiaoyuan Yi, Xiting Wang, Laks Lakshmanan, V.S. and Xing Xie . . . . . . . . . . . . . 8760
                 What does the Failure to Reason with Respectively” in Zero/Few-Shot Settings Tell Us about Language
                 Models?
                              Ruixiang Cui, Seolhwa Lee, Daniel Hershcovich and Anders Søgaard . . . . . . . . . . . . . . . . . . . . 8786
                 BLIND: Bias Removal With No Demographics
Hadas Orgad and Yonatan Belinkov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8801
                 How do humans perceive adversarial text? A reality check on the validity and naturalness of word-
                 based adversarial attacks
                     Salijona Dyrmishi, Salah GHAMIZI and Maxime Cordy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8822
                 Soft Alignment Objectives for Robust Adaptation of Language Generation
                      Michal ˇ
                Stefánik, Marek Kadlcik and Petr Sojka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8837
                 The CRINGE Loss: Learning what language not to model
                                                  Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar and Jason Weston
                 8854
                 Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process
       Fanghua Ye, zhiyuan hu and Emine Yilmaz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8875
                 Towards Identifying Fine-Grained Depression Symptoms from Memes
                                                    Shweta Yadav, Cornelia Caragea, Chenye Zhao, Naincy Kumari, Marvin A Solberg and Tanmay
                 Sharma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8890
                 SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks
                                                  Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-
                 Lun Wu, Hung-yi Lee, Karen Livescu and Shinji Watanabe. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8906
                                                                cv

# My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave

Pavan Holur, David Chong, Timothy R Tangherlini and Vwani Roychowdhury . . . . . . . . . . . . 8938

# Characterizing and Measuring Linguistic Dataset Drift

Tyler A Chang, Kishaloy Halder, Neha Anna John, Yogarshi Vyas, Yassine Benajiba, Miguel Ballesteros and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8953

# WebCPM: Interactive Web Search for Chinese Long-form Question Answering

Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun and Jie Zhou . . . 8968

# Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model

Hongwei Zeng, Bifan Wei, Jun Liu and Weiping Fu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8989

# FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction

Chen-Yu Lee, Chun-Liang Li, Hao Zhang, Timothy Dozat, Vincent Perot, Guolong Su, Xiang Zhang, Kihyuk Sohn, NIKOLAY GLUSHNEV, Renshen Wang, Joshua Ainslie, Shangbang Long, Siyang Qin, Yasuhisa Fujii, Nan Hua and Tomas Pfister . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9011

# MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies

Shiyue Zhang, Shijie Wu, Ozan Irsoy, Steven Lu, Mohit Bansal, Mark Dredze and David Rosenberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9027

# Knowledgeable Parameter Efficient Tuning Network for Commonsense Question Answering

Ziwang Zhao, Linmei Hu, Hanyu Zhao, Yingxia Shao and Yequan Wang . . . . . . . . . . . . . . . . . 9051

# BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric

Mingda Chen, Paul-Ambroise Augustin Duquenne, Pierre Y Andrews, Justine T Kao, Alexandre Mourachko, Holger Schwenk and Marta R. Costa-jussà . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9064

# NLPositionality: Characterizing Design Biases of Datasets and Models

Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke and Maarten Sap . . . 9080

# Backpack Language Models

John Hewitt, John Thickstun, Christopher D. Manning and Percy Liang . . . . . . . . . . . . . . . . . . 9103

# WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models

Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang and Jonathan May . . . . . . . . . . . . 9126

# Grounded Multimodal Named Entity Recognition on Social Media

Jianfei Yu, Ziyan Li, Jieming Wang and Rui Xia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9141

# Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference

Junhao Zheng, Qianli Ma, Shengjie Qiu, Yue Wu, Peitian Ma, Junlong Liu, Huawen Feng, Xichen Shang and Haibin Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9155

# Translation-Enhanced Multilingual Text-to-Image Generation

Yaoyiran Li, Ching-Yun Chang, Stephen Rawls, Ivan Vulić and Anna Korhonen . . . . . . . . . . . 9174

# Benchmarking Large Language Model Capabilities for Conditional Generation

Joshua Maynez, Priyanka Agrawal and Sebastian Gehrmann . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9194

# lilGym: Natural Language Visual Reasoning with Reinforcement Learning

Anne Wu, Kiante Brantley, Noriyuki Kojima and Yoav Artzi . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9214

# Unsupervised Melody-to-Lyrics Generation

Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar A Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang and Nanyun Peng . . . . . . . . . . . . . . . . . . 9235

# Causality-aware Concept Extraction based on Knowledge-guided Prompting

Siyu Yuan, Deqing Yang, Jinxi Liu, Shuyu Tian, Jiaqing Liang, Yanghua Xiao and Rui Xie.9255

# Span-level Aspect-based Sentiment Analysis via Table Filling

Mao Zhang, Yongxin Zhu, Zhen Liu, Zhimin Bao, Yunfei Wu, Xing Sun and Linli Xu . . . . . 9273

# Limitations of Language Models in Arithmetic and Symbolic Induction

Jing Qian, Hong Wang, Zekun Li, Shiyang Li and Xifeng Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . 9285

# EEL: Efficiently Encoding Lattices for Reranking

Prasann Singhal, Jiacheng Xu, Xi Ye and Greg Durrett . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9299

# CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training

Zhenhui Ye, Rongjie Huang, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Xiang Yin and Zhou Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9317

# Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation

Yulong Chen, Huajian Zhang, Yijie Zhou, Xuefeng Bai, Yueguan Wang, Ming Zhong, Jianhao Yan, Yafu Li, Judy Li, Xianchao Zhu and Yue Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9332

# Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation

Xiaohang Tang, Yi Zhou and Danushka Bollegala . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9352

# How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech

Aditya Yedetore, Tal Linzen, Robert Frank and R. Thomas McCoy . . . . . . . . . . . . . . . . . . . . . . . 9370

# GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator

Jian Yang, Shuming Ma, Li Dong, Shaohan Huang, Haoyang Huang, Yuwei Yin, Dongdong Zhang, Liqun Yang, Furu Wei and Zhoujun Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9394

# Linear Guardedness and its Implications

Shauli Ravfogel, Yoav Goldberg and Ryan Cotterell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9413

# Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM’s Translation Capability

Eleftheria Briakou, Colin Cherry and George Foster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9432

# Open Set Relation Extraction via Unknown-Aware Training

Jun Zhao, Xin Zhao, WenYu Zhan, Qi Zhang, Tao Gui, Zhongyu Wei, Yun Wen Chen, Xiang Gao and Xuanjing Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9453

# Learning to Imagine: Visually-Augmented Natural Language Generation

Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen . . . . . . . . 9468

# Generating Hashtags for Short-form Videos with Guided Signals

Tiezheng Yu, Hanchao Yu, Davis Liang, Yuning Mao, Shaoliang Nie, Po-Yao Huang, Madian Khabsa, Pascale Fung and Yi-Chia Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9482

# NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints

Mohaddeseh Bastan, Mihai Surdeanu and Niranjan Balasubramanian . . . . . . . . . . . . . . . . . . . . 9496

# The Best of Both Worlds:

# Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning

Zhuang Li, Lizhen Qu, Philip Cohen, Raj V Tumuluri and Gholamreza Haffari . . . . . . . . . . . . 9511

# Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the "What" and Focus on the "How"!

Chen Chen, Dylan Walker and Venkatesh Saligrama . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9529

# Unsupervised Extractive Summarization of Emotion Triggers

Tiberiu Sosea, Hongli Zhan, Junyi Jessy Li and Cornelia Caragea . . . . . . . . . . . . . . . . . . . . . . . . 9550

# Document-Level Event Argument Extraction With a Chain Reasoning Paradigm

Jian Liu, Chen Liang, Jinan Xu, Haoyan Liu and Zhe Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9570

# Pre-training Multi-party Dialogue Models with Latent Discourse Inference

Yiyang Li, Xinting Huang, Wei Bi and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9584

# Interpreting Positional Information in Perspective of Word Order

Zhang Xilong, Liu Ruochen, Liu Jin and Liang Xuefeng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9600

# I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation

Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West and Yejin Choi . . . . . . . . . . . . . . . . . . . . . . . . . 9614

# More than Classification: A Unified Framework for Event Temporal Relation Extraction

Quzhe Huang, Yutong Hu, Shengqi Zhu, Yansong Feng, Chang Liu and Dongyan Zhao . . . . 9631

# Multi-Source Test-Time Adaptation as Dueling Bandits for Extractive Question Answering

Hai Ye, Qizhe Xie and Hwee Tou Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9647

# Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery

Yutao Mou, Xiaoshuai Song, Keqing He, Chen Zeng, Pei Wang, Jingang Wang, Yunsen Xian and Weiran Xu. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9661

# DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering

Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu and Minlie Huang . . . . . . 9676

# Backdooring Neural Code Search

Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang and Bin Luo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9692

# Concise Answers to Complex Questions: Summarization of Long-form Answers

Abhilash C Potluri, Fangyuan Xu and Eunsol Choi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9709

# Towards Better Entity Linking with Multi-View Enhanced Distillation

Yi Liu, Yuan Tian, Jianxun Lian, xinlong wang, Yanan Cao, Fang Fang, Wen Zhang, Haizhen Huang, Weiwei Deng and Qi Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9729

# A Measure-Theoretic Characterization of Tight Language Models

Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner and Ryan Cotterell 9744

# PAED: Zero-Shot Persona Attribute Extraction in Dialogues

Luyao Zhu, Wei Li, Rui Mao, Vlad Pandelea and Erik Cambria . . . . . . . . . . . . . . . . . . . . . . . . . . 9771

# PromptRank: Unsupervised Keyphrase Extraction Using Prompt

Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun and Xiaoyan Bai . . 9788

# When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories

Alex Troy Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi and Hannaneh Haji-shirzi

9802

# infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information

Jaehyung Kim, Yekyung Kim, Karin Johanna Denton de Langis, Jinwoo Shin and Dongyeop Kang

9823

# SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models

Akshita Jha, Aida Mostafazadeh Davani, Chandan K Reddy, Shachi Dave, Vinodkumar Prabhakaran and Sunipa Dev

9851

# Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations

Lucy Lu Wang, Yulia Otmakhova, Jay DeYoung, Thinh Hung Truong, Bailey E. Kuehl, Erin A Bransom and Byron C. Wallace

9871

# Say What You Mean!

Large Language Models Speak Too Positively about Negative Commonsense Knowledge

Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li and Yanghua Xiao

9890

# An Inner Table Retriever for Robust Table Question Answering

Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria de Gispert and Gonzalo Iglesias

9909

# SIMSUM: Document-level Text Simplification via Simultaneous Summarization

Sofia Blinova, Xinyu Zhou, Martin Jaggi, Carsten Eickhoff and Seyed Ali Bahrainian

9927

# SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation

Junkai Zhou, Liang Pang, Huawei Shen and Xueqi Cheng

9945

# NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic

Zi’ou Zheng and Xiaodan Zhu

9960

# Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction

Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, David Wadden, Khendra G Lucas, Adam Miner, Theresa Nguyen and Tim Althoff

9977

# Dating Greek Papyri with Text Regression

John Pavlopoulos, Maria Konstantinidou, Isabelle Marthot-Santaniello, Holger Essler and Asimina Paparigopoulou

10001

# Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot and Ashish Sabharwal

10014

# Direct Fact Retrieval from Knowledge Graphs without Entity Linking

Jinheon Baek, Alham Fikri Aji, Jens Lehmann and Sung Ju Hwang

10038

# DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering

Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor and Omri Abend

10056

# A New Direction in Stance Detection: Target-Stance Extraction in the Wild

Yingjie Li, Krishna K Garg and Cornelia Caragea

10071

Improved Instruction Ordering in Recipe-Grounded Conversation
                     Duong Minh Le, Ruohao Guo, Wei Xu and Alan Ritter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10086
                 Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Pre-
                 dictions
    Byung-Doh Oh and William Schuler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10105
                 Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization
    Xinyu Wang, Lin Gui and Yulan He . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10118
                 Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training
                                      Zhenyu Zhang, Lei Shen, Yuming Zhao, Meng Chen and Xiaodong He . . . . . . . . . . . . . . . . . 10134
                 Language Detoxification with Attribute-Discriminative Latent Space
                     Jin Myung Kwak, Minseon Kim and Sung Ju Hwang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10149
                 Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction
                                            Changrong Min, Ximing Li, Liang Yang, Zhilin Wang, Bo Xu and Hongfei LIN . . . . . . . . . 10172
                 Adaptive and Personalized Exercise Generation for Online Language Learning
Peng Cui and Mrinmaya Sachan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10184
                 NLP Reproducibility For All: Understanding Experiences of Beginners
                        Shane Storks, Keunwoo Peter Yu, Ziqiao Ma and Joyce Chai . . . . . . . . . . . . . . . . . . . . . . . . . . . 10199
                 Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA
                                          Elias Stengel-Eskin, Jimena Guallar-Blasco, Yi Zhou and Benjamin Van Durme . . . . . . . . . 10220
                 UMRSpell: Unifying the Detection and Correction Parts of Pre-trained Models towards Chinese Mis-
                 sing, Redundant, and Spelling Correction
               Zheyu He, Yujin Zhu, Linlin Wang and Liang Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10238
                 LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction
                                                     Jeremiah Milbauer, Annie Louis, Mohammad Javad Hosseini, Alex Fabrikant, Donald Metzler
                 and Tal Schuster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10251
                 Local Interpretation of Transformer Based on Linear Decomposition
                                           Sen Yang, Shujian Huang, wei zou, Jianbing Zhang, Xinyu Dai and Jiajun CHEN . . . . . . . . 10270
                 DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
                                             Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu and Graham Neubig . . . . . . . . 10288
                 Multilingual Event Extraction from Historical Newspaper Adverts
                        Nadav Borenstein, Natália da Silva Perez and Isabelle Augenstein . . . . . . . . . . . . . . . . . . . . . . . 10304
                 BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency
                                                       Zhenyu Lei, Herun Wan, Wenqian Zhang, Shangbin Feng, Zilong Chen, Jundong Li, Qinghua
                 Zheng and Minnan Luo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10326
                 Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions
                                                 Mayur Patidar, Prayushi Faldu, Avinash Kumar Singh, Lovekesh Vig, Indrajit Bhattacharya and
                 Mausam - . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10341
                 Understanding Client Reactions in Online Mental Health Counseling
                                                    Anqi Li, Lizhi Ma, Yaling Mei, Hongliang He, Shuai Zhang, Huachuan Qiu and Zhenzhong Lan
                 10358
                                                                 cx

# Nonlinear Structural Equation Model Guided Gaussian Mixture Hierarchical Topic Modeling

HeGang Chen, Pengbo Mao, Yuyin Lu and Yanghui Rao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10377

# Revisiting Token Dropping Strategy in Efficient BERT Pretraining

Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du and Dacheng Tao 10391

# The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers

Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim and Eyal Shnarch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10406

# FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering

Anku Rani, S.M Towhidul Islam Tonmoy, Dwip D Dalal, Shreya Gautam, Megha Chakraborty, Aman Chadha, Amit Sheth and Amitava Das . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10421

# Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages

Arnav Anil Mhaske, Harshit Kedia, Sumanth Doddapaneni, Mitesh M. Khapra, Pratyush Kumar, Rudra Murthy and Anoop Kunchukuttan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10441

# CREPE: Open-Domain Question Answering with False Presuppositions

Xinyan Velocity Yu, Sewon Min, Luke Zettlemoyer and Hannaneh Hajishirzi . . . . . . . . . . . . 10457

# Joint Document-Level Event Extraction via Token-Token Bidirectional Event Completed Graph

Qizhi Wan, Changxuan Wan, Keli Xiao, Dexi Liu, Chenliang Li, Bolong Zheng, Xiping Liu and Rong Hu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10481

# Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering

Xiaolin Zheng, Mengling Hu, Weiming Liu, Chaochao Chen and Xinting Liao . . . . . . . . . . . 10493

# Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention

Rongchuan Tang, Yang Zhao, Chengqing Zong and Yu Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . 10508

# What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization

Griffin Adams, Bichlien H Nguyen, Jake Allen Smith, Yingce Xia, Shufang Xie, Anna Ostropolets, Budhaditya Deb, Yuan-Jyue Chen, Tristan Naumann and Noémie Elhadad . . . . . . . . . . . . . . . . 10520

# Annotating Mentions Alone Enables Efficient Domain Adaptation for Coreference Resolution

Nupoor Gandhi, Anjalie Field and Emma Strubell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10543

# A Universal Discriminator for Zero-Shot Generalization

Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng and Zhilin Yang . . . . . . . . . . . . . . . . . . . . . . . 10559

# Syntax and Geometry of Information

Raphaël Bailly, Laurent Leblond and Kata Gábor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10576

# GreenKGC: A Lightweight Knowledge Graph Completion Method

Yun Cheng Wang, Xiou Ge, Bin Wang and C.-C. Jay Kuo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10596

# Unsupervised Open-domain Keyphrase Generation

Lam Thanh Do, Pritom Saha Akash and Kevin Chen-Chuan Chang . . . . . . . . . . . . . . . . . . . . . 10614

# A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment

Jiyue Jiang, Sheng Wang, Qintong Li, Lingpeng Kong and Chuan Wu . . . . . . . . . . . . . . . . . . . 10628

# Plug-and-Play Knowledge Injection for Pre-trained Language Models

Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun and Jie Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10641

# Two Birds One Stone: Dynamic Ensemble for OOD Intent Classification

Yunhua Zhou, Jianqiang Yang, Pengyu Wang and Xipeng Qiu . . . . . . . . . . . . . . . . . . . . . . . . . . 10659

# SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages

Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq Joty, Caiming Xiong and Chien-Sheng Jason Wu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10674

# Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?

Juanhui Li, Harry Aaron Shomer, Jiayuan Ding, Yiqi Wang, Yao Ma, Neil Shah, Jiliang Tang and Dawei Yin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10696

# A dynamic programming algorithm for span-based nested named-entity recognition in O(n2)

Caio Corro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10712

# Target-Side Augmentation for Document-Level Machine Translation

Guangsheng Bao, ZHIYANG TENG and Yue Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10725

# Rethinking Masked Language Modeling for Chinese Spelling Correction

Hongqiu Wu, Shaohua Zhang, Yuchen Zhang and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10743

# A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues

Yunxin Li, Baotian Hu, Chen Xinyu, Yuxin Ding, Lin Ma and Min Zhang . . . . . . . . . . . . . . . 10757

# Simple and Effective Unsupervised Speech Translation

Changhan Wang, Hirofumi Inaguma, Peng-Jen Chen, Ilia Kulikov, Yun Tang, Wei-Ning Hsu, Michael Auli and Juan Pino . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10771

# Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation

Xuan Long Do, Bowei Zou, Shafiq Joty, Tran Anh Tai, Liangming Pan, Nancy Chen and Ai Ti Aw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10785

# CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification

Meiqi Chen, Yixin Cao, Yan Zhang and Zhiwei Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10804

# f-Divergence Minimization for Sequence-Level Knowledge Distillation

Yuqiao Wen, Zichao Li, Wenyu Du and Lili Mou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10817

# Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations

Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou and Songlin Hu . . . . . . . . . . . . . . . . . . . . . . . . . . 10835

# A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction

Ruoyu Zhang, Yanzeng Li and Lei Zou. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10853

# A Synthetic Data Generation Framework for Grounded Dialogues

Jianzhu Bao, Rui Wang, Yasheng Wang, Aixin Sun, Yitong Li, Fei Mi and Ruifeng Xu. . . .10866

# MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages

Cheikh M. Bamba Dione, David Ifeoluwa Adelani, Peter Nabende, Jesujoba Alabi, Thapelo Andrew Sindane, Happy Buzaaba, Shamsuddeen Hassan Muhammad, Chris Chinenye Emezue, Perez Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jonathan Mukiibi, Blessing K Siban- da, Bonaventure F. P. Dossou, Andiswa Bukula, Rooweither Mabuya, Allahsera Auguste Tapo, Edwin

Munkoh-Buabeng, victoire Memdjokam Koagne, Fatoumata Ouoba Kabore, Amelia Taylor, Godson K
                 KALIPE, Tebogo Macucwa, Vukosi Marivate, Tajuddeen Gwadabe, Mboning Tchiaze Elvis, Ikechu-
                 kwu Ekene Onyenwe, Gratien G. Atindogbe, Tolulope Anu Adelani, Idris Akinade, Olanrewaju Sam-
                 uel, Marien NAHIMANA, Théogène MUSABEYEZU, Emile Niyomutabazi, Ester Chimhenga, Kudzai
                 Gotosa, Patrick Mizha, Apelete AGBOLO, SEYDOU TRAORE, Chinedu Uchechukwu, Aliyu Yakubu
                 Yusuf, Muhammad Sulaiman Abdullahi and Dietrich Klakow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10883
                 Semantic Structure Enhanced Event Causality Identification
                                                       Zhilei Hu, Zixuan Li, Xiaolong Jin, Long Bai, Saiping Guan, Jiafeng Guo and Xueqi Cheng10901
                 Weakly-Supervised Spoken Video Grounding via Semantic Interaction Learning
                                                       Ye Wang, Wang Lin, Shengyu Zhang, Tao Jin, Linjun Li, Xize Cheng and Zhou Zhao . . . . 10914
                 Rehearsal-free Continual Language Learning via Efficient Parameter Isolation
                                                             Zhicheng Wang, Yufang Liu, Tao Ji, xiaoling Wang, Yuanbin Wu, congcong jiang, ye chao,
                 zhencong han, ling wang, xu shao and wenqiu zeng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10933
                 Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification
                                Chih Yao Chen, Tun Min Hung, Yi-Li Hsu and Lun-Wei Ku . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10947
                 Combo of Thinking and Observing for Outside-Knowledge VQA
                                           Qingyi Si, Yuchen Mo, Zheng Lin, HUISHAN JI and Weiping Wang . . . . . . . . . . . . . . . . . . . 10959
                 AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model
                                                I-Hung Hsu, Zhiyu Xie, Kuan-Hao Huang, Prem Natarajan and Nanyun Peng . . . . . . . . . . . . 10976
                 Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through
                 Modeling Social Relationships
                                                 David Jurgens, Agrima Seth, Jackson Sargent, Athena Aghighi and Michael Geraci . . . . . . . 10994
                 TART: Improved Few-shot Text Classification Using Task-Adaptive Reference Transformation
                                            Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen and Chang-Tien Lu . . . . . . . . . . . . . . . 11014
                 How Do In-Context Examples Affect Compositional Generalization?
                                                               Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang LOU and Dongmei
                 Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11027
                 Attractive Storyteller: Stylized Visual Storytelling with Unpaired Text
Dingyi Yang and Qin Jin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11053
                 Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation
                                                         Robert Giaquinto, Dejiao Zhang, Benjamin Kleiner, Yang Li, Ming Tan, Parminder Bhatia, Ra-
                 mesh Nallapati and Xiaofei Ma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11067
                 WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction
                         Qiyu Wu, Masaaki Nagata and Yoshimasa Tsuruoka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11084
                 Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models
           Junmo Kang, Wei Xu and Alan Ritter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11100
                 OD-RTE: A One-Stage Object Detection Framework for Relational Triple Extraction
                                                  Jinzhong Ning, Zhihao Yang, Yuanyuan Sun, Zhizheng Wang and Hongfei LIN . . . . . . . . . . 11120
                 I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons
                 and Dragons
                                                     Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi and
                 Prithviraj Ammanabrolu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11136
                                                               cxiii

# Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning

Tianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu and Xuanjing Huang . . . . . . . . . . . . . . . . . 11156

# Is GPT-3 a Good Data Annotator?

BOSHENG DING, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty and Lidong Bing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11173

# Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog

Fanqi Wan, Weizhou Shen, Ke Yang, Xiaojun Quan and Wei Bi . . . . . . . . . . . . . . . . . . . . . . . . 11196

# Few-shot Event Detection: An Empirical Study and a Unified View

Yubo Ma, Zehao Wang, Yixin Cao and Aixin Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11211

# How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases

Aaron Mueller and Tal Linzen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11237

# ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations

Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi and Chandra Bhagavatula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11253

# HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation

Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi and Matthew Peters 11272

# Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations

Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen and He He . . . . . . . . . . . . . . 11289

# An Inclusive Notion of Text

Ilia Kuznetsov and Iryna Gurevych . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11311

# AlignScore: Evaluating Factual Consistency with A Unified Alignment Function

Yuheng Zha, Yichi Yang, Ruichen Li and Zhiting Hu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11328

# Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation

Liqiang Jing, Xuemeng Song, Kun Ouyang, Mengzhao Jia and Liqiang Nie . . . . . . . . . . . . . . 11349

# Counterfactual Active Learning for Out-of-Distribution Generalization

Xun Deng, Wenjie Wang, Fuli Feng, Hanwang Zhang, Xiangnan He and Yong Liao . . . . . . 11362

# Multi-granularity Temporal Question Answering over Knowledge Graphs

Ziyang Chen, Jinzhi Liao and Xiang Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11378

# A New Aligned Simple German Corpus

Vanessa Toborek, Moritz Busch, Malte Boßert, Christian Bauckhage and Pascal Welke . . . 11393

# Introducing Semantics into Speech Encoders

Derek Q Xu, Shuyan Annie Dong, Changhan Wang, Suyoun Kim, Zhaojiang Lin, Bing Liu, Akshat Shrivastava, Shang-Wen Li, Liang-Hsuan Tseng, Guan-Ting Lin, Alexei Baevski, Hung-yi Lee, Yizhou Sun and Wei Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11413

# Constrained Tuple Extraction with Interaction-Aware Network

Xiaojun Xue, Chunxia Zhang, Tianxiang Xu and Zhendong Niu . . . . . . . . . . . . . . . . . . . . . . . . 11430

# MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning

Zhiyang Xu, Ying Shen and Lifu Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11445

# Single Sequence Prediction over Reasoning Graphs for Multi-hop QA

Gowtham Ramesh, Makesh Narsimhan Sreedhar and Junjie Hu . . . . . . . . . . . . . . . . . . . . . . . . . 11466

# Contrastive Error Attribution for Finetuned Language Models

Faisal Ladhak, Esin Durmus and Tatsunori Hashimoto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11482

# DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications

Adam Daniel Ivankay, Mattia Rigotti and Pascal Frossard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11499

# Neural Machine Translation for Mathematical Formulae

Felix Petersen, Moritz Schubotz, Andre Greiner-Petter and Bela Gipp . . . . . . . . . . . . . . . . . . . 11534

# Query-Efficient Black-Box Red Teaming via Bayesian Optimization

Deokjae Lee, JunYeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo Lee, Hwaran Lee and Hyun Oh Song . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11551

# SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control

Xiaochuang Han, Sachin Kumar and Yulia Tsvetkov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11575

# Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing

Chengyue Jiang, Wenyang Hui, Yong Jiang, Xiaobin Wang, Pengjun Xie and Kewei Tu . . . 11597

# MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition

Yuchen Hu, Chen Chen, Ruizhe Li, Heqing Zou and Eng Siong Chng . . . . . . . . . . . . . . . . . . . 11610

# Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors

Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin F Rousseau and Greg Durrett . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11626

# GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding

Jia-Chen Gu, Zhenhua Ling, Quan Liu, Cong Liu and Guoping Hu . . . . . . . . . . . . . . . . . . . . . 11645

# Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks

Artem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, Alexander Panchenko, Maxim Panov, Mikhail Burtsev and Artem Shelmanov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11659

# BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting

Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, KHALID ALMUBARAK, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Edward Raff, Dragomir Radev and Vassilina Nikoulina . . . . . . . . 11682

# Logic-driven Indirect Supervision: An Application to Crisis Counseling

Mattia Medina Grespan, Meghan Broadbent, Xinyao Zhang, Katherine E Axford, Brent Kious, Zac Imel and Vivek Srikumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11704

# Grounding Characters and Places in Narrative Text

Sandeep Soni, Amanpreet Sihra, Elizabeth F. Evans, Matthew Wilkens and David Bamman . . . 11723

# From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models

Shangbin Feng, Chan Young Park, Yuhan Liu and Yulia Tsvetkov . . . . . . . . . . . . . . . . . . . . . . . 11737

# SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT

Aditya Yadavalli, Alekhya Yadavalli and Vera Tobin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11763

# Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models

Albert Xu, Xiang Ren and Robin Jia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11778

# Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?

Chengwei Qin, Shafiq Joty, Qian Li and Ruochen Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11802

# Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale

Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11833

# Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya

Fitsum Gaim, Wonsuk Yang, Hancheol Park and Jong Park . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11857

# ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain

Mike Zhang, Rob van der Goot and Barbara Plank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11871

# CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval

Minghan Li, Sheng-Chieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wen-tau Yih and Xilun Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11891

# MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning

Bang Yang, Fenglin Liu, Xian Wu, Yaowei Wang, Xu Sun and Yuexian Zou . . . . . . . . . . . . . 11908

# Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Keith Luby, Christian C. Luhmann and H. Andrew Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11923

# In-sample Curriculum Learning by Sequence Completion for Natural Language Generation

Qi Jia, Yizhu Liu, Haifeng Tang and Kenny Q. Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11937

# Product Question Answering in E-Commerce: A Survey

Yang Deng, Wenxuan Zhang, Qian Yu and Wai Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11951

# Towards Domain-Agnostic and Domain-Adaptive Dementia Detection from Spoken Language

Shahla Farzana and Natalie Parde . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11965

# Generalizing Backpropagation for Gradient-Based Interpretability

Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alex Warstadt and Ryan Cotterell . . . . 11979

# UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language

Xinyi Mou, Zhongyu Wei, Qi Zhang and Xuanjing Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11996

# Generic Temporal Reasoning with Differential Analysis and Explanation

Yu Feng, Ben Zhou, Haoyu Wang, Helen Jin and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12013

# Model-Based Simulation for Optimising Smart Reply

Benjamin Towle and Ke Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12030

# Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval

John Wieting, Jonathan Clark, William Cohen, Graham Neubig and Taylor Berg-Kirkpatrick 12044

# On the Blind Spots of Model-Based Evaluation Metrics for Text Generation

Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass and Yulia Tsvetkov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12067

# Dealing with Semantic Underspecification in Multimodal NLP

Sandro Pezzelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12098

# Trigger Warning Assignment as a Multi-Label Document Classification Problem

Matti Wiegmann, Magdalena Wolska, Christopher Schröder, Ole Borchardt, Benno Stein and Martin Potthast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12113

# WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings

Wenjie Zhuo, Yifan Sun, Xiaohan Wang, Linchao Zhu and Yi Yang . . . . . . . . . . . . . . . . . . . . . 12135

# Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms

Tianshu Zhang, Changchang Liu, Wei-Han Lee, Yu Su and Huan Sun . . . . . . . . . . . . . . . . . . . 12149

# Causality-Guided Multi-Memory Interaction Network for Multivariate Stock Price Movement Prediction

Di Luo, Weiheng Liao, Shuqi Li, Xin Cheng and Rui Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12164

# DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization

SongYang Gao, Shihan Dou, Yan Liu, Xiao Wang, Qi Zhang, Zhongyu Wei, Jin Ma and Ying Shan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12177

# A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires

Hoyun Song, Jisu Shin, Huije Lee and Jong Park . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12190

# Downstream Datasets Make Surprisingly Good Pretraining Corpora

Kundan Krishna, Saurabh Garg, Jeffrey P. Bigham and Zachary Lipton . . . . . . . . . . . . . . . . . . 12207

# Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach

Liyan Xu, Chenwei Zhang, Xian Li, Jingbo Shang and Jinho D. Choi . . . . . . . . . . . . . . . . . . . 12223

# XDailydialog: A Multilingual Parallel Dialogue Corpus

Zeming Liu, Ping Nie, Jie Cai, Haifeng Wang, Zheng-Yu Niu, PENG ZHANG, Mrinmaya Sachan and Kaiping Peng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12240

# PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent

Kshitij Mishra, Priyanshu Priya and Asif Ekbal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12254

# Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis

Yue Deng, Wenxuan Zhang, Sinno Jialin Pan and Lidong Bing . . . . . . . . . . . . . . . . . . . . . . . . . 12272

# Contrastive Decoding: Open-ended Text Generation as Optimization

Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer and Mike Lewis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12286

# Resolving Indirect Referring Expressions for Entity Selection

Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti and Annie Louis . . . . . . . . . . . . . . 12313

# Accelerating Transformer Inference for Translation via Parallel Decoding

Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin and Emanuele Rodola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12336

# Hard Sample Aware Prompt-Tuning

Yuanjian Xu, Qi An, Jiahuan Zhang, Peng Li and Zaiqing Nie . . . . . . . . . . . . . . . . . . . . . . . . . . 12356

# WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events

Marco Antonio Stranisci, Rossana Damiano, Enrico Mensa, Viviana Patti, Daniele P. Radicioni and Tommaso Caselli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12370

Best-k Search Algorithm for Neural Text Generation
                      Jiacheng Xu, Caiming Xiong, silvio savarese and Yingbo Zhou . . . . . . . . . . . . . . . . . . . . . . . . . 12385
                 Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models
                 for Indic Languages
                                                     Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra,
                 Anoop Kunchukuttan and Pratyush Kumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12402
                 Transforming Visual Scene Graphs to Image Captions
                                                    Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Songfang Huang,
                 Fei Huang, Zhangzikang Li and Yu Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12427
                 Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks
                                                    Yun Tang, Anna Y Sun, Hirofumi Inaguma, Xinyue Chen, Ning Dong, Xutai Ma, Paden D Toma-
                 sello and Juan Pino . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12441
                 Improving Domain Generalization for Prompt-Aware Essay Scoring via Disentangled Representation
                 Learning
                      Zhiwei Jiang, Tianyi Gao, Yafeng Yin, Meng Liu, Hua Yu, Zifeng Cheng and Qing Gu . . . 12456
                 What’s the Meaning of Superhuman Performance in Today’s NLU?
                      Simone Tedeschi, Johan Bos, Thierry Declerck, Jan Hajiˇ
                                                                                                        c, Daniel Hershcovich, Eduard H Ho-
                 vy, Alexander Koller, Simon Krek, Steven Schockaert, Rico Sennrich, Ekaterina Shutova and Roberto
                 Navigli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12471
                 PromptNER: Prompt Locating and Typing for Named Entity Recognition
                                                      Yongliang Shen, Zeqi Tan, Shuhui Wu, Wenqi Zhang, Rongsheng Zhang, Yadong Xi, Weiming
                 Lu and Yueting Zhuang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12492
                 Hints on the data for language modeling of synthetic languages with transformers
Rodolfo Joel Zevallos and Nuria Bel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12508
                 Neural Machine Translation Methods for Translating Text to Sign Language Glosses
                  Dele Zhu, Vera Czehmann and Eleftherios Avramidis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12523
                 Revisiting Event Argument Extraction: Can EAE Models Learn Better When Being Aware of Event Co-
                 occurrences?
       Yuxin He, Jingyue Hu and Buzhou Tang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12542
                 HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation
                                                        Qianyu He, Yikai Zhang, Jiaqing Liang, Yuncheng Huang, Yanghua Xiao and Yunwen Chen
                 12557
                 Large-scale Lifelong Learning of In-context Instructions and How to Tackle It
                      Jisoo Mok, Jaeyoung Do, Sungjin Lee, Tara Taghavi, Seunghak Yu and Sungroh Yoon . . . . 12573
                 Controllable Text Generation via Probability Density Estimation in the Latent Space
                                                         Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Lingyuan Zhang, Heng Gong, Weihong
                 Zhong and Bing Qin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12590
                 Learning Latent Relations for Temporal Knowledge Graph Reasoning
                      Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu and Liang Wang . . . . . . . . . . . . . . . . . . . . . . . 12617
                 DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value
                 Function
                                                  Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han
                 Shi, Yujun Li, lin li, Jian Yin, Zhenguo Li and Xiaodan Liang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12632
                                                             cxviii

# Unsupervised Selective Rationalization with Noise Injection

Adam Storek, Melanie Subbiah and Kathleen McKeown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12647

# Understanding In-Context Learning via Supportive Pretraining Data

Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz and Tianlu Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12660

# ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation

Zhexin Zhang, Jiaxin Wen and Minlie Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12674

# Effective Contrastive Weighting for Dense Query Expansion

Xiao Wang, Sean MacAvaney, Craig Macdonald and Iadh Ounis . . . . . . . . . . . . . . . . . . . . . . . 12688

# Improving the Detection of Multilingual Online Attacks with Rich Social Media Data from Singapore

Janosch Haber, Bertie Vidgen, Matthew S Chapman, Vibhor Agarwal, Roy Ka-Wei Lee, Yong Keong Yap and Paul R¨ottger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12705

# Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model

Jakob Prange and Man Ho Ivy Wong. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12722

# Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization

Artidoro Pagnoni, Alex Fabbri, Wojciech Kryscinski and Chien-Sheng Jason Wu . . . . . . . . . 12737

# MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering

Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier and Julian Martin Eisenschlos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12756

# MGR: Multi-generator Based Rationalization

Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, YuanKai Zhang and Yang Qiu12771

# BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics

Liang Ma, Shuyang Cao, Robert L Logan IV, Di Lu, Shihao Ran, Ke Zhang, Joel Tetreault and Alejandro Jaimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12788

# Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection

Rheeya Uppaal, Junjie Hu and Yixuan Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12813

# UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization

Yulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng and Yue Zhang 12833

# RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue

Zhengliang Shi, Weiwei Sun, Shuo Zhang, Zhen Zhang, Pengjie Ren and Zhaochun Ren . . 12856

# An AMR-based Link Prediction Approach for Document-level Event Argument Extraction

Yuqing Yang, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu and Zheng Zhang . . . . . 12876

# PuMer: Pruning and Merging Tokens for Efficient Vision Language Models

Qingqing Cao, Bhargavi Paranjape and Hannaneh Hajishirzi . . . . . . . . . . . . . . . . . . . . . . . . . . . 12890

# Gloss-Free End-to-End Sign Language Translation

Kezhou Lin, Xiaohan Wang, Linchao Zhu, Ke Sun, bang zhang and Yi Yang . . . . . . . . . . . . . 12904

# TAGPRIME: A Unified Framework for Relational Structure Extraction

I-Hung Hsu, Kuan-Hao Huang, Shuning Zhang, Wenxin Cheng, Prem Natarajan, Kai-Wei Chang and Nanyun Peng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12917

# Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers

Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao and Xia Song

Page: 12933

# BITE: Textual Backdoor Attacks with Iterative Trigger Injection

Jun Yan, Vansh Gupta and Xiang Ren

Page: 12951

# A Crosslingual Investigation of Conceptualization in 1335 Languages

Yihong Liu, Haotian Ye, Leonie Weissweiler, Philipp Wicke, Renhao Pei, Robert Zangenfeind and Hinrich Schütze

Page: 12969

# Exploring and Verbalizing Academic Ideas by Concept Co-occurrence

Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang and Chenghu Zhou

Page: 13001

# mCLIP: Multilingual CLIP via Cross-lingual Transfer

Guanhua Chen, Lu Hou, Yun Chen, Wenliang Dai, Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan and Wenping Wang

Page: 13028

# Distantly Supervised Course Concept Extraction in MOOCs with Academic Discipline

Mengying Lu, Yuquan Wang, Jifan Yu, Yexing Du, Lei Hou and Juanzi Li

Page: 13044

# Extrinsic Evaluation of Machine Translation Metrics

Nikita Moghe, Tom Sherborne, Mark Steedman and Alexandra Birch

Page: 13060

# ExplainMeetSum: A Dataset for Explainable Meeting Summarization Aligned with Human Intent

Hyun Kim, Minsoo Cho and Seung-Hoon Na

Page: 13079

# A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation

Xiaoheng Zhang and Yang Li

Page: 13099

# CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning

Weiqi Wang, Tianqing Fang, Baixuan Xu, Chun Yi Louis Bo, Yangqiu Song and Lei Chen

Page: 13111

# The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research

Mohamed Abdalla, Jan Philip Wahle, Terry Lima Ruas, Aurélie Névéol, Fanny Ducel, Saif M. Mohammad and Karen Fort

Page: 13141

# Language of Bargaining

Mourad Heddaya, Solomon E Dworkin, Chenhao Tan, Rob Voigt and Alexander K Zentefis

Page: 13161

# Do Question Answering Modeling Improvements Hold Across Benchmarks?

Nelson F. Liu, Tony Lee, Robin Jia and Percy Liang

Page: 13186

# VLN-Trans: Translator for the Vision and Language Navigation Agent

Yue Zhang and Parisa Kordjamshidi

Page: 13219

# Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models

Qinhong Zhou, Zonghan Yang, Peng Li and Yang Liu

Page: 13234

# Continual Contrastive Finetuning Improves Low-Resource Relation Extraction

Wenxuan Zhou, Sheng Zhang, Tristan Naumann, Muhao Chen and Hoifung Poon

Page: 13249

# KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment

Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong and Hongzhi Yin

Page: 13264

UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language
                                 Nuwa Xi, Sendong Zhao, Haochun Wang, Chi Liu, Bing Qin and Ting Liu . . . . . . . . . . . . . . 13277
                 Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive
                 Multi-hop Paths
Xiangqing Shen, Siwei Wu and Rui Xia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13292
                 Shrinking Embeddings for Hyper-Relational Knowledge Graphs
             Bo Xiong, Mojtaba Nayyeri, Shirui Pan and Steffen Staab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13306
                 CTC-based Non-autoregressive Speech Translation
                                                   Chen Xu, Xiaoqian Liu, Xiaowen Liu, Qingxuan Sun, Yuhao Zhang, Murun Yang, Qianqian
                 Dong, Tom Ko, Mingxuan Wang, Tong Xiao, Anxiang Ma and Jingbo Zhu . . . . . . . . . . . . . . . . . . . 13321
                 Attention as a Guide for Simultaneous Speech Translation
  Sara Papi, Matteo Negri and Marco Turchi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13340
                 On Complementarity Objectives for Hybrid Retrieval
                                            Dohyeon Lee, Seung-won Hwang, Kyungjae Lee, Seungtaek Choi and Sunghyun Park . . . . 13357
                 C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection
      Chenye Zhao, Yingjie Li and Cornelia Caragea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13369
                 Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding
                                            Haoli Bai, Zhiguang Liu, Xiaojun Meng, li wentao, Shuang Liu, Yifeng LUO, nian xie, Rongfu
                 Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, Xin Jiang and Qun Liu . . . . . . . . . . . . . . . . . . . . . 13386
                 PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts
                      Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang and Yongbin Li . . . . . . . . . . . 13402
                 MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning
                                             Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, ruyi gan, Jiaxing Zhang and Liwei Wang
                 13417
                 PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation
          Shaolin Zhu, Shangjie Li, Yikun Lei and Deyi Xiong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13433
                 Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection
           Erik Arakelyan, Arnav Arora and Isabelle Augenstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13448
                 DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles
                                               Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, N M Anoop Krishnan and Mau-
                 sam - . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13465
                 Self-Instruct: Aligning Language Models with Self-Generated Instructions
                                            Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi and
                 Hannaneh Hajishirzi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13484
                 Disentangled Phonetic Representation for Chinese Spelling Correction
       Zihong Liang, Xiaojun Quan and Qifan Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13509
                 Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis
                            Ta-Chung Chi, Ting-Han Fan, alexander rudnicky and Peter J Ramadge . . . . . . . . . . . . . . . . . 13522
                 CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models
                                      Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen and Mykola Pechenizkiy. . . . . . .13538
                                                               cxxi

Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human
                 feedback
                                                            Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau and Jason Weston . 13557
                 Uncovering and Categorizing Social Biases in Text-to-SQL
                                                Yan Liu, Yan Gao, Zhe Su, Xiaokang Chen, Elliott Ash and Jian-Guang LOU . . . . . . . . . . . . 13573
                 On the Compositional Generalization in Versatile Open-domain Dialogue
                            Tingchen Fu, Xueliang Zhao, Lemao Liu and Rui Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13585
                 What is the Real Intention behind this Question? Dataset Collection and Intention Classification
                                  Maryam Sadat Mirzaei, Kourosh Meshgi and Satoshi Sekine . . . . . . . . . . . . . . . . . . . . . . . . . . . 13606
                 Conjunct Resolution in the Face of Verbal Omissions
                   Royi Rassin, Yoav Goldberg and Reut Tsarfaty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13623
                 Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts
                                                               Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather M Foran and Y-Lan Boureau
                 13641
                 Learning In-context Learning for Named Entity Recognition
                                                         Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han
                 and Le Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13661
                 Holistic Prediction on a Time-Evolving Attributed Graph
                                           Shohei Yamasaki, Yuya Sasaki, Panagiotis Karras and Makoto Onizuka . . . . . . . . . . . . . . . . . 13676
                 Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional
                 Random Field
                                       Zixia Jia, Zhaohui Yan, Wenjuan Han, Zilong Zheng and Kewei Tu . . . . . . . . . . . . . . . . . . . . . 13695
                 Training Trajectories of Language Models Across Scales
                                                               Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi
                 Chen, Luke Zettlemoyer and Veselin Stoyanov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13711
                 A Diverse Set of Freely Available Linguistic Resources for Turkish
Duygu ALTINOK. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13739
                 Measuring Consistency in Text-based Financial Forecasting Models
                  Linyi Yang, Yingpeng Ma and Yue Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13751
                 Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation
                                            Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida and André Martins . . . . . . . . . . . . . . . 13766
                 RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank
                                                             Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Wei Wu, Yunsen Xian, Dongyan Zhao, Kai
                 Chen and Rui Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13785
                 Entailment as Robust Self-Learner
                           Jiaxin Ge, Hongyin Luo, Yoon Kim and James Glass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13803
                 ReCode: Robustness Evaluation of Code Generation Models
                                                             Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Ku-
                 mar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan,
                 Dan Roth and Bing Xiang. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13818
                                                               cxxii

# EPIC: Multi-Perspective Annotation of a Corpus of Irony

Simona Frenda, Alessandro Pedrani, Valerio Basile, Soda Marem Lo, Alessandra Teresa Cigna-rella, Raffaella Panizzon, Cristina Marco, Bianca Scarlini, Viviana Patti, Cristina Bosco and Davide Bernardi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13844

# Dialogue Summarization with Static-Dynamic Structure Fusion Graph

Shen Gao, Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao and Rui Yan13858

# Large-Scale Correlation Analysis of Automated Metrics for Topic Models

Jia Peng Lim and Hady Lauw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13874

# U-CREAT: Unsupervised Case Retrieval using Events extrAcTion

Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella and Ashutosh Modi . . . . . . . . . . . . . . . . . 13899

# ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis

Omkar Jayant Joshi, Priya N Pitre and Yashodhara Haribhakta . . . . . . . . . . . . . . . . . . . . . . . . . . 13916

# Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework

Mingqi Gao, Xiaojun Wan, Jia Su, Zhefeng Wang and baoxing Huai . . . . . . . . . . . . . . . . . . . . 13932

# Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker

Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi and Yulia Tsvetkov . . . . 13960

# Don’t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text

Ashim Gupta, Carter Wood Blum, Temma Choji, Yingjie Fei, Shalin Shah, Alakananda Vempala and Vivek Srikumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13981

# Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring

Cong Wang, Zhiwei Jiang, Yafeng Yin, Zifeng Cheng, Shiping Ge and Qing Gu . . . . . . . . . . 13999

# Mitigating Label Biases for In-context Learning

Yu Fei, Yifan Hou, Zeming Chen and Antoine Bosselut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14014

# QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations

Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee and Kristina Toutanova . . 14032

# Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering

Yujie Wang, Hu Zhang, Jiye Liang and Ru Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14048

# Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation

Hao Li, Viktor Schlegel, Riza Batista-Navarro and Goran Nenadic . . . . . . . . . . . . . . . . . . . . . . 14064

# Ambiguous Learning from Retrieval: Towards Zero-shot Semantic Parsing

Shan Wu, Chunlei Xin, Hongyu Lin, Xianpei Han, Cao Liu, Jiansong Chen, Fan Yang, Guanglu Wan and Le Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14081

# Explicit Syntactic Guidance for Neural Text Generation

Yafu Li, Leyang Cui, Jianhao Yan, Yongjing Yin, Wei Bi, Shuming Shi and Yue Zhang . . . 14095

# What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric

Enrico Liscio, Oscar Araque, Lorenzo Gatti, Ionut L. Constantinescu, Catholijn M Jonker, Kyriaki Kalimeri and Pradeep Kumar Murukannaiah . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14113

# Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning

Ziran Liang, Yuyin Lu, HeGang Chen and Yanghui Rao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14133

# Multimodal Persona Based Generation of Comic Dialogs

Harsh Agrawal, Aditya M. Mishra, Manish Gupta and Mausam - . . . . . . . . . . . . . . . . . . . . . . . 14150

# LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion

Dongfu Jiang, Xiang Ren and Bill Yuchen Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14165

# Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation

Weihao Zeng, Lulu Zhao, Keqing He, Ruotong Geng, Jingang Wang, Wei Wu and Weiran Xu 14179

# Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization

Minghang Zheng, Shaogang Gong, Hailin Jin, Yuxin Peng and Yang Liu . . . . . . . . . . . . . . . . 14197

# IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages

Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra and Raj Dabre . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14210

# Weaker Than You Think: A Critical Look at Weakly Supervised Learning

Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Joseph Stephan and Dietrich Klakow 14229

# Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases

Yingji Li, Mengnan Du, Xin Wang and Ying Wang. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14254

# Towards Understanding Omission in Dialogue Summarization

Yicheng Zou, Kaitao Song, Xu Tan, Zhongkai Fu, Qi Zhang, Dongsheng Li and Tao Gui . . 14268

# Python Code Generation by Asking Clarification Questions

Haau-Sing (Xiaocheng) Li, Mohsen Mesgar, André Martins and Iryna Gurevych . . . . . . . . . 14287

# A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports

Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, Che Lin and Chuan-Ju Wang . . . . . . . . . . . . 14307

# Improving the robustness of NLI models with minimax training

Michalis Korakakis and Andreas Vlachos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14322

# USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis

Zepeng Zhai, Hao Chen, Ruifan Li and Xiaojie WANG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14340

# PAD-Net: An Efficient Framework for Dynamic Networks

Shwai He, Liang Ding, Daize Dong, Boan Liu, Fuqiang Yu and Dacheng Tao . . . . . . . . . . . . 14354

# Resolving Ambiguities in Text-to-Image Generative Models

Ninareh Mehrabi, Palash Goyal, Apurv Verma, Jwala Dhamala, Varun Kumar, Qian Hu, Kai-Wei Chang, Richard Zemel, Aram Galstyan and Rahul Gupta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14367

# Knowledge Unlearning for Mitigating Privacy Risks in Language Models

Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran and Minjoon Seo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14389

# Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor

Or Honovich, Thomas Scialom, Omer Levy and Timo Schick . . . . . . . . . . . . . . . . . . . . . . . . . . 14409

# To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering

Dheeru Dua, Emma Strubell, Sameer Singh and Pat Verga . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14429

# A Survey for Efficient Open Domain Question Answering

Qin Zhang, Shangsi Chen, Dongkuan Xu, Qingqing Cao, Xiaojun Chen, Trevor Cohn and Meng Fang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14447

# Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities

Sina Ahmadi and Antonios Anastasopoulos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14466

# Compositional Generalization without Trees using Multiset Tagging and Latent Permutations

Matthias Lindemann, Alexander Koller and Ivan Titov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14488

# ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning

Xiao Xu, Bei Li, Chenfei Wu, Shao-Yen Tseng, Anahita Bhiwandiwalla, Shachar Rosenman, Vasudev Lal, Wanxiang Che and Nan Duan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14507

# Finding the Pillars of Strength for Multi-Head Attention

Jinjie Ni, Rui Mao, Zonglin Yang, Han Lei and Erik Cambria . . . . . . . . . . . . . . . . . . . . . . . . . . 14526

# Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation

yandan zheng, Anran Hao and Anh Tuan Luu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14541

# Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering

Jiajie Zhang, Shulin Cao, Tingjian Zhang, Xin Lv, Juanzi Li, Lei Hou, Jiaxin Shi and Qi Tian 14556

# Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation

Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi and Heng Ji . . . . . . . 14571

# A Length-Extrapolatable Transformer

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song and Furu Wei. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14590

# A Survey of Deep Learning for Mathematical Reasoning

Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck and Kai-Wei Chang. . . . . . . . . . . . . . . . . . . . . . 14605

# A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training

Nitay Calderon, Subhabrata Mukherjee, Roi Reichart and Amir Kantor . . . . . . . . . . . . . . . . . . 14632

# Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation

Chaoya Jiang, Wei Ye, Haiyang Xu, Songfang Huang, Fei Huang and Shikun Zhang . . . . . . 14660

# Tell2Design: A Dataset for Language-Guided Floor Plan Generation

Sicong Leng, Yang Zhou, Mohammed Haroon Dupty, Wee Sun Lee, Sam C Joyce and Wei Lu 14680

# Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations

Bingsheng Yao, Prithviraj Sen, Lucian Popa, James Hendler and Dakuo Wang . . . . . . . . . . . 14698

cxxv

# Rethinking Annotation: Can Language Learners Contribute?

Haneul Yoo, Rifki Afina Putri, Changyoon Lee, Youngin Lee, So-Yeon Ahn, Dongyeop Kang and Alice Oh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14714

# Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling

Shengqiong Wu, Hao Fei, Yixin Cao, Lidong Bing and Tat-Seng Chua . . . . . . . . . . . . . . . . . . 14734

# MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations

Tao Shi and Shao-Lun Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14752

# Learning Language-Specific Layers for Multilingual Machine Translation

Telmo Pires, Robin M. Schmidt, Yi-Hsiu Liao and Stephan Peitz . . . . . . . . . . . . . . . . . . . . . . . 14767

# Personality Understanding of Fictional Characters during Book Reading

Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Zhou Xiao, Fandong Meng and Jie Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14784

# StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Xuekai Zhu, Jian Guan, Minlie Huang and Juan Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14803

# Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models

Qingyu Tan, Hwee Tou Ng and Lidong Bing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14820

# Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings

Daniel Rotem, Michael Hassid, Jonathan Mamou and Roy Schwartz . . . . . . . . . . . . . . . . . . . . 14836

# Large Language Models Are Reasoning Teachers

Namgyu Ho, Laura Schmid and Se-Young Yun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14852

# Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations

Wenting Zhao, Justin Chiu, Claire Cardie and Alexander Rush . . . . . . . . . . . . . . . . . . . . . . . . . 14883

# PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification

Yau-Shian Wang, Ta-Chung Chi, Ruohong Zhang and YIMING YANG . . . . . . . . . . . . . . . . . 14897

# Visually-augmented pretrained language models for NLP tasks without images

Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Qinyu Zhang and Ji-Rong Wen . . . . . . . . . . . . . . 14912

# Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning

Armineh Nourbakhsh, Sameena Shah and Carolyn Rosé . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14930

# A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization

Lining Zhang, Simon Mille, Yufang Hou, Daniel Deutsch, Elizabeth Clark, Yixin Liu, Saad Mahamood, Sebastian Gehrmann, Miruna Adriana Clinciu, Khyathi Raghavi Chandu and João Sedoc 14944

# TAVT: Towards Transferable Audio-Visual Text Generation

Wang Lin, Tao Jin, Wenwen Pan, Linjun Li, Xize Cheng, Ye Wang and Zhou Zhao . . . . . . . 14983

# MeetingQA: Extractive Question-Answering on Meeting Transcripts

Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt and Mohit Bansal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15000

# FERMAT: An Alternative to Accuracy for Numerical Reasoning

Jasivan Alex Sivakumar and Nafise Sadat Moosavi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15026

# Don’t Forget Your ABC’s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems

Sarah E. Finch, James D. Finch and Jinho D. Choi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15044

# Decoder Tuning: Efficient Language Understanding as Decoding

Ganqu CUI, Wentao Li, Ning Ding, Longtao Huang, Zhiyuan Liu and Maosong Sun . . . . . 15072

# The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

Akshatha Arodi, Martin P¨ omsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu and Jackie Chi Kit Cheung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15088

# CREST: A Joint Framework for Rationalization and Counterfactual Text Generation

Marcos Treviso, Alexis Ross, Nuno M. Guerreiro and André Martins . . . . . . . . . . . . . . . . . . . 15109

# Towards Unifying Multi-Lingual and Cross-Lingual Summarization

Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu and Jie Zhou 15127

# On Improving Summarization Factual Consistency from Natural Language Feedback

Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker, Dragomir Radev and Ahmed Hassan Awadallah . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15144

# From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models

Julia Mendelsohn, Ronan Le Bras, Yejin Choi and Maarten Sap . . . . . . . . . . . . . . . . . . . . . . . . 15162

# Exploring Large Language Models for Classical Philology

Frederick Riemenschneider and Anette Frank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15181

# LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding

Yi Tu, Ya Guo, Huan Chen and jinyang tang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15200

# Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition

Yuchen Hu, Ruizhe Li, Chen Chen, Chengwei Qin, Qiu-Shi Zhu and Eng Siong Chng . . . . 15213

# An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation

Xuancheng Huang, Zijun Liu, Peng Li, Tao Li, Maosong Sun and Yang Liu . . . . . . . . . . . . . . 15233

# Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion

Hongcai Xu, Junpeng Bao and Wenbo Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15257

# Dual Cache for Long Document Neural Coreference Resolution

Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu and Zheng Zhang . . . . . . . . . . . . . . . . . . 15272

# Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation

Kaiyu Huang, Peng Li, Jin Ma, Ting Yao and Yang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15286

# DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media

Mario Ezra Aragon, Adrian Pastor Lopez Monroy, Luis C Gonzalez, David E. Losada and Manuel Montes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15305

# Toward Interactive Dictation

Belinda Z. Li, Jason Eisner, Adam Pauls and Sam Thomson . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15319

# CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors

Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang and Xipeng Qiu 15339

# Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning

Barun Patra, Saksham Singhal, Shaohan Huang, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary and Xia Song . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15354

# Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension

Xiao Zhang, Heyan Huang, Zewen Chi and Xian-Ling Mao . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15374

# LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming

Jingsheng Gao, Yixin Lian, Ziyi Zhou, yuzhuo fu and Baoyuan Wang . . . . . . . . . . . . . . . . . . . 15387

# Prompting PaLM for Translation: Assessing Strategies and Performance

David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar and George Foster 15406

# Exploring Lottery Prompts for Pre-trained Language Models

Yulin Chen, Ning Ding, Xiaobin Wang, Shengding Hu, Haitao Zheng, Zhiyuan Liu and Pengjun Xie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15428

# A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations

Wenjie Zheng, Jianfei Yu, Rui Xia and Shijin Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15445

# TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline

Jiang Li, Xiangdong Su and Guanglai Gao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15460

# Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition

Yuwei Bao, Barrett Martin Lattimer and Joyce Chai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15475

# Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination

Adam Przepiórkowski and Michał Wo´zniak. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15494

# LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development

Ilias Chalkidis, Nicolas Garneau, Catalina Goanta, Daniel Katz and Anders Søgaard . . . . . . 15513

# Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge

Xuebo Liu, Yutong Wang, Derek F. Wong, Runzhe Zhan, Liangxuan Yu and Min Zhang . . 15536

# NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models

Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang and Shiqing Ma . . . . . . . . . . . . . . . . . . . . . . . 15551

# Revisiting Relation Extraction in the era of Large Language Models

Somin Wadhwa, Silvio Amir and Byron C. Wallace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15566

# Pre-trained Language Models Can be Fully Zero-Shot Learners

Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu and Lei Li . . . . . . . . . . . . . . . . . . . . . . . . 15590

# Can Large Language Models Be an Alternative to Human Evaluations?

Cheng-Han Chiang and Hung-yi Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15607

# HyperMixer: An MLP-based Low Cost Alternative to Transformers

Florian Mai, Arnaud Pannatier, Fabio J Fehr, Haolin Chen, Francois Marelli, Francois Fleuret and James Henderson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15632

# UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units

Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe and Juan Pino . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15655

# Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression

Wen Wu, Chao Zhang and Philip C. Woodland. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15681

# Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation

Wei Liu and Michael Strube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15696

# Plug-and-Play Document Modules for Pre-trained Models

Chaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min Chan, Yankai Lin, Zhiyuan Liu, xiangyang li, Zhonghua Li, Zhao Cao and Maosong Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15713

# An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models

Zhongbin Xie and Thomas Lukasiewicz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15730

# Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models

Lijing Wang, Yingya Li, Timothy A Miller, Steven Bethard and Guergana Savova . . . . . . . . 15746

# A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models

Krithika Ramesh, Arnav Chavan, Shrey Pandit and Sunayana Sitaram . . . . . . . . . . . . . . . . . . . 15762

# Ranking-Enhanced Unsupervised Sentence Representation Learning

Yeon Seonwoo, Guoyin Wang, Changmin Seo, Sajal Choudhary, Jiwei Li, Xiang Li, Puyang Xu, Sunghyun Park and Alice Oh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15783

# To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support

Gabriella Skitalinskaya and Henning Wachsmuth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15799

# Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments

Ethan Adrian Mendes, Yang Chen, Wei Xu and Alan Ritter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15817

# Composition-contrastive Learning for Sentence Embeddings

Sachin J. Chanchani and Ruihong Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15836

# Causes and Cures for Interference in Multilingual Translation

Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy and Shruti Bhosale . . . . . . . . . . 15849

# Understanding and Bridging the Modality Gap for Speech Translation

Qingkai Fang and Yang Feng. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15864

# Few-shot Reranking for Multi-hop QA via Language Model Prompting

Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee and Lu Wang . . . . . . . . 15882

# DICE: Data-Efficient Clinical Event Extraction with Generative Models

Mingyu Derek Ma, Alexander K Taylor, Wei Wang and Nanyun Peng . . . . . . . . . . . . . . . . . . . 15898

# XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations

Yusen Zhang, Jun Wang, Zhiguo Wang and Rui Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15918

# INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation

Wenhao Zhu, Jingjing Xu, Shujian Huang, Lingpeng Kong and Jiajun CHEN . . . . . . . . . . . . 15948

# Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction

Qi Sun, Kun Huang, Xiaocui Yang, Pengfei Hong, Kun Zhang and Soujanya Poria . . . . . . . 15960

# Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning

Shivaen Ramshetty, Gaurav Verma and Srijan Kumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15974

# Crosslingual Generalization through Multitask Finetuning

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, KHALID ALMUBARAK, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff and Colin Raffel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15991

# Evaluate AMR Graph Similarity via Self-supervised Learning

Ziyi Shou and Fangzhen Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16112

# Analyzing Transformers in Embedding Space

Guy Dar, Mor Geva, Ankit Gupta and Jonathan Berant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16124

# Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning

Alexander Hanbo Li, Mingyue Shang, Evangelia Spiliopoulou, Jie Ma, Patrick Ng, Zhiguo Wang, Bonan Min, William Yang Wang, Kathleen McKeown, Vittorio Castelli, Dan Roth and Bing Xiang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16171

# FactKG: Fact Verification via Reasoning on Knowledge Graphs

Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne and Edward Choi . . . . . . . 16190

# DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains

Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille and Pierre-Antoine Gourraud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16207

# Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction

Changsen Yuan, Heyan Huang, Yixin Cao and Yonggang Wen . . . . . . . . . . . . . . . . . . . . . . . . . 16222

# Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks

Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min, Liang Yang and Hongfei LIN . . . . . . . . 16235

# SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations

Paul-Ambroise Augustin Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswami, Changhan Wang, Juan Pino, Benoît Sagot and Holger Schwenk . . . . . . . . . . . . . . . . . . . . 16251

# Character-Aware Models Improve Visual Text Rendering

Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi and Noah Constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16270

# IDRISI-RA: The First Arabic Location Mention Recognition Dataset of Disaster Tweets

Reem Suwaileh, Muhammad Imran and Tamer Elsayed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16298

# FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction

Tianshuo Peng, Zuchao Li, Lefei Zhang, Bo Du and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . 16318

# What Do NLP Researchers Believe? Results of the NLP Community Metasurvey

Julian Michael, Ari Holtzman, Alicia Parrish, Aaron Mueller, Alex Wang, Angelica Chen, Divyam Madaan, Nikita Nangia, Richard Yuanzhe Pang, Jason Phang and Samuel R. Bowman . . . . . . . . . 16334

# Prototype-Guided Pseudo Labeling for Semi-Supervised Text Classification

Weiyi Yang, Richong Zhang, Junfan Chen, Lihong Wang and Jaein Kim . . . . . . . . . . . . . . . . .16369

# LENS: A Learnable Evaluation Metric for Text Simplification

Mounica Maddela, Yao Dou, David Heineman and Wei Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16383

# MeetingBank: A Benchmark Dataset for Meeting Summarization

Yebowen Hu, Timothy Jeewun Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Fo-roosh and Fei Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16409

# UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective

yang ping, JunYu Lu, ruyi gan, Junjie Wang, Yuxiang Zhang, Pingjian Zhang and Jiaxing Zhang 16424

# DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification

Regina Stodden, Omar Momen and Laura Kallmeyer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16441

# A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text

Yunxin Li, Baotian Hu, Yuxin Ding, Lin Ma and Min Zhang . . . . . . . . . . . . . . . . . . . . . . . . . . . 16464

# RARR: Researching and Revising What Language Models Say, Using Language Models

Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan and Kelvin Guu . . . . . . . . . . . . . . . . . . . . . . 16477

