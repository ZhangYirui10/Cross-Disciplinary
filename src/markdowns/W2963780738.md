# Accepted Manuscript

# Haze Visibility Enhancement: A Survey and Quantitative Benchmarking

# Computer Vision and Image Understanding

Yu Li, Shaodi You, Michael S. Brown, Robby T. Tan

PII: S1077-3142(17)30159-5

DOI: 10.1016/j.cviu.2017.09.003

Reference: YCVIU 2616

To appear in: Computer Vision and Image Understanding

Received date: 5 August 2016

Revised date: 27 July 2017

Accepted date: 17 September 2017

Please cite this article as: Yu Li, Shaodi You, Michael S. Brown, Robby T. Tan, Haze Visibility Enhancement: A Survey and Quantitative Benchmarking, Computer Vision and Image Understanding (2017), doi: 10.1016/j.cviu.2017.09.003

This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.

# ACCEPTED MANUSCRIPT

# Haze Visibility Enhancement: A Survey and Quantitative Benchmarking

# Yu Lia, Shaodi Youb, Michael S. Brownc, Robby T. Tand

aAdvanced Digital Sciences Center, Singapore

bData61-CSIRO, Australia and Australian National University, Australia

cYork University, Canada

dYale-NUS College and National University of Singapore

# Abstract

This paper provides a comprehensive survey of methods dealing with visibility enhancement of images taken in hazy or foggy scenes. The survey begins with discussing the optical models of atmospheric scattering media and image formation. This is followed by a survey of existing methods, which are categorized into: multiple image methods, polarizing filter-based methods, methods with known depth, and single-image methods. We also provide a benchmark of a number of well-known single-image methods, based on a recent dataset provided by Fattal [1] and our newly generated scattering media dataset that contains ground truth images for quantitative evaluation. To our knowledge, this is the first benchmark using numerical metrics to evaluate dehazing techniques. This benchmark allows us to objectively compare the results of existing methods and to better identify the strengths and limitations of each method.

# Keywords

Scattering media, visibility enhancement, dehazing, defogging

# 1. Introduction

Fog and haze are two of the most common real-world phenomena caused by atmospheric particles. Images captured in foggy and hazy scenes suffer from noticeable degradation of visibility and significant reduction of contrast, as shown in Figure 1. To visually recover scenes from haze or fog can be critical for image processing and computer vision algorithms. Haze-free photographs with clear visual content are what consumers desired when shooting target objects or landscapes; hence, cameras or image-editing softwares that can recover scenes from haze or fog are useful for consumer markets. In addition, many computer vision systems, particularly those for outdoor scenes (e.g., surveillance, intelligent vehicle systems, remote sensing systems), assume clear scenes under good weather. This is because the underlying algorithms, such as object detection, tracking, segmentation, optical flow, obstruction detection, stereo vision are designed with such an assumption. However, mist, fog, and haze are natural phenomena that are inevitable and thus have to be resolved.

The degradation in hazy and foggy images can be physically attributed to floating particles in the atmosphere that absorb and scatter light in the environment [2]. This scattering and absorption reduce the direct transmission from the scene to the camera and add another layer of the scattered light, known as airlight [3]. The attenuated direct transmission causes the intensity from the scene to be weaker, while the airlight causes the appearance of the scene to be washed out.

In the past two decades, there has been significant progress in methods that use images taken in hazy scenes. Early work by Cozman and Krotkov [4] and Nayar and Narasimhan [5, 6] uses atmospheric cues to estimate depth. Since then, a number of methods have been developed.

Email addresses: yul@illinois.edu (Yu Li), shaodi.you@anu.edu.au (Shaodi You), mbrown@eecs.yorku.ca (Michael S. Brown), robby.tan@nus.edu.sg (Robby T. Tan)

Preprint submitted to Computer Vision and Image Understanding September 21, 2017

# ACCEPTED MANUSCRIPT

have been introduced to explicitly enhance visibility, which can be categorized into ([7, 8]): multi-image-

based methods (e.g., [9, 10, 11, 12]), polarizing filter-

based methods (e.g., [13, 14]), methods using known

depth or geometrical information (e.g., [15, 16, 17, 18]), and single-image methods (e.g., [8, 19, 20, 7, 21, 22, 23, 24, 1, 25, 26]).

Chronologically, Oakley and Satherleyâ€™s study [15], published in 1998, was the pioneer in proposing a method dealing with poor visibility conditions. The method, however, requires known geometrical information. In 2000, Narasimhan and Nayarâ€™s [9] introduced a method that uses multiple images to solve the ill-posedness nature of the problem. It assumes the images are taken under different atmospheric conditions â€“ that is when taking the input images, we need to wait for some time until the fog or haze density levels change, which is impractical for many applications. Subsequently in 2001, polarizing filter-based methods were proposed ([13]). This approach can resolve the problem, since we do not need to wait for atmospheric conditions to change when taking the input images. However, it assumes that the scene is static when the filter is rotated, which still poses problems for real-time applications. More importantly, these two approaches cannot process a single input image. To address this problem, an approach that uses a single input image with additional depth constraints was introduced in 2007 [17]. Motivated by these problems, in 2008, two methods based on a single input image without known geometrical information were proposed [8, 19]. From that point forward, many single input approaches were proposed to address this problem.

In this paper, one of the contributions is to provide a detailed survey on dehazing methods. Our survey provides a holistic view of most of the existing methods. After starting with a brief introduction of the atmospheric scattering optics in Section 2, in Section 3 we provide the survey, where a particular emphasis is placed on the last category of single-image methods, reflecting the recent progress in the field. As part of this survey, we also provide a quantitative benchmarking of a number of the single-image methods. Obtaining quantitative results is challenging as it is difficult to capture ground truth examples for where the same scene has been imaged with and without scattering particles. The work by Fattal [1] synthesizes a dataset by using natural images which associate depth maps that can be used to simulate the spatially varying attenuation in haze and fog images. We have generated an additional dataset using a physically based rendering to simulate environments with scattered particles. Section 4 provides the results of the different methods using both Fattalâ€™s dataset [1] and our newly generated benchmark dataset. Our paper is concluded in Section 5 with a discussion on the current state of image dehazing methods and the findings from the benchmark results. In particular, we discuss current limitations with existing methods and possible avenues for research for future methods.

# 2. Atmospheric Scattering Model

Haze is a common atmospheric phenomenon resulting from air pollution, such as dust, smoke, and other dry particles that obscure the clarity of the sky. Sources for haze particles include farming, traffic, industry, and wildfire. As listed in Table 1, the particle size varies from 10âˆ’2 to 1Î¼m and the density varies from 10 to 103 per cm3. The particles cause visibility degradation and also color shift. Depending on the view-angle with respect to the sun and the types of the particles, haze may appear brownish or yellowish [28].

Unlike haze, fog or mist is caused by water droplets and/or ice crystals suspended in the air close to the earthâ€™s surface [29]. As listed in Table 1, the particle size varies from 1 to 10Î¼m and the density varies from 10 to 100 per cm3. Generally, fog particles do not have their own color, and thus their color appearance depends mostly on the surrounding light colors.

# Table 1: Weather condition and the particle type, size, and density [27]

|Weather|Particle type|Particle radius (ðm)|Density (ð’„ð’Žâˆ’3)|
|---|---|---|---|
|Clean air|Molecule|10âˆ’4|1019|
|Haze|Aerosol|10âˆ’2âˆ’ 1|10âˆ’ 103|
|Fog|Water droplets|1 âˆ’ 10|10 âˆ’ 100|

In this section, we review the derivation of the optical model for haze or fog, which is known as Koschmiederâ€™s law [2]. Discussing the derivation is necessary to understand the physics behind the model. The discussion is based on Narasimhan and Nayar [9] and McCartney [3].

# 2.1. Optical Modeling

As illustrated in Figure 2(a), when a ray of light hits a particle, the particle will scatter the light to all directions, with magnitudes depending on the particleâ€™s size, shape, and incident light wavelengths. Since the directions of scattered rays are moving away from the particle, they are known as outbound rays or out-scattering.

# ACCEPTED MANUSCRIPT

Integrating this equation between x = 0 and x = d1 gives us: E(d, Î») = E0(Î»)eâˆ’Î²(Î»)dx, where E0 is the irradiance. This formula is known as the Beer-Lambert law.

# Intensity distribution of scattered light

# (a) Single particle scattering

Incident light

Observed intensity

Incident light

ð¸(ðœ†)

ðœƒ

ðœƒ

ð¸(ðœ†)

# (b) Unit volume scattering

Attenuated light

ð¸(0,ðœ†)

ð¸(ð‘¥, ðœ†)

Incident light

ð‘¥ = 0

ð‘‘ð‘¥

ð‘¥ = ð‘‘

# (c) Attenuation over distance

Figure 2: (a) Single particle scattering; (b) unit volume scattering; and (c) light attenuation over distance raised by scattering.

The rays arriving from all directions that hit a particle are referred to as inbound rays or in-scattering rays. As well exploited by Minnaert [30], for a given particle type and incident light wavelength, the outbound light intensity can be modeled as a function between the angle of inbound and outbound light. In this paper, we are more interested in the statistical properties over a large number of particles. Thus, considering the particle density (Table 1), and that each particle can be considered as an independent particle, we can have the statistical relationship between inbound light intensity E and outbound light intensity I [3]:

I(Î¸, Î») = Î²p,x(Î¸, Î»)E(Î»),  (1)

where Î²p,x(Î¸, Î») is called the angular scattering coefficient. The subindices of Î² are defined with p indicating its dependency on particle type and density, and x indicating the dependency spatially. By integrating Eq. (1) over all spherical directions, we obtain the total scattering coefficient:

I(Î») = Î²p,x(Î»)E(Î»). (2)

# Direct Transmission

If we assume a particle medium consists of a small chunk with thickness dx, and a parallel light ray passes through every sheet, as illustrated in Figure 2(c), then the change in irradiance at location x is expressed as:

dE(x, Î») = âˆ’Î²p,x(Î»)d. (3)

where E(x, Î»)

1 We use d for differential and italic d for depth.

# ACCEPTED MANUSCRIPT

The first term is the direct transmission, and the second term is the airlight. The model is known as Koschmiederâ€™s law [2]. The term I is the image intensity as an RGB color vector, while x is the 2D image spatial location. The term Lâˆž is the atmospheric light that is assumed to be globally constant and independent from location x. The term L represents the atmospheric light, the camera gain, and the squared distance, L = Lâˆžg/d2. The term Ï is the reflectance of an object, Î² is the atmospheric attenuation coefficient, and d is the distance between an object and the camera. The term Î² is assumed to be independent from wavelengths, which is a common assumption as we are dealing with particles whose size is larger compared with the wavelength of light, such as, fog, haze, and aerosol [3]. Moreover, Î² is independent from the spatial image location for homogeneous distribution of atmospheric particles.

In this paper, we denote scene reflection as:

R(x) = LÏ(x). (13)

The estimation of Eq. 13 terms is the ultimate goal of dehazing or visibility enhancement, since these terms represent the scene that has not been affected by medium-sized scattered particles. The term A(x) represents the airlight, and can be denoted as:

A(x) = Lâˆž(1 âˆ’ eâˆ’Î²d(x)). (14)

The function t(x) represents the transmission, as t(x) = eâˆ’Î²d(x). Hence, the scattering model in Eq. (12) can be written as:

I(x) = D(x) + A(x), (15)

where D(x) = R(x)t(x), the direct transmission. The above scattering model assumes the images are three channel RGB images. For gray images, we can write a similar formula by transforming the color vectors to scalar variables:

I(x) = D(x) + A(x), (16)

where D(x) = t(x)R(x), with R(x) = LÏ(x), and, A(x) = Lâˆž(1 âˆ’ eâˆ’Î²d(x)).

# 3. Survey on Dehazing Methods

The general goal of dehazing is to recover the clear scene reflection R (and transmission t, atmosphere light intensity to a camera is linearly proportional to the cameraâ€™s pixel values, the scattered light in the atmosphere captured by the camera can be modeled as:

I(x) = LÏ(x)eâˆ’Î²d(x) + Lâˆž(1 âˆ’ eâˆ’Î²d(x)). (12)

2That is to say we have three sets of equations for wavelength Î» at red, green and blue channel separately. The bold fonts indicate this color vector.

# ACCEPTED MANUSCRIPT

color Lâˆž) from input I. It is an ill-posed problem as it requires one to infer many unknown parameters from only one equation. In order to make the problem plausible to solve, other information is required. Many early methods propose to use multiple images (e.g., [9]) or use information from other modalities (e.g., depth [18]) to dehaze the images. Compared with dehazing with multiple images as input, single-image dehazing is more challenging. A milestone in single-image dehazing was made with the concurrent publications of Tan [8] and Fattal [19] that propose methods that can automatically dehaze a single image without additional information, such as known geometrical information. These two methods are based on their observations of the characteristics of the hazy and clean images. These characteristics are used as image priors to solve the dehazing problem. Following this trend, different haze-related priors (including the well-received dark channel prior [20]) were proposed and single-image dehazing became the dominant research topic in the field. Recently, a number of methods attempted to use learning frameworks [24, 25] to solve the single-image haze removal problem and demonstrated good results.

# 3.1. Multiple Images

Narasimhan-Nayar 2000 [9] extends the analysis of the dichromatic scattering model of [5], which is described as:

I(x) = p(x) Ë†D(x) + q(x) Ë†A(x),

where Ë†D and Ë†A are the chromaticity values of the direct transmission and the airlight. The terms p and q are the magnitude of the direct transmission and the airlight, respectively. The paper calls the equation the dichromatic scattering model, where the word â€˜dichromaticâ€™ is borrowed from [31] due to the similarity of the models.

The method uses multiple images of the same scene taken in different haze density. It works by supposing there are two images taken from the same scene, which share the same color of atmospheric light but have different direct transmission colors. From this, two planes can be formed in the RGB space that intersect each other. In their work [9] utilizes the intersection to estimate the atmospheric light chromaticity, Ë†A, which is similar to Tominaga and Wandellâ€™s method [32] for estimating a light color from specular reflection. The assumption that the images of the same scene have different colors of direct transmission, however, might produce inaccurate estimation since, in many cases, the colors of the direct transmission of the same scene are similar.

The method then introduces the concept of iso-depth, which is the ratio of the direct transmission magnitudes under two different weather conditions. Referring to Eq. (17), and applying it to two images, we have:

p2(x) = Lâˆž2 eâˆ’(Î²â‚‚âˆ’Î²â‚)d(x),

p1(x) Lâˆž1

where p is the magnitude of the direct transmission. From this equation, we can infer that if two pairs of pixels have the same ratio, then they must have the same depth: p2(xi) = p2(xj). To calculate these ratios, the method provides a solution by utilizing the analysis of the planes formed in the RGB space by the scattering dichromatic model in Eq. (17).

# ACCEPTED MANUSCRIPT

# Early Work in Depth Estimation

Cozman-Krotkov 1997 [4] is one of the earliest methods to analyze images of scenes captured in scattering media. The goal in this work is to extract scene depth by exploiting the presence of the atmospheric scattering effects. This work inspired Nayar-Narasimhan 1999 [5], who proposed a few methods to estimate depth from hazy scenes. Unlike [4], however, this work does not assume that the haze-free image is provided. While these two methods do not assume that the haze-free image is provided, the method provides a solution by utilizing the analysis of the planes formed in the RGB space by the scattering dichromatic model in Eq. (17).

Having obtained the ratios for all pixels, the method proceeds with the estimation of the scene structure, which is calculated by:

(Î²2 âˆ’ Î²1)d(x) = log(Lâˆž2 âˆ’ log(p2(x)).

Lâˆž1 p1(x)

# ACCEPTED MANUSCRIPT

# Table 2: An overview of existing works on vision through atmospheric scattering media.

|Method|Category|Known parameters (input)|Estimation (output)|Key idea|
|---|---|---|---|---|
|Nayar â€“ Narasimham 2000|Multiâ€‘images|Two RGB images I(x) with different weather conditions ð›½1,ð›½2|t(x), d(x)|Iso â€“depth: comparing different ð›½;color decomposition|
|Nayar â€“ Narasimham 2003a|Multiâ€‘images|Two grayscale or RGB images I(x) with different weather conditions ð›½, ð›½1, ð›½2|t(x), d(x), A(x) and scene reflection R(x)|Isoâ€“ depth|
|Caraffaâ€‘Tarel 2012|Multiâ€‘images|Stereo images|d(x), R(x)|Depth from scattering; depth from stereo; spatial smoothness|
|Li et al. 2015|Multiâ€‘images|Monocular video|t(x), d(x), R(x)|Depth from monocular video; depth from scattering; photoconsistency|
|Schechner et al. 2001|Polarizing filter|Two images with different polarization under same weather condition A(x), t(x), d(x), R(x)|Assuming direct transmission D(x) has insignificant polarization| |
|Schartz et al. 2006|Polarizing filter|Two images with different polarization under same weather condition A(x), t(x), d(x), R(x)|Direct transmission D(x) has insignificant polarization; A(x) and D(x) are statistically independent| |
|Oakley â€“ Satherley 1998|Known depth|Single grayscale image I(x)|Atmospheric light: ð‘³âˆž, Depth d(x)|Mean square optimization; color of the scene is uniform|
|Nayar â€“ Narasimham 2003b|Known depth|Single RGB image I(x)|User specified less hazed and more hazed regions|R(x)|
|Nayar â€“ Narasimham 2003b|Known depth|Single RGB image I(x)|User specified vanishing point, min depth and max depth|R(x)|
|HautiÃ¨ re et al. 2007|Known depth|Single image I(x)|ð‘³âˆž, R(x)|Depth from calibrated camera; Scene of flat ground|
|Kopf et al. 2008|Known depth|Single image I(x)|t(x), R(x)|Transmission estimation using averaged texture from same depth; Brightest value assumption for atmospheric light ð¿âˆž|
|Tan 2008|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Estimation; maximal contrast assumption for scene reflection R(x) estimation|
|Fattal 2008|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Shading and transmission are locally and statistically uncorrelated|
|He et al. 2009|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Dark channel: outdoor objects in clear weather have at least one color channel that is significantly dark|
|Tarel â€“ HautiÃ¨ re 2009|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Maximal contrast assumption; normalized air light is upper-bounded; Scene reflection R(x) and airlight A(x) are statistically independent|
|Kratzâ€“ Nishino 2009|Single image|Single RGB image I(x)|t(x), R(x)|Layer separation|
|Ancutiâ€‘Ancuti 2010|Single image|Single RGB image I(x)|A(x), R(x)|Grayâ€‘world color constancy; global contrast enhancement|
|Meng et al. 2013|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Dark channel for transmission t(x)|
|Tang et al. 2014|Single image|Single RGB image I(x)|t(x), R(x)|Learning for transmission t(x)|
|Fattal 2014|Single image|Single RGB image I(x)|ð‘³âˆž, t(x), R(x)|Color line: small image patch has uniform color and depth but different shading|
|Cai et al. 2016|Single image|Single RGB image I(x)|t(x), R(x)|Learning of t(x) in CNN framework|
|Berman et al. 2016|Single image|Single RGB image I(x)|t(x), R(x)|Nonâ€‘local haze line; finite color approximation|

# ACCEPTED MANUSCRIPT

To be able to estimate the depth, the last equation requires the knowledge of the values of Lâˆž1 and Lâˆž2, which are obtained by solving the equation:

c(x) = Lâˆž2 - p2(x) Lâˆž1, (20)

where c is the magnitude of a vector indicating the distance between the origin of I1 to the origin of I2 in the direction of the airlight chromaticity in RGB space, while p2(x) is the ratio, which had been computed.

For the true scene color restoration, employing the estimated atmospheric light, the method computes the airlight magnitude of Eq. (17) using:

q(xi) = Lâˆž1 (1 - e-Î²d(xi)), (21)

where:

Î²d(xi) = Î²d(xj) d(xi), (22)

and d(xi) is computable using Eq. (19). Î²d(xj) is a chosen reference point. This is obtained by assuming there is at least one pixel in the image for which the true value of the direct transmission, D, is known (e.g., a black object), since, in this case I(x) = A(x), and Î²d(x) can be directly computed. The method also proposes how to find such a pixel automatically. Note that knowing the value of q(xi) in Eq. (21) enables us to dehaze the images straightforwardly.

Narasimhan-Nayar 2003 In a subsequent publication, Narasimhan and Nayar [10] introduce a technique that works for gray or colored images: contrast restoration of iso-depth regions, atmospheric light estimation, and contrast restoration.

In the contrast restoration of iso-depth regions, the method forms an equation that assumes the depth segmentation is provided (e.g., manually by the user) and the atmospheric light is known:

Ï(xi) = 1 - âˆ‘i (1 - Ï(xi)) âˆ‘j (Lâˆž - I(xi)), (23)

where the sums are over the same depth regions. As can be seen in the equation, Ï(xi) can be estimated up to a linear factor âˆ‘j Ï(xj). By setting Ïmin = 0 and Ïmax = 1 and adjusting the value âˆ‘j Ï(xj), the contrast of regions with the same depth can be restored.

To estimate the atmospheric lights, the method utilizes two gray images of the same scene yet different atmospheric lights. Based on the scattering model in Eq. (12), scene reflectance Ï is eliminated. The two

# ACCEPTED MANUSCRIPT

for the estimated disparity Î´ and the estimated clean left image I0L. The optimization to estimate the two variables Î´ and I0L is done in a two-step fashion that in each time only one of the variables is optimized, with the other one fixed and then alternate. After a few iterations, it will converge with the solution of Î´ and I0L.

Li et al. 2015 [12] jointly estimates scene depth and enhances visibility in a foggy video, which, unlike Caraffa-Tarelâ€™s method [11], uses a monocular video. Following the work of Zhang et al. [34], it estimates the camera parameters and the initial depth of the scene, which is erroneous particularly for dense fog regions due to the photoconsistency problem in the data term.

Similar to [11], Li et al.â€™s method [12] introduces a photoconsistency data term that involves effect of fog:

E(d) = 1 / âˆ‘n âˆ‘nâ€²âˆˆN(n) |N(n)| ||Inâ€²(x) âˆ’ Inâ€²(lnâ†’nâ€²(x, dn(x)))||, where lnâ†’tâ€²(x, dn(x)) projects the pixel x with inverse depth dn(x) in frame n to frame nâ€². The intensity,

Inâ€²(x) = (In(x) âˆ’ Lâˆž) nâ†’ntn(x)n + Lâˆž, is a synthetic intensity value obtained from the transmission, tn, which is computable by knowing dn (note that, in the paper, the scattering coefficient Î² and the atmospheric light, Lâˆž, are estimated separately). The projection function Ï€nâ†’nâ€²(x, tn(x)) computes the corresponding transmission in the nâ€²-th frame for the pixel x in the n-th frame with transmission tn(x). The denominator N(t) represents the neighboring frames of frame n and |N(n)| is the number of neighboring frames. By having Î²(x) estimated separately, tn(x) depends only on dn(x), and thus dn is the only unknown in the last equation. The whole idea in the photoconsistency term here is to generate a synthetic intensity value of each pixel from known depth, d, atmospheric light, Lâˆž, and the particle scattering coefficient, Î². Note that the paper assumes Î² and Lâˆž are uniform across the video sequence. Therefore, if those three values are correctly estimated, the generated synthetic intensity values must be correct.

Aside from the photoconsistency term, the method also uses Laplacian smoothing as the transmission smoothness prior. Together with the geometric coherent term and disparity smoothness term, the problem is formulated in a Markov Random Field (MRF) for dense image labeling. After a few iterations, the outcomes are estimated depth maps and defogged images.

# 3.2. Polarizing Filter

Schechner et al. 2001 addresses the issue appearing in the work of Narasimhan and Nayar [9], where it requires at least two images of the same scene taken under different particle densities (i.e., we have to wait until the fog density changes considerably). Unlike [9], Schechner et al.â€™s [13] uses multiple images captured using polarizing filters, which does not require the fog density to change.

In this case, the challenge lies in estimating W given [Imax, Imin]T to produce D and A accurately.

# ACCEPTED MANUSCRIPT

The method claims that while the airlight and direct transmission are in fact statistically dependent there are transformations that can relax this dependency. The method therefore transforms the input data using a wavelet transformation and solves the ICA problem by using an optimization method in the wavelet domain. Aside from P, the method also needs to estimate Lâˆž, which is done by labeling certain regions manually to have two pixels that have the same values of the direct transmission yet different values of the airlight.

The first method requires the user to select a region with less haze and a region with more haze of the same reflection as the first oneâ€™s. From these two inputs, the approach estimates the dichromatic plane and dehaze pixels that have the same color as the region with less haze. This method assumes the pixels represent scene points that have the same reflection. The second method asks the user to indicate the vanishing point and to input the maximum and minimum distance from the camera. This information is used to interpolate the distance to estimate the clear scene in between.

# 3.3. Known Depth

Oakley-Satherley 1998 is one of the early methods dealing with visibility enhancement in a single foggy image. The enhancement is done in two stages: parameter estimation followed by contrast enhancement. The basic idea of the parameter estimation is to employ the sum of squares method to minimize an error function, between the image intensity and some parameters of the physical model, by assuming the reflectance of the scene can be approximated by a single value representing the mean of the scene reflectance. With these assumptions, the minimization is done to estimate three global parameters: the atmospheric light (Lâˆž), the mean reflectance of the whole scene (Ï), and the scattering coefficient (Î²):

Err = M âˆ‘ (I(x) âˆ’ Lâˆž(1 + (Ï âˆ’ Î²d(x)))Â²)

The last equation assumes that L = Lâˆž. Having estimated the three global parameters by minimizing function Err, the airlight is then computed using:

A(x) = Lâˆž(1 âˆ’ eâˆ’Î²d(x))

Consequently, the end result is obtained by computing:

R(x) = Lmax I(x) âˆ’ A(x) eÎ²d(x)Â².Â²

where Lmax is a constant depending on the maximum gray level of the image display device, and the power is the gamma correction.

The main drawbacks of this method are the assumption that the depth of the scene is known, and the mean reflectance for the whole image is used in the minimization and in computing the airlight. The latter is acceptable if the color of the scene is somehow uniform, which is not the case for general scenes. Tan and Oakleyâ€™s extended the work of Oakley and Satherley to handle color images by taking into account a colored scattering coefficient and colored atmospheric light.

d2 = Îº or âˆš(y âˆ’ yh)Â² + (x âˆ’ xh)Â²

# ACCEPTED MANUSCRIPT

where Îº â‰¥ c. The first heuristic is used to model vertical planes like buildings and the second heuristic is used for modeling cylindrical scenes like rural roads. The two parameters c and Îº are obtained in an optimization process with a proposed image quality attribute. The final depth excluding the sky region is estimated as

d = min(d1, d2).

By doing this, the airlight A, can be transformed from color vectors into scalars, A. Hence, the visibility enhancement problem can be solved if we know the scalar value of the airlight, A, for every pixel:

eÎ²d(x) = c L2c A(x) âˆ‘c L2c,

where c represents the index of RGB channels, and ËœR is the light normalized color of the scene reflection, R. The values of A range from 0 to âˆ‘c L2c. The key idea of the method is to find a value of A(x) from that range that maximizes the local contrast of ËœR(x). The local contrast is defined as:

Contrast(ËœR(x)) = âˆ‘x,c |âˆ‡RS(x)|,

where S is a local window whose size is empirically set to 5 Ã— 5. It was found that the correlation between the airlight and the contrast is convex.

The problem can be cast into an MRF framework and optimized using graphcuts to estimate the values of the airlight across the input image. The method works for both color and gray images and was shown able to handle relatively thick fog. One of the drawbacks of the method is the appearance of halos around depth discontinuity due to the local window-based operation. Another drawback is that when the input regions have no textures, the quantity of local contrast will be constant even when the airlight value changes.

Prior to the 2008 publication, Tan et al. [37] introduced a fast single dehazing method that uses a color constancy method [38] to estimate the color of the atmospheric light, and utilizes the Y channel of the YIQ color space as an approximation to dehaze.

# 3.4. Single-Image Methods

Tan 2008 [8] is based on two basic observations: first, images on a clear day have more contrast than images in bad weather; second, the airlight whose variation mainly depends on the depth, tends to be smooth. Given an input image, the method estimates the atmospheric light, Lâˆž from the brightest pixels in the input image, and normalizes the color of the input image, from I to ËœI by dividing I by the chromaticity of Lâˆž, element-wise. The chromaticity of Lâˆž is the same

lâˆ’1(x) = A(1 âˆ’ I(x)/||Lâˆž||) + ||Lâˆž||,

t(x) = 1 âˆ’ IA(x) âˆ’ Râ€²/||Lâˆž||.

# ACCEPTED MANUSCRIPT

mission function. The IA and IRâ€² are defined as:

where Lâˆž is obtained by picking the top 0.1 % brightest pixels in the dark channel. Finally, to have a smooth and robust estimation of t(x) that can avoid the halo effects due to the use of patches, the method employs the matting Laplacian in [40]. One can interpret the dark channel prior as the maximum possible value of the airlight in a local patch, following [8], since the maximum possible value of the airlight is the minimum over the color components.

Assuming Lâˆž can be obtained from the sky regions, Î· is estimated by assuming the shading and the transmission functions are statistically uncorrelated over a certain region Î©. This implies that CÎ©(lâˆ’1, t) = 0, where function CÎ© is the sample covariance. Hence, Î· can be defined based on CÎ©(lâˆ’1, t) = 0:

Î·(x) = CÎ©(IA(x), h(x)),

where h(x) = (||Lâˆž|| âˆ’ IA(x))/IRâ€²(x). Obtaining the values of t(x) and Lâˆž will eventually solve the estimation of the scene reflection, R(x).

The success of the method relies on whether the statistical decomposition of shading and transmission can be optimum, and whether they are truly independent. Moreover, while it works for haze, the approach was not tried on foggy scenes.

He et al. 2009. The work in [20, 39] observed an interesting phenomenon of outdoor natural scenes with clear visibility. They found that most outdoor objects in clear weather have at least one color channel that is significantly dark. They argue that this is because natural outdoor images are colorful (i.e., the brightness varies significantly in different color channels) and full of shadows. Hence, they define a dark channel as:

Jdark = min(min Rc(y)).

Because of the observation that, Jdark â†’ 0, He et al. [20] refer to this as the dark channel prior. The dark channel prior is used to estimate the transmission as follows. Based on Eq. (12), we can express:

Ic(x) = t(x)Rc(x) + 1 âˆ’ t(x).

Assuming that we work on a local patch Î©(x) and denote the patchâ€™s transmission as t(x), the overall objective function can be expressed as:

min(min Ic(x)) = t(x) min(min Rc(x)) + 1 âˆ’ t(x),

and consequently, due to the dark channel prior:

t(x) = 1 âˆ’ min(min Ic(x)).

# ACCEPTED MANUSCRIPT

Hence, in terms of the factorial MRF, ËœcI is the observed field, and Cc and D are the two separated hidden fields. Each node in the MRF will connect to the corresponding node in the observed field and to its neighboring nodes within the same field. The goal is then to estimate the value of Cc for all color channels and the depth, D. The objective function consists of the likelihood and the priors Cc and D. The prior of Cc is based on the exponential power distribution of the chromaticity gradients (from natural images), while the prior of D is manually selected from a few different models, depending on the input scene (e.g., either cityscape or terrain). To solve the decomposition problem, the method utilizes an EM algorithm that decouples the estimation of the two hidden fields. In each step, graphcuts are used to optimize the values, resulting in a high computational cost. To make the iteration more efficient good initializations are required. The initialization for the depth is:

Dinit(x) = maxcâˆˆR,G,B Ic(x),

which means the upper bound on the depth value at each pixel is assumed to be corresponding to the maximum of observed RGB color values and the maximum value can be used as the initial estimate of the depth layer.

In the Bayesian direction, a different method in [45] is later proposed with a novel MRF model and planar constraint. This approach is able to produce better results, especially on road images. Ancuti-Ancuti 2010. The methods in [21] [22] propose an approach based on image fusion. The idea is to blend information from two images derived from the input image: a white-balanced image, I1, by using the gray-world color constancy method [46], and a global contrast enhanced image, I2, which is calculated by I2(x) = Î³(I(x) âˆ’ Â¯I), where Â¯I is the average intensity of the whole input image and Î³ is a weighting factor. From both I1 and I2, the weights in terms of the luminance, chromaticity, and saliency are calculated. Based on the weights, the output of the dehazing algorithm is:

Ëœt(x) = w1(x)I1 + w2(x)I2,

where wk is the normalized weights and the index k is either 1 or 2, such that wk(x) = wkwkwk and Ëœwk = wk / âˆ‘k=12 wk. The subscripts l, c, s represent luminance, chromaticity, and saliency, respectively. The method employs a L1-based regularization formulation to obtain a more robust and smooth transmission map.

Tang et al. 2014 [24], unlike the previous methods, introduces a learning-based method to estimate the transmission. The method gathers multiscale features, such as dark channel [39], local maximum contrast [8].

# ACCEPTED MANUSCRIPT

hue disparity, and local maximum saturation, and uses 550 lines in the RGB space, and the atmospheric lightâ€™s vector which starts from the origin must intersect with the two straight lines. Second, knowing the normalized color vector, it tries to estimate the magnitude of the atmospheric light. The idea is to dehaze the image using the estimated normalized light vector, and then to minimize the distance between the estimated shading and the estimated transmission for the top 1% brightness value found at each transmission level.

Cai et al. 2016 [25] proposes a learning-based framework similar to [24] that trains a regressor to predict the transmission value t(x) at each pixel (16 Ã— 16) from its surrounding patch. Unlike [24], which used a hand-crafted features, Cai et al. [25] applied a convolutional neural network (CNN) framework with special network design. The network, termed DehazeNet is conceptually formed by four sequential operations (feature extraction, multi-scale mapping, local extremum, and non-linear regression), which consist of 3 convolution layers, a max-pooling, a maxout unit, and a bilateral rectified linear unit (BReLU, a nonlinear activation function extended from standard ReLU [50]). The training set used is similar to that in [24]â€“namely, they gathered haze-free patches from Internet to generate hazy patches using the hazy imaging model with random transmissions t and assuming white atmosphere light color (Lâˆž = [1 1 1]>). Once all the weights in the network are obtained from the training, the transmission estimation for a new hazy image patch is simply forward propagation using the network. To handle the block artifact caused by the patch-based estimation, guided filtering [51] is used to refine the transmission map before recovering the scene.

Berman et al. 2016 [26] proposes an algorithm based on a new, non-local prior. This is a departure from existing methods (e.g., [8, 20, 23, 1, 24, 25]) that use patch-based transmission estimation. The algorithm by [26] relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, that form tight clusters in RGB space and pixels in a cluster are often non-local (spread in the whole image). The presence of haze will elongate the shape of each cluster to a line in color space as the pixels may be affected by different transmission coefficients due to their different distances to the camera. The line, termed haze-line, is informative in estimating the transmission factors. In their algorithm, they first proposed a clustering method to group the pixels and each cluster becomes a haze-line. Then the maximum radius of each cluster is calculated and used to estimate the transmission. A final regulation step is performed to enforce the smoothness of the transmission map.

# ACCEPTED MANUSCRIPT

# Table 3: Single-image dehazing methods we compared.

The programming language use is denoted as: M for matlab, P for python, C for C/C++. The average runtime is tested on images of resolution 720 Ã— 480 using a desktop with Xeon E5 3.5GHz CPU and 16GB RAM. Source of the results is denoted as: (No symbol) is code from the authors, (*) is our implementation, (â€ ) is result images that are directly provided by the authors.

|Methods|Pub. venue|Code|Runtime(s)|
|---|---|---|---|
|Ancuti 13 [22]|TIP 2013|M*|3.0|
|Tan 08 [8]|CVPR 2008|C|3.3|
|Fattal 08 [19]|ToG 2008|Mâ€ |141.1|
|He 09 [20]|CVPR 2009|M*|20|
|Tarel 09 [7]|ICCV 2009|M|12.8|
|Kratz 09 [42]|ICCV 2009|P|124.2|
|Meng 13 [23]|ICCV 2013|M|1.0|
|Fattal 14 [1]|ToG 2014|Câ€ |1.9|
|Berman 16 [26]|CVPR 2016|M|1.8|
|Tang 14 [24]|CVPR 2014|M*|10.4|
|Cai 16 [25]|TIP 2016|M*|1.7|

# 4. Quantitative Benchmarking

In this section, we benchmark several well-known visibility enhancement methods. Our focus is on recent single-image-based methods. Compared with other approaches, single-image-based approaches are more practical and thus have more potential applications. By benchmarking the methods in this approach, we consider it will be beneficial, since one can know the comparisons of the methods quantitatively.

# 4.1. Evaluation on Fattalâ€™s Dataset [1]

To compare all methods quantitatively we need to test on a dataset with ground truth. Ideally, similar to what Narasimhan et al. [52] did, the dataset should be created from real atmospheric scenes taken over a long period of time to have all possible atmospheric conditions ranging from light mist to dense fog with various backgrounds of scenes. While it may be possible, it is not trivial, since it has to be done at certain times and locations where fog and haze are present frequently. In addition, the illumination in the scene should keep fixed which means clouds and sunlight distribution should be about the same. Unfortunately, these conditions rarely met. Moreover, it is challenging to have a pixel-wise ground truth of a scene without the effect of particles even on a clear day, particularly for distant objects, as significant amounts of atmospheric particles are always present. These reasons motivated us to use synthesized data. We first performed dehazing evaluations on a recent dataset provided by Fattal [1]. In addition, we created a new dataset using a physics-based rendering technique for the evaluation. In the following sections, we will describe the details of the dataset and present the results of different dehazing methods on.

We excluded the Doll scene due to invalid link on the page.

3 http://www.cs.huji.ac.il/~raananf/projects/dehaze_cl/results/index_comp.html

# ACCEPTED MANUSCRIPT

# Table 4: The mean absolute difference of transmission estimation results on Fattalâ€™s dataset [1]. The three smallest values are highlighted.

|Methods|Church|Couch|Flower1|Flower2|Lawn1|Lawn2|Mansion|Moebius|Reindeer|Road1|Road2|
|---|---|---|---|---|---|---|---|---|---|---|---|
|Tan 08 [8]|0.167|0.367|0.216|0.294|0.275|0.281|0.316|0.219|0.372|0.257|0.186|
|Fattal 08 [19]|0.377|0.090|0.089|0.075|0.317|0.323|0.147|0.111|0.070|0.319|0.347|
|Kratz 09 [42]|0.147|0.096|0.245|0.275|0.089|0.093|0.146|0.239|0.142|0.120|0.118|
|He 09 [20]|0.052|0.063|0.164|0.181|0.105|0.103|0.061|0.208|0.115|0.092|0.079|
|Meng 13 [23]|0.113|0.096|0.261|0.268|0.140|0.131|0.118|0.228|0.128|0.114|0.096|
|Tang 14 [24]|0.141|0.074|0.044|0.055|0.118|0.127|0.096|0.070|0.097|0.143|0.158|
|Fattal 14 [1]|0.038|0.090|0.047|0.042|0.078|0.064|0.043|0.145|0.066|0.069|0.060|
|Cai 16 [25]|0.061|0.114|0.112|0.126|0.097|0.102|0.072|0.096|0.095|0.092|0.088|
|Berman 16 [26]|0.047|0.051|0.061|0.115|0.032|0.041|0.080|0.153|0.089|0.058|0.062|

# Table 5: The mean signed difference of transmission estimation results on Fattalâ€™s dataset [1].

|Methods|Church|Couch|Flower1|Flower2|Lawn1|Lawn2|Mansion|Moebius|Reindeer|Road1|Road2|
|---|---|---|---|---|---|---|---|---|---|---|---|
|Tan 08 [8]|0.013|-0.339|-0.117|-0.268|-00.083|-0.089|-0.301|-0.160|-0.358|-0.148|-0.117|
|Fattal 08 [19]|0.376|0.088|0.088|0.071|0.317|0.323|0.143|0.073|0.063|0.312|0.327|
|Kratz 09 [42]|-0.006|0.010|-0.220|-0.267|0.003|-0.013|-0.114|-0.236|-0.083|-0.030|0.067|
|He 09 [20]|-0.035|-0.045|-0.162|-0.180|-0.091|-0.086|-0.041|-0.208|-0.105|-0.054|-0.047|
|Meng 13 [23]|-0.112|-0.003|-0.259|-0.266|-0.139|-0.130|-0.101|-0.223|-0.086|-0.109|-0.089|
|Tang 14 [24]|0.133|0.054|-0.008|-0.046|0.059|0.067|0.089|-0.051|0.013|0.094|0.123|
|Fattal 14 [1]|-0.019|0.086|-0.021|-0.019|0.063|0.045|0.002|-0.105|0.006|0.005|-0.015|
|Cai 16 [25]|-0.002|0.086|-0.096|-0.118|0.012|0.017|-0.028|-0.070|0.044|0.001|0.023|
|Berman 16 [26]|0.009|-0.014|-0.051|-0.115|-0.008|-0.013|-0.076|-0.152|-0.059|-0.041|-0.021|

# Table 6: The mean absolute difference of final dehazing results On Fattalâ€™s dataset [1]. The three smallest values are highlighted.

|Methods|Church|Couch|Flower1|Flower2|Lawn1|Lawn2|Mansion|Moebius|Reindeer|Road1|Road2|
|---|---|---|---|---|---|---|---|---|---|---|---|
|Tan 08 [8]|0.109|0.139|0.098|0.134|0.146|0.146|0.154|0.131|0.150|0.111|0.139|
|Fattal 08 [19]|0.158|0.055|0.028|0.022|0.116|0.123|0.071|0.039|0.034|0.135|0.165|
|Kratz 09 [42]|0.099|0.060|0.155|0.161|0.055|0.059|0.085|0.155|0.083|0.073|0.088|
|He 09 [20]|0.036|0.038|0.078|0.080|0.056|0.057|0.034|0.121|0.061|0.051|0.052|
|Tarel 09 [7]|0.173|0.112|0.130|0.120|0.146|0.161|0.113|0.143|0.179|0.148|0.176|
|Ancuti 13 [22]|0.188|0.078|0.276|0.219|0.128|0.144|0.109|0.189|0.145|0.135|0.142|
|Meng 13 [23]|0.052|0.060|0.114|0.106|0.055|0.055|0.048|0.096|0.065|0.052|0.054|
|Tang 14 [24]|0.087|0.048|0.017|0.019|0.072|0.078|0.053|0.031|0.053|0.088|0.106|
|Fattal 14 [1]|0.025|0.053|0.019|0.015|0.035|0.033|0.022|0.076|0.034|0.033|0.038|
|Cai 16 [25]|0.042|0.069|0.045|0.049|0.061|0.0652|0.040|0.043|0.053|0.057|0.065|
|Berman 16 [26]|0.032|0.031|0.022|0.045|0.026|0.031|0.049|0.081|0.045|0.040|0.042|

# ACCEPTED MANUSCRIPT

Input Tarel 09 Ancuti 13 Tan 08 Fattal 08 Kratz 09

Ioo Tang 14 He 09 Cai 16 Meng 13 Fattal 14 Berman 16

Figure 4: Final haze removal results on the churchcase.

|Average transmission MAD|Average transmission MSD|Average color MAD|
|---|---|---|
|Tan 08|Tan 08|Tan 08|
|Fattal 08|Fattal 08|Fattal 08|
|Kratz 09|Kratz 09|Kratz 09|
|He 09|He 09|He 09|
| |Tarel 09| |
|Meng 13|Meng 13|Ancuti 13|
|Tang 14|Tang 14|Meng 13|
|Fattal 14|Fattal 14|Tang 14|
|Cai 16|Cai 16|Fattal 14|
|Berman 16|Berman 16|Cai 16|
| | |Berman 16|

-0.2 -0. 0.2 05 15

Fig. 1. The average performance of different dehazing methods on Fattalâ€™s dataset [?].

Figure 5: The average performance of different dehazing methods on Fattalâ€™s dataset [1].

# 680 Transmission Map Evaluation

Table 4 lists the mean absolute difference (MAD) of the estimated transmissions (excluding sky regions) of each method to the ground truth transmission. Note that two methods, Tarel 09 [7] and Ancuti 13 [22], are not included, as Tarel 09 [7] directly estimated airlight A in Eq. (14) and Ancuti 13 [22] does not require the transmission estimation. The three smallest errors for each image are highlighted. We can see no single method can be outstanding for all cases. The recent methods Fattal 14 [1] and Berman 16 [26] can obtain more accurate estimation of the transmission for most cases. The early work of Tan 08 [8] gives less precise estimation. Another early work, Fattal 08 [19], is not stable and it obtains accurate estimation in a few cases (e.g., flower2, reindeer) while it obtains the largest error in some other cases (e.g., church, road1).

We plot the average MAD over all 11 cases in Figure 5. It is noticed that in general, the latest methods perform better in the transmission estimation. The methods of Fattal 14 [1] and Berman 16 [26] rank at the top, while the two learning-based methods, Tang 14 [24] and Cai 16 [25], are in the second place. However, we noticed in our experiments that the learning-based methods heavily rely on the white balance step.

We further test the mean signed difference (MSD) on the transmission estimation results (excluding sky regions) as MSD = 1/N âˆ‘ (ti - t), where i is the pixel index, N is the total number of pixels, ti is the estimated transmission, and t is the ground truth transmission. By doing so, we can test whether a method overestimates (positive signed difference) or underestimates (negative signed difference) the transmission, which cannot be revealed using the previous MAD metrics. The MSDs are listed in Table 5 and the average MSDs are plotted in Figure 5. It is observed that Tan 08 [8] mostly underestimates the transmission and as a result it obtains oversaturated dehaze results. Fattal 08 [19], on the other hand, likely overestimates the transmission, leading to results with haze still presented in the output. The two methods He 09 [20] and Meng 13 [23] also slightly.

# ACCEPTED MANUSCRIPT

underestimate the transmission due to the fact they essentially predict the lower bound of transmission.

# Dehazing Results Evaluation

We evaluate the dehazing results. The mean absolute difference (MAD) of each method (excluding sky regions) to the ground truth clean image is listed in Table 6 and the dehazing results on the church case are shown in Figure 4. In Table 6, the three smallest errors for each image are highlighted. Again, no one method can be outstanding for all cases. It is observed that non-model-based method Ancuti 13 [22] obtains the largest error in the recovery. The visual qualities of their results are also rather inferior compared with other methods (as can be seen in Figure 4). This shows that the image contrast enhancement operation without the haze image model Eq. (12) cannot achieve satisfactory results.

Among the rest of the model-based methods, the latest methods, Meng 13 [23], Tang 14 [24], Fattal 14 [1], Cai 16 [25], and Berman 16 [26], and also He 09 [20] generally perform better than early dehazing methods Tan 08 [8], Fattal 08 [19], Tarel 09 [7], and Kratz 09 [42]. Fattal 14 [1] and Berman 16 [26] are the best two methods that can provide dehazing results that are the closest to the ground truth. This quantitative ranking corresponds well to the overall visual quality for the example shown in Figure 4.

# Evaluation with Various Haze Levels

Additionally, we test the performance of each method for different haze levels. In Fattalâ€™s dataset [1], he provides a subset of images (lawn1, mansion, reindeer, road1) that are synthesized with three different haze levels by controlling the scattering coefficient Î². As Î² increases, denser haze effects will appear. We measure the transmission estimation error and final dehazing error using the mean absolute difference, and the average results over all scenes are plotted in Figure 6. It is clearly observed that Fattal 14 [1] stably stands out in achieving fewer errors in both transmission estimation and final dehazing at different haze levels. Fattal 08 [19] works well only at low haze levels and the performance drops at medium and high haze levels.

Looking at the transmission results, we can see Tan 08 [8]â€™s, He 09 [20]â€™s, and Meng 13 [23]â€™s estimation becomes more accurate when haze level increases. This demonstrates that the priors of these three methods are correlated with haze so that these priors can tell more information with more haze. The difference is that He 09 [20], and Meng 13 [23] can achieve much smaller transmission errors than Tan 08 [8], showing the superiority of dark channel prior [20] and boundary constraint [23] against the local contrast [8] for this task. This can be explained by the fact that with heavier haze, the priors can provide better estimations.

We have evaluated 9 methods on our dataset (Fattal 08 [19]â€™s and Fattal 14 [1]â€™s results are not available on our dataset). As the test images in our dataset are rendered with the Monte-Carlo sampling-based ray tracing algorithm, we cannot obtain the transmission map explicitly. Therefore, we quantify the visibility enhancement outputs by comparing them with their respective ground truths. The quantitative measurement is done by using the structural similarity index (SSIM) [55]. While MAD directly measures the closeness of the pixel value to the ground truth, SSIM is more consistent with human visual perception, especially in the cases of dehazing for denser haze levels (haze level beyond 3 in our dataset). SSIM is a popular choice to compute the visibility enhancement.

# ACCEPTED MANUSCRIPT

# Average transmission MAD at different haze levels

|0.25|Average color MAD at different haze levels|
|---|---|
|low|IoW|
|medium|medium|
|high|high|
|0.15| |

Tan 08 Fattal 08 Kratz 09 He 09 Meng 13 Tang 14 Fattal 14 Cai 16 Berman 16

Tan 08 Fattal 08 Kratz 09 He 09 Tarel 09 Ancuti 13 Meng 13 Tang 14 Fattal 14 Cai 16 Berman 16

# Fig. 1. Comparisons of the results for different haze levels.

# Figure 6: Comparisons of the results for different haze levels.

# Figure 7: Samples of our synthetic data with increasing haze levels.

structure similarity of two images in image restoration. Unlike MAD, a higher value in SSIM indicates a better match as it is a similarity measurement.

# Figure 8 shows the performance of each method in terms of SSIM.

It is observed that again the latest methods Tang 14 [24], Cai 16 [25], and Berman 16 [26] generally performed better than others. He 09 [20] also performs very well, especially in heavier haze levels. This is consistent with our experiment in Section 4.1.

# 4.3. Qualitative Results on Real Images

We also list three qualitative examples of the dehazing results on real hazy images by different methods in Figure 9 (more visual comparisons can be found in the previous dehazing paperâ€“e.g., [1, 26]). The visual comparison here confirms our findings in the previous benchmarking that Fattal 14 [1] and Berman 16 [26] are the best two methods that can consistently provide excellent dehazing results. Some early methods, like Kratz 09 [42], Tarel 09 [7], and Ancuti 13 [22] exhibit noticeable limitations in the dehazing results (e.g., oversaturation, boundary artifacts, color shift). He 09 [20] and Meng 13 [23] also perform well and obtain similar results as they essentially both predict the lower bound of the transmission.

The learning-based methods Tang 14 [24] and Cai 16 [25] produce appealing results but tend to leave a noticeable amount of haze in the image.

We have also conducted the first quantitative benchmark for most representative single-image dehazing methods. Our primary finding from the benchmark is that recent works [1],[26] generally perform better in the dehazing. Machine learning based methods [24, 25] can also get decent results, but their performance is likely to.

# ACCEPTED MANUSCRIPT

# Average SSIM at different haze levels

| |0.8|0.6|0.5|
|---|---|---|---|
|Tan|08|Kratz|09|
|He|09|Tareb| |
|Ancuti|13|Meng|13|
|Tang|14|Cai|16|
|Berman|16| | |

Figure 8: The performance of each method on our dataset on 5 haze levels (l=1,2,3,4,5, low to high) in terms of SSIM.

be affected by the white balancing step. Therefore we still recommend the prior-based methods [1],[26] over the learning-based methods [24, 25] in practical use for robustness. We also found that the popular dark channel prior [20] is an effective prior in dehazing, especially for denser haze levels.

For the dataset used in the benchmark, we picked a dataset from Fattal [1] and also our newly introduced synthetic dataset, which provides ground truth images and haze images with different haze levels. We hope the community can benefit from our dataset by being able to assess new methods more objectively.

# Discussion

When fog is considerably thick, the problem of visibility enhancement becomes harder. This is because scene reflection is â€œburiedâ€ further underneath the airlight (A) and transmission (t). Considering the scattering model in Eq. (12), when the scattering coefficient Î² is largeâ€“that is, in a thick fog sceneâ€“the transmission (t = eâˆ’Î²d) is small. Consequently, the airlight (A = (1 âˆ’ t)Lâˆž) is dominated by the atmospheric light, Lâˆž, and thus the veiling component takes up a greater portion in the image intensity. Also, since the transmission is small, the contribution of scene reflection in the image intensity becomes reduced significantly, due to the multiplication of R with a fractionally small value of t. The combined airlight and transmission components hide the underlying scene reflection information in the image intensities.

Based on this, some questions might arise: how do we know whether the information of scene reflection is too minuscule to be recovered? How thick is the fog that we cannot extract the scene reflection any longer? Answering such questions is important theoretically, since then we can know the limit of visibility enhancement in bad weather. Furthermore, in thick foggy scenes, due to absorption and scattering to directions other than the line of sight, image blur will be present more prominently and it is not modeled in the current model.

Another issue to note is the application of various onboard camera photo-finishing routines, such as tone-mapping and color manipulation. Although many methods do not explicitly mention the assumption of linearity between the flux of incoming light and the pixel intensity values, based on the scattering model (Eq. (12)), there is an assumption that the image is acting as a linear light-measuring device. While for the purpose of visibility enhancement this might not be an issue, for physically correct scene reflection recovery, the non-linearity of real camera outputs can be a significant issue that needs to be carefully considered.

Our synthetic dataset is still limited in size. Modelling and rendering a large set of data using physics-based rendering takes a great deal of time and effort. However, continued efforts in producing a larger dataset would be of continued benefit for future work.

# Acknowledgment

This study is supported by an Nvidia GPU Grant and a Canadian NSERC Discovery grant. R. T. Tans work in this research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its.

# ACCEPTED MANUSCRIPT

|Input: House|Tan 08|Fattal 08|Kratz 09|Tarel 09|He 09| | |
|---|---|---|---|---|---|---|---|
|Ancuti 13|Meng 13|Tang 14|Fattal 14|Cai 16|Berman 16| | |
|Input: Train|Tan 08| |Kratz 09|Tarel 09|He 09| | |
| |Meng 13|Tang 14|Fattal 14|Cai 16|Berman 16| | |
|Input: Cityscape|Tan 08|Tarel 09|He 09|Meng 13|Fattal 14|Cai 16|Berman 16|

Figure 9: Example comparisons on real images.

International Research Centre in Singapore Funding Initiative.

# References

1. R. Fattal, Dehazing using color-lines, ACM Trans. Graph. 34 (1) (2014) 13:1â€“13:14.
2. H. Koschmieder, Theorie der horizontalen Sichtweite: Kontrast und Sichtweite, Keim & Nemnich, 1925.
3. E. J. McCartney, Optics of the atmosphere: scattering by molecules and particles, New York, John Wiley and Sons, Inc., 1976. 421 p.
4. F. Cozman, E. Krotkov, Depth from scattering, in: IEEE Conf. Computer Vision and Pattern Recognition, 1997.
5. S. K. Nayar, S. G. Narasimhan, Vision in bad weather, in: IEEE Intâ€™l Conf. Computer Vision, 1999.
6. S. G. Narasimhan, S. K. Nayar, Vision and the atmosphere, Intâ€™l J. Computer Vision 48 (3) (2002) 233â€“254.
7. J.-P. Tarel, N. HautiÃ¨re, Fast visibility restoration from a single color or gray level image, in: IEEE Intâ€™l Conf. Computer Vision, 2009.
8. R. T. Tan, Visibility in bad weather from a single image, in: IEEE Conf. Computer Vision and Pattern Recognition, 2008.
9. S. G. Narasimhan, S. K. Nayar, Chromatic framework for vision in bad weather, in: IEEE Conf. Computer Vision and Pattern Recognition, 2000.
10. S. G. Narasimhan, S. K. Nayar, Contrast restoration of weather degraded images, IEEE Trans. Pattern Analysis and Machine Intelligence 25 (6) (2003) 713â€“724.
11. L. Caraffa, J.-P. Tarel, Stereo reconstruction and contrast restoration in daytime fog, in: Asian Conf. Computer Vision.

# ACCEPTED MANUSCRIPT

2012.

|[12]|Z. Li, P. Tan, R. T. Tan, S. Z. Zhou, L.-F. Cheong, Simultaneous video defogging and stereo reconstruction, in: IEEE Conf. Computer Vision and Pattern Recognition, 2015.|
|---|---|
|[13]|Y. Y. Schechner, S. G. Narasimhan, S. K. Nayar, Instant de-hazing of images using polarization, in: IEEE Conf. Computer Vision and Pattern Recognition, 2001.|
|[14]|S. Shwartz, E. Namer, Y. Y. Schechner, Blind haze separation, in: IEEE Conf. Computer Vision and Pattern Recognition, 2006.|
|[15]|J. P. Oakley, B. L. Satherley, Improving image quality in poor visibility conditions using a physical model for contrast degradation, IEEE Trans. Image Processing 7 (2) (1998) 167â€“179.|
|[16]|S. G. Narasimhan, S. K. Nayar, Interactive (de) weathering of an image using physical models, in: IEEE Workshop on Color and Photometric Methods in Computer Vision, 2003.|
|[17]|N. HautiÃ¨re, J.-P. Tarel, D. Aubert, Towards fog-free in-vehicle vision systems through contrast restoration, in: IEEE Conf. Computer Vision and Pattern Recognition, 2007.|
|[18]|J. Kopf, B. Neubert, B. Chen, M. F. Cohen, D. Cohen-Or, O. Deussen, M. Uyttendaele, D. Lischinski, Deep photo: Model-based photograph enhancement and viewing, ACM Trans. Graphics 27 (5) (2008) 116:1â€“116:10.|
|[19]|R. Fattal, Single image dehazing, ACM Trans. Graphics 27 (3) (2008) 72.|
|[20]|K. He, J. Sun, X. Tang, Single image haze removal using dark channel prior, in: IEEE Conf. Computer Vision and Pattern Recognition, 2009.|
|[21]|C. O. Ancuti, C. Ancuti, P. Bekaert, Effective single image de-hazing by fusion, in: IEEE Intâ€™l Conf. Image Processing, 2010.|
|[22]|C. O. Ancuti, C. Ancuti, Single image dehazing by multi-scale fusion, IEEE Trans. Image Processing 22 (8) (2013) 3271â€“3282.|
|[23]|G. Meng, Y. Wang, J. Duan, S. Xiang, C. Pan, Efficient image dehazing with boundary constraint and contextual regularization, in: IEEE Intâ€™l Conf. Computer Vision, 2013.|
|[24]|K. Tang, J. Yang, J. Wang, Investigating haze-relevant features in a learning framework for image dehazing, in: IEEE Conf. Computer Vision and Pattern Recognition, 2014.|
|[25]|B. Cai, X. Xu, K. Jia, C. Qing, D. Tao, Dehazenet: An end-to-end system for single image haze removal, IEEE Trans. Image Processing 25 (11) (2016) 5187â€“5198.|
|[26]|D. Berman, T. Treibitz, S. Avidan, Non-local image dehazing, in: IEEE Conf. Computer Vision and Pattern Recognition, 2016.|
|[27]|G. M. Hidy, M. Kerker, Aerosols and Atmospheric Chemistry: The Kendall Award Symposium Honoring Milton Kerker, at the Proceedings of the American Chemical Society, Los Angeles, California, March 28-April 2, 1971, Academic Press, 1972.|
|[28]|M. O. Codes, International codesâ€“wmo no. 306, Genevaâ€“Switzerland: World Meteorological.|
|[29]|C. D. Ahrens, Meteorology today: an introduction to weather, climate, and the environment, West Publishing Company New York, 1991.|
|[30]|M. G. J. Minnaert, The Nature of Light and Colour in the Open Air: Transl.[By] HM Krener-Priest, Rev.[By] KE Brian Jay, Dover, 1954.|
|[31]|S. A. Shafer, Using color to separate reflection components, Color Research & Application 10 (4) (1985) 210â€“218.|
|[32]|S. Tominaga, B. A. Wandell, Standard surface-reflectance model and illuminant estimation, J. Opt. Soc. Am. A 6 (4) (1989) 576â€“584.|
|[33]|L. Caraffa, J.-P. Tarel, Combining stereo and atmospheric veil depth cues for 3d reconstruction, IPSJ Transactions on Computer Vision and Applications 6 (2014) 1â€“11.|
|[34]|G. Zhang, J. Jia, T.-T. Wong, H. Bao, Consistent depth maps recovery from a video sequence, IEEE Trans. Pattern Analysis and Machine Intelligence 31 (6) (2009) 974â€“988.|

