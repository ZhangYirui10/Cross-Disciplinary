# Privacy Preserving Vertical Federated Learning for Tree-based Models

# [Technical Report]

# Yuncheng Wu†, Shaofeng Cai†, Xiaokui Xiao†, Gang Chen‡, Beng Chin Ooi†

{ †National University of Singapore, ‡Zhejiang University }

wuyc@comp.nus.edu.sg, shaofeng@comp.nus.edu.sg, xiaoxk@comp.nus.edu.sg, ooibc@comp.nus.edu.sg, cg@zju.edu.cn

# ABSTRACT

Federated learning (FL) is an emerging paradigm that enables multiple organizations to jointly train a model without revealing their private data to each other. This paper studies vertical federated learning, which tackles the scenarios where (i) collaborating organizations own data of the same set of users but with disjoint features, and (ii) only one organization holds the labels. We propose Pivot, a novel solution for privacy preserving vertical decision tree training and prediction, ensuring that no intermediate information is disclosed other than those the clients have agreed to release (i.e., the final tree model and the prediction output). Pivot does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m−1 out of m clients. We further identify two privacy leakages when the trained decision tree model is released in plaintext and propose an enhanced protocol to mitigate them. The proposed solution can also be extended to tree ensemble models, e.g., random forest (RF) and gradient boosting decision tree (GBDT) by treating single decision trees as building blocks. Theoretical and experimental analysis suggest that Pivot is efficient for the privacy achieved.

# PVLDB Reference Format:

Yuncheng Wu, Shaofeng Cai, Xiaokui Xiao, Gang Chen, Beng Chin Ooi. Privacy Preserving Vertical Federated Learning for Tree-based Models. PVLDB, 13(11): 2090-2103, 2020.

DOI: https://doi.org/10.14778/3407790.3407811

# 1. INTRODUCTION

There has been a growing interest in exploiting data from distributed databases of multiple organizations, for providing better customer service and acquisition. Federated learning (FL) (or collaborative learning) is an emerging paradigm for machine learning that enables multiple data owners (i.e., clients) to jointly train a model without revealing their private data to each other. The basic idea of FL is to iteratively let each client (i) perform some local computations on her data to derive certain intermediate results, and then (ii) exchange these results with other clients in a secure manner to advance the training process, until a final model is obtained. The advantage of FL is that it helps each client protect her data assets, so as to abide by privacy regulations (e.g., GDPR and CCPA) or to maintain a competitive advantage from proprietary data.

Existing work on FL has mainly focused on the horizontal setting, which assumes that each client’s data have the same schema, but no tuple is shared by multiple clients. In practice, however, there is often a need for vertical federated learning, where all clients hold the same set of records, while each client only has a disjoint subset of features. For example, a digital banking scenario, where a bank and a Fintech company aim to jointly build a machine learning model that evaluates credit card applications. The bank has some partial information about the users (e.g., account balances), while the Fintech company has some other information (e.g., the users’ online transactions). In this scenario, vertical FL could enable the bank to derive a more accurate model, while the Fintech company could benefit from a pay-per-use model for its contribution to the training and prediction.

To our knowledge, there exist only a few solutions for privacy preserving vertical FL. These solutions, however, are insufficient in terms of either efficiency or data privacy. In particular, some assume that the labels in the training data could be shared with all participating clients in plaintext, whereas in practice, the labels often exist in one client’s data only and could not be revealed to other clients without violating privacy. For instance, in the scenario illustrated, the training data could be a set of historical credit card applications, and each label would be a ground truth that indicates whether the application should have been approved. In this case, the labels are only available to the bank and could not be directly shared with the Fintech company. As a consequence, the solutions are inapplicable. Meanwhile, others assume that some intermediate results during the execution could be revealed in plaintext; nevertheless, such intermediate results could be exploited by an adversarial client to infer the sensitive information in other clients’ data. One solution, on the other hand, relies on secure hardware for privacy protection, but such secure hardware may not be available.

This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.

Proceedings of the VLDB Endowment, Vol. 13, No. 11

ISSN 2150-8097.

DOI: https://doi.org/10.14778/3407790.3407811

not be trusted by all parties [81] and could be vulnerable to side channel attacks [76]. The method in [57] utilizes secure multiparty computation (MPC) [78], but assumes that each client’s data could be outsourced to a number of non-colluding servers. This assumption is rather strong, as it is often challenging in practice to ensure that those servers do not collude and to convince all clients about it.

To address the above issues, we propose Pivot, a novel and efficient solution for vertical FL that does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m − 1 out of m clients. Pivot is a part of our Falcon¹ (federated learning with privacy protection) system, and it ensures that no intermediate information is disclosed during the training or prediction process. Specifically, Pivot is designed for training decision tree (DT) models, which are well adopted for financial risk management [21, 50], healthcare analytics [6], and fraud detection [16] due to their good interpretability.

The core of Pivot is a hybrid framework that utilizes both threshold partially homomorphic encryption (TPHE) and MPC, which are two cryptographic techniques that complement each other especially in the vertical FL setting: TPHE is relatively efficient in terms of communication cost but can only support a restrictive set of computations, whereas MPC could support an arbitrary computation but incurs expensive communication overheads. Pivot employs TPHE as much as possible to facilitate clients’ local computation, and only invokes MPC in places where TPHE is inadequate in terms of functionality. This leads to a solution that is not only secure but also highly efficient for vertical tree models, as demonstrated in Section 8. Specifically, we make the following contributions:

- We propose a basic protocol of Pivot that supports the training of both classification trees and regression trees, as well as distributed prediction using the tree models obtained. This basic protocol guarantees that each client only learns the final tree model but nothing else. To our knowledge, Pivot is the first vertical FL solution that achieves such a guarantee.
- We enhance the basic protocol of Pivot to handle a more stringent case where parts of the final tree model need to be concealed for better privacy protection. In addition, we propose extensions of Pivot for training several ensemble tree-based models, including random forest (RF) and gradient boosting decision trees (GBDT).
- We implement DT, RF, and GBDT models based on Pivot and conduct extensive evaluations on both real and synthetic datasets. The results demonstrate that Pivot offers accuracy comparable to non-private algorithms and provides high efficiency. The basic and enhanced protocols of Pivot achieve up to 37.5x and 4.5x speedup (w.r.t. training time) over an MPC baseline.

# 2. PRELIMINARIES

# 2.1 Partially Homomorphic Encryption

A partially homomorphic encryption (PHE) scheme is a probabilistic asymmetric encryption scheme for restricted computation over the ciphertexts. In this paper, we utilize the Paillier cryptosystem [61], which consists of three algorithms (Gen, Enc, Dec):

1. The key generation algorithm (sk, pk) = Gen(keysize) which returns secret key sk and public key pk, given a security parameter keysize.
2. The encryption algorithm c = Enc(x, pk), which maps a plaintext x to a ciphertext c using pk.
3. The decryption algorithm x = Dec(c, sk), which reverses the encryption by sk and outputs the plaintext x.

Interested readers are referred to [27] for the exact construction of Enc and Dec. For simplicity, we omit the public key pk in the Enc algorithm and write Enc(x) as [x] in the rest of the paper. Let x1, x2 denote two plaintexts. We utilize the following properties of PHE:

- Homomorphic addition: given two ciphertexts [x1], [x2], the ciphertext of the sum x1 + x2 can be obtained by multiplying the ciphertexts, i.e., [x1]⊕ [x2</sub] ∶ [x1]⋅ [x2] = [x1 + x2]
- Homomorphic multiplication: given a plaintext x1 and a ciphertext [x2], the ciphertext of the product x1x2 can be obtained by raising [x2] to the power x1: x1 ⊗ [x2] ∶ [x2]ˣ1 = [x1x2]
- Homomorphic dot product: given a ciphertext vector [v] = ([v1],⋯,[vm])ᵀ and a plaintext vector x = (x1,⋯, xm), the ciphertext of the dot product v ⋅ x can be obtained by: x⊙ [v] ∶ (x1 ⊗ [v1])⊕ ⋯ ⊕ (xm ⊗ [vm]) = [x1v1 + ⋯ + xmvm] = [x ⋅ v]

We utilize a threshold variant of the PHE scheme (i.e., TPHE) with the following additional properties. First, the public key pk is known to everyone, while each client only holds a partial secret key. Second, the decryption of a ciphertext requires inputs from a certain number of clients. In this paper, we use a full threshold structure, which requires all clients to participate in order to decrypt a ciphertext.

# 2.2 Secure Multiparty Computation

Secure multiparty computation (MPC) allows participants to compute a function over their inputs while keeping the inputs private. In this paper, we utilize the additive secret sharing scheme SPDZ [28] for MPC. We refer to a value a ∈ Zq that is additively shared among clients as a secretly shared value, and denote it as ⟨a⟩ = (⟨a⟩1,⋯,⟨a⟩m), where ⟨a⟩i is a random share of ⟨a⟩ hold by client i. To reconstruct a secretly shared value ⟨a⟩, i.e., Rec(⟨a⟩), every client can send its own share to a specific client who computes a = (∑i=1m ⟨a⟩i) mod q. Given secretly shared values, we have the following secure computation primitives. For ease of exposition, we omit the modular operation in the following formulations.

- Secure addition: given two secretly shared values ⟨a⟩ and ⟨b⟩, the secretly shared sum c = a + b can be obtained by having client i non-interactively compute ⟨c⟩i = ⟨a⟩i + ⟨b⟩i. Then ⟨c⟩i is a share of ⟨c⟩ owned by client i.
- Secure multiplication: given two secretly shared values ⟨a⟩ and ⟨b⟩, the secretly shared multiplication c = a⋅b can be obtained using Beaver’s pre-computed multiplication triplet technique [7]. Assuming that the clients have already shared ⟨u⟩, ⟨v⟩, ⟨z⟩ where u, v are random values in Zq and z = u⋅ v mod q, then client i locally computes ⟨e⟩i = ⟨a⟩i − ⟨u⟩i

# Algorithm 1: CART(F, Y, D)

Input: F : feature set, Y : label set, D: sample set

Output: T: decision tree

1. if prune conditions satisfied then
2. classification: return leaf node with majority class
3. regression: return leaf node with mean label value
4. else
5. determine the best split feature j and value s
6. split D into 2 partitions Dl, Dr
7. return a tree with feature a that has two edges, call CART(F − j, Y, Dl) and CART(F − j, Y, Dr)

⟨f⟩ = ⟨b⟩ −⟨v⟩ , and the clients run Rec(⟨e⟩)and Rec(⟨f⟩).

Finally, every client i computes ⟨c⟩i = −i⋅ e⋅ f +f ⋅ ⟨a⟩i +e ⋅ ⟨b⟩i + ⟨z⟩i. Then ⟨c⟩i is a share of ⟨c⟩ owned by client i.

Secure comparison: given two shared values ⟨a⟩ and ⟨b⟩, the secure comparison operation (e.g., ⟨a⟩ > ⟨b⟩) returns a secretly shared ⟨0⟩ orₖ⟨1⟩. The basic idea is to first truncate the two values by 2, then execute ⟨a⟩ − ⟨b⟩, and finally output the secretly shared sign bit after dividing by 2ᵏ−1. We refer the interested readers to [17, 18] for details.

Based on the above primitives, other primitives including secure division and secure exponential can be approximated, which are also supported in SPDZ [18, 28, 5]. In this paper, we use these secure computations in SPDZ as building blocks by default for the calculation concerning secretly shared values, which means that the outputs are also secretly shared values unless they are reconstructed. The secret sharing based MPC has two phases: an offline phase that is independent of the function and generates pre-computed Beaver’s triplets, and an online phase that computes the designated function using these triplets.

# 2.3 Tree-based Models

In this paper, we consider the classification and regression trees (CART) algorithm [13] with binary structure, while we note that other variants (e.g., ID3 [64], C4.5 [65]) can be easily generalized. We assume there is a training dataset D with n data points {x₁,⋯,xₙ} each containing d features and the corresponding output label set Y = {y₁,⋯, yₙ}.

Algorithm 1 describes the CART algorithm, which builds a tree recursively. For each tree node, it first decides whether some pruning conditions are satisfied, e.g., feature set is empty, tree reaches the maximum depth, the number of samples is less than a threshold. If any condition is satisfied, then it returns a leaf node with the class of majority samples for classification or the mean label value for regression. Otherwise, it determines the best split to construct two sub-trees that are built recursively. In order to find the best split feature and split threshold, CART uses Gini impurity [13] as a metric in classification. Let c be the number of classes and K = {1,⋯, c} be the class set. Let D be sample set on a given node, the Gini impurity is:

IG(D) = 1 − Qk∈K(pₖ)²

where pₖ is the fraction of samples in D labeled with class k. Let F be the set of available features, given any split feature j ∈ F and split value τ ∈ Domain(j), the sample set D can be split into two partitions Dₗ and Dᵣ. Then, the impurity gain of this split is as follows:

gain = IG(D) − (wₗ ⋅ IG(Dₗ) + wᵣ ⋅ IG(Dᵣ))

= wₗ Qk∈K(pl,k)² + wᵣ Qk∈K(pr,k)² − Qk∈K(pₖ)²

where wₗ = SDₗS~SDS and wᵣ = SDᵣS~SDS, and pl,k (resp. pr,k) is the fraction of samples in Dₗ (resp. Dᵣ) that are labeled with class k ∈ K. The split with the maximum impurity gain is considered the best split of the node. For regression, CART uses the label variance as a metric. Let Y be the set of labels of D, then the label variance is:

IV(D) = E(Y)² − (E(Y))² = 1/n Qi=1ⁿ(yi)² − (1/n Qi=1ⁿ(yi))²

Similar to Eqn (5), the best split is determined by maximizing the variance gain. With CART, ensemble models can be trained to obtain better predictive performance, such as random forest (RF) [12], gradient boosting decision tree (GBDT) [35, 36], XGBoost [20], etc.

# 3. SOLUTION OVERVIEW

# 3.1 System Model

We consider a set of m distributed clients (or data owners) {u₁,⋯, uₘ} who want to train a decision tree model by consolidating their respective dataset {D₁,⋯, Dₘ}. Each row in the datasets corresponds to a data sample, and each column corresponds to a feature. Let n be the number of samples and di be the number of features in Di, where i ∈ {1,⋯, m}. We denote Di = {xit}ⁿ where xit represents the t-th sample of Di.

Let Y = {yₜ}ⁿ be the set of sample labels.

**Table 1: Frequently Used Notations**
|Notation|Description|
|---|---|
|m|Number of clients|
|n|Number of samples|
|di|Number of features in dataset Di|
|Di|Dataset of client i|
|Y|Set of sample labels|

Pivot focuses on the vertical federated learning scenario [77], where the datasets {D₁,⋯, Dₘ} share the same sample ids while with different features. In particular, we assume that the clients have determined and aligned their common samples using private set intersection techniques [54, 62, 19, 63] without revealing any information about samples not in the intersection. In addition, we assume that the label set Y is held by only one client (i.e., super client) and cannot be directly shared with other clients.

# 3.2 Threat Model

We consider the semi-honest model [57, 75, 74, 23, 22, 38] where every client follows the protocol exactly as specified, but may try to infer other clients’ private information based on the messages received. Like any other client, no additional trust is assumed of the super client. We assume that an adversary A can corrupt up to m −1 clients and the adversary’s corruption strategy is static, such that the set of corrupted clients is fixed before the protocol execution and remains unchanged during the execution.

# 3.3 Problem Formulation

To protect the private data of honest clients, we require that an adversary learns nothing more than the data of the clients he has corrupted and the final output. Similar to previous work [57, 81, 23], we formalize our problem under the ideal/real paradigm. Let F be an ideal functionality such that the clients send their data to a trusted third party for computation and receive the final output from that party. Let π be a real world protocol executed by the clients. We say a real protocol π behaves indistinguishably as the ideal functionality F if the following formal definition is satisfied.

Definition 1. ([15, 25, 57]). A protocol π securely realizes an ideal functionality F if for every adversary A attacking the real interaction, there exists a simulator S attacking

# Table 1: Summary of notations

|Notation|Description|
|---|---|
|m|number of clients|
|n|number of samples in the training dataset|
|d|number of total features|
|Di|training dataset hold by client i|
|Y|label set of training dataset|
|di|number of features in Di|
|b|maximum split number for any feature|
|h|maximum tree depth|
|pk, sk|public key and secret key pair|
|[α]|encrypted mask vector for a tree node|

the ideal interaction, such that for all environments Z, the following quantity is negligible (in λ):

TPr[real(Z,A, π, λ) = 1]−Pr[ideal(Z,S,F, λ) = 1]T.◻

In this paper, we identify two ideal functionalities FDTT and FDTP for the model training and model prediction, respectively. In FDTT, the input is every client’s dataset while the output is the trained model that all clients have agreed to release. In FDTP, the input is the released model and a sample while the output is the predicted label of that sample. The output of FDTT is part of the input of FDTP. Specifically, in our basic protocol (Section 4), we assume that the output of FDTT is the plaintext tree model, including the split feature and the split threshold on each internal node, and the label for prediction on each leaf node. While in our enhanced protocol (Section 5), the released plaintext information is assumed to include only the split feature on each internal node, whereas the split threshold and the leaf label are concealed for better privacy protection.

# 4. BASIC PROTOCOL

In this section, we present our basic protocol of Pivot. The output of the model training stage is assumed to be the whole plaintext tree model. Note that prior work [71, 44, 67, 68, 69, 21, 50] is not applicable to our problem since they simplify the problem by revealing either the training labels or intermediate results in plaintext, which discloses too much information regarding the client’s private data.

To satisfy Definition 1 for vertical tree training, a straightforward solution is to directly use the MPC framework. For example, the clients can apply the additive secret sharing scheme (see Section 2.2) to convert private datasets and labels into secretly shared data, and train the model by secure computations. However, this solution incurs high communication complexity because it involves O(nd) secretly shared values and most secure computations are communication intensive. On the other hand, while TPHE could enable each client to compute encrypted split statistics at local by providing the super client’s encrypted label information, it does not support some operations (e.g., comparison), which are needed in best split determination. Based on these observations and inspired by [81], we design our basic protocol using a hybrid framework of TPHE and MPC for vertical tree training. The basic idea is that each client executes as many local computations (e.g., computing split statistics) as possible with the help of TPHE and uses MPC only when TPHE is insufficient (e.g., deciding the best split). As a consequence, most computations are executed at local and the secretly shared values involved in MPC are reduced to O(db), where b denotes the maximum number of split values for any feature and db is the number of total splits.

# 4.1 Classification Tree Training

In our training protocol, the clients use a mask vector of size n to indicate which samples are available on a tree node, but keep the vector in an encrypted form to avoid disclosing the sample set. Specifically, let α = (α₁,⋯, αₙ) be an indicator vector for a tree node. Then, for any i ∈ {1,⋯, n}, αi = 1 indicates that the i-th sample is available on the node, and αi = 0 otherwise. We use [α]= ([α₁],⋯,[αₙ]) to denote the encrypted version of α, where [⋅] represents homomorphic encrypted values (see Section 2.1).

Before the training starts, each client initializes a decision tree with only a root node, and associates the root node with an encrypted indicator vector [α] where all elements are [1] (since all samples are available on the root node). Then, the clients work together to recursively split the root node. In what follows, we will use an example to illustrate how our protocol decides the best split for a given tree node based on Gini impurity.

Consider the example in Figure 2, where we have three clients u₁, u₂, and u₃. Among them, u₁ is the super client.

# Algorithm 2: Conversion to secretly shared value

|[6&]=([1],[0],[1],[0],[0])|income label| |
|---|---|---|
|[6%]= ([0],[1],[0],[0], [1])|2500|Class1|
| |1500|Class2|
|!(superclient)|3500|Class1|
| |5000|Class1|
| |2000|Class2|

computeGini impurity by MPC

Consider a split value

|age|deposit|
|---|---|
|30|10000|
|20| |
|45|5000|
|35|30000|
|55|12000|

Figure 2: Classification tree training example

Suppose that each client computes the encrypted numbers and she owns the labels with two classes, 1 and 2. There are five training samples with three features (i.e., income, age, and deposit), and each client holds one feature. Suppose that the clients are to split a tree node whose encrypted mask vector is [α]= ([1],[1],[1],[0],[1]), i.e., Samples 1, 2, 3, and 5 are on the node. Then, u₁ computes an encrypted indicator vector for each class, based on [α] and her local labels. For example, for Class 1, u₁ derives a temporary indicator vector (1,0,1,1,0), which indicates that Samples 1, 3, and 4 belong to Class 1. Next, u₁ uses the indicator vector to perform an element-wise homomorphic multiplication with [α], which results in an encrypted indicator vector [γ₁] = ([1],[0],[1],[0],[0]). This vector indicates that Samples 1 and 3 are on the node to be split, and they belong to Class 1. Similarly, u₁ also generates an encrypted indicator vector [γ₂] for Class 2. After that, u₁ broadcasts [γ₁] and [γ₂] to all clients.

After receiving [γ₁] and [γ₂], each client combines them with her local training data to compute several statistics that are required to choose the best split of the current node. In particular, to evaluate the quality of a split based on Gini impurity (see Section 2.3), each client needs to examine the two child nodes that would result from the split, and then compute the following statistics for each child node: (i) the total number of samples that belong to the child node, and (ii) the number of samples among them that are associated with label class k, for each k ∈ K.

For example, suppose that u₃ considers a split that divides the current node based on whether the deposit values are larger than 15000. Then, u₃ first examines her local samples, and divides them into two partitions. The first partition (referred to as the left partition) consists of Samples 1, 2, and 4, i.e., the local samples whose deposit values are no more than 15000. Meanwhile, the second partition (referred to as the right partition) contains Samples 3 and 5. Accordingly, for the left (resp. right) partition, u₃ constructs an indicator vector vₗ = (1,1,0,1,0) (resp. vᵣ = (0,0,1,0,1)) to specify the samples that it contains. After that, u₃ performs a homomorphic dot product between vₗ and [γ₁] to obtain an encrypted number [gl,₁]. Observe that gl,₁ equals the exact number of Class 1 samples that belong to the left child node of the split. Similarly, u₃ uses vₗ and [γ₂] to generate [gl,₂], an encrypted version of the number of Class 2 samples that belong to the left child node. Using the same approach, u₃ also computes the encrypted numbers of Classes 1 and 2 samples associated with the right child node. Further, u₃ derives an encrypted total number of samples in the left (resp. right) child node, using a homomorphic dot product between vₗ and [α] (resp. vᵣ and [α]).

MPC computation. After the clients generate the encrypted statistics mentioned above (i.e., [gl,k], [gr,k], [nₗ], [nᵣ]), they execute an MPC protocol to identify the best split of the current node. Towards this end, the clients first invoke Algorithm 2 to convert each encrypted number [x] into a set of secret shares {⟨x⟩i}ᵐ, where ⟨x⟩i is given to uᵢ. The general idea of Algorithm 2 is from [24, 28, 81]. We use ⟨x⟩ to denote that the x is secretly shared among the clients.

After the above conversion, the clients obtain secretly shared x.

# Algorithm 3: Pivot DT training (basic protocol)

Input: {Di}m: local datasets, {Fi}m: local features

Yi=1: label set, [α]: encrypted mask vector

pk: the public key, {ski}m: partial secret keys

Output: T: decision tree model

1. if prune conditions satisfied then
2. classification: return leaf node with majority class

regression: return leaf node with mean label value

else

Using these statistics, the clients identify the best split of the current node as follows.

Consider a split τ and the two child nodes that it induces. To evaluate the Gini impurity of the left (resp. right) child node, the clients need to derive, for each class k ∈ K, the fraction pl,k (resp. pr,k) of samples on the node that are labeled with k.

Observe that

pl,k = gl,k, pr,k = gr,k.

In addition, recall that the clients have obtained, for each class k ∈ K, the secretly shared values ⟨gl,k⟩ and ⟨gr,k⟩. Therefore, the clients can jointly compute ⟨pl,k⟩ and ⟨pr,k⟩ using the secure addition and secure division operators in SPDZ (see Section 2.2), without disclosing pl,k and pr,k to any client.

With the same approach, the clients use ⟨nl⟩ and ⟨nr⟩ to securely compute ⟨wl⟩ and ⟨wr⟩, where wl = nl and wr = nr.

Given ⟨pl,k⟩, ⟨pr,k⟩, ⟨wl⟩, and ⟨wr⟩, the clients can then compute the impurity gain of each split τ (see Eqn. (5)) in secretly shared form, using the secure addition and secure multiplication operators in SPDZ.

Finally, the clients jointly determine the best split using a secure maximum computation as follows. First, each client ui assigns an identifier (i, j, s) to the s-th split on the j-th feature that she holds. Next, the clients initialize four secretly shared values ⟨gainmax⟩, ⟨i*⟩, ⟨j*⟩, ⟨s*⟩, all with ⟨−1⟩.

After that, they will compare the secretly shared impurity gains of all splits, and securely record the identifier and impurity gain of the best split in ⟨i*⟩, ⟨j*⟩, ⟨s*⟩, and ⟨gainmax⟩, respectively.

Specifically, for each split τ, the best split identifier can be decided based on Eqn. (6). The clients compare its impurity gain ⟨gainτ⟩ with ⟨gainmax⟩ using secure comparison (see Section 2.2).

Let ⟨sign⟩ be the result of the secure comparison, i.e., sign = 1 if gainτ > gainmax, and sign = 0 otherwise. Then, the clients securely update ⟨gainmax⟩ using the secretly shared values, such that gainmax = gainmax ⋅ (1 − sign) + gainτ ⋅ sign.

The best split identifier is updated in the same manner. After examining all splits, the clients obtain the secretly shared best split identifier (⟨i*⟩,⟨j*⟩,⟨s*⟩).

# Model update.

Recall that in the basic protocol, the tree model can be released in plaintext. Therefore, the clients compute the encrypted indicator vectors for the child nodes given the best split.

# 4.3 Tree Model Prediction

# Algorithm 4: Pivot DT prediction (basic protocol)

After releasing the plaintext tree model, the clients can jointly make a prediction given a sample. In vertical FL, the features of a sample are distributed among the clients. Figure 3a shows an example of a released model, where each internal node represents a feature with a split threshold owned by a client, and each leaf node represents a predicted label on that path. To predict a sample, a naive method is to let the super client coordinate the prediction process [21]: starting from the root node, the client who has the node feature compares its value with the split threshold, and notifies super client the next branch; then the prediction is forwarded to the next node until a leaf node is reached. However, this method discloses the prediction path, from which a client can infer the other client’s feature value along that path.

To ensure that no additional information other than the predicted output is leaked, we propose a distributed prediction method, as shown in Algorithm 4. Let z = (z₁,⋯, zₜ₊₁) be the leaf label vector of the leaf nodes in the tree model, where t is the number of internal nodes. Note that all clients know z since the tree model is public in this protocol. Given a sample, clients collaborate to update an encrypted prediction vector [η]= ([1],⋯,[1]) with size t+1 in a round-robin manner. Each element in [η] indicates if a prediction path is possible with encrypted form.

Without loss of generality, we assume that the prediction starts with u₁ and ends with uₘ. If a prediction path is possible from the perspective of a client, then the client multiplies the designated element in [η] by 1 using homomorphic multiplication, otherwise by 0. Figure 3b illustrates an example of this method. Starting from u₃, given the feature value ‘deposit = 6000’, u₃ initializes [η] and updates it to ([0],[1],[1],[0],[1]), since she can eliminate the first and fourth prediction paths after comparing her value with the split threshold ‘deposit = 5000’. Then, u₃ sends [η] to the next client for updates. After all clients’ updates, there is only one [1] in [η], which indicates the true prediction path. Finally, u₁ computes z⊙ [η] to get the encrypted prediction output, and decrypts it jointly with all clients.

# 4.4 Security Guarantees

Theorem 1. The basic protocol of Pivot securely realizes the ideal functionalities FDTT and FDTP against a semi-honest adversary who can statically corrupt up to m −1 out of m clients.

Proof Sketch. We need to show that, in the view of an adversary A, any information learned by the protocol can be learned directly from the input it has corrupted and the output it receives.

For model training, the proof can be reduced to the computations on one tree node because each node can be computed separately given that its output is public [47, 48]. There are two cases. First, when a given node is an internal node: (i) if the super client is corrupted, nothing is revealed in the local computation step regarding the honest client’s data; while the MPC conversion [24] and additive secret sharing scheme [28] are secure, thus, the MPC computation step is secure; finally, in the model update step, if i∗ is an honest client, the transmitted message [α] is secure for the threshold Paillier scheme [61] is secure. (ii) if the super client is not corrupted, the only difference is the transmitted encrypted label information [L], which is also secure.

Second, if a given node is a leaf node: (i) if the super client is corrupted, nothing is revealed since the honest client does not have the labels; (ii) if the super client is not corrupted, the transmitted messages are the encrypted sample number of each class (for classification) and the encrypted mean label (for regression), which are secure. Therefore, A learns no additional information from the protocol execution, the security follows.

# 5. ENHANCED PROTOCOL

The basic protocol guarantees that no intermediate information is disclosed. However, after obtaining the public model, colluding clients may extract private information of a target client’s training dataset, with the help of their own datasets. We first present two possible privacy leakages in Section 5.1 and then propose an enhanced protocol that mitigates this problem by concealing some model information in Section 5.2. The security analysis is given in Section 5.3.

# 5.1 Privacy Leakages

We identify two possible privacy leakages: the training label leakage and the feature value leakage, regarding a target client’s training dataset. The intuition behind the leakages is that the colluding clients are able to split the sample set based on the split information in the model and their own datasets. We illustrate them by the following two examples given the tree model in Figure 3.

Example 1. (Training label leakage). Assume that u₂ and u₃ collude, let us see the right branch of the root node. u₂ knows exactly the sample set in this branch, say Dage > 30, as all samples are available on the root node and he can just split his local samples based on ‘age = 30’. Then, u₃ can classify this set into two subsets given the ‘deposit=5000’ split, say Dage > 30⋀ deposit ≤ 5000 and Dage > 30 ⋀deposit > 5000, respectively. Consequently, according to the plaintext class labels on the two leaf nodes, colluding clients may infer that the samples in Dage > 30⋀deposit ≤ 5000 are with class 2 and vice versa, with high probability.

Example 2. (Feature value leakage). Assume that u₁ and u₂ collude, let us see the path of u₂ → u₁ → u₃ (with red arrows). Similar to Example 1, u₁ and u₂ can exactly know

the training sample set on the ‘u₃’ node before splitting, say D′. In addition, recall that u₁ is the super client who has all sample labels, thus, he can easily classify D′ into two sets by class, say D′1 and D′2, respectively. Consequently, the colluding clients may infer that the samples in D2 have ‘deposit ≤ 5000’ and vise versa, with high probability. Note that these two leakages happen when the clients (except the target client) along a path collude. Essentially, given the model, the colluding clients (without super client) may infer labels of some samples in the training dataset if there is no feature belongs to the super client along a tree path; similarly, if the super client involves in collusion, the feature values of some samples in the training dataset of a target client may be inferred.

# 5.2 Hiding Label and Split Threshold

Our observation is that these privacy leakages can be mitigated if the split thresholds on the internal nodes and the leaf labels on the leaf nodes in the model are concealed from all clients. Without such information, the colluding clients can neither determine how to split the sample set nor what leaf label a path owns. We now discuss how to hide these information in the model.

For the leaf label on each leaf node, the clients can convert it to an encrypted value, instead of reconstructing its plaintext. Specifically, after obtaining the secretly shared leaf label (e.g., ⟨k⟩) using secure computations (Lines 1-3 in Algorithm 3), each client encrypts her own share of ⟨k⟩ and broadcasts to all clients. Then, the encrypted leaf label can be computed by summing up these encrypted shares using homomorphic addition. As such, the leaf label is concealed.

For the split threshold on each internal node, the clients hide it by two additional computations in the model update step. Recall that in the basic protocol, the best split identifier (⟨i∗⟩,⟨j∗⟩,⟨s∗⟩) is revealed to all clients after the MPC computation in each iteration. In the enhanced protocol, we assume that ⟨s∗⟩ is not revealed, thus the split threshold can be concealed. To support the tree model update without disclosing s∗ to the i∗-th client, we first use the private information retrieval (PIR) [75, 74] technique to privately select the split indicator vectors of s∗.

# Private split selection.

Let n′ = SSijS denote the number of splits of the j∗-th feature of the i∗-th client. We assume n′ is public for simplicity. Note that the clients can further protect n′ by padding placeholders to a pre-defined threshold number. Instead of revealing ⟨s∗⟩ to the i∗-th client, the clients jointly convert ⟨s∗⟩ into an encrypted indicator vector [λ] = ([λ₁],⋯,[λₙ′])ᵀ, such that λₜ = 1 when t = s∗ and λₜ = 0 otherwise, where t ∈ {1,⋯, n}. This vector is sent to the i∗-th client for private split selection at local. Let Vn×ⁿ′ = (v₁,⋯,vₙ′) be the split indicator matrix, where vₜ is the split indicator vector of the t-th split of the j∗-th feature (see Section 4.1). The following theorem [74] suggests that the i∗-th client can compute the encrypted indicator vector for the s∗-th split without disclosing s∗.

# Theorem 2.

Given an encrypted indicator vector [λ] = ([λ1],⋯,[λn])T such that [λs∗] = [1] and [λt] = [0] for all t ≠ s∗, and the indicator matrix Vs∗ = (v₁,⋯,vₙ′), then [vs∗] = V@[λ]. ◻

The notion @ represents the homomorphic matrix multiplication, which executes homomorphic dot product operations between each row in V and [λ]. We refer the interested readers to [74] for the details. For simplicity, we denote the selected [vs∗] as [v]. The encrypted split threshold can also be obtained by homomorphic dot product between the encrypted indicator vector [λ] and the plaintext split value vector of the j∗-th feature.

# Encrypted mask vector updating.

After finding the encrypted split vector [v], we need to update the encrypted mask vector [α] for protecting the sample set recursively. This requires element-wise multiplication between [α] and [v]. Thanks to the MPC conversion algorithm, we can compute [α] ⋅ [v] as follows [24]. For each element pair [αj] and [vj] where j ∈ [1, n], we first convert [αj] into ⟨αj⟩ = (⟨αj⟩₁,⋯,⟨αj⟩ₘ) using Algorithm 2, where ⟨αj⟩i(i ∈ {1,⋯, m}) is the share hold by ui; then each client ui executes homomorphic multiplication ⟨αj⟩i ⊗ [vj] = [⟨αj⟩i ⋅ vj] and sends the result to the i∗-th client; finally, the i∗-th client can sum up the results using homomorphic addition:

[α′] = [⟨αj⟩ ⋅ vj]⊕ ⋯ ⊕ [⟨αj⟩ ⋅ vj]= [αj ⋅ vj]

After updating [α], the tree can also be built recursively, similar to the basic protocol.

# Secret sharing based model prediction.

The prediction method in the basic protocol is not applicable here as the clients cannot directly compare their feature values with the encrypted split thresholds. Hence, the clients first convert the encrypted split thresholds and encrypted leaf labels into secretly shared form and make predictions on the secretly shared model using MPC. Let ⟨z⟩ with size (t +1) denote the secretly shared leaf label vector, where t is the number of internal nodes.

To make the prediction given a sample, the clients also provide the distributed feature values in secretly shared form. Similar to the prediction in the basic protocol, the clients initialize a secretly shared prediction vector ⟨η⟩ with size (t + 1), indicating if a prediction path is possible. Then, they compute this vector as follows.

The clients initialize a secretly shared marker ⟨1⟩ for the root node. Starting from root node, the clients recursively compute the markers of its child nodes until all leaf nodes are reached. Then, the marker of each leaf node is assigned to the corresponding position in ⟨η⟩, and there is only one ⟨1⟩ element in ⟨η⟩, specifying the real prediction path in a secret manner. Specifically, each marker is computed by secure multiplication between its parent node’s marker and a secure comparison result (between the secretly shared feature value and split threshold on this node). For example, in Figure 3a, the split threshold on the root node will be ⟨30⟩ while the feature value will be ⟨25⟩, then ⟨1⟩ is assigned to its left child and ⟨0⟩ to its right child. The clients know nothing about the assigned markers due to the computations are secure. Finally, the secretly shared prediction output can be computed easily by a dot product between ⟨z⟩ and ⟨η⟩, using secure computations.

# Discussion.

A noteworthy aspect is that the clients can also choose to hide the feature ⟨j∗⟩ by defining n′ as the total number of splits on the i∗-th client, or even the client ⟨i∗⟩ that has the best feature by defining n′ as the total number of splits among all clients. By doing so, the leakages could be further alleviated. However, the efficiency and interpretability would be degraded greatly. In fact, there is a trade-off between privacy and efficiency (interpretability).

# Table 2: Theoretical analysis

| |Pivot basic protocol|Pivot enhanced protocol|
|---|---|---|
|Model training|O(ncdbt)Ce + O(cdbt)(Cd + Cs) + O(dbt)Cc|O(ncdbt)Ce + O(cdbt + nt)Cd + O(cdbt)Cs + O(dbt)Cc|
|– local computation|O(ncdb)Ce|O(ncdb)Ce|
|– mpc computation|O(cdb)(Cd + Cs) + O(db)Cc|O(cdb)(Cd + Cs) + O(db)Cc|
|– model update|O(n)Ce|O(nb)Ce + O(n)Cd|
|Model prediction|O(mt)Ce + O(1)Cd|O(t)(Cs + Cc)|

for the released model. The less information the model reveals, the higher privacy while the lower efficiency and less interpretability the clients obtain, and vise versa.

# 5.3 Security Guarantees

Theorem 3. The enhanced protocol of Pivot securely realizes the ideal functionalities FDTT and FDTP against a semi-honest adversary who can statically corrupt up to m−1 out of m clients.

Proof Sketch. For model training, the only difference from the basic protocol is the two additional computations (private split selection and encrypted mask vector updating) in the model update step, which are computed using threshold Paillier scheme and MPC conversion. Thus, the security follows. For model prediction, since the additive secret sharing scheme is secure and the clients compute a secretly shared marker for every possible path, the adversary learns nothing except the final prediction output. ◻

# 6. THEORETICAL ANALYSIS

We theoretically analyze the Pivot basic protocol and Pivot enhanced protocol in terms of computational cost for model training and model prediction, as summarized in Table 2. Let Ce and Cs roughly denote the costs for computations on a homomorphic encrypted value and on a secretly shared value, respectively. Due to that the threshold decryption (involving decryption of each client and combination via network communication) and secure comparison (involving multi-round network communications among the clients) are more time-consuming than the other computations, we consider these two operations separately for better analysis, and denote the costs of them by Cd and Cc, respectively. Let d = max({di}i=1) be the maximum number of features any client holds, b be the maximum number of splits any feature has, and c be the number of classes.

# Model training.

With the basic protocol, the computational cost of a client in each iteration includes: (i) local computation step: the encrypted label vectors computed by the super client, i.e., O(nc)Ce, and the encrypted statistics computed by the clients, i.e., O(nc¯ db)Ce, where db is the number of local splits; (ii) MPC computation step: the MPC conversion for encrypted statistics of total splits, i.e., O(cdb)Cd, and the best split determined using O(cdb) statistics, i.e., O(cdb)Cs + O(db)Cc, where db is the number of total splits; and (iii) model update step: the update of encrypted mask vectors, i.e., O(n)Ce. Thus, the total cost is O(nc¯ dbt)Ce + O(cdbt)(Cd + Cs) + O(dbt)Cc, where t is the number of internal nodes in the tree model. With the enhanced protocol, the only difference is the two additional computations in the model update step: private split selection on b split indicator vectors, i.e., O(nb)Ce, and encrypted mask vector update that mainly requires n threshold decryption operations, i.e., O(n)Cd. Thus, the total cost is O(nc¯ dbt)Ce + O(cdbt + nt)Cd + O(cdbt)Cs + O(dbt)Cc.

# 7. EXTENSIONS TO OTHER ML MODELS

So far, Pivot supports a single tree model. Now we briefly present how to extend the basic protocol to ensemble tree models, including random forest (RF) [12] and gradient boosting decision tree (GBDT) [35, 36] in Section 7.1 and Section 7.2, respectively. Same as the basic protocol, we assume that all the trees can be released in plaintext. The extension to other machine learning models is discussed in Section 7.3.

# 7.1 Random Forest

RF constructs a set of independent decision trees in the training stage and outputs the class that is the mode of the classes (for classification) or mean prediction (for regression) of those trees in the prediction stage.

For model training, the extension from a single decision tree is natural since each tree can be built (using Algorithm 3) and released separately. For model prediction, after obtaining the encrypted predicted label of each tree, the clients can easily convert these encrypted labels into secret shares for majority voting using secure maximum computation (for classification) or compute the encrypted mean prediction by homomorphic computations (for regression).

# 7.2 Gradient Boosting Decision Trees

GBDT uses decision trees as weak learners and improves model quality with a boosting strategy [34]. The trees are built sequentially where the training labels for the next tree are the prediction losses between the ground truth labels and the prediction outputs of previous trees.

Model training. The extension to GBDT is non-trivial, since we need to prevent the super client from knowing the training labels of each tree except the first tree (i.e., intermediate information) while facilitating the training process.

We first consider GBDT regression. Let W be the number of rounds and a regression tree is built in each round. Let Yw be the training label vector of the w-th tree. We aim to protect Yw by keeping it in an encrypted form. After building the w-th tree where w ∈ {1,⋯, W −1}, the clients jointly make predictions for all training samples to get an encrypted estimation vector ¯Yw; then the clients can compute the encrypted training labels [Yw+1] of the (w +1)-th tree given [Yw] and ¯Yw. Besides, note that in Section 4.2, an encrypted label square vector [γw+1] is needed, which is computed by element-wise homomorphic multiplication between βw+1 and [α]. However, βw+1 is not plaintext here since the training labels are ciphertexts. Thus, the clients need expensive element-wise ciphertext multiplications (see Section 5.2) between [βw+1] and [α] in each iteration. To optimize this computation, we slightly modify our basic protocol. Instead of letting the super client compute [γw+1] in each iteration, we now let the client who has the best split update [γw+1] along with [α] using the same split indicator vector and broadcast them to all clients. In this way, the clients only need to compute [γw+1] using [βw+1] and [α] once at the beginning of each round, which reduces the cost.

For GBDT classification, we use the one-vs-the-rest technique by combining a set of binary classifiers. Essentially, the clients need to build a GBDT regression forest for each class, resulting in W ∗ c regression trees in total (c is the number of classes). After each round in the training stage, the clients obtain c trees; and for each training sample, the clients make a prediction on each tree, resulting in c encrypted prediction outputs. Then, the clients jointly convert them into secretly shared values for computing secure softmax (which can be constructed using secure exponential, secure addition, and secure division, as mentioned in Section 2.2), and convert them back into an encrypted form as encrypted estimations. The rest of the computation is the same as regression.

# 7. Model prediction.

For GBDT regression, the prediction output can be decrypted after homomorphic aggregating the encrypted predictions of all trees. For GBDT classification, the encrypted prediction for each class is the same as that for regression; then the clients jointly convert these encrypted results into secretly shared values for deciding the final prediction output by secure softmax function.

# 7.3 Other Machine Learning Models

Though we consider tree-based models in this paper, the proposed solution can be easily adopted in other vertical FL models, such as logistic regression (LR), neural networks, and so on. The rationale is that these models can often be partitioned into the three steps described in Section 4.1. As a result, the TPHE primitives, conversion algorithm, and secure computation operators can be re-used.

For example, the clients can train a vertical LR model as follows. To protect the intermediate weights of the LR model during the training, the clients initialize an encrypted weight vector, [θ] = ([θ1],⋯,[θm]), where [θi] corresponds to the encrypted weights of features held by client i. In each iteration, for a Sample t, each client i first locally aggregates an encrypted partial sum, say [ξit], by homomorphic dot product between [θi] and Sample t’s local features xit. Then the clients jointly convert {[ξit]}mi=1 into secretly shared values using Algorithm 2, and securely aggregate them before computing the secure logistic function. Meanwhile, the super client also provides Sample t’s label as a secretly shared value, such that the clients can jointly compute the secretly shared loss of Sample t. After that, the clients convert the loss back into the encrypted form (see Section 5.2), and each client can update her encrypted weights [θi] using homomorphic properties, without knowing the loss. Besides, the model prediction is a half component of one iteration in training, which can be easily computed.

# 8. EXPERIMENTS

We evaluate the performance of Pivot basic protocol (Section 4) and Pivot enhanced protocol (Section 5) on the decision tree model, as well as the ensemble extensions (Section 7). We present the accuracy evaluation in Section 8.2 and the efficiency evaluation in Section 8.3. We implement Pivot in C++ and employ the GMP2 library for big integer computation and the libhcs3 library for operations of the threshold Paillier scheme. We utilize the SPDZ4 library for semi-honest additive secret sharing computations. Besides, we apply the libscapi5 library to provide network communications among clients. Since the cryptographic primitives only support big integer computations, we convert the floating point datasets into fixed-point integer representation.

# 8.1 Experimental Setup

We conduct experiments on a cluster of machines in a local area network (LAN). Each machine is equipped with Intel (R) Xeon (R) CPU E5-1650 v3 @ 3.50GHz×12 and 32GB of RAM, running Ubuntu 16.04 LTS. Unless noted otherwise, the keysize of threshold Paillier scheme is 1024 bits and the security parameter of SPDZ configuration is 128 bits.

# Datasets.

We evaluate the model accuracy using three real-world datasets: credit card data (30000 samples with 25 features) [79], bank marketing data (4521 samples with 17 features) [58], and appliances energy prediction data (19735 samples with 29 features) [14]. The former two datasets are for classification while the third dataset is for regression.

We evaluate the efficiency using synthetic datasets, which are generated with sklearn6 library. Specifically, we vary the number of samples (n) and the number of total features (d) to generate datasets and then equally split these datasets w.r.t. features into m partitions, which are held by m clients, respectively. We denote d̄ = d/m as the number of features each client holds. For classification tasks, the number of classes is set to 4, and only one client holds the labels.

# Baselines.

For accuracy evaluation, we adopt the non-private decision tree (NP-DT), non-private random forest (NP-RF), and non-private gradient-boosting decision tree (NP-GBDT) algorithms from sklearn for comparison. For a fair comparison, we adopt the same hyper-parameters for both our protocols and the baselines, e.g., the maximum tree depth, the pruning conditions, the number of trees, etc.

For efficiency evaluation, to our knowledge, there is no existing work providing the same privacy guarantee as Pivot. Therefore, we implement a secret sharing based decision tree.

# Table 3: Model accuracy comparison with non-private baselines

|Dataset|Pivot-DT|NP-DT|Pivot-RF|NP-RF|Pivot-GBDT|NP-GBDT|
|---|---|---|---|---|---|---|
|Bank market|0.886077|0.886188|0.888619|0.890497|0.891271|0.892044|
|Credit card|0.821526|0.821533|0.823056|0.823667|0.825167|0.827167|
|Appliances energy|212.05281|211.45229|211.55175|211.32113|211.35326|210.75291|

algorithm using the SPDZ library (namely, SPDZ-DT) as a baseline. The security parameter of SPDZ-DT is also 128 bits. Besides, we also implement a non-private distributed decision tree (NPD-DT) algorithm as another baseline to illustrate the overhead of protecting the data privacy. In NPD-DT, the super client broadcasts plaintext labels to all clients, each client computes split statistics and exchanges them in plaintext with others to decide the best split.

# Metrics

For model accuracy, we measure the number of samples that are correctly classified over the total testing samples for classification; and the mean square error (MSE) between the predicted labels and the ground truth labels for regression. For efficiency, we measure the total running time of the model training stage and the prediction running time per sample of the model prediction stage. In all experiments, we report the running time of the online phase because SPDZ did not support the offline time benchmark.

# 8.3 Evaluation of Accuracy

In terms of accuracy, we compare the performance of the proposed decision tree (Pivot-DT), random forest (Pivot-RF) and gradient boosting decision tree (Pivot-GBDT) algorithms with their non-private baselines on three real world datasets. In these experiments, the keysize of threshold Paillier scheme is set to 512 bits. We conduct 10 independent trials of each experiment and report the average result. Table 3 summarizes the comparison results. We can notice that the Pivot algorithms achieve accuracy comparable to the non-private baselines. There are two reasons for the slight loss of accuracy. First, we use the fixed-point integer to represent float values, whose precision is thus truncated. Second, Pivot has only implemented the basic algorithms, which are not optimized as the adopted baselines.

# 8.3.1 Evaluation on Training Efficiency

Varying m. Figure 4a shows the performance for varying m. The training time of all algorithms increases as m increases because the threshold decryptions and secure computations need more communication rounds. Pivot-Basic always performs better than Pivot-Enhanced since Pivot-Enhanced has two additional computations in the model update step, where the O(n) ciphertexts multiplications dominate the cost. Besides, we can see that Pivot-Enhanced-PP that parallelizes only the threshold decryptions could reduce the total training time by up to 2.7 times.

Varying n. Figure 4b shows the performance for varying n. The relative comparison of the Pivot-Basic and Pivot-Enhanced is similar to Figure 4a, except that the training time of Pivot-Basic increases slightly when n goes up. The reason is that, in Pivot-Basic, the cost of encrypted statistics computation (proportional to O(n)) is only a small part of the total training time; the time-consuming parts are the MPC conversion that requires O(cdb) threshold decryptions. The training time of Pivot-Enhanced scales linearly with n because of the threshold decryptions for encrypted mask vector updating are proportional to O(n). For example, when n = 200K, the training time of Pivot-Enhanced is about 12 hours while that of Pivot-Basic is only 35 minutes.

Varying d, b. Figure 4c-4d show the performance for varying d and b, respectively. The trends of the four algorithms in these two experiments are similar, i.e., the training time all scales linearly with d or b since the number of total splits is O(db). In addition, the gap between Pivot-Basic and Pivot-Enhanced is stable as d or b increases. This is because that d does not affect the additional costs in Pivot-Enhanced, and b only has small impact via private split selection (i.e., O(nb) ciphertext computations) which is negligible comparing to the encrypted mask vector updating computation.

Varying h. Figure 4e shows the performance for varying h. Since the generated synthetic datasets are sampled uniformly, the trained models tend to construct a full binary tree, where the number of internal nodes is 2ʰ −1 given the maximum tree depth h. Therefore, the training time of all algorithms approximately double when h increases by one.

Varying W. Figure 4f shows the performance for varying W in ensemble methods. RF classification is slightly slower than RF regression as the default c is 4 in classification comparing to 2 in regression. GBDT regression is slightly slower than RF regression, since additional computations are required by GBDT to protect intermediate training labels. Besides, the training time of GBDT classification is much longer than GBDT regression for two overheads: one is the one-vs-the-rest strategy, which means W∗c trees are trained; the other is the secure softmax computation on c encrypted.

# Table 4: Parameters adopted in the evaluation

|Parameter|Description|Range|Default|
|---|---|---|---|
|m|number of clients|[2,10]|3|
|n|number of samples|[5K,200K]|50K|
|d|number of features|[5,120]|15|
|b|maximum splits|[2,32]|8|
|h|maximum tree depth|[2,6]|4|
|W|number of trees|[2,32]|-|

# 8.3.2 Evaluation on Prediction Efficiency

Varying m. Figure 4g compares the prediction time per sample for varying m. Results show that the prediction time of Pivot-Enhanced is higher than Pivot-Basic, because the cost of secure comparisons is higher than the homomorphic computations. Besides, the prediction time of Pivot-Basic increases faster than that of Pivot-Enhanced as m increases. The reason is that the communication round for distributed prediction in Pivot-Basic scales linearly with m; while in Pivot-Enhanced, the number of secure comparisons remains the same, the increasing of m only incurs slight overhead.

We can notice that Pivot-Basic and Pivot-Enhanced can achieve up to about 19.8x and 4.5x speedup over SPDZ-DT, respectively.

# Varying n.

Figure 5b shows the comparison for varying n. Both Pivot-Enhanced and SPDZ-DT scale linearly to n and SPDZ-DT increases more quickly than Pivot-Enhanced. When n is small (e.g., n = 5K), the three algorithms achieve almost the same performance. While when n = 200K, Pivot-Basic and Pivot-Enhanced are able to achieve about 37.5x and 1.8x speedup over SPDZ-DT.

# Varying h.

Figure 4h compares the prediction time per sample for varying h. When h = 2, Pivot-Enhanced takes less prediction time because the number of internal nodes (i.e. secure comparisons) is very small. Pivot-Basic outperforms Pivot-Enhanced when h ≥ 3 and this advantage increases as h increases for two reasons. Firstly, the number of internal nodes is proportional to 2 − 1. Secondly, as described in Figure 4g, the number of clients dominates the prediction time of Pivot-Basic; although the size of the prediction vector also scales to h, its effect is insignificant since the size is still very small, leading to stable performance.

# 9. FURTHER PROTECTIONS

This section extends Pivot to account for malicious adversaries (in Section 9.1), and to incorporate differential privacy for enhanced protection (in Section 9.2).

# 9.1 Extension to Malicious Model

We demonstrate how to extend Pivot to account for malicious adversaries. Recall that we assumed a semi-honest adversary in Pivot, which means the clients do the executions correctly. In the malicious model, an adversary may deviate from the specified protocol to infer the private data. For example, in Algorithm 2, if u₁ only adds its own encrypted share [r₁] to compute [e] (line 4), then it can infer the private data x after the threshold decryption. To prevent such malicious behaviors, we let each client prove that it executes the specified protocol on the correct data (i.e., the data a client promises to use) step by step. Once a client deviates from the protocol or uses incorrect data in any step, the other clients will detect it and abort the execution.

For this purpose, we extend Pivot using zero-knowledge proofs (ZKP) and authenticated shares in SPDZ, inspired by [81]. We first present some building blocks in Section 9.1.1, then we introduce the extension to the basic protocol and the enhanced protocol in Section 9.1.2 and Section 9.1.3, respectively.

|3000|Pivot-Basic|1600|Pivot-Basic|
|---|---|---|---|
| |Pivot-Enhanced| |Pivot-Enhanced|
|2000|SPDZ-DT|1200|SPDZ-DT|
| |NPD-DT| |NPD-DT|
|1000| |800| |
|400| | | |

Training Time (min)
(a) Training time vs. m

(b) Training time vs. n

Figure 5: Comparison with baselines

# 9.1.1 Building Blocks

Zero-knowledge proofs (ZKP). We use ZKP [24, 11, 26, 81] to ensure that each client performs the local computation correctly, even if up to m − 1 clients collude maliciously. Generally, ZKP enables a prover to prove to a verifier that a certain statement is true, without conveying any secret information for the statement. We mainly use the following existing building blocks of Σ-protocol for ZKP.

- Proof of plaintext knowledge (POPK): it takes a ciphertext [a] as input and proves that the prover knows the plaintext a∗ such that a∗ = Dec([a]) [24].
- Proof of plaintext-ciphertext multiplication (POPCM): it takes three ciphertexts [a],[b],[c] as input and proves that the prover knows the plaintext a∗ such that a∗ = Dec([a]) and Dec([c]) = Dec([a]) ⋅Dec([b]) [24].
- Proof of homomorphic dot product (POHDP): it takes two ciphertext vectors [a],[b] and a ciphertext [c] as inputs and proves that the prover knows the plaintext vector a∗ such that a∗ = Dec([a]) and Dec([c]) = Dec([a]) ⊙ Dec([b]) [81].

Note that the interactive Σ-protocol with honest verifier can be transformed into efficient non-interactive zero-knowledge (with random oracle assumption) and full zero-knowledge using existing techniques [31, 37, 81].

SPDZ authenticated shares. SPDZ can ensure malicious security even up to m − 1 clients may deviate arbitrarily from the protocol using the information-theoretic message authentication code (MAC) [28, 46]. The secure computation building blocks described in Section 2.2 are supported accordingly. In SPDZ, given a value a ∈ Zq, its authenticated secretly shared value is represented by ⟨a⟩ = (⟨a⟩₁,⋯,⟨a⟩ₘ,⟨δ⟩₁,⋯,⟨δ⟩ₘ,⟨∆⟩₁,⋯,⟨∆⟩ₘ), such that client i holds the random share ⟨a⟩ᵢ, the random MAC share ⟨δ⟩ᵢ and the fixed MAC key share ⟨∆⟩ᵢ, and the MAC relation δ = a⋅∆ holds. The MAC-related shares ensure that no client can modify ⟨a⟩ᵢ without being detected. When reconstructing a secretly shared value ⟨a⟩, every client i ∈ {1,⋯, m} first broadcast their shares ⟨a⟩ᵢ and compute a = ∑ᵐᵢ=1 ⟨a⟩ᵢ. To ensure that a is correct, every client then checks the MAC by computing and opening ⟨δ⟩ᵢ−a⋅⟨∆⟩ᵢ, then checking these shares sum up to zero. If the MAC is incorrect, then the malicious behavior can be detected.

Modified MPC conversion. Since Pivot applies a hybrid framework of TPHE and MPC, we need to modify Algorithm 1 as follows to make the MPC conversion process satisfy malicious security [24]. Specifically, we further let each client i: (i) broadcast [ri] together with POPK (line 3), ensuring that client i knows ri; (ii) compute [e] and call threshold decryption (line 4), ensuring that every client have computed the same e; and (iii) broadcast [xi] together with POPK (lines 6-8) for committing its own share. Then, the verifier can easily compute [e −r₁] (if i = 1) or [−rᵢ] (if i ≠ 1) using homomorphic properties (since both [e] and [ri] are known to all), and check if it matches [xi] using a secure equality protocol under malicious model (e.g., [45]).

# 9.1.2 Basic Protocol Extension

We first discuss the extension of the classification tree training (Section 4.1) and then the model prediction (Section 4.3) of the basic protocol. The extension to the regression tree training follows the same way.

# Classification tree training.

Before training, each client commits its local training data by encrypting and broadcasting it to other clients, which will be used for ZKP verification during the whole training process. The committed data includes the pre-computed split indicator vectors vₗ and vᵣ for each local split, and the label indicator vector βₖ of each class k ∈ K that is only committed by the super client. Each client uses POPK to prove that it knows the plaintext of the committed data (e.g., [vₗ], [vᵣ], [βₖ]). Besides, the super client initializes an encrypted mask vector [α] with [1] and broadcasts it. It can be easily verified by threshold decryption since the initial α is public.

# Local computation.

In this step, the super client first computes and broadcasts a set of encrypted indicator vectors [L] = ⋃ₖ∈K{[γₖ]} by element-wise homomorphic multiplication on βₖ using [α] for k ∈ K. Note that [βₖ] has been committed by the super client and [α] is also known to all, the super client can use POPCM to prove that she executes the homomorphic multiplication correctly. After that, each client computes 2c+2 encrypted statistics for each local split, where the only operation is the homomorphic dot product computation, e.g., [nₗ] = vₗ ⊙ [z]. Therefore, each client can broadcast these encrypted statistics and prove that she performs these computations correctly using POHDP.

# MPC computation.

After utilizing the modified MPC conversion algorithm, the clients obtain random shares for the encrypted statistics. To ensure that the random shares are not modified before combining with the MAC shares and computing with SPDZ, the clients also need to verify that these shares (along with the MAC shares) are valid and indeed match with the converted encrypted values [81]. The rest of SPDZ computations are malicious secure as the authenticated secret sharing scheme is malicious secure, and the best split identifier ⟨i∗⟩,⟨j∗⟩,⟨s∗⟩ can be found and revealed to all clients.

# Model update.

In this step, client i∗ first selects the corresponding split indicator vectors vₗ and vᵣ of the s∗-th split of the j∗-th feature, and computes [αₗ] and [αᵣ] by element-wise homomorphic multiplication using [α], which can be proved by POPCM. Note that the other clients can select the corresponding [vₗ] and [vᵣ] for the verification given the best split identifier, as they have been committed beforehand.

Similarly, the pruning condition check and leaf label computation can be proved. For example, if any pruning condition is satisfied, the super client can first compute and broadcast the encrypted number of samples for each class k (say [gₖ]) by summing up all elements in [γₖ], where k ∈ K. Note that the verifier can execute the same computations (since [γₖ] is known to all) and use the secure equality protocol.

tocol (e.g., [45]) to verify the correctness. Then the clients convert them into shares, i.e., ⟨gₖ⟩, and find the leaf label ⟨k⟩ that has the maximum ⟨gₖ⟩ using the secure maximum operation (see Section 4.1). The correctness can be ensured by the modified MPC conversion algorithm and the MAC-based SPDZ scheme. Finally, the leaf label can be revealed.

The above constructions guarantee that each client correctly follows the specified training protocol and uses the same data (i.e., split indicator vectors, and label indicator vectors, as committed before training) during the whole process. If the verification of any computation is incorrect, the execution will be aborted.

# Model prediction.

To ensure malicious security in the model prediction process, each client also needs to prove that she executes the specified computations using the correct data. Similar to the model training stage, each client can commit her data by encrypting and broadcasting the indicator of each sample’s value comparing to the corresponding split threshold in the tree model. For example, the clients can commit their testing data for prediction along with the training data using the split indicator vectors; then a verifier can retrieve the other clients’ committed indicators (for both left branch and right branch) given the sample index, client index, feature index and split index.

As described in Algorithm 4, the clients execute the prediction process in a round-robin manner. At first, client m initializes an encrypted prediction vector [η] = ([1],⋯,[1]) with size t +1 and broadcasts it to the other clients, where t is the number of internal nodes. The clients can jointly decrypt [η] to check the correctness since the elements in this vector are known to all at the beginning. Then client m updates [η] using her local sample indicators. For example, in Figure 3, there is one feature in the tree model that belongs to u₃, and the indicators are 0 (for the left branch) and 1 (for the right branch). For any tree node in the model, the client updates the corresponding leaf indexes in [η] using the indicators. For example, u₃ updates the first and fourth elements by homomorphic multiplication using 0 while the second and fifth elements by homomorphic multiplication using 1. For any update, the client broadcasts the updated [η] together with POPCM such that the other clients can verify the correctness. After [η] is updated by the last client (i.e., u₁ in Algorithm 4), each client can homomorphicly aggregate [η] and call threshold decryption to check if the sum is 1, ensuring that there is only one valid prediction path. Consequently, each client computes the homomorphic dot product operation between [η] and the leaf label vector z and calls threshold decryption to get the prediction output.

# 9.1.3 Enhanced Protocol Extension

# Model training.

The commitment and the local computation step are exactly the same as those of the basic protocol. In the MPC computation step, instead of revealing ⟨s∗⟩, the clients compute a secretly shared indicator vector ⟨λ⟩ = (⟨λ₁⟩,⋯,⟨λₙ′⟩) using SPDZ, where λₜ = 1 when t = s∗ and λₜ = 0 otherwise. This step is malicious secure since SPDZ is malicious secure. Then, for each value in ⟨λ⟩, the clients convert it into an encrypted value, by encrypting and broadcasting each share, and homomorphicly aggregating them together. The clients also need to verify that each encrypted value indeed match with the converted secretly shared value [81].

# Algorithm 5: Randomly sample a secretly shared value from Laplace distribution

Input: μ: location parameter, b: scale parameter, Output: ⟨X⟩: secretly shared value

1. ⟨U⟩ ← sample a uniformly random secretly shared value within (−1, 1) using SPDZ
2. initialize secretly shared values ⟨Us⟩ and ⟨Ua⟩
3. if ⟨U⟩ >⟨0⟩ then
4. ⟨Us⟩ =⟨1⟩,  ⟨Ua⟩ = ⟨U⟩
5. else if ⟨U⟩ = ⟨0⟩ then
6. ⟨Us⟩ =⟨0⟩,  ⟨Ua⟩ = ⟨0⟩
7. else
8. ⟨X⟩⟨Us⟩ =⟨−1⟩, ⟨Ua⟩ = ⟨−U⟩
9. = μ −b⋅⟨Us⟩⋅ ln(1− 2⋅ ⟨Ua⟩) ~~ compute using SPDZ
10. return ⟨X⟩

# Model update step.

In the model update step, client i∗ first computes a private split selection operation using [λ] and V n×n′, where V is the split indicator matrix for all the splits of the j∗ feature and has been committed before training. The private split selection actually executes n homomorphic dot product operations, which can be proved using POHDP. After that, client i∗ executes an element-wise ciphertext multiplication between the selected encrypted split indicator vector [v] and the encrypted mask vector [α] (see Section 5.2). The correctness can be ensured by the modified MPC conversion algorithm, the homomorphic multiplication together with POPCM, and the conversion from secretly shared value to ciphertext (as discussed above). Similarly, after obtaining the secretly shared leaf label, the clients can jointly convert it into ciphertext, instead of revealing it. Therefore, the model training satisfies malicious security.

# Model prediction.

Recall that in the enhanced protocol, the clients first convert the tree model (with an encrypted split threshold on each internal node and encrypted leaf label on each leaf node) into secretly shared tree model, as well as convert their input feature values into secretly shared form. The conversion can be performed by the modified MPC conversion algorithm together with additional verification of the SPDZ authenticated shares (as discussed in Section 9.1.1). After that, the rest of the computations can be executed using malicious secure SPDZ, and the prediction output can be obtained.

# 9.2 Incorporating Differential Privacy

We can incorporate differential privacy (DP) [30, 42, 66, 22, 72, 41] to provide further protection, ensuring that the released model (even in the plaintext form) leaks limited information about individual’s private data in the training dataset. In a nutshell, a computation is differentially private if the probability of producing a given output does not depend very much on whether a particular sample is included in the input dataset [30, 66]. Formally, for any two datasets D and D′ differing in a single sample and any output O of a function f,

Pr[f (D) ∈ O] ≤ e ⋅ Pr[f(D′) ∈ O] (11)

The parameter is the differential privacy budget that controls the tradeoff between the accuracy of f and how much information it discloses.

In our case, f trains a CART tree model with multiple

iterations where a tree node is built in each iteration. In the

# Algorithm 6: Randomly select a secretly shared index using exponential mechanism

Input:   {⟨score1⟩, ⋯,⟨scoreR⟩}: secretly shared scores

: differential privacy budget

∆: score function sensitivity

Output:  ⟨index⟩: secretly shared index

1  for r ∈ [1, R] do  ⋅⟨score ⟩
2    ⟨probr ⟩ = exp      2∆ r  ~~ compute secretly shared probability for each score
3  ⟨P⟩ = ∑R ⟨prob ⟩
⟨F ⟩ =  r=1   r
4   0   ⟨0⟩
5  for r ∈ [1, R] do
6    ⟨prob′ ⟩ = ⟨probʳ⟩  ~~ normalize the secretly shared probabilities such that their sum is ⟨1⟩
⟨Fr ⟩ = ⟨Fr−1⟩ + ⟨prob′ ⟩ ~~ cumulative probability for scores {1,⋯, r}     r
7  (⟨0⟩, ⟨F1⟩), (⟨F1⟩, ⟨F2⟩),⋯, (⟨FR−1⟩, ⟨1⟩) ← arrange R secretly shared sub-intervals within (⟨0⟩,⟨1⟩)
⟨U⟩ ← sample a uniformly random secretly shared value
8   within (0,1) using SPDZ
9  ⟨index⟩ =⟨−1⟩
10  for r ∈ [1, R] do
11    if ⟨U⟩ > ⟨Fr−1⟩ ∧ ⟨U⟩ ≤ ⟨Fr ⟩ then
12          ⟨index⟩ = ⟨r⟩
13    else
14          ⟨index⟩ = ⟨index⟩
15  return ⟨index⟩

after computing the impurity gain for all possible splits, the clients obtain a set of secretly shared impurity gains, which can be viewed as scores of the splits. The clients can use Algorithm 6 to decide the best split while satisfying DP.

We assign a DP budget to each query. First, the clients can compute [¯n] by homomorphicly aggregating [α] and convert it to secretly shared ⟨n¯⟩. Before checking the condition, the clients jointly add a secretly shared random noise ⟨Lap(∆~)⟩ to ⟨¯n⟩ according to the Laplace mechanism [30], where ∆ is the sensitivity of the query, denoting the largest possible difference that one sample can have on the output of the query. Here ∆ = 1 since the count query can affect the output by maximum 1. Note that the random noise can be easily generated in an MPC way since the required primitives are all supported in SPDZ, such that no client knows the plaintext noise. Algorithm 5 describes how to sample a secretly shared value from Laplace distribution using SPDZ.

There are two steps: (i) uniformly samples a secretly shared value ⟨U⟩ within (−¹, 1) (line 1), the primitive is also supported in SPDZ [4, 28]; and (ii) computes the secretly shared value ⟨X⟩ = μ −b ⋅ sgn(⟨U⟩)ln(1 −2⋅ S⟨U⟩S) (line 2-9), where μ and b are the location parameter and scale parameter of the Laplace distribution. According to the inverse transform sampling [29, 70, 2], the result ⟨X⟩ follows the Laplace distribution with parameters μ and b. In our case, μ = 0 and b = ∆. Consequently, the clients obtain the desired secretly shared random noise to be added on ⟨¯n⟩, and no one learns the plaintext noise.

Second, if the condition is not satisfied, the clients compute the best split as described in Section 4.1, then the clients can jointly use the exponential mechanism [30] to choose the best split where the sensitivity of the Gini impurity gain is ∆ = 2 [33]. Algorithm 6 describes the random selection using SPDZ based on the exponential mechanism, where the inputs are a number of R secretly shared scores, the differential privacy budget, and the sensitivity of the score function. The clients first compute the secretly shared probabilities according to the exponential mechanism (line 1-2). Next, the clients normalize these probabilities such that the sum is ⟨1⟩; meanwhile, the clients compute the secretly shared cumulative probability for each index (line 3-7), which will be used for randomly sampling from discrete distribution (as what does in the exponential mechanism).

A noteworthy aspect is that, since both the Laplace noise generation (Algorithm 5) and the random selection using exponential mechanism (Algorithm 6) are computed using SPDZ, we can easily incorporate DP into the malicious model (see Section 9.1) by replacing the semi-honest SPDZ scheme with the authenticated SPDZ scheme.

# 10. RELATED WORK

The works most related to ours are [71, 67, 68, 69, 21, 50, 44] for privacy preserving vertical tree models. None of

# ACKNOWLEDGEMENTS

We thank Xutao Sun for his early contribution to this work. This research is supported by Singapore Ministry of Education Academic Research Fund Tier 3 under MOEs official grant number MOE2017-T3-1-007.

# REFERENCES

1. California consumer privacy act. bill no. 375 privacy: personal information: businesses. https://leginfo.legislature.ca.gov/. 2018-06-28.
2. Laplace distribution. https://en.wikipedia.org/wiki/laplace_distribution.
3. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). oj, 2016-04-27.
4. data61/mp-spdz: Versatile framework for multiparty computation. https://github.com/data61/mp-spdz. Accessed: 2019-11-25.
5. T. Araki, A. Barak, J. Furukawa, M. Keller, Y. Lindell, K. Ohara, and H. Tsuchida. Generalizing the SPDZ compiler for other protocols. In CCS, pages 880–895, 2018.
6. A. T. Azar and S. M. El-Metwally. Decision tree classifiers for automated medical diagnosis. Neural Computing and Applications, 23(7-8):2387–2403, 2013.
7. D. Beaver. Efficient multiparty protocols using circuit randomization. In CRYPTO, 1991.
8. A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. B. Calo. Analyzing federated learning through an adversarial lens. In ICML, pages 634–643, 2019.
9. K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In CCS, pages 1175–1191, 2017.
10. R. Bost, R. A. Popa, S. Tu, and S. Goldwasser. Machine learning classification over encrypted data. In NDSS, 2015.
11. F. Boudot. Efficient proofs that a committed number lies in an interval. In EUROCRYPT, pages 431–444, 2000.
12. L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
13. L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and regression trees. 1984.
14. L. M. Candanedo, V. Feldheim, and D. Deramaix. Data driven prediction models of energy use of appliances in a low-energy house. Energy and Buildings, 140:81–97, 2017.
15. R. Canetti. Universally composable security: A new paradigm for cryptographic protocols. In FOCS, pages 136–145, 2001.
16. S. Cao, X. Yang, C. Chen, J. Zhou, X. Li, and Y. Qi. Titant: Online real-time transaction fraud detection in ant financial. PVLDB, 12(12):2082–2093, 2019.
17. O. Catrina and S. de Hoogh. Improved primitives for secure multiparty integer computation. In SCN, pages 182–199, 2010.

# CONCLUSIONS

We have proposed Pivot, a privacy preserving solution with two protocols for vertical tree-based models. With the basic protocol, Pivot guarantees that no intermediate information is disclosed during the execution. With the enhanced protocol, Pivot further mitigates the possible privacy leakages occurring in the basic protocol. To our best knowledge, this is the first work that provides strong privacy guarantees for vertical tree-based models. The experimental results demonstrate Pivot achieves accuracy comparable to non-private algorithms and is highly efficient.

# References

1. O. Catrina and A. Saxena. Secure computation with fixed-point numbers. In FC, pages 35–50, 2010.
2. H. Chen, K. Laine, and P. Rindal. Fast private set intersection from homomorphic encryption. In CCS, pages 1243–1255, 2017.
3. T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In SIGKDD, pages 785–794, 2016.
4. K. Cheng, T. Fan, Y. Jin, Y. Liu, T. Chen, and Q. Yang. Secureboost: A lossless federated learning framework. CoRR, abs/1901.08755, 2019.
5. A. R. Chowdhury, C. Wang, X. He, A. Machanavajjhala, and S. Jha. Crypt: Crypto-assisted differential privacy on untrusted servers. SIGMOD, 2020.
6. M. D. Cock, R. Dowsley, C. Horst, R. S. Katti, A. C. A. Nascimento, W. Poon, and S. Truex. Efficient and private scoring of decision trees, support vector machines and logistic regression models based on pre-computation. IEEE TDSC, 16(2):217–230, 2019.
7. R. Cramer, I. Damg˚ard, and J. B. Nielsen. Multiparty computation from threshold homomorphic encryption. In EUROCRYPT, pages 280–299, 2001.
8. R. Cramer, I. B. Damgrd, and J. B. Nielsen. Secure multiparty computation and secret sharing. 2015.
9. I. Damg˚ard. On σ-protocol. In Lecture Notes, 2010.
10. I. Damg˚ard and M. Jurik. A generalisation, a simplification and some applications of paillier’s probabilistic public-key system. In Public Key Cryptography, pages 119–136, 2001.
11. I. Damg˚ard, V. Pastro, N. P. Smart, and S. Zakarias. Multiparty computation from somewhat homomorphic encryption. In CRYPTO, pages 643–662, 2012.
12. L. Devroye. Non-Uniform Random Variate Generation. Springer, 1986.
13. C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.
14. S. Faust, M. Kohlweiss, G. A. Marson, and D. Venturi. On the non-malleability of the fiat-shamir transform. In INDOCRYPT, pages 60–79, 2012.
15. S. Fletcher and M. Z. Islam. Decision tree classification with differential privacy: A survey. ACM Comput. Surv., 52(4):83:1–83:33, 2019.
16. A. Friedman and A. Schuster. Data mining with differential privacy. In SIGKDD, pages 493–502, 2010.
17. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28:2000, 1998.
18. J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189–1232, 2000.
19. F. Fu, J. Jiang, Y. Shao, and B. Cui. An experimental evaluation of large scale GBDT systems. PVLDB, 12(11):1357–1370, 2019.
20. J. A. Garay, P. D. MacKenzie, and K. Yang. Strengthening zero-knowledge protocols using signatures. In EUROCRYPT, pages 177–194, 2003.
21. C. Ge, I. F. Ilyas, and F. Kerschbaum. Secure multi-party functional dependency discovery. PVLDB, 13(2):184–196, 2019.
22. R. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig, and J. Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In ICML, pages 201–210, 2016.
23. J. Goldstick. Introduction to statistical computing, statistics 406, notes - lab 5. Lecture Notes, Department of Statistics, 2009.
24. M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private histograms through consistency. PVLDB, 3(1):1021–1032, 2010.
25. X. He, A. Machanavajjhala, C. J. Flynn, and D. Srivastava. Composing differential privacy and secure computation: A case study on scaling private record linkage. In CCS, pages 1389–1406, 2017.
26. B. Hitaj, G. Ateniese, and F. P´erez-Cruz. Deep models under the GAN: information leakage from collaborative deep learning. In CCS, pages 603–618, 2017.
27. Y. Hu, D. Niu, J. Yang, and S. Zhou. FDML: A collaborative machine learning framework for distributed features. In SIGKDD, pages 2232–2240, 2019.
28. M. Kantarcioglu and O. Kardes. Privacy-preserving data mining in the malicious model. IJICS, 2(4):353–375, 2008.
29. M. Keller, E. Orsini, and P. Scholl. MASCOT: faster malicious arithmetic secure computation with oblivious transfer. In CCS, pages 830–842, 2016.
30. Y. Lindell and B. Pinkas. Privacy preserving data mining. In CRYPTO, pages 36–54, 2000.
31. Y. Lindell and B. Pinkas. Secure multiparty computation for privacy-preserving data mining. J. Priv. Confidentiality, 1(1), 2009.
32. J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network predictions via minionn transformations. In CCS, pages 619–631, 2017.
33. Y. Liu, Y. Liu, Z. Liu, J. Zhang, C. Meng, and Y. Zheng. Federated forest. CoRR, abs/1905.10053, 2019.
34. F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shafi, V. Shanbhogue, and U. R. Savagaonkar. Innovative instructions and software model for isolated execution. In HASP, page 10, 2013.
35. H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas. Federated learning of deep networks using model averaging. CoRR, abs/1602.05629, 2016.
36. H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. In ICLR, 2018.
37. C. A. Meadows. A more efficient cryptographic matchmaking protocol for use in the absence of a continuously available third party. In IEEE S&P, pages 134–137, 1986.
38. L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting unintended feature leakage in collaborative learning. In IEEE S&P, pages 691–706, 2019.
39. I. Mironov, O. Pandey, O. Reingold, and S. P. Vadhan. Computational differential privacy. In CRYPTO, pages 126–142, 2009.
40. P. Mohassel and Y. Zhang. Secureml: A system for scalable privacy-preserving machine learning. In IEEE

# References

1. S&P, pages 19–38, 2017.
2. S. Moro, P. Cortez, and P. Rita. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22–31, 2014.
3. V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-preserving ridge regression on hundreds of millions of records. In IEEE S&P, pages 334–348, 2013.
4. O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and M. Costa. Oblivious multi-party machine learning on trusted processors. In USENIX Security Symposium, pages 619–636, 2016.
5. P. Paillier. Public-key cryptosystems based on composite degree residuosity classes. In EUROCRYPT, pages 223–238, 1999.
6. B. Pinkas, T. Schneider, G. Segev, and M. Zohner. Phasing: Private set intersection using permutation-based hashing. In USENIX Security Symposium, pages 515–530, 2015.
7. B. Pinkas, T. Schneider, and M. Zohner. Scalable private set intersection based on OT extension. ACM Trans. Priv. Secur., 21(2):7:1–7:35, 2018.
8. J. R. Quinlan. Induction of decision trees. Mach. Learn., 1(1):81–106, Mar. 1986.
9. J. R. Quinlan. C4.5: Programs for machine learning. Morgan Kaufmann Publishers Inc., 1993.
10. R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In CCS, pages 1310–1321, 2015.
11. J. Vaidya and C. Clifton. Privacy-preserving decision trees over vertically partitioned data. In DBSec, pages 139–152, 2005.
12. J. Vaidya, C. Clifton, M. Kantarcioglu, and A. S. Patterson. Privacy-preserving decision trees over vertically partitioned data. TKDD, 2(3):14:1–14:27, 2008.
13. J. Vaidya, B. Shafiq, W. Fan, D. Mehmood, and D. Lorenzi. A random decision tree framework for privacy-preserving data mining. IEEE TDSC, 11(5):399–411, 2014.
14. C. R. Vogel. Computational Methods for Inverse Problems, volume 23 of Frontiers in Applied Mathematics. SIAM, 2002.
15. K. Wang, Y. Xu, R. She, and P. S. Yu. Classification spanning private databases. In AAAI, pages 293–298, 2006.
16. N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu. Collecting and analyzing multidimensional data with local differential privacy. In ICDE, pages 638–649, 2019.
17. D. J. Wu, J. Zimmerman, J. Planul, and J. C. Mitchell. Privacy-preserving shortest path computation. In NDSS, 2016.
18. Y. Wu, K. Wang, R. Guo, Z. Zhang, D. Zhao, H. Chen, and C. Li. Enhanced privacy preserving group nearest neighbor search. IEEE TKDE, 2019.
19. Y. Wu, K. Wang, Z. Zhang, W. Lin, H. Chen, and C. Li. Privacy preserving group nearest neighbor search. In EDBT, pages 277–288, 2018.
20. Y. Xu, W. Cui, and M. Peinado. Controlled-channel attacks: Deterministic side channels for untrusted operating systems. In IEEE S&P, pages 640–656, 2015.
21. Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications. ACM TIST, 10(2):12:1–12:19, 2019.
22. A. C. Yao. Protocols for secure computations (extended abstract). In FOCS, pages 160–164, 1982.
23. I. Yeh and C. Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Syst. Appl., 36(2):2473–2480, 2009.
24. W. Zheng, A. Dave, J. G. Beekman, R. A. Popa, J. E. Gonzalez, and I. Stoica. Opaque: An oblivious and encrypted distributed analytics platform. In NSDI, pages 283–298, 2017.
25. W. Zheng, R. A. Popa, J. E. Gonzalez, and I. Stoica. Helen: Maliciously secure coopetitive learning for linear models. In IEEE S&P, pages 915–929, 2019.

