# Mitigating GenAI-powered Evidence Pollution for Out-of-Context Multimodal Misinformation Detection - Article (Preprint v1) by Zehong Yan et al.

While large generative artificial intelligence (GenAI) models have achieved significant success, they also raise growing concerns about online information security due to their potential misuse for generating deceptive content. Out-of-context (OOC) multimodal misinformation detection, which often retrieves Web evidence to identify the repurposing of images in false contexts, faces the issue of

# Reasoning Over GenAI-Polluted Evidence to Derive Accurate Predictions

Existing works simulate GenAI-powered pollution at the claim level with stylistic rewriting to conceal linguistic cues, and ignore evidence-level pollution for such information-seeking applications. In this work, we investigate how polluted evidence affects the performance of existing OOC detectors, revealing a performance degradation of more than 9 percentage points. We propose two strategies, cross-modal evidence reranking and cross-modal claim-evidence reasoning, to address the challenges posed by polluted evidence. Extensive experiments on two benchmark datasets show that these strategies can effectively enhance the robustness of existing out-of-context detectors amidst polluted evidence.

Peer-approved
Preprints

There is a newer version available for this {{ publicationType }}.
View latest version

</publication-authors-facepile>

</publication-basic-stats>

View Latest Version

<v-popover>

Create new version

Data

<bookmark-button with-status-message :item="publication"></bookmark-button>

</v-popover>

# Field

Report

PDF

Cite

<share-publication-dropdown v-if="publication.published_at" :publication="publication"></share-publication-dropdown>

# </history-modal>
<create-new-version-modal ref="createNewVersionModal" :publication="publication"></create-new-version-modal>
<ancillary-files-modal ref="ancillaryFilesModal"></ancillary-files-modal>
<report-modal ref="reportModal" :item="publication"></report-modal>
# </how-to-cite-modal>

{{ publication.field_name }}

# Subfield

{{ publication.subfield_name }}

# Open Peer Review

<rating-overview :publication="publication" is-clickable @click.native="showModal('receivedReviewsModal')" style="cursor: pointer"></rating-overview>
<compose-review-cta :publication="publication"></compose-review-cta>
a.id === $authUser.id && !a.is_vacant_author)) && publication.auth_can_create_new_version" type="button" @click="showModal('createNewVersionModal')" class="hollow-btn blue">
New version

</publication-reviews-modal>

{{ publication.article_category }}
{{ publicationDate }}

<v-popover v-if="!publication.is_echo">
CC BY

Copyright: Â© {{ publicationYear }} {{ publication.presentation_authors[0].full_name + (publication.presentation_authors.length > 1 ? ' et al' : '') }}. This is an open access publication distributed under the terms of the CC BY 4.0 License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.

</v-popover>
<v-popover v-else>
Lic. Info

Check the {{ publicationType | capitalize }} Source for copyright and license information.

</v-popover>

{{ publication.doi_url }}

# {{ publication.title }}

</publication-authors-as-text>

# Abstract

# Source

</publication-reviews-section>

Listen on

<ancillary-files-management></ancillary-files-management>

# Mitigating GenAI-powered Evidence Pollution for Out-of-Context Multimodal Misinformation Detection

While large generative artificial intelligence (GenAI) models have achieved significant success, they also raise growing concerns about online information security due to their potential misuse for generating deceptive content. Out-of-context (OOC) multimodal misinformation detection, which often retrieves Web evidence to identify the repurposing of images in false contexts, faces the issue of reasoning over GenAI-polluted evidence to derive accurate predictions. Existing works simulate GenAI-powered pollution at the claim level with stylistic rewriting to conceal linguistic cues, and ignore evidence-level pollution for such information-seeking applications. In this work, we investigate how polluted evidence affects the performance of existing OOC detectors, revealing a performance degradation of more than 9 percentage points. We propose two strategies, cross-modal evidence reranking and cross-modal claim-evidence reasoning, to address the challenges posed by polluted evidence. Extensive experiments on two benchmark datasets show that these strategies can effectively enhance the robustness of existing out-of-context detectors amidst polluted evidence.

# 1. Introduction

The rapid development of generative artificial intelligence (GenAI) technologies has led to a surge of synthetic data in the Web [1][2][3]. *According to Gartner’s prediction, by 2025, generative AI will account for 10% of all data produced, up from less than 1% today*1. While GenAI mitigates the problem of data scarcity to some extent [4][5][6], it also facilitates the spread of realistic-looking yet non-factual misinformation [7][8]. Specifically, large language models (LLMs) like GPT-4 [9] produce both deliberate disinformation and unintentional hallucinations [1].

the growing use of diffusion models for visual manipulation exacerbates these safety issues [10][11]. Therefore, it is urgent to develop robust methods for information-seeking applications to mitigate pollution in the era of GenAI.

Existing studies has predominantly examined the GenAI-posed threats at the claim level [12][13][14][15]. To bypass detectors that rely upon superficial features such as language style for detection [7], nefarious users typically transform sensational language into a neutral, formal style [14]. For example, Figure 1(a) illustrates the scenario where a sensational claim has been rewritten in the style of the New York Times to elude detection.

Figure 1. Example of how misinformation detectors are misled by *claim-level versus evidence-level* pollution posed by GenAI. Faces of individuals are obscured to reduce privacy risks and mitigate the effects of misinformation exposure.

On the other hand, evidence-level threats primarily target information-seeking systems that retrieve related evidence for inference (such as question answering [16][1] and fact-checking systems [17][18]), by contaminating the evidence corpus with false information. As shown in Figure 1(b), malicious users exploit GenAI technologies to generate texts and images that support the misinformation about Taylor Swift’s pregnancy, leading to incorrect predictions by the detectors. Existing works on evidence-level threats have focused on textual pollution within fixed, highly structured evidence corpora like Wikipedia pages. However, this narrow focus results in a considerable gap for misinformation detectors in the real-world where evidence retrieved from the web are typically unstructured, noisy and polluted.

Out-of-context (OOC) misinformation, where an authentic image is paired with false narratives to create misleading news, is one of the easiest and most effective ways to mislead audiences and has garnered increasing attention [19][20]. To combat OOC misinformation, [21][22][23][24] retrieve related news from web searches for each modality as a supplement to measure the cross-modal inconsistency. These works assume that the retrieved evidence contains only factual information, making the detectors vulnerable to data pollution caused by GenAI, an issue that remains underexplored.

In this work, we explore how GenAI models contribute to the pollution of evidence affecting the performance of OOC detectors. Figure 2 shows an example of

how diverse multimodal evidence that closely resembles the original claim can be generated using GPT-4 [9] and Stable Diffusion 2 [11]. The generated evidence is mixed with evidence retrieved from the web before feeding into an OOC misinformation detector. Preliminary experiments reveal that existing OOC detectors are susceptible to this type of pollution, with detection efficacy decreasing by more than 9 percentage points.

Figure 2. An illustrated example of claim-conditioned generated evidence, accompanied by clean evidence retrieved from the Web.

We propose two strategies to enhance the robustness of existing OOC detectors: cross-modal evidence reranking and cross-modal claim-evidence reasoning. Cross-modal reranking prioritizes the most contextually relevant retrieved textual evidence based on the claim image, as well as the most relevant retrieved visual evidence based on the claim caption. Cross-modal claim-evidence reasoning provides an additional layer of analysis by identifying inconsistencies between the claim image and the top-ranked textual evidence retrieved. Our main contributions are as follows:

We construct a large diverse collection of multimodal evidence to simulate the challenges posed by GenAI-based pollution for OOC misinformation detectors.
We propose cross-modal evidence reranking and cross-modal claim-evidence reasoning to significantly enhance the robustness of OOC detectors against evidence pollution.
Extensive experiments reveal the susceptibility of OOC detectors in the presence of evidence pollution and the effectiveness of the proposed strategies to mitigate such threats.

# 2. Related Work

Out-of-Context Misinformation Detection. Early works in OOC misinformation detection [25][19][26] focus on verifying claims by analyzing the consistency of the image-caption pairs. These methods employ knowledge-rich pre-trained models, such as VGG-19 [27], CLIP [28] and VisualBERT [29] to assess consistency. However, they tend to miss complex misinformation [7] as they focus solely on the content of claims without considering external information like metadata [30][31] and web search results [32][20].

For external evidence reasoning, [20] first collects multimodal evidence from the Web and use a Consistency-Checking Network (CCN) to analyze the consistency between the claim and retrieved evidence. [22] introduces the RED-DOT model, which ranks and filters evidence based on similarity scores to determine its relevance to the claim before using them for verification. [23]

extends this approach by employing stance extraction networks to analyze whether the evidence supports or refutes the claim.

To improve the explainability of the veracity prediction, [33] integrates multi-clue feature extraction, multi-level reasoning, and a decoder into a unified framework to explain the reasoning behind predictions. [24] introduces SNIFFER, an explainable multimodal large language model that uses a two-stage instruction tuning process and three-stage reasoning framework. Despite these advancements, these works assume the factual integrity of retrieved evidence, which might not hold in real-world scenarios where evidence can be tainted with misleading or fabricated content.

Fact Checking with Polluted Evidence. While substantial progress has been made in developing automated fact checking systems [34][35][36] that verify claims based on reference knowledge bases, these systems suffer a marked decrease in performance when faced with compromised evidence. [17] utilizes language models to generate coherent yet false evidence which is then inserted into the evidence base. Building on this, [18] proposes a taxonomy of pollution strategies targeting evidence, including planting and camouflaging, which expose the susceptibility of current fact-checking systems to manipulation. While these studies provide insights into evidence pollution, they focus on textual pollution in a controlled and highly structured evidence source, such as Wikipedia. Our work considers more complex and realistic scenarios posed by GenAI, examining how such technologies affect fact-checking across a diverse range of evidence sources in an open-domain setting.

# 3. Methodology

In this section, we first simulate the scenarios where GenAI technologies are used to create realistic multimodal evidence pollution. Then we introduce two strategies, namely cross-modal reranking and cross-modal claim-evidence reasoning, to improve the robustness of OOC detectors against pollution.

# 3.1. Base OOC Detector

Figure 3 gives an overview of a typical framework of OOC misinformation detector. Given a claim comprising of an image \(I^q\) and a caption \(T^q\), we first retrieve visual and textual evidence from the web using Google Vision and Google Custom Search. The claim and the retrieved evidence undergo a framework comprising of three key modules: visual, textual and image-caption consistency reasoning. The visual reasoning module examines the relevance between the claim image \(I^q\) and the polluted image evidence  \(\{I^c, I^g\}\). The textual reasoning module assesses how well the query caption \(T^q\) corresponds with the polluted text evidence  \(\{T^c, T^g\}\). Beyond these individual assessments, the consistency reasoning module checks the consistency between the claim image and the caption. The outputs from these reasoning modules are combined through a fusion module and then passed to a classifier to determine the veracity.

Figure 3. Overview of a typical OOC detection framework.

# 3.2. Evidence Pollution with GenAI

Polluted evidence poses significant challenges for both visual and textual reasoning modules, as they are susceptible to distractions from noisy or conflicting information, leading to inaccurate predictions. Unlike previous works [20]

that assume a clean evidence corpus, we consider the scenario where the evidence on the Web is polluted with highly similar yet potentially false information, thus challenging the robustness of evidence-based detectors.

For textual evidence pollution, we utilize LLMs to obtain realistic textual evidence for pollution at scale. Specifically, we employ GPT-4 in a zero-shot manner and prompt it with two types of instructions motivated by real-world scenarios where noisy and conflicting information is prevalent, especially on social media platforms. The first type of instruction is used to generate textual evidence related to the entity mentioned in the caption: *Write a short text about the main entity mentioned in the caption. Caption:&nbsp;\\(&lt;\\)INPUT\\(&gt;\\) . The second type of instruction generates textual evidence that either supports or refutes the claim caption: Write a piece of evidence to support or refute the given caption. Caption:&nbsp;\\(&lt;\\)INPUT\\(&gt;\\).* Since LLMs are prone to hallucinate, the generated text may contain inaccuracies.

Visual evidence also exhibits significant diversity across various domains, particularly in news, where different outlets may display different images of the same event. To simulate such diversity in real-world visual information, we employ the entity-preserving capabilities of Depth-Conditional Stable Diffusion to generate visual evidence with varied camera angles and scene compositions, thereby providing a more challenging visual context for evaluating multimodal claims.

Recall the multimodal claim in Figure 2. The generated visual evidence shows variations of the same individual in the image, enriched with contextual details, visual modifications, and different backgrounds. With the claim caption, LLM generates the text based on the main entity, where the description of “British television channel” is factual. However, it also produces hallucinations, such as “BBC3 primarily focused on political debates”, which is incorrect, as BBC3 targets a younger audience and does not specifically focus on political content. Additionally, the generated support and refute textual evidence tends to extend beyond the context of the caption and produce nonfactual statements like “BBC3 won the BAFTA, not the RTS award”.

# 3.3. Proposed Strategies

OOC detectors assess the information authenticity and the consistency between text and associated images. However, the sophistication of LLMs introduces a new layer of complexity as it generates convincing polluted evidence that is not easily detected as LLM-generated content. We demonstrate this by evaluating the Vicuna-13B model, an open-source detector, on a dataset comprising of 10,000 pieces of textual evidence, evenly split between human-written and LLM-generated texts. The model achieves only a 41.3% accuracy in identifying LLM-generated content. This motivates us to develop two strategies, cross-modal evidence reranking and cross-modal claim-evidence reasoning, to enhance the robustness of OOC detectors (see Figure 4).

Figure

OOC misinformation detection framework in the presence of polluted evidence with proposed cross-modal reranking and cross-modal claim-evidence reasoning strategies.

|Dataset|NewsCLIPpings|VERITE| | |
|---|---|---|---|---|
|Train|Validation|Test|Test| |
|Claim|71,072|7,024|7,264|662|
|Evidence|&nbsp;|&nbsp;|&nbsp;|&nbsp;|
|Clean Text|689,995|58,388|60,848|1,261|
|Generated Text|903,067|82,112|67,016|2,002|
|Clean Image|650,738|64,562|66,772|8,309|
|Generated Image|655,848|65,082|67,092|8,389|

Table 1. Dataset statistics.

Cross-modal Evidence Reranking. This strategy *addresses the issue of OOC detectors inadvertently focus on polluted evidence by giving priority to evidence that best aligns with the claim.* Inspired by [36], we use CLIP to identify the most contextually relevant textual evidence from a corpus that may contain polluted information, based on the claim image. Similarly, this method is employed to determine the most relevant visual evidence based on the claim caption. Algorithm 1 gives the details. Specifically, we utilize CLIP embeddings to compute cross-modal similarity scores and obtain the re-ranked lists of visual and textual evidence. The top-k visual and textual evidence are then passed to the visual reasoning module and textual reasoning module respectively.

Cross-modal Claim-Evidence Reasoning. Cross-modal claim-evidence reasoning goes beyond traditional caption-image consistency check, which often misses critical contextual details provided by external evidence. For example, a false caption may correctly describe the visible elements in an image but misrepresent its context, such as attributing a news event to the wrong location or time. These discrepancies can only be verified using external information that is most pertinent to the main entity in the caption. As such, we use the most relevant textual evidence related to the caption for a consistency check with the claim image, ensuring the model’s robustness even when confronted with polluted evidence. Algorithm 2 gives the details.

The two proposed strategies can be utilized in a plug-and-play manner, allowing for easier integration into real world applications, without the need for re-training. Further, these strategies are adaptable to various types of pollutions with the emphasis on enhancing semantic-level reasoning rather than the feature distribution of a specific pollution model.

# 4. Performance Study

# 4.1. Experimental Setup

Datasets. We use two datasets in our experiments:

NewsCLIPpings [19] is the largest synthetic benchmark for OOC misinformation detection. It synthesizes out-of-context samples by replacing the images in the original image-caption pairs with retrieved images that are semantically related but belong to different news events. [20] extends this dataset by supplementing both textual and visual evidence using Google Search APIs.
VERITE [40] is a real-world benchmark for

evaluating multimodal misinformation detection. It consists of real and out-of-context pairs from fact-checking websites. We use the corresponding multimodal evidence from [22].

For each piece of textual evidence, we randomly apply one of the LLM instruction to create the corresponding polluted entity-based, supporting or refuting evidence. For each piece of visual evidence, we use Depth-conditioned Stable Diffusion to generate the corresponding images. These generated evidence are added to the original clean evidence corpus. Table 1 shows the statistics for the two datasets.

Table 2. OOC detection performance (%) under evidence pollution of different modalities. The first row (Clean) refers to the original performance without any pollution introduced. The absolute change compared to the Clean setting is highlighted in red.

Baselines. We use the following OOC misinformation detectors in our experiments:

CCN [20]. This employs attention-based memory networks for visual and textual reasoning between the claim and evidence, and a fine-tuned CLIP component to check the claim image and caption consistency.
RED-DOT [22]. This leverages the pre-trained CLIP as the backbone to extract visual and textual features. Transformer-based fusion module is used to facilitate interaction and reasoning among these features.
SNIFFER [24]. This is the state-of-the-art multimodal large language model designed for OOC misinformation detection. It employs a two-stage instruction tuning on InstructBLIP for the cross-modal consistency checks.
GPT-4o [41]. This is currently one of the most powerful multimodal large language models. We utilize GPT-4o in a zero-shot manner with step-by-step instructions for OOC detection. Details are provided in Appendix.

# 4.2. Effect of Evidence Pollution on OOC Detectors

Table 3 shows the OOC detection performance across different evidence modalities. We observe that: 1) The combination of polluted text and image poses a significant threat to OOC detectors. Specifically, the accuracy of all detectors drop by more than 9 percentage points, revealing the vulnerabilities of existing OOC detectors against generated multimodal pollution. 2) Textual pollution has a greater impact than visual pollution, indicating that existing OOC detectors are more dependent on textual information. This modality bias may stem from the fact that textual evidence often provides more semantics such as relationships between entities compared to images. 3) Detection of false claims in the presence of with polluted evidence proves to be more challenging than true claims. Specifically, CCN experiences a significant drop of 35.67 points in the F1 score for false claims on the VERITE dataset, highlighting the difficulties in reasoning with contradictory evidence.

Table 3. OOC detection performance (%) with the proposed strategies under the evidence pollution. The first row (None) refers to the original performance under multimodal pollution. The absolute change to the original one is highlighted in blue.

Quantitative Analysis. Figure 5a shows the performance of SNIFFER when we vary the proportion of polluted evidence. We see that the accuracy of the model drops as the proportion of pollution increases. Even a small amount of pollution can significantly affect the model’s detection capabilities where introducing 25% of polluted evidence results in a decrease of 7.63 points. The impact of varying pollution ratios on different models such as CCN and different types of

textual evidence pollution are given in the Appendix.

Generalization Analysis. Figure 5b shows the impact of pollution in textual and visual modalities under different generative models. Notably, for visual pollution, advanced models like DALL-E, which significantly improves image quality and resolution, further amplify the effects of visual pollution.

Human Evaluation. We conduct a human evaluation on ten randomly selected misinformation samples with polluted evidence. Twenty participants were asked to judge each piece of evidence’s authenticity and each claim’s veracity before and after reading the polluted evidence. The results show that (a) only 49.39% of the generated evidence was correctly identified as AI-generated; (b) 41.84% of the initially correct veracity judgments for misinformation samples were reversed to wrong predictions after reading the polluted evidence.

Figure 5. SNIFFER’s performance across varying proportion of polluted evidence and GenAI models on NewsCLIPpings.

# 4.3. Effect of Proposed Strategies

Table 4 shows the performance of the various OOC misinformation detectors when we incorporate the proposed defense strategies. We see that: 1) The combination of both strategies yields the best results, increasing the overall accuracy to 88.82% (+12.40) and 75.44% (+11.15) for SNIFFER on the NewsCLIPpings and VERITE dataset respectively. This indicates that the two strategies complement each other, enhancing the model’s robustness against multimodal pollution. Further, the strategies can be generalized to the real-world VERITE dataset. 2) Incorporating cross-modal evidence re-ranking significantly boosts performance. The overall accuracy of SNIFFER increases to 87.68%, marking an improvement of 11.26%, on the NewsCLIPpings dataset. This strategy also enhances the detection of true and false claims to 87.74% (+10.27) and 87.62% (+12.31), respectively. The results suggest that re-ranking evidence and focusing on the top relevant evidence greatly aids in reconciling discrepancies introduced by multimodal pollution. 3) Similar to cross-modal reranking, cross-modal claim-evidence reasoning module also shows substantial gains, particularly in the detection of true claims.

Table 4 further compares the performance of LLM-based detectors with three general approaches under evidence pollution. The extra detector approach involves adding an auxiliary classifier to filter out the generated evidence, the vigilant prompting approach introduces hints at the presence of false evidence in the prompt, and the reader ensemble approach combines multiple judgments based on different evidence by voting [1]. SNIFFER, equipped with our proposed solution, achieves the highest performance across two datasets, with significant improvements of 12.40% on NewsCLIPpings and 13.41% on VERITE, demonstrating its superiority in the presence of polluted evidence. Notably, our approaches can be easily integrated into existing OOC detection frameworks, whereas the prompting-based and voting-based approaches are restricted to LLM-based detectors.

Table 4. Performance comparison of different strategies. The first row (None) refers to the original performance under multimodal pollution.

# 4.4. Case Study

Figure 6 presents a case study under evidence pollution. Initially, in the clean setting, the model correctly identifies that the image, depicting Tim Henman, is irrelevant to the political figures mentioned in the caption (Nick Clegg, Elwyn Watkins, Simon Hughes). However, after exposure to pollution, SNIFFER erroneously asserts that the image is relevant, citing visual evidence of a man in a suit speaking to another man and textual evidence mentioning both Nick Clegg and Tim Henman. Additionally, it also incorrectly emphasizes a weak connection between the image and textual evidence, leading to an incorrect prediction.

Incorporating the two proposed strategies enables SNIFFER to recognize the inconsistency between the image and the caption, and confirm that the image indeed features Tim Henman which does not match the caption’s context. This leads to the

correction prediction.

Figure 6. Case study of SNIFFER’s justification outputs under clean and polluted settings. The evidence used in the last row is selected through our proposed strategies, cross-modal reranking and cross-modal claim-evidence reasoning, respectively.

# 5. Conclusion

In this paper, we reveal the critical vulnerabilities of existing out-of-context multimodal misinformation detectors when confronted with evidence polluted by large generative models. To counteract this, we introduced and evaluated two innovative strategies: cross-modal evidence reranking and cross-modal claim-evidence reasoning. Our comprehensive experiments across multiple detectors and two benchmarks have shown that these strategies significantly enhance the detectors’ resilience against multimodal evidence pollution. We believe this study paves the way for further research into robust misinformation detection in the era of GenAI.

# Appendix A. Task Formulation

Figure 7 provides an overview of an out-of-context (OOC) detection system in the era of GenAI. The input claim is processed through a retriever module to gather relevant textual and visual evidence from the Web. LLMs and Stable Diffusion models play a role in generating and simulating pollution. The claim and evidence are then passed to the OOC detector, which evaluates the claim’s veracity. Here, we further summarize the task components and evidence pollution posed by large generative models as follows:

Figure 7. An overview of out-of-context detection system under evidence pollution. A claim image and its caption are processed by retrievers to gather textual and visual evidence from the web (green). Conditioned on the claim, we employ large language models (LLMs) and stable diffusion (SD) models to generate pollution, which is then inject then into original evidence corpus (purple). Finally, the claim, along with the textual and visual evidence, is fed into an OOC detector to determine its veracity.

# Appendix B. Implementation Details

We use CCN [20] and SNIFFER’s [24] public model checkpoints fine-tuned on the NewsCLIPpings training set. We use the InstructBLIP [42] as our captioner for visual reasoning path without fine-tuning. We leverage the the CLIP (ViT-L/14) as the cross-modal reranking module and select the top-1 sentence and top-5 images for textual and visual evidence. For augmented reasoning, we reuse the original CLIP component from CCN and internal checking from SNIFFER. All models are trained and evaluated on 8 Nvidia H100 (80G) GPUs. We generate textual pollution with GPT-4 (gpt-4) [9], which is configured with a temperature of 1.2, a maximum token length of 64, and a top-P setting of 0.95. We employ the variant of Stable Diffusion v2 models (stabilityai/stable-diffusion-2-depth) to generate visual pollution. We report accuracy over all samples, and F1 score for the true and false samples, respectively.

# Appendix C. Visualization of Similarity Distribution

To assess the similarity between the generated evidence and the original clean evidence, we conducted an analysis of similarity for both textual and visual evidence. We then examined the distribution between the clean and generated evidence. For clearer visualization, We randomly select a evidence subset of 500 claims from the test set. As shown in Figure 8(a)and Figure 8(b), the distribution is centered around zero, indicating that the generated evidence closely resembles the original clean evidence.

Additionally, we applied t-SNE to visualize the latent spaces. The results prove that our approach is able to generate evidence that not only closely mirrors the original clean evidence but also exhibits greater similarity to the input claim, thereby effectively contaminating the clean evidence while preserving high semantic similarity. This demonstrates the effectiveness of our approach in generating evidence that can blend seamlessly into the original clean evidence set.

Figure 8. (a): Distribution of differences in CLIP scores between input image and textual evidence. The X-axis represents the difference calculated as the CLIP score of the image-evidence (generated) minus the CLIP score of the image-evidence (clean), while the Y-axis shows the count of these occurrences. (b): Distribution of differences in CLIP scores between input caption and visual evidence. (c)-(d): t-SNE visualization of latent space of clean and generated evidence.

# Appendix D. Performance Analysis of Varying Proportion of Polluted Evidence

In addition to SNIFFER, we present the results of the CCN model [20] under different proportions of polluted evidence, as illustrated in Figure 9. The accuracy of CCN demonstrates a marked decline as the level of evidence pollution increases. Furthermore, the results highlight CCN’s heavy reliance on the text modality for misinformation identification, making it particularly vulnerable to pollution introduced by LLMs.

Figure 9. CCN’s performance across varying proportion of polluted evidence on NewsCLIPpings dataset.

# Appendix E. Comparative Analysis of Types of Textual Pollution

In this section, we study the effects of different ways when generating textual evidence pollution. Figure 10 shows the impact of different types of textual evidence pollution on the performance of CCN and SNIFFER. We see that CCN is more affected by the generated entity based text, while SNIFFER shows the largest decline in the presence of generated supporting and refuting evidence.

Figure 10. OOC detection performance (%) comparison among different types of textual pollution.

# Appendix F. Performance of Cross-modal Reranking

Table 5 shows the percentage of clean evidence within the top-k results after applying the cross-modal re-ranking. By leveraging the capabilities of pre-trained encoder CLIP to facilitate cross-modal semantic matching between textual and visual modalities, we have effectively increased the probability of utilizing clean evidence for misinformation detection.

|Reranker|Evidence|Query|R@1|R@3|R@5|R@10|
|---|---|---|---|---|---|---|
|CLIP (ViT-B/32)|Polluted Text|Image|70.56%|64.14%|59.98%|55.57%|
|CLIP (ViT-B/32)|Polluted Image|Caption|64.88%|61.05%|57.00%|49.74%|
|CLIP (ViT-L/14)|Polluted Text|Image|72.78%|66.73%|62.67%|57.89%|
|CLIP (ViT-L/14)|Polluted Image|Caption|76.38%|72.23%|67.83%|56.73%|

Table 5. Performance evaluation of CLIP-based re-rankers in NewsCLIPpings dataset. The retrieval effectiveness is measured at multiple cutoff points. R@k indicates the percentage of clean evidence is found within the top-k retrieved results.

# Appendix G. Comparison of Related

# Works

Table 6 presents a comparison of related work, each evaluated across different criteria: Textual Modality, Visual Modality, Use of Large Language Models, Targeted Evidence Source, and Stance Diversity. Our work distinctly integrates all these aspects in an open-domain OOC misinformation detection task, which requires reasoning over evidence retrieved from the Web with various sources. We simulate a more realistic pollution posed by the GenAI, calling for an early evaluation. Furthermore, unlike previous efforts that focus solely on textual pollution, our proposed pollution pipeline is the first work to introduce multimodal pollution.

|Targeted Task|Textual Modality|Visual Modality|Use LLM|Targeted Evidence|Stance Diversity|
|---|---|---|---|---|---|
|News Veracity Classification&nbsp;[17]|✔|✖|✖|Wikipedia, S2ORC, Reddit|✖|
|News Veracity Classification&nbsp;[18]|✔|✖|✖|Wikipedia|✖|
|Question Answering&nbsp;[16]|✔|✖|✖|Wikipedia|✖|
|Question Answering&nbsp;[1]|✔|✖|✔|Wikipedia, WMT News|Supporting|
|OOC Misinformation Detection (Ours)|✔|✔|✔|Web|Supporting, Refuting|

Table 6. Comparison of related work on evidence pollution.

# Appendix H. Detecting Polluted Evidence

Along with the rapid development of LLMs, the issue of data pollution has become increasingly important and observed in the research community&nbsp;[1][39]. There has been increasing attention on detecting LLM-generated data in recent studies&nbsp;[2]. Following&nbsp;[2], we adopt the prompt for detection. We randomly select a set of 10,000 pieces of textual evidence samples as the test set, equally divided into human-written clean samples and LLM-generated samples, and use open-source Vicuna-13B model to detect LLM-generated content. The results show that LLM detector can hardly identify LLM-generated text with an overall accuracy of just 41.3%. We found that LLMs focus on grammar, sentence structure, and specific contextual details such as events and people, as well as vocabulary usage. Such traditional linguistic scopes are not enough because advanced large generative technologies, like GPT-4, are exceptionally proficient at mimicking human-like text, underscoring the need for more sophisticated approaches.

# Appendix I. Prompt to Detect the OOC Misinformation

Figure 11 illustrates the prompt utilized for asking GPT-4o to identify inconsistencies between the claim image and its caption. The preliminary step is to retrieve multimodal evidence. For each claim, we retrieve textual and visual evidence (converted to text via image captioning) separately and then pass them to GPT-4o to process.

Figure 11. Prompt used to ask GPT-4o to detect out-of-context misinformation.

# Footnotes

1 https://www.gartner.com/en/newsroom

# References

-

a,&nbsp;b,&nbsp;c,&nbsp;d,&nbsp;e,&nbsp;f

Pan Y, Pan L, Chen W, Nakov P, Kan MY, Wang W. "On the Risk of Misinformation Pollution with Large Language Models." In: *Findings of the Association for Computational Linguistics: EMNLP 2023*. 2023 Dec; p. 1389-1403. doi:10.18653/v1/2023.findings-emnlp.97.
-

a,&nbsp;b,&nbsp;c,&nbsp;d

Chen C, Shu K. Can LLM-Generated Misinformation Be Detected? In: *The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*; 2024. Available from: https://openreview.net/forum?id=ccxD4mtkTU. [cited 2024 Aug 7].
-

a,&nbsp;b

Wu J, Yang S, Zhan R, Yuan Y, Wong DF, Chao LS (2023). "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions". *CoRR*. abs/2310.14724. doi:10.48550/arXiv.2310.14724. dblp computer science bibliography.
-

^

Babbar R, Schölkopf B (2019). "Data scarcity, robustness and extreme multi-label classification". *Machine Learning*. 108 (8): 1329–1351.
-

^

Kim S, Bae S, Shin J, Kang S, Kwak D, Yoo K, Seo M. Aligning large language models through synthetic feedback. In: *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 2023:13677-13700. doi:10.18653/v1/2023.emnlp-main.844. Available from: https://aclanthology.org/2023.emnlp-main.844.
-

^

Villalobos P, Ho A, Sevilla J, Besiroglu T, Heim L, Hobbhahn M (2024). "Position: Will we run out of data? Limits of LLM scaling based on human-generated data." In: *Forty-first International Conference on Machine Learning, ICML 2024*. Available from: https://openreview.net/forum?id=ViZcgDQjyG. [cited 2024 Sep 2].
-

a,&nbsp;b,&nbsp;c

Guo Z, Schlichtkrull M, Vlachos A (2022). "A Survey on Automated Fact-Checking". *Transactions of the Association for Computational Linguistics*. 10: 178–206. doi:10.1162/tacl_a_00454.
-

^

Zhang S, Dong L, Li X, Zhang S, Sun X, Wang S, Li J, Hu R, Zhang T, Wu F, Wang G (2023). "Instruction Tuning for Large

-
Language Models: A Survey. *CoRR*. abs/2308.10792. doi:10.48550/arXiv.2308.10792. arXiv:2308.10792.
-
a,&nbsp;b,&nbsp;c,&nbsp;d
OpenAI (2023). "GPT-4 Technical Report". *CoRR*. abs/2303.08774. doi:10.48550/ARXIV.2303.08774. arXiv:2303.08774. Available from: https://dblp.org/rec/journals/corr/abs-2303-08774.bib.
-
^
Ramesh A, Dhariwal P, Nichol A, Chu C, Chen M (2022). "Hierarchical Text-Conditional Image Generation with CLIP Latents". *CoRR*. abs/2204.06125. doi:10.48550/arXiv.2204.06125. ePrint 2204.06125. Bibsource dblp computer science bibliography. BibURL https://dblp.org/rec/journals/corr/abs-2204-06125.bib.
-
a,&nbsp;b,&nbsp;c
Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution image synthesis with latent diffusion models. *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022:10684-10695.
-
^
Atanasova P, Wright D, Augenstein I (2020). "Generating label cohesive and well-formed adversarial claims". *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*. 3168--3177. doi:10.18653/v1/2020.emnlp-main.256.
-
^
Russo D, Kaszefski-Yaschuk S, Staiano J, Guerini M. Countering misinformation via emotional response generation. In: *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 2023 Dec; p. 11476-11492. doi:10.18653/v1/2023.emnlp-main.703.
-
a,&nbsp;b
Wu J, Guo J, Hooi B (2024). "Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks." In: *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. pp. 3367–3378. doi:10.1145/3637528.3671977. Source.
-
^
Yerukola A, Zhou X, Clark E, Sap M. "Don't Take This Out of Context!: On the Need for Contextual Models

-

a,&nbsp;b

Pan L, Chen W, Kan MY, Wang WY. "Attacking Open-domain Question Answering by Injecting Misinformation." In: *Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)*. 2023 Nov; p. 525-539. doi:10.18653/v1/2023.ijcnlp-main.35.
-

a,&nbsp;b,&nbsp;c

Du Y, Bosselut A, Manning CD. "Synthetic disinformation attacks on automated fact verification systems." In: *Proceedings of the AAAI Conference on Artificial Intelligence*. 2022; 36: 10581--10589.
-

a,&nbsp;b,&nbsp;c

Abdelnabi S, Fritz M. "Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems." In: Calandrino JA, Troncoso C, editors. *32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023*. 2023. p. 6719-6736. Available from: https://www.usenix.org/conference/usenixsecurity23/presentation/abdelnabi. [cited 2023 Oct 18].
-

a,&nbsp;b,&nbsp;c

Luo G, Darrell T, Rohrbach A. "NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media." *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*. 2021:6801-6817. doi:10.18653/v1/2021.emnlp-main.545.
-

a,&nbsp;b,&nbsp;c,&nbsp;d,&nbsp;e,&nbsp;f,&nbsp;g,&nbsp;h

Abdelnabi S, Hasan R, Fritz M (2022). "Open-domain, content-based, multi-modal fact-checking of out-of-context images via online resources". *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. pages 14940–14949.
-

^

Zhang Y, Trinh L, Cao D, Cui Z, Liu Y (2023). "Detecting out-of-context multimodal misinformation with interpretable neural-symbolic model". *CoRR*. abs/2304.07633. doi:10.48550/arXiv.2304.07633. Source.
-

a,&nbsp;b,&nbsp;c,&nbsp;d,&nbsp;e

Papadopoulos SI, Koutlis C,

Papadopoulos S, Petrantonakis PC (2023). "RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection". *CoRR*. abs/2311.09939. doi:10.48550/ARXIV.2311.09939. ePrint 2311.09939.

-
a,&nbsp;b
Yuan X, Guo J, Qiu W, Huang Z, Li S. Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 2023:4268-4280. doi:10.18653/v1/2023.emnlp-main.259.
-
a,&nbsp;b,&nbsp;c,&nbsp;d,&nbsp;e
Qi P, Yan Z, Hsu W, Lee ML (2024). "*SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection". In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. pp. 13052–13062.
-
^
Jaiswal A, Sabir E, AbdAlmageed W, Natarajan P (2017). "Multimedia semantic integrity assessment using joint embedding of images and text". In: *Proceedings of the 25th ACM international conference on Multimedia*. p. 1465–1471.
-
^
Papadopoulos SI, Koutlis C, Papadopoulos S, Petrantonakis P. Synthetic misinformers: Generating and combating multimodal misinformation. In: *Proceedings of the 2nd ACM International Workshop on Multimedia AI against Disinformation*. 2023. p. 36–44.
-
^
Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Bengio Y, LeCun Y, editors. *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*; 2015. Available from: http://arxiv.org/abs/1409.1556. [cited 2019 Jul 17]. Source: dblp computer science bibliography.
-
^
Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry G, Askell A, Mishkin P, Clark J, et al. Learning transferable visual models from natural language supervision. In: *International conference on machine learning*. PMLR; 2021. p. 8748-8763.
-
^
Li LH, Yatskar M, Yin D, Hsieh CJ, Chang KW (2019). "VisualBERT: A Simple and Performant Baseline for Vision and Language". *CoRR*. abs/1908.03557. Available from: http://arxiv.org/abs/1908.03557. [cited 2019 Aug 19].
-
^
Sabir E, AbdAlmageed W, Wu Y, Natarajan P. "Deep multimodal image-repurposing detection." In: *Proceedings of the 26th ACM international conference on Multimedia*. 2018. p. 1337–1345.

-
^
Aneja S, Bregler C, Nießner M (2021). "Catching Out-of-Context Misinformation with Self-supervised Learning". *CoRR*. abs/2101.06278. Available from: https://arxiv.org/abs/2101.06278.
-
^
Müller-Budack E, Theiner J, Diering S, Idahl M, Ewerth R (2020). "Multimodal analytics for real-world news using measures of cross-modal entity consistency". *Proceedings of the 2020 international conference on multimedia retrieval*. 2020: 16–25.
-
a, b
Zhang F, Liu J, Zhang Q, Sun E, Xie J, Zha ZJ. "ECENet: Explainable and context-enhanced network for multi-modal fact verification." In: *Proceedings of the 31st ACM International Conference on Multimedia*. 2023. p. 1231-1240.
-
^
Thorne J, Vlachos A. "Evidence-based factual error correction." In: *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*. 2021 Aug; p. 3298-3309. doi:10.18653/v1/2021.acl-long.256. Available from: https://aclanthology.org/2021.acl-long.256.
-
a, b
Chakraborty M, Pahwa K, Rani A, Chatterjee S, Dalal D, Dave H, G R, Gurumurthy P, Mahor A, Mukherjee S, Pakala A, Paul I, Reddy J, Sarkar A, Sensharma K, Chadha A, Sheth A, Das A. "FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering." In: *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*. 2023 Dec; p. 15282-15322. doi:10.18653/v1/2023.emnlp-main.945.
-
a, b
Yao BM, Shah A, Sun L, Cho JH, Huang L (2023). "End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models." In: *Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR '23, New York, NY, USA, pp. 2733–2743. doi:10.1145/3539618.3591879.
-
^
Cao M, Dong Y, Cheung J (2022). "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization". *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*. 3340–3354. doi:10.18653/v1/2022.acl-long.236.
-
^
Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y, Madotto A, Fung P (2023). "Survey of Hallucination in Natural Language Generation". *ACM Comput. Surv.*. 55 (12): 248:1–248:38. doi:10.1145/3571730.

1.
a,&nbsp;b
Xiang C, Wu T, Zhong Z, Wagner DA, Chen D, Mittal P (2024). "Certifiably robust RAG against retrieval corruption". *CoRR*. abs/2405.15556. doi:10.48550/arXiv.2405.15556. ePrint&nbsp;2405.15556.
2.
^
Papadopoulos SI, Koutlis C, Papadopoulos S, Petrantonakis PC (2024). "VERITE: a robust benchmark for multimodal misinformation detection accounting for unimodal bias". *International Journal of Multimedia Information Retrieval*. 13 (1): 4.
3.
^
OpenAI. *Hello {GPT-4o}*, 2024. Accessed: 2024-06-07. Available from: https://openai.com/index/hello-gpt-4o/.
4.
^
Dai W, Li J, Li D, Tiong AMH, Zhao J, Wang W, Li B, Fung P, Hoi SCH. "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning." In: *Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023*, 2023. Available from: https://dblp.org/rec/conf/nips/Dai0LTZW0FH23.bib.

# Researchers

|Full Name|Research Institution|ORCID ID|Research Field|Profile URL|
|---|---|---|---|---|
|Peng Qi|National University of Singapore, Singapore|0000-0001-7747-3293|Computer Science|Profile|
|Wynne Hsu|National University of Singapore, Singapore| | |Profile|
|Mong Li Lee|National University of Singapore, Singapore|0000-0002-9636-388X|Computer Science|Profile|

99818","follow_endpoint":"https:\/\/www.qeios.com\/follow\/user\/
99818","unfollow_endpoint":"https:\/\/www.qeios.com\/unfollow\/user\/
99818","pivot":{"authorable_id":10523,"user_id":99818,"authorable_type":"App\\
Models\\Article","created_at":"2025-02-07T13:53:22.000000Z","updated_at":"2025-
02-07T13:53:22.000000Z","sort_index":3},"research_institution":
{"id":7052,"name":"National University of
Singapore","email_domain":"nus.edu.sg","country_name":"Singapore","city_name":nu
ll,"website":"http:\/\/
www.nus.edu.sg\/","subscription_expires_at":null,"ror_id":"https:\/\/ror.org\/
01tgyzw49","has_active_subscription":false},"subscriptions":
[]}],"publication_type":"article","version_label":"Preprint
v1","newer_published_version_exists":false,"is_versionable":true,"is_reviewable"
:true,"read_url":"https:\/\/www.qeios.com\/read\/WZW860","doi_url":"https:\/\/
doi.org\/10.32388\/WZW860","doi":"10.32388\/
WZW860","doi_for_altmetric_mentions":"10.32388\/WZW860","edit_url":"https:\/\/
www.qeios.com\/compose\/edit\/
WZW860","spotify_podcast_url":null,"domain_name":"Physical
Sciences","field_name":"Computer Science","subfield_name":"Artificial
Intelligence","first_non_preprint_version_published_at":null,"vacant_authors":
[],"used_definitions":[],"ancillary_files":
[],"auth_is_bookmarker":false,"auth_is_admin":false,"auth_can_publish":false,"au
th_can_delete":false,"auth_can_quit":false,"auth_can_modify_authorship":false,"a
uth_can_give_administration":false,"auth_can_create_new_version":false,"all_vers
ions_views_count":153,"all_versions_downloads_count":32,"all_versions_average_ra
ting":null,"all_versions_published_reviewers_count":0,"all_versions_average_subr
ating_for_clarity":null,"all_versions_average_subrating_for_novelty":null,"all_v
ersions_average_subrating_for_impact":null,"is_peer_approved":false,"peer_approv
al_statement":null};</script><script>window.GLOB = window.GLOB ||
{};GLOB.authUser = null;GLOB.userTimezone = "UTC";GLOB.serverTimezone =
"UTC";GLOB.appConfig = {"mailRecipients":
{"support":"info@qeios.com","institutionRequest":"info@qeios.com"},"twitterUsern
ame":"@qeios","issn":"2632-3834"};GLOB.thirdPartyServicesConfig = {"pusher":
{"appKey":"8f3f6208e182be17dd23","appCluster":"eu"}};</script><script>window.GLO
B = window.GLOB || {};GLOB.authUser = null;GLOB.userTimezone =
"UTC";GLOB.serverTimezone = "UTC";GLOB.appConfig = {"mailRecipients":
{"support":"info@qeios.com","institutionRequest":"info@qeios.com"},"twitterUsern
ame":"@qeios","issn":"2632-3834"};GLOB.thirdPartyServicesConfig = {"pusher":
{"appKey":"8f3f6208e182be17dd23","appCluster":"eu"}};</script>
    <script type="text/javascript">
    const Ziggy = {"url":"https:\/\/www.qeios.com","port":null,"defaults":
{},"routes":{"cashier.payment":{"uri":"stripe\/payment\/{id}","methods":
["GET","HEAD"]},"ignition.healthCheck":{"uri":"_ignition\/health-
check","methods":["GET","HEAD"]},"ignition.executeSolution":{"uri":"_ignition\/
execute-solution","methods":["POST"]},"ignition.updateConfig":
{"uri":"_ignition\/update-config","methods":
["POST"]},"postme_subdomain.new_article":{"uri":"\/","methods":
["GET","HEAD"],"domain":"postme.qeios.com"},"postme_subdomain.article_new_versio
n":{"uri":"new-version","methods":
["GET","HEAD"],"domain":"postme.qeios.com"},"postme_subdomain.accept_invitation"
:{"uri":"accept-invitation","methods":
["GET","HEAD"],"domain":"postme.qeios.com"},"accept_to_publish_article":
{"uri":"accept-to-publish-article","methods":["GET","HEAD"]},"terms":
{"uri":"terms","methods":["GET","HEAD"]},"privacy_policy":
{"uri":"privacy","methods":["GET","HEAD"]},"publishing_policy":
{"uri":"publishing-policy","methods":["GET","HEAD"]},"conduct_policy":
{"uri":"conduct-policy","methods":["GET","HEAD"]},"reviewer_guidelines":
{"uri":"reviewer-guidelines","methods":["GET","HEAD"]},"about":
{"uri":"about","methods":["GET","HEAD"]},"editorial_team":
{"uri":"about#editorial-team","methods":["GET","HEAD"]},"plans":
{"uri":"plans","methods":["GET","HEAD"]},"indexing":{"uri":"indexing","methods":
["GET","HEAD"]},"ethics":{"uri":"ethics","methods":
["GET","HEAD"]},"pro_plan_paid_by_funder":{"uri":"pro-plan-paid-by-
funder","methods":["GET","HEAD"]},"peer_approved_articles":{"uri":"peer-

approved-articles","methods":["GET","HEAD"]},"preprint_articles":
{"uri":"preprints","methods":["GET","HEAD"]},"archive":
{"uri":"archive","methods":["GET","HEAD"]},"login":{"uri":"login","methods":
["GET","HEAD"]},"logout":{"uri":"logout","methods":["POST"]},"register":
{"uri":"signup","methods":["GET","HEAD"]},"register.presave_orcid_lacks":
{"uri":"signup\/presave-orcid-lacks","methods":
["POST"]},"register.verification_email_sent":{"uri":"signup\/verification-email-
sent","methods":["GET","HEAD"]},"register.change_email":{"uri":"signup\/change-
email","methods":["GET","HEAD"]},"email.verify":{"uri":"verifyemail\/
{token}","methods":["GET","HEAD"]},"email.verify.resend":{"uri":"resend-
verification-email","methods":["POST"]},"password.request":{"uri":"password\/
reset","methods":["GET","HEAD"]},"password.email":{"uri":"password\/
email","methods":["POST"]},"password.reset":{"uri":"password\/reset\/{token}\/
{email}","methods":["GET","HEAD"]},"authenticate_via_magic_token":
{"uri":"authenticate-via-magic-token","methods":
["GET","HEAD"]},"orcid.authorize":{"uri":"orcid\/authorize","methods":
["GET","HEAD"]},"orcid.authorization_completed":{"uri":"orcid\/authorization-
completed","methods":["GET","HEAD"]},"home":{"uri":"\/","methods":
["GET","HEAD"]},"cortex":{"uri":"cortex","methods":
["GET","HEAD"]},"cortex.submit_request":{"uri":"cortex\/submit-
request","methods":["POST"]},"definitions.get_clusters":{"uri":"definitions\/
get-clusters","methods":["GET","HEAD"]},"definitions.relate":
{"uri":"definitions\/relate\/{definition}\/{cluster?}","methods":
["POST"],"bindings":{"definition":"id"}},"relate_suggestions":{"uri":"relate-
suggestions\/{publication}","methods":["GET","HEAD"],"bindings":
{"publication":"id"}},"profile":{"uri":"profile\/{user?}","methods":
["GET","HEAD"]},"profile.followers":{"uri":"profile\/{user}\/
followers","methods":["GET","HEAD"],"bindings":
{"user":"id"}},"profile.following":{"uri":"profile\/{user}\/
following","methods":["GET","HEAD"],"bindings":{"user":"id"}},"my_subscription":
{"uri":"my-subscription","methods":["GET","HEAD"]},"subscribe_to_personal_pro":
{"uri":"subscribe-to-personal-pro","methods":["GET","HEAD"]},"manage_billing":
{"uri":"manage-billing","methods":["GET","HEAD"]},"settings.account":
{"uri":"settings\/account","methods":["GET","HEAD"]},"settings.account.save":
{"uri":"settings\/account\/save","methods":["POST"]},"settings.notification":
{"uri":"settings\/notification","methods":
["GET","HEAD"]},"settings.notification.save":{"uri":"settings\/notification\/
save","methods":["POST"]},"email_unsubscribe.authoring_request":{"uri":"email-
unsubscribe\/authoring-request\/{user}","methods":["GET","HEAD"],"bindings":
{"user":"id"}},"email_unsubscribe.article_publishing_prompt":{"uri":"email-
unsubscribe\/article-publishing-prompt\/{user}","methods":
["GET","HEAD"],"bindings":{"user":"id"}},"email_unsubscribe.activity":
{"uri":"email-unsubscribe\/activity\/{user}","methods":
["GET","HEAD"],"bindings":{"user":"id"}},"change_email.send":{"uri":"change-
email\/send","methods":["POST"]},"change_email.verify":{"uri":"change-email\/
verify\/{token}","methods":["GET","HEAD"]},"avatar.add":{"uri":"profile\/
{user}\/add-avatar","methods":["POST"],"bindings":
{"user":"id"}},"avatar.remove":{"uri":"profile\/{user}\/remove-
avatar","methods":["POST"],"bindings":
{"user":"id"}},"article_direct_submission.form":{"uri":"article-submission-
form\/{unconfirmedSubmission?}","methods":["GET","HEAD"],"bindings":
{"unconfirmedSubmission":"id"}},"article_direct_submission.confirm":
{"uri":"article-submission-confirm\/{submission}","methods":
["GET","HEAD"],"bindings":{"submission":"id"}},"compose.new_definition":
{"uri":"compose\/new-definition","methods":
["GET","HEAD"]},"compose.new_article":{"uri":"compose\/new-article","methods":
["GET","HEAD"]},"compose.new_review":{"uri":"compose\/new-review","methods":
["GET","HEAD"]},"compose.edit":{"uri":"compose\/edit\/{publication}","methods":
["GET","HEAD"],"bindings":
{"publication":"id"}},"compose.new_review.with_prefilled_reviewable":
{"uri":"compose\/new-review\/with-prefilled-reviewable","methods":
["GET","HEAD"]},"compose.drafts":{"uri":"compose\/drafts\/{type?}","methods":
["GET","HEAD"]},"compose.remove_authors":{"uri":"compose\/remove-authors\/
{publication}","methods":["POST"],"bindings":

{"publication":"id"}},"compose.give_administration":{"uri":"compose\/give-
administration\/{publication}\/{receiver}","methods":["POST"],"bindings":
{"receiver":"id"}},"get_online_writer":{"uri":"compose\/get-online-writer\/
{publication}","methods":["POST"],"bindings":
{"publication":"id"}},"compose.mark_presence":{"uri":"compose\/mark-presence\/
{publication}","methods":["POST"],"bindings":
{"publication":"id"}},"read_latest_version":{"uri":"read\/latest-
{publication}","methods":["GET","HEAD"]},"read":{"uri":"read\/
{publication}","methods":["GET","HEAD"]},"read.pdf":{"uri":"read\/
{publication}\/pdf","methods":["GET","HEAD"]},"read.grammar_correction_diff":
{"uri":"read\/{publication}\/grammar-correction-diff","methods":
["GET","HEAD"]},"publication_ancillary_file":{"uri":"work-supplementary-data\/
{publication}\/{filename}","methods":["GET","HEAD"]},"follow.user":
{"uri":"follow\/user\/{user}","methods":
["GET","HEAD","POST","PUT","PATCH","DELETE","OPTIONS"],"bindings":
{"user":"id"}},"unfollow.user":{"uri":"unfollow\/user\/{user}","methods":
["POST"],"bindings":{"user":"id"}},"search_on_qeios":{"uri":"search","methods":
["GET","HEAD"]},"notifications":{"uri":"notifications","methods":
["GET","HEAD"]},"authoring_request.approve":{"uri":"authoring-requests\/
{authoringRequest}\/approve","methods":
["GET","HEAD","POST","PUT","PATCH","DELETE","OPTIONS"]},"authoring_request.deny"
:{"uri":"authoring-requests\/{authoringRequest}\/deny","methods":
["GET","HEAD","POST","PUT","PATCH","DELETE","OPTIONS"]},"claiming_invitation.acc
ept":{"uri":"claiming\/{invitation}\/accept","methods":
["GET","HEAD","POST","PUT","PATCH","DELETE","OPTIONS"]},"claiming_invitation.dec
line":{"uri":"claiming\/{invitation}\/decline","methods":
["GET","HEAD","POST","PUT","PATCH","DELETE","OPTIONS"]},"request_premium_service
_on_article":{"uri":"request-premium-service-on-article\/{article}","methods":
["GET","HEAD"],"bindings":{"article":"id"}},"reference.search_qeios":
{"uri":"reference\/search","methods":["GET","HEAD"]},"editor_upload.image":
{"uri":"editor-upload\/image","methods":["POST"]},"super_admin.":
{"uri":"admin\/send-reminders-of-acceptances-to-publish-article","methods":
["POST"]},"super_admin.analytics":{"uri":"admin\/analytics","methods":
["GET","HEAD"]},"super_admin.researcher_creation":{"uri":"admin\/users\/
researcher-creation","methods":
["GET","HEAD"]},"super_admin.researcher_creation.create":{"uri":"admin\/users\/
researcher-creation\/create","methods":
["POST"]},"super_admin.verified_researchers":{"uri":"admin\/users\/verified-
researchers","methods":["GET","HEAD"]},"super_admin.unverified_researchers":
{"uri":"admin\/users\/unverified-researchers","methods":
["GET","HEAD"]},"super_admin.create_user_as_stripe_customer":{"uri":"admin\/
users\/add-to-stripe","methods":["GET","HEAD"]},"super_admin.deactivate_user":
{"uri":"admin\/users\/deactivate","methods":
["GET","HEAD"]},"super_admin.publications.unpublished":{"uri":"admin\/
publications\/unpublished\/{publicationType?}","methods":
["GET","HEAD"]},"super_admin.publications.retraction":{"uri":"admin\/
publications\/retraction","methods":
["GET","HEAD"]},"super_admin.publications.xml_files":{"uri":"admin\/
publications\/xml-files","methods":
["GET","HEAD"]},"super_admin.publications.xml_files.download":{"uri":"admin\/
publications\/xml-files\/download","methods":
["GET","HEAD"]},"super_admin.publications.files_for_portico":{"uri":"admin\/
publications\/files-for-portico","methods":
["GET","HEAD"]},"super_admin.publications.files_for_portico.download":
{"uri":"admin\/publications\/files-for-portico\/download","methods":
["GET","HEAD"]},"super_admin.publications.in_external_venues":{"uri":"admin\/
publications\/in-external-venues","methods":
["GET","HEAD"]},"super_admin.publications.in_external_venues.add":
{"uri":"admin\/publications\/in-external-venues\/add","methods":
["POST"]},"super_admin.publications.existing_draft_publishing":{"uri":"admin\/
publications\/publish-existing-draft","methods":
["GET","HEAD"]},"super_admin.publications.update_authors":{"uri":"admin\/
publications\/update-authors","methods":
["GET","HEAD"]},"super_admin.publications.update_affiliations":{"uri":"admin\/

# publications/update-affiliation

|super_admin.publications.update_authorship_with_unstructured_data|{"uri":"admin/publications/update-authorship-with-unstructured-data","methods":["GET","HEAD"]}|
|---|---|
|super_admin.publications.publish_new_version|{"uri":"admin/publications/publish-new-version","methods":["GET","HEAD"]}|
|super_admin.publications.copy_content_between_publications|{"uri":"admin/publications/copy-content-between-publications","methods":["GET","HEAD"]}|
|super_admin.publications.replace_pdf|{"uri":"admin/publications/replace-pdf","methods":["GET","HEAD"]}|
|super_admin.publications.contents_diff|{"uri":"admin/publications/contents-diff","methods":["GET","HEAD"]}|
|super_admin.publications.normalize_typesetting|{"uri":"admin/publications/normalize-typesetting","methods":["GET","HEAD"]}|
|super_admin.publications.normalize_references|{"uri":"admin/publications/normalize-references","methods":["GET","HEAD"]}|
|super_admin.publications.promote_on_social_media|{"uri":"admin/publications/promote-on-social-media","methods":["GET","HEAD"]}|
|super_admin.publications.add_spotify_podcast_link|{"uri":"admin/publications/add-spotify-podcast-link","methods":["GET","HEAD"]}|
|super_admin.publications.communicate_grammar_correction_result|{"uri":"admin/publications/communicate-grammar-correction-result","methods":["GET","HEAD"]}|
|super_admin.publications.send_peer_approval_communications_to_authors|{"uri":"admin/publications/send-peer-approval-communications-to-authors","methods":["GET","HEAD"]}|
|super_admin.article_direct_submissions|{"uri":"admin/article-direct-submissions","methods":["GET","HEAD"]}|
|super_admin.research_institutions.creation|{"uri":"admin/research-institutions/creation","methods":["GET","HEAD"]}|
|super_admin.delete_comment|{"uri":"admin/delete-comment","methods":["GET","HEAD"]}|
|super_admin.authors_of_external_works|{"uri":"admin/authors-of-external-works","methods":["GET","HEAD"]}|
|super_admin.authors_of_external_works.add_many|{"uri":"admin/authors-of-external-works/add-many","methods":["POST"]}|
|super_admin.authors_of_external_works.mark_as_uncontactable|{"uri":"admin/authors-of-external-works/mark-as-uncontactable","methods":["POST"]}|
|super_admin.view_acceptances_to_publish_article|{"uri":"admin/view-acceptances-to-publish-article","methods":["GET","HEAD"]}|
|super_admin.send_reminders_of_acceptances_to_publish_article|{"uri":"admin/send-reminders-of-acceptances-to-publish-article","methods":["GET","HEAD"]}|

function n() {
return u(t, arguments, e(this).constructor);
}

function");if(void 0!==r){if(r.has(t))return r.get(t);r.set(t,n)}function n(){return u(t,arguments,e(this).constructor)}return n.prototype=Object.create(t.prototype,{constructor:{value:n,enumerable:!1,writable:!0,configurable:!0}}),o(n,t)},f(t)}var a=String.prototype.replace,c=/%20/g,l="RFC3986",s={default:l,formatters:{RFC1738:function(t){return a.call(t,c,"+")},RFC3986:function(t){return String(t)}},RFC1738:"RFC1738",RFC3986:l},v=Object.prototype.hasOwnProperty,p=Array.isArray,y=function(){for(var t=[],r=0;r<256;++r)t.push("%"+((r<16?"0":"")+r.toString(16)).toUpperCase());return t}(),d=function(t,r){for(var n=r&&r.plainObjects?Object.create(null):{},e=0;e<t.length;++e)void 0!==t[e]&&(n[e]=t[e]);return n},b={arrayToObject:d,assign:function(t,r){return Object.keys(r).reduce(function(t,n){return t[n]=r[n],t},t)},combine:function(t,r){return[].concat(t,r)},compact:function(t){for(var r=[{obj:{o:t},prop:"o"}],n=[],e=0;e<r.length;++e)for(var o=r[e],i=o.obj[o.prop],u=Object.keys(i),f=0;f<u.length;++f){var a=u[f],c=i[a];"object"==typeof c&&null!==c&&-1===n.indexOf(c)&&(r.push({obj:i,prop:a}),n.push(c))}return function(t){for(;t.length>1;){var r=t.pop(),n=r.obj[r.prop];if(p(n)){for(var e=[],o=0;o<n.length;++o)void 0!==n[o]&&e.push(n[o]);r.obj[r.prop]=e}}}(r),t},decode:function(t,r,n){var e=t.replace(/\+/g," ");if("iso-8859-1"===n)return e.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(e)}catch(t){return e}},encode:function(t,r,n,e,o){if(0===t.length)return t;var i=t;if("symbol"==typeof t?i=Symbol.prototype.toString.call(t):"string"!=typeof t&&(i=String(t)),"iso-8859-1"===n)return escape(i).replace(/%u[0-9a-f]{4}/gi,function(t){return"%26%23"+parseInt(t.slice(2),16)+"%3B"});for(var u="",f=0;f<i.length;++f){var a=i.charCodeAt(f);45===a||46===a||95===a||126===a||a>=48&&a<=57||a>=65&&a<=90||a>=97&&a<=122||o===s.RFC1738&&(40===a||41===a)?u+=i.charAt(f):a<128?u+=y[a]:a<2048?u+=y[192|a>>6]+y[128|63&a]:a<55296||a>=57344?u+=y[224|a>>12]+y[128|a>>6&63]+y[128|63&a]:(a=65536+((1023&a)<<10|1023&i.charCodeAt(f+=1)),u+=y[240|a>>18]+y[128|a>>12&63]+y[128|a>>6&63]+y[128|63&a])}return u},isBuffer:function(t){return!(!t||"object"!=typeof t||!(t.constructor&&t.constructor.isBuffer&&t.constructor.isBuffer(t)))},isRegExp:function(t){return"[object RegExp]"===Object.prototype.toString.call(t)},maybeMap:function(t,r){if(p(t)){for(var n=[],e=0;e<t.length;e+=1)n.push(r(t[e]));return n}return r(t)},merge:function t(r,n,e){if(!n)return r;if("object"!=typeof n){if(p(r))r.push(n);else{if(!r||"object"!=typeof r)return[r,n];(e&&(e.plainObjects||e.allowPrototypes)||!v.call(Object.prototype,n))&&(r[n]=!0)}return r}if(!r||"object"!=typeof r)return[r].concat(n);var o=r;return p(r)&&!p(n)&&(o=d(r,e)),p(r)&&p(n)?(n.forEach(function(n,o){if(v.call(r,o)){var i=r[o];i&&"object"==typeof i&&n&&"object"==typeof n?r[o]=t(i,n,e):r.push(n)}else r[o]=n}),r):Object.keys(n).reduce(function(r,o){var i=n[o];return r[o]=v.call(r,o)?t(r[o],i,e):i,r},o)}},h=Object.prototype.hasOwnProperty,m={brackets:function(t){return t+"[]"},comma:"comma",indices:function(t,r){return t+"["+r+"]"},repeat:function(t){return t}},g=Array.isArray,j=String.prototype.split,w=Array.prototype.push,O=function(t,r){w.apply(t,g(r)?r:[r])},E=Date.prototype.toISOString,R=s.default,S={addQueryPrefix:!1,allowDots:!1,charset:"utf-8",charsetSentinel:!1,delimiter:"&",encode:!0,encoder:b.encode,encodeValuesOnly:!1,format:R,formatter:s.formatters[R],indices:!1,serializeDate:function(t){return E.call(t)},skipNulls:!1,strictNullHandling:!1},T=function t(r,n,e,o,i,u,f,a,c,l,s,v,p,y){var d,h=r;if("function"==typeof f?h=f(n,h):h instanceof Date?h=l(h):"comma"===e&&g(h)&&(h=b.maybeMap(h,function(t){return t instanceof Date?l(t):t})),null===h){if(o)return u&&!p?u(n,S.encoder,y,"key",s):n;h=""}if("string"==typeof(d=h)||"number"==typeof d||"boolean"==typeof d||"symbol"==typeof d||"bigint"==typeof d||b.isBuffer(h)){if(u){var m=p?n:u(n,S.encoder,y,"key",s);if("comma"===e&&p){for(var w=j.call(String(h),","),E="",R=0;R<w.length;++R)E+=(0===R?"":",")+v(u(w[R],S.encoder,y,"value",s));return[v(m)+"="+E]}return[v(m)+"="+v(u(h,S.encoder,y,"value",s))]}return[v(n)+"="+v(String(h))]}var T,k=[];if(void 0===h)return k;if("comma"===e&&g(h))T=[{value:h.length>0?

h.join(",")||null:void 0}];else if(g(f))T=f;else{var x=Object.keys(h);T=a?
x.sort(a):x}for(var N=0;N<T.length;++N){var C=T[N],D="object"==typeof C&&void 0!
==C.value?C.value:h[C];if(!i||null!==D){var F=g(h)?"function"==typeof e?
e(n,C):n:n+(c?"."+C:"["+C+"]");O(k,t(D,F,e,o,i,u,f,a,c,l,s,v,p,y))}}return
k},k=Object.prototype.hasOwnProperty,x=Array.isArray,N={allowDots:!
1,allowPrototypes:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!
1,decoder:b.decode,delimiter:"&",depth:5,ignoreQueryPrefix:!
1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!
1,strictNullHandling:!1},C=function(t){return
t.replace(/&#(\d+);/g,function(t,r){return
String.fromCharCode(parseInt(r,10))})},D=function(t,r){return
t&&"string"==typeof t&&r.comma&&t.indexOf(",")>-1?
t.split(","):t},F=function(t,r,n,e){if(t){var o=n.allowDots?t.replace(/\.([^.[]
+)/g,"[$1]"):t,i=/(\[[^[\]]*])/g,u=n.depth>0&&/(\[[^[\]]*])/.exec(o),f=u?
o.slice(0,u.index):o,a=[];if(f){if(!
n.plainObjects&&k.call(Object.prototype,f)&&!
n.allowPrototypes)return;a.push(f)}for(var c=0;n.depth>0&&null!
==(u=i.exec(o))&&c<n.depth;){if(c+=1,!
n.plainObjects&&k.call(Object.prototype,u[1].slice(1,-1))&&!
n.allowPrototypes)return;a.push(u[1])}return u&&a.push("["+o.slice(u.index)
+"]"),function(t,r,n,e){for(var o=e?r:D(r,n),i=t.length-1;i>=0;--i){var
u,f=t[i];if("[]"===f&&n.parseArrays)u=[].concat(o);else{u=n.plainObjects?
Object.create(null):{};var a="["===f.charAt(0)&&"]"===f.charAt(f.length-1)?
f.slice(1,-1):f,c=parseInt(a,10);n.parseArrays||""!==a?!isNaN(c)&&f!
==a&&String(c)===a&&c>=0&&n.parseArrays&&c<=n.arrayLimit?(u=[])
[c]=o:"__proto__"!==a&&(u[a]=o):u={0:o}}o=u}return o}(a,r,n,e)}},$=function(t,r)
{var n=function(t){if(!t)return N;if(null!=t.decoder&&"function"!=typeof
t.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!
==t.charset&&"utf-8"!==t.charset&&"iso-8859-1"!==t.charset)throw new
TypeError("The charset option must be either utf-8, iso-8859-1, or
undefined");return{allowDots:void 0===t.allowDots?N.allowDots:!!
t.allowDots,allowPrototypes:"boolean"==typeof t.allowPrototypes?
t.allowPrototypes:N.allowPrototypes,arrayLimit:"number"==typeof t.arrayLimit?
t.arrayLimit:N.arrayLimit,charset:void 0===t.charset?
N.charset:t.charset,charsetSentinel:"boolean"==typeof t.charsetSentinel?
t.charsetSentinel:N.charsetSentinel,comma:"boolean"==typeof t.comma?
t.comma:N.comma,decoder:"function"==typeof t.decoder?
t.decoder:N.decoder,delimiter:"string"==typeof t.delimiter||
b.isRegExp(t.delimiter)?t.delimiter:N.delimiter,depth:"number"==typeof
t.depth||!1===t.depth?+t.depth:N.depth,ignoreQueryPrefix:!
0===t.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof
t.interpretNumericEntities?
t.interpretNumericEntities:N.interpretNumericEntities,parameterLimit:"number"==t
ypeof t.parameterLimit?t.parameterLimit:N.parameterLimit,parseArrays:!1!
==t.parseArrays,plainObjects:"boolean"==typeof t.plainObjects?
t.plainObjects:N.plainObjects,strictNullHandling:"boolean"==typeof
t.strictNullHandling?t.strictNullHandling:N.strictNullHandling}}(r);if(""===t||
null==t)return n.plainObjects?Object.create(null):{};for(var e="string"==typeof
t?function(t,r){var
n,e={},o=(r.ignoreQueryPrefix?t.replace(/^\?/,""):t).split(r.delimiter,Infinity=
==r.parameterLimit?void 0:r.parameterLimit),i=-
1,u=r.charset;if(r.charsetSentinel)for(n=0;n<o.length;+
+n)0===o[n].indexOf("utf8=")&&("utf8=%E2%9C%93"===o[n]?u="utf-8":"utf8=
%26%2310003%3B"===o[n]&&(u="iso-8859-1"),i=n,n=o.length);for(n=0;n<o.length;+
+n)if(n!==i){var f,a,c=o[n],l=c.indexOf("]="),s=-1===l?c.indexOf("="):l+1;-
1===s?(f=r.decoder(c,N.decoder,u,"key"),a=r.strictNullHandling?null:""):
(f=r.decoder(c.slice(0,s),N.decoder,u,"key"),a=b.maybeMap(D(c.slice(s+1),r),func
tion(t){return
r.decoder(t,N.decoder,u,"value")})),a&&r.interpretNumericEntities&&"iso-8859-
1"===u&&(a=C(a)),c.indexOf("[]=")>-1&&(a=x(a)?[a]:a),e[f]=k.call(e,f)?
b.combine(e[f],a):a}return e}(t,n):t,o=n.plainObjects?Object.create(null):
{},i=Object.keys(e),u=0;u<i.length;++u){var f=i[u],a=F(f,e[f],n,"string"==typeof
t);o=b.merge(o,a,n)}return b.compact(o)},A=/*#__PURE__*/function(){function
 t(t,r,n){var e,o;this.name=t,this.definition=r,this.bindings=null!

(e=r.bindings)?e:{},this.wheres=null!=(o=r.wheres)?o:{},this.config=n}var n=t.prototype;return n.matchesUrl=function(t){var r=this;if(!this.definition.methods.includes("GET"))return!1;var n=this.template.replace(/(\/?){([^}?]*)(\??)}/g,function(t,n,e,o){var i,u="(?<"+e+">"+((null==(i=r.wheres[e])?void 0:i.replace(/(^\^)|(\$$)/g,""))||"[^/?]+")+")";return  o?"("+n+u+")?":""+n+u}).replace(/^\w+:\/\//,""),e=t.replace(/^w+:\/\//,"").split("?"),o=e[0],i=e[1],u=new RegExp("^"+n+"/?$").exec(o);return!!u&&{params:u.groups,query:$(i)}},n.compile=function(t){var r=this,n=this.parameterSegments;return n.length?this.template.replace(/{([^}?]+)(\??)}/g,function(e,o,i){var u,f,a;if(!i&&[null,void 0].includes(t[o]))throw new Error("Ziggy error: '"+o+"' parameter is required for route '"+r.name+"'.");if(n[n.length-1].name===o&&".*"===r.wheres[o])return encodeURIComponent(null!=(a=t[o])?a:"").replace(/%2F/g,"/");if(r.wheres[o]&&!new RegExp("^"+(i?"("+r.wheres[o]+")?":r.wheres[o])+"$").test(null!=(u=t[o])?u:""))throw  new Error("Ziggy error: '"+o+"' parameter does not match required format '"+r.wheres[o]+"' for route '"+r.name+"'.");return encodeURIComponent(null!=(f=t[o])?f:"")}).replace(/\/+$/,""):this.template},r(t,[{key:"template",get:function(){return((this.config.absolute?this.definition.domain?""+this.config.url.match(/^\w+:\/\//)[0]+this.definition.domain+(this.config.port?":"+this.config.port:""):this.config.url:"")+"/"+this.definition.uri).replace(/\/+$/,"")}},{key:"parameterSegments",get:function(){var t,r;return null!=(t=null==(r=this.template.match(/{[^}?]+\??}/g))?void 0:r.map(function(t){return{name:t.replace(/{|\??}/g,""),required:!/\?}$/.test(t)}}))?t:[]}}]),t}(),P=/*#__PURE__*/function(t){var e,i;function u(r,e,o,i){var u;if(void 0===o&&(o=!0),(u=t.call(this)||this).t=null!=i?i:"undefined"!=typeof Ziggy?Ziggy:null==globalThis?void 0:globalThis.Ziggy,u.t=n({},u.t,{absolute:o}),r){if(!u.t.routes[r])throw new Error("Ziggy error: route '"+r+"' is not in the route list.");u.i=new A(r,u.t.routes[r],u.t),u.u=u.l(e)}return u}i=t,(e=u).prototype=Object.create(i.prototype),e.prototype.constructor=e,o(e,i);var f=u.prototype;return f.toString=function(){var t=this,r=Object.keys(this.u).filter(function(r){return!t.i.parameterSegments.some(function(t){return t.name===r})}).filter(function(t){return"_query"!==t}).reduce(function(r,e){var o;return n({},r,((o={})[e]=t.u[e],o))},{});return this.i.compile(this.u)+function(t,r){var n,e=t,o=function(t){if(!t)return S;if(null!=t.encoder&&"function"!=typeof t.encoder)throw new TypeError("Encoder has to be a function.");var r=t.charset||S.charset;if(void 0!==t.charset&&"utf-8"!==t.charset&&"iso-8859-1"!==t.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var n=s.default;if(void 0!==t.format){if(!h.call(s.formatters,t.format))throw new TypeError("Unknown format option provided.");n=t.format}var e=s.formatters[n],o=S.filter;return("function"==typeof t.filter||g(t.filter))&&(o=t.filter),{addQueryPrefix:"boolean"==typeof t.addQueryPrefix?t.addQueryPrefix:S.addQueryPrefix,allowDots:void 0===t.allowDots?S.allowDots:!!t.allowDots,charset:r,charsetSentinel:"boolean"==typeof t.charsetSentinel?t.charsetSentinel:S.charsetSentinel,delimiter:void 0===t.delimiter?S.delimiter:t.delimiter,encode:"boolean"==typeof t.encode?t.encode:S.encode,encoder:"function"==typeof t.encoder?t.encoder:S.encoder,encodeValuesOnly:"boolean"==typeof t.encodeValuesOnly?t.encodeValuesOnly:S.encodeValuesOnly,filter:o,format:n,formatter:e,serializeDate:"function"==typeof t.serializeDate?t.serializeDate:S.serializeDate,skipNulls:"boolean"==typeof t.skipNulls?t.skipNulls:S.skipNulls,sort:"function"==typeof t.sort?t.sort:null,strictNullHandling:"boolean"==typeof t.strictNullHandling?t.strictNullHandling:S.strictNullHandling}}(r);"function"==typeof o.filter?e=(0,o.filter)("",e):g(o.filter)&&(n=o.filter);var i=[];if("object"!=typeof e||null===e)return"";var u=m[r&&r.arrayFormat in m?r.arrayFormat:r&&"indices"in r?r.indices?"indices":"repeat":"indices"];n||(n=Object.keys(e)),o.sort&&n.sort(o.sort);for(var f=0;f<n.length;++f){var a=n[f];o.skipNulls&&null===e[a]||O(i,T(e[a],a,u,o.strictNullHandling,o.skipNulls,o.encode?o.encoder:null,o.filter,o.sort,o.allowDots,o.serializeDate,o.format,o.formatter,

o.encodeValuesOnly,o.charset))}var c=i.join(o.delimiter),l=!
0===o.addQueryPrefix?"?":"";return o.charsetSentinel&&(l+="iso-8859-
1"===o.charset?"utf8=%26%2310003%3B&":"utf8=%E2%9C%93&"),c.length>0?l+c:""}
(n({},r,this.u._query),{addQueryPrefix:!
0,arrayFormat:"indices",encodeValuesOnly:!0,skipNulls:!0,encoder:function(t,r)
{return"boolean"==typeof t?Number(t):r(t)}})},f.v=function(t){var r=this;t?
this.t.absolute&&t.startsWith("/")&&(t=this.p().host+t):t=this.h();var
e={},o=Object.entries(this.t.routes).find(function(n){return e=new
A(n[0],n[1],r.t).matchesUrl(t)})||[void 0,void 0];return n({name:o[0]},e,
{route:o[1]})},f.h=function(){var
t=this.p(),r=t.pathname,n=t.search;return(this.t.absolute?
t.host+r:r.replace(this.t.url.replace(/^\w*:\/\/[^/]+/,""),"").replace(/^\/
+/,"/"))+n},f.current=function(t,r){var
e=this.v(),o=e.name,i=e.params,u=e.query,f=e.route;if(!t)return o;var a=new
RegExp("^"+t.replace(/\./g,"\\.").replace(/\*/g,".*")+"$").test(o);if([null,void
0].includes(r)||!a)return a;var c=new A(o,f,this.t);r=this.l(r,c);var
l=n({},i,u);return!(!Object.values(r).every(function(t){return!t})||
Object.values(l).some(function(t){return void 0!==t}))||
Object.entries(r).every(function(t){return l[t[0]]==t[1]})},f.p=function(){var
t,r,n,e,o,i,u="undefined"!=typeof window?window.location:
{},f=u.host,a=u.pathname,c=u.search;return{host:null!
=(t=null==(r=this.t.location)?void 0:r.host)?t:void 0===f?"":f,pathname:null!
=(n=null==(e=this.t.location)?void 0:e.pathname)?n:void 0===a?"":a,search:null!
=(o=null==(i=this.t.location)?void 0:i.search)?o:void
0===c?"":c}},f.has=function(t){return
Object.keys(this.t.routes).includes(t)},f.l=function(t,r){var e=this;void
0===t&&(t={}),void 0===r&&(r=this.i),t=["string","number"].includes(typeof t)?
[t]:t;var o=r.parameterSegments.filter(function(t){return!
e.t.defaults[t.name]});if(Array.isArray(t))t=t.reduce(function(t,r,e){var
i,u;return n({},t,o[e]?((i={})[o[e].name]=r,i):"object"==typeof r?r:((u={})
[r]="",u))},{});else if(1===o.length&&!
t[o[0].name]&&(t.hasOwnProperty(Object.values(r.bindings)[0])||
t.hasOwnProperty("id"))){var i;(i={})[o[0].name]=t,t=i}return
n({},this.m(r),this.g(t,r))},f.m=function(t){var r=this;return
t.parameterSegments.filter(function(t){return
r.t.defaults[t.name]}).reduce(function(t,e,o){var i,u=e.name;return n({},t,
((i={})[u]=r.t.defaults[u],i))},{})},f.g=function(t,r){var
e=r.bindings,o=r.parameterSegments;return Object.entries(t).reduce(function(t,r)
{var i,u,f=r[0],a=r[1];if(!a||"object"!=typeof a||Array.isArray(a)||!
o.some(function(t){return t.name===f}))return n({},t,((u={})[f]=a,u));if(!
a.hasOwnProperty(e[f])){if(!a.hasOwnProperty("id"))throw new Error("Ziggy error:
object passed as '"+f+"' parameter is missing route model binding key '"+e[f]
+"'.");e[f]="id"}return n({},t,((i={})[f]=a[e[f]],i))},{})},f.valueOf=function()
{return this.toString()},f.check=function(t){return this.has(t)},r(u,
[{key:"params",get:function(){var t=this.v();return
n({},t.params,t.query)}}]),u}(/*#__PURE__*/f(String));return function(t,r,n,e)
{var o=new P(t,r,n,e);return t?o.toString():o}});
</script>
    <script  src="/js/app.js?id=0fddab3f6bf71c68a0d6f57cd050a315"></script>
    <!-- In  order to show math formulas that need MathJax to be rendered
(because they are created with MathJax) -->
    <script  type="text/x-mathjax-config">MathJax.Hub.Config({ messageStyle:
'none' });</script>
    <script
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-
AMS_HTML"></script>
    <!-- Embedly setup for conversion of media embedded in publication content
-->
    <script  charset="utf-8"
src="https://cdn.embedly.com/widgets/platform.js"></script>
    <script  src="https://www.qeios.com/js/lib/convert-oembed.js"></script>

<script
src="https://www.qeios.com/js/lib/jquery-collapser/jquery.collapser.min.js"></
script>
        <!-- Altmetric Embed -->
    <script type='text/javascript'
src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
        <script src="/js/pages/read/article.js?
id=d1be8023ab27f6f5966fff5281584b52"></script>
</body>
</html>

