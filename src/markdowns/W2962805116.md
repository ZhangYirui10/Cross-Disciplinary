# BiRank: Towards Ranking on Bipartite Graphs

# Xiangnan He, Ming Gao Member, IEEE, Min-Yen Kan Member, IEEE and Dingxian Wang

Abstract—The bipartite graph is a ubiquitous data structure that can model the relationship between two entity types: for instance, users and items, queries and webpages. In this paper, we study the problem of ranking vertices of a bipartite graph, based on the graph’s link structure as well as prior information about vertices (which we term a query vector). We present a new solution, BiRank, which iteratively assigns scores to vertices and finally converges to a unique stationary ranking. In contrast to the traditional random walk-based methods, BiRank iterates towards optimizing a regularization function, which smooths the graph under the guidance of the query vector. Importantly, we establish how BiRank relates to the Bayesian methodology, enabling the future extension in a probabilistic way. To show the rationale and extendability of the ranking methodology, we further extend it to rank for the more generic n-partite graphs. BiRank’s generic modeling of both the graph structure and vertex features enables it to model various ranking hypotheses flexibly. To illustrate its functionality, we apply the BiRank and TriRank (ranking for tripartite graphs) algorithms to two real-world applications: a general ranking scenario that predicts the future popularity of items, and a personalized ranking scenario that recommends items of interest to users. Extensive experiments on both synthetic and real-world datasets demonstrate BiRank’s soundness (fast convergence), efficiency (linear in the number of graph edges) and effectiveness (achieving state-of-the-art in the two real-world tasks).

Index Terms—Bipartite graph ranking, graph regularization, n-partite graphs, popularity prediction, personalized recommendation.

# 1 INTRODUCTION

Graphs provide a universal language to represent relationships between entities. In real-world applications, not only should the relationships between entities of the same type be considered, but the relationships between different types of entities should also be modeled. Such relationships naturally form a bipartite graph, containing rich information to be mined from. For example, in YouTube, the videos and users form a bipartite relationship where edges indicate a viewing action; in Web search, the relationships between queries and search engine result pages are user actions (“clicks”), which provide important relevance judgments from the user’s perspective.

A fundamental task in the mining of bipartite graphs is to rank vertices against a specific criterion. Depending on the setting, assigning each vertex a ranking score can be used for many tasks, including the estimation of vertex importance (popularity prediction) and the inference of similar vertices to a target vertex (similarity search), and edge suggestion for connecting a target vertex (link prediction and recommendation). Existing work on graph ranking have largely focused on unipartite graphs, including PageRank, HITS, and many of their variants. Although several works have considered ranking on bipartite graphs, they have either focused on a specific application or adapted existing algorithms to handle the bipartite case. In our opinion, the work up to the current time, lacks a thorough theoretical analysis.

In this paper, we focus on the problem of ranking vertices of bipartite graphs. We formulate the ranking problem in a generic manner – accounting for both the graph’s structural information and the proper incorporation of any prior information for vertices, where such vertex priors can be used to encode any features of vertices. The main contributions of this paper are summarized as follows:

- We develop a new algorithm – BiRank – for addressing the ranking problem on bipartite graphs, and show its convergence to a unique stationary point;
- We analyze BiRank through the formalism of graph regularization, and present a complementary Bayesian view. These two views enable future extensions to be grounded and compelling from a theoretically principled way (either algebraically or probabilistically).
- We deploy BiRank to the general ranking scenario of item popularity prediction, illustrating how to parameterize it to encode several ranking hypotheses;
- We extend the methodology to rank on the more generic n-partite graphs, and employ it for a personalized ranking scenario by mining tripartite graphs.
- We conduct extensive experiments to justify our methods for the two real-world ranking scenarios of popularity prediction and personalized recommendation.

The paper is organized as follows. After reviewing related works in Section 2, we formalize the problem in Section 3. Then we describe the BiRank algorithm in Section 4, and interpret it from two views in Section 5. In Section 6, we discuss how to apply BiRank to popularity prediction and personalized recommendation. We conduct experiments in Section 7, before concluding the paper in Section 8.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 2 RELATED WORK

Agarwal [16] further extended the regularization framework BiRank, which ranks vertices of a bipartite graph, can be categorized as a link-based object ranking method under the paradigm of link mining [11]. In this section, we focus on related work that contribute in the ranking method, and omit discussion of other relevant issues such as efficiency and evolving graphs. We then review work that can benefit from such bipartite graph ranking, forming the potential downstream applications of BiRank.

# 2.1 Graph Ranking Methods

In the context of web graph ranking, PageRank [2] and HITS [3] are the most prominent methods. PageRank estimates the importance score of vertices as the stationary distribution of a random walk process – starting from a vertex, the surfer randomly jumps to a neighbor vertex according to the edge weight. HITS assumes each vertex has two roles: hub and authority, transforming the graph to a bipartite graph. A vertex has a high authority score if it is linked by many vertices with hub score, and a vertex has a high hub score if it links to many authoritative vertices.

Many variants start from the basic themes of PageRank and HITS. Ng et al. [12] studied the stability of the two algorithms, finding HITS more sensitive to small perturbations in the graph structure under certain situations. They proposed two variants — Randomized HITS and Subspace HITS — that yield more stable rankings. Similarly, Lempel et al. [5] found that applying HITS on graphs with TKCs (tightly knit communities, i.e., small but highly interconnected set of vertices) fails to identify meaningful authority vertices. They devised SALSA as a stochastic variant of HITS, for alleviating the TKC effect. Haveliwala [4] proposed topic-sensitive PageRank (also known as personalized PageRank) by replacing the uniform teleportation vector with a non-uniform vector that encodes each vertex’s topic score (cf. query vector in our BiRank context). Later on, Ding et al. [13] unified HITS and PageRank under a normalized ranking framework. Inspired by the discrete-time Markov process explanation of PageRank, Liu et al. [6] also proposed BrowseRank based on continuous time Markov process, exploiting user behavior data for page importance ranking. To incorporate side information on vertices and edges into ranking, Gao et al. [7] extended PageRank in a semi-supervised way by learning the transition matrix based on the features on vertices and edges.

Similar to their ranking algorithms, our proposed BiRank is also a propagation-based method; however, the main difference lies in the normalization strategy used in the iterative process. The symmetric normalization used in BiRank normalizes an edge weight by both of its vertex ends, which accords a smoothing on the graph that can be explained by the regularization theory [18]. As a result, extensions to the algorithm, such as incorporating more features about vertices and edges, can be achieved in a theoretically principled way. More importantly, we believe such a bridge with the graph regularization theory and Bayesian framework allows BiRank a broader algorithmic extensions that are difficult to achieve by PageRank, HITS and their variants.

# 2.2 Ranking on Bipartite Graphs

There are other algorithms developed for bipartite graph ranking that target specific applications. As a natural way to represent relationship between two types of entities, bipartite graphs have been widely used across domains. As a consequence, ranking on bipartite graph data have been explored to address many applications. For example, in Web search, Deng et al. [8] modeled queries and URLs for query suggestion, Cao et al. [20] considered the co-occurrence between entities and queries for entity ranking, Li et al. [10] modeled users and their search sessions for detecting click spam, and Rui et al. [21] mined visual features and the surrounding texts for Web image annotation. In practical recommender systems, bipartite graphs methods have been used for Twitter user recommendation [22] and YouTube video recommendation [23]. In the domain of natural language processing, Parveen et al. [24] generated multi-document summarization based on the relationship of sentences and lexical entities.

In terms of the ranking technique, these works share the same cornerstone — they all rank by iteratively propagating scores on the graph; either through a PageRank-like random walk or a HITS-like iterative process – which is adjusted for use on bipartite graphs. The prominent advantage of such propagation-based methods is that the global structure of the graph can be implicitly considered, which is an effective way to deal with the data sparsity and make use of the graph structure.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 3 PROBLEM FORMULATION

We first present the bipartite graph model and then give the notation convention used. We then formalize the ranking problem that we address in this paper.

# Notations.

Let G = (U ∪ P, E) be a bipartite graph, where U and P represent the vertex sets of the two entity types respectively, and E represents the edge set (n.b., bipartite graphs have edges only between vertices of the two different types). Figure 1 shows an example of the bipartite structure.

|u₁|u₂|u₃|… …| |
|---|---|---|---|---|
|w|w₁₃|w₂₂|w₃₂|w|
|w₁₂| |w₃₃| | |
|p₁|p₂|p₃|… …| |

We use u to denote the i-th vertex in U, and p to denote the j-th vertex in P, where 1 ≤ i ≤ |U| and 1 ≤ j ≤ |P|; set cardinality |U| denotes number of elements in U. Edges carry non-negative weights wij, modeling the relationship strength between the connected vertices ui and pj (if ui and pj are not connected, their edge weight wij is zero). As such, we can represent all edge weights of the graph as a |U| × |P| matrix W = [wij]. For each vertex ui, we denote its weighted degree (i.e., sum of connected edges’ weights) as di, and use a diagonal matrix Du to denote the weighted degrees of all vertices in U such that (Du)ii = di; and similarly, for dj and Dp. Note that in this paper, we deal with undirected bipartite graphs, i.e., we do not model any directionality in the edges.

# Problem Definition.

In a nutshell, the general graph ranking problem is to assign each vertex a score s.t. a given expectation is satisfied. For example, PageRank [2] infers an importance score for each vertex to capture the intuition that an important vertex should be linked by many other important vertices. As in many applications, a ranking simply based on the graph structure is insufficient; often, there also exists some prior information (or features) on the vertices. For example, in webpage ranking, we already know some webpages are important (e.g., official sites), and wish to incorporate this knowledge into the ranking process; in the application of recommendation, we need to consider a user’s historical actions as the prior knowledge of the user’s preference. We term such prior knowledge as a query vector, which encodes the prior belief of the score of vertices with respect to the ranking criterion. In this paper, we study the bipartite graph ranking problem where a query vector is given, formally defined as:

Input: A bipartite graph G = (U ∪ P, E) with its weight matrix W. A query vector u0, p0 encodes the prior belief concerning the vertices in U and P, respectively, with respect to the ranking criterion.

Output: A function f : P ∪ U → R, which maps each vertex in G to a real number. The function value f(ui) and f(pj) form the ranking score of vertex ui and pj, respectively. To keep the notation simple, we also use ui and pj to denote the ranking score.

To account for the query vector p0 and u0 that encode the prior belief on the importance of the vertices, one can either opt for 1) incorporating the graph ranking results for combination in post-processing (a.k.a late fusion), or 2) factoring the query vector directly into the ranking process. The first way of post-processing yields a ranking that is a compromise between two rankings; for scenarios that the query vector defines a full ranking of vertices, this ensemble approach might be suitable. However, when the query vector only provides partial information – i.e., only a small portion of the vertices are specified – the ranking process needs to be adjusted accordingly.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# Algorithm 1: The Iterative BiRank Algorithm

Input: Weight matrix W, query vector p0, u0, and hyper-parameters α, β;

Output: Ranking vectors p, u;

Symmetrically normalize W: S = D−1 W D−1;

1. Randomly initialize p and u; u2 p2
2. while Stopping criteria is not met do
3. p ← αST u + (1 − α) p0;
4. u ← βS p + (1 − β) u0;
5. end
6. return p and u

Moreover, our empirical experience shows that BiRank has a very fast convergence rate — 10 iterations are usually enough for convergence. One reason is that it can be seen as optimizing a convex function effectively using alternating optimization, discussed later in Section 5.

# 4.2 Convergence Analysis of BiRank

We show that BiRank can converge to a stationary and unique solution regardless of the initialization, followed by a theoretical analysis of the convergence speed.

# 4.2.1 Proof of Convergence

It is clear that the behavior of BiRank depends on the hyper-parameters α and β, which are in the range [0,1]. To make a thorough analysis, we need to carefully consider the boundary conditions. Considering the two boundaries 0 and 1, we divide the proof into the following three cases:

Proof.

1. α = 0 or β = 0. When α = 0, the vector p = p0 is unchanged across iterations. Thus u, which depends on p and u0, will also be unchanged after the first iteration. Similarly for the case of β = 0.
2. α = 1 and β = 1. In this case, the query vectors do not have any impact on the ranking, and the ranking is solely determined by the graph structure. The iterative update rule then reduces to Eq. (1), whose matrix form is p = ST u, u = S p. By further reducing this, we obtain:
3. p(k) = (STS)p(k−1) = ... = (STS)kp(0),

u(k) = (SST)u(k−1) = ... = (SST)ku(0),

Normal cases. We now consider the normal ranking scenarios that α and β are in the range of (0,1), or one of α, β is 1, meaning that both the graph structure and query vectors can affect the ranking process. Without loss of generality, we prove the convergence of p.

# 4.1.1 Time Complexity Analysis

It is easy to show that a direct implementation of BiRank iteration in Eq. (3) has a time complexity of O(|P| · |U|), mainly due to the multiplication of ST u and S p. However, note that in real-world applications, the matrix S is typically very sparse; for example in recommender systems, the user–item matrix to model is always over 99% sparse (e.g., Netflix challenge dataset). In this case, a representation of sparse matrix only needs to account for non-zero entries (which correspond to the edges of the bipartite graph), instead of all |P| · |U| entries. As such, the real-time cost needed for BiRank is O(c|E|), where c denotes the number of iterations executed to converge, and |E| denotes number of edges in the graph. Thus, BiRank is linear with respect to number of edges, ensuring good scalability to large-scale graphs.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

First, we eliminate u in p’s update rule:

p = αβ(STS) p + α(1 − β)ST u0 + (1 − α)p0. (5)

Let matrix M be αβ(STS) and vector z0 be α(1 − β)ST u0 + (1 − α) p0, which are both invariant across iterations. Then, we have:

p(k) = Mp(k−1) + z0 = ... = Mkp(0) + ∑t=0k−1 Mtz0, (6)

where k denotes the number of iterations, and p(0) denotes the initial ranking vector of p. Assuming M’s eigenvalues are in the range of (-1, 1), we can obtain:

limk→∞ Mkp(0) = 0, and limk→∞ ∑t=0k−1 Mt = (I − M)−1.

where I denotes the identity matrix. In this case, we can derive the stationary solution of p as:

p* = (I − M)−1z0. (7)

However, the above stationary solution is derived based on the assumption that M’s eigenvalues are in the range (-1,1). Next, we prove the correctness of this assumption.

# Theorem 1.

M’s eigenvalues are in the range [−αβ, αβ].

# Proof.

Recall that M is defined as:

M = αβ(STS) = αβ(D−1 W D−1)T(D−1 W D−1)

= αβ(D−1 WTD−1 W D−1).

To see M’s eigenvalues are bounded by αβ, we first construct a variant Mv that has the same eigenvalues2 as M:

Mv = D1 M D−1 = αβ(WTD−1W D−1).

Note that matrix WT D−1W D−1 is a stochastic matrix in which the entries of each column sum to 1. By standard linear algebra [25], for a stochastic matrix, its largest absolute value of the eigenvalues is always 1. Thus, the eigenvalues of Mv are in the range [−αβ, αβ], and same must hold for M as they have exactly the same eigenvalues. The proof of M’s eigenvalues is finished.

As M’s eigenvalues are theoretically guaranteed in the range [−αβ, αβ] and in the normal cases α, β are in the range (0,1), the assumption that M’s eigenvalues are in the range (−1,1) holds. Therefore, we conclude that Eq. (7) indeed forms the stationary solution of p. To round out the proof, we give the stationary solution of BiRank as follows:

p* = (I − αβST S)−1[α(1 − β)ST u0 + (1 − α)p0],

u* = (I − αβSST)−1[β(1 − α)S p0 + (1 − β) u0.

The convergence proof of BiRank is finished. This convergence proof gives an elegant closed-form solution – for any non-trivial initializations, the iterative algorithm of BiRank will converge to Eq. (8). As such, an alternative method to our iterative BiRank is to direct calculate the closed-form stationary solution. Even so, we suggest the practitioners following the iterative procedure for two reasons. First, in real-world practice, when there is a large number of vertices to rank, the matrix inversion operation is very expensive, making the calculation of the closed-form solution inefficient. More specifically, matrix inversion is usually assumed to take O(N3) time [26]; thus the time complexity of directly calculating the stationary solution is O(|P|3 + |U|3), which even can be much higher than the upper bound of the iterative solution O(c|P||U|). Second, the iterative process emphasizes the underlying motivation that reinforcing a vertex’s importance from its neighbors and the query vector. As such, one does not have to run the iterations until convergence; instead, one can compute the scores by starting from any initialization and performing a fixed number of iterations.

# 4.2.2 Speed of Convergence

Since the behavior of BiRank depends on the graph structure, query vector and hyper-parameters α, β, we analyze how do these factors impact BiRank’s convergence speed.

In each BiRank iteration, the score of a vertex comes from both its neighbors and the query vector. Since the query vector is static that remains unchanged across iterations, it cannot contribute to any form of divergence in the rankings; thus the main uncertainty for convergence stems from the part of the score diffusion from neighbors. As such, the number of iterations required to converge will increase as α and β increase (the empirical evidence in Figure 6 also verifies this property). Clearly, the slowest convergence is when α and β are set to 1, where the effect of query vector is eliminated. When both α and β are set to 1, the update of p at the iteration k can be written as: p(k) = (STS)p(k−1), which essentially can be seen as the power method for the symmetric eigenvalue problem (Chapter 8.2 of [25]). It is known that the convergence of power method is determined by the second dominant eigenvalue of the transition matrix. In spite of the slight difference that the power iteration requires an additional L2 normalization on the ranking vector (while our BiRank does not), we point out that BiRank shares the same property of convergence speed.

# Theorem 2.

The convergence rate of BiRank depends on the second largest eigenvalue of the matrix ST S in magnitude.

# Proof.

As ST S is a symmetric matrix, it is guaranteed to have n eigenvalues which are real numbers (n = |P|). Let its eigenvalues be λ1, λ2, ..., λn, where |λ1| ≥ |λ2| ≥ ... ≥ |λn|, and vectors x1, x2, ..., xn be the corresponding eigenvectors. Then, the starting vector p(0) can be expressed as:

p(0) = ∑i=1n cixi, where {ci} are constant coefficients.

Then the update of p(k) can be written as:

p(k) = c1(ST S)kx1 + c2(ST S)kx2 + ... + cn(ST S)kxn

= c1λ1k(x1 + ∑i=2n ci(λi/λ1)kxi).
(9)
Here we use the fact that (ST S)xi = λixi. Hence, we see that the non-essential quantities decay at a rate of approximately |λ2/λ1|.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 5 FOUNDATIONS OF BIRANK

In contrast to the traditional graph ranking algorithms (e.g., PageRank and HITS), BiRank iterations are implicitly optimizing an objective function. This is analogous to the manifold ranking algorithm on graphs [14]. In what follows, we investigate the regularization framework for BiRank and present a Bayesian explanation of the ranking algorithm. These two views shed important insight into the basis of BiRank, allowing future extensions in a theoretically principled way. To show its extendability, we finally generalize the methodology to rank for the more general n-partite graphs.

# 4.3 Connection with Other Algorithms

There are some bipartite graph ranking algorithms [8], [21], [20] that share a similar spirit with BiRank, though originally developed for specific applications with varying ranking targets. Specifically, in terms of the iterative ranking process, they have the same update rule form as Eq. (3); the main difference is in how to generate the transition matrices (S and ST for updating u and p, respectively). It is instructive to clarify the difference with these algorithms.

**TABLE 1: Transition matrices (i.e., S and ST in Eq. (3)) of different bipartite graph ranking algorithms. Note that ST here denotes a matrix, rather than just the transpose of S.**
|Method|Definition of Transition Matrices|
|---|---|
|HITS (Kleinberg [3])|S = W; ST = WT|
|Co-HITS (Deng et al.[8])|S = W D−1; ST = WT D−1|
|BGER (Cao et al.[20])|S = D−1pT −1 uT; S = Dp W|
|BGRM (Rui et al.[21])|S = D−1W D−1; ST = D−1WT D−1|
|BiRank (our proposal)|S = D−² W D−²; ST = D−² WT D−²|

Table 1 summarizes the ways of constructing transition matrices using our symbol notation. From a high-level view, these algorithms differ in how they utilize the vertex degree to normalize each edge weight (except that HITS does not account for the query vector). HITS, the earliest proposed algorithm, uses the original weight matrix W as-is; although the convergence can be guaranteed in theory, HITS is sensitive to outliers in graph [12] and suffers from the tightly knit communities phenomenon [5]. Co-HITS [8] normalizes each column of W (and WT) stochastically, having an explanation of simulating random walks on the graph. However, random walk methods can be biased towards the high-degree vertices [23]. While BGER [20] avoids this defect by normalizing each row of W (and WT) stochastically, yielding an effect of suppressing the scores of high-degree vertices. However, the one-side normalization of BGER does not account the degrees of p vertices when updating u, allowing high-degree p vertices to exert a stronger impact in the diffusion process; and vice versa. Similar with our proposed BiRank, BGRM also applies a symmetric normalization on W, while the level of normalization differs (the sum of normalization exponents is −2 and −1 for BGRM and BiRank, respectively). Although it is difficult to tell which way between them is more advantageous, we point out that BiRank employs the matrix STS in a similar fashion to a stochastic matrix (the same eigenvalues, see Theorem 1) and corresponds to a regularization framework, both of which are nice properties that BGRM lacks.

# 5.1 Regularization Framework

Inspired from the discrete graph theory [18], we construct the regularization function as follows:

R( ,  ) = ∑ ∑wij (pj − ui)2 + γ ∑(pj − p0)2 + η∑(ui − u0)2,

where γ and η are the regularization parameters to combine different components (they are constants corresponding to α and β in BiRank). Next, we first show that optimizing Eq. (10) leads to the iterative BiRank algorithm, and then interpret the meaning of the regularization function.

# 5.1.1 Relationship with BiRank

Eq. (10) defines an objective function with the ranking scores as model parameters. To optimize the objective function, a common strategy is performing the coordinate descent [27]. Let us first calculate its first-order derivatives with respect to each model parameter:

∂R/∂pj = (2 + 2γ)pj − 2γpj − 2∑i=1|U| wijui/√di√dj

∂R/∂ui = (2 + 2η)ui − 2ηui − 2∑j=1|P| wijpj/√di√dj.

Setting the derivatives to 0, we can obtain:

pj = 1 + γ∑i=1|U| wijui/√di√dj + 1 + γpj,

ui = 1 + η∑j=1|P| wijpj/√di√dj + 1 + ηui,

which exactly recovers the BiRank iteration Eq. (2) by plugging γ = 1−α, η = 1−β into the equation. As such, we see that BiRank is actually iterating towards optimizing the regularization function Eq. (10).

As we have shown BiRank converges to a stationary solution in Section 4.2.1. Now a question arises: does the solution found by BiRank lead to the regularization function’s global optimum? In fact it does, as the regularization function is strictly convex in both pj and ui.

# Theorem 3

The regularization function R(u, p) defined by Eq.(10) is strictly convex and only one global minimum exists.

Proof. According to convex optimization theory, a continuous, twice differentiable function is strictly convex if and only if its Hessian is positive definite.

only if its Hessian matrix is positive definite. As R(u,p) is a continuous function, we now prove it is twice differentiable and that its Hessian matrix is positive definite.

The second order derivative of R(u,p) is:

∂2R ∂pj∂pj = 2 + 2γ; ∂ui∂ui = 2 + 2η; ∂pj∂ui = 2 di√dj.

We can see R(u,p) is twice differentiable.

Let matrix A be the (|U| + |P|) × (|U| + |P|) weighted adjacency matrix of the bipartite graph. Then the Hessian of R(u,p) can be written as: 2(I − D−1AD−1) + 2B, where D is a diagonal matrix where each entry Dkk denotes the weighted degree of k-th vertex (can be of either side); B is a diagonal matrix that each entry Bkk is γ or η, dependant on the choice of origin (side) for the k-th vertex.

Note that the matrix (I − D−1AD−1) is the normalized Laplacian matrix of the graph. By spectral graph theory, the normalized Laplacian matrix of a graph is positive semi-definite. Meanwhile, B is also positive definite because its eigenvalues are all positive (eigenvalues of a diagonal matrix are its diagonal values). Finally, according to the standard linear algebra, the addition of a positive semi-definite matrix and positive definite matrix is also positive definite. Thus, we reach the conclusion that the Hessian matrix must be positive definite. The proof is finished.

# 5.2 Bayesian Explanation

On the basis of the above regularization framework, we now present a Bayesian explanation for BiRank. Figure 2 shows the graphical model representation of the ranking method. We model the query vectors p0 and u0 as observations, which are generated by the latent factors p and u (distributions), serving as the importance scores of the vertices; the weight matrix W forms the prior for generating the latent factors. The goal is to infer the latent factors p and u that generate the observations p0 and u0.

The MAP (maximum a posteriori) estimation is given by:

arg max p(u, p | u0, p0, W).

By Bayes’ rule and the conditional independence indicated in the graphical model, we have:

p(u, p | u0, p0, W) = p(u0, p0 | u, p) · p(u, p | W) / p(u0, p0)

∝ p(u0, p0 | u, p) · p(u, p | W)

∝ p(u0 | u) · p(p0 | p) · p(u, p | W).

Note that p and u are not conditionally independent with each other given the prior W, as a vertex’s score is also influenced by its neighbors’ scores. Taking the logarithm, MAP estimation is then equivalent to:

arg max {ln p(u, p | W) + ln p(u0 | u) + ln p(p0 | p)}.

We then devise the conditional probabilities as follows:

p(u, p | W) = Zup e−Rs(u, p),

p(p0 | p) = e−γRf(p), p(u0 | u) = e−ηRf(u),

where Zup, Zp and Zu are normalization constants, and where Rs(u, p) is the smoothness term, and Rf(p) and Rf(u) are the fitting terms defined in Eq. (13). From this formalization, we can see that minimizing the regularization function is equivalent to maximizing the posteriori probability of generating the query vector.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 6 APPLICATIONS

This shows the equivalence between BiRank’s ranking process and a Bayesian network. We map the ranking problem to probabilistic graphical modeling, allowing the extension of BiRank in a probabilistic way, which is more flexible and adaptable for different applications. For example, if there is additional prior knowledge or context for the vertices, we can model them as priors of pu and use the desired distributions; moreover, aside from MAP, other inference techniques can also be applied to infer the ranking scores, such as variational inference and MCMC sampling.

# 6.1 Popularity Prediction

Predicting the popularity of web content has a wide range of applications, such as online marketing and recommender systems. In what follows, we first briefly introduce the task, and then show how to customize BiRank to address the problem.

# 6.1.1 Task Introduction

A direct and objective metric to measure an item’s popularity is the view count, which evaluates users’ attention on the item. Thereby, previous works have primarily focused on modeling the view histories of items and casted the prediction as a regression task. However, for some external services (who are not the content providers themselves), items’ view histories are not easily accessible. Specifically, most websites do not explicitly provide the view history for an item. Even in the cases where a website like YouTube and Flickr provides the current number of views, one will have to repeatedly and periodically crawl the item pages to build view histories, a very bandwidth intensive activity.

To assist external observers in predicting items’ popularity, we are more interested in an alternative and more viable solution — modeling the affiliated user comments of items. In contrast to view count, the advantage of comments is the exposure of users’ commenting activities up to the current time — crawling once, one can get the previous history and perform the prediction directly. However, the key deficiency is that the comment history can be much sparser than view history, since a user viewing an item may not comment on it. For example, it is common that two items have no comments during the time interval, while they attract views at a different rate. As such, existing view-based solutions (which are mostly regression-based methods) can fail to leverage the comment series to predict popularity accurately.

To tackle the sparsity issue in user comments for quality prediction, we need to account for more popularity signals in addition to the comment count. Here, we propose three ranking hypotheses observed from user comments that we wish to incorporate into our popularity prediction solution:

- H1. Temporal Factor: If an item has received many comments recently, it is more likely to be popular in the near future. More recent comments are a salient signal that more users focused on the item recently.
- H2. User Social Influence: If the users commenting on an item are more influential, the item is more likely to receive more views in the future. This is enabled by the...

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# Item Current Popularity

If an item has already been popular, it is likely to garner more views in the future. This is partially affected by the existing visual interfaces of Web 2.0 systems: the more views an item has, the more likely it will be promoted to users.

# 6.1.2 BiRank Customization

To customize BiRank for a certain ranking purpose, we need to construct the weighted bipartite graph model and devise the query vectors.

# Bipartite Graph Construction

As we deal with users’ commenting behaviors on items, we model their relationship as a bipartite graph – users and items form the two sides of vertices U and P, respectively, and edges E represent comments. If and only if a user has commented on an item, there is an edge between them. We use the edge weight to model the respective comment’s contribution towards the item’s future popularity. As the hypothesis H1 shows a strong near-term correlation, we assign w based on temporal considerations. Specifically, recent (older) comments should contribute more (less) to an item’s future popularity. To achieve this, we choose a monotonically decreasing exponential decay function:

$$w_{ij} = \delta a(t^0 - t_{ij}^1) + b$$

where δ is the decay parameter that controls the decay rate, t is the ranking time and t is the commenting time; a and b are constants, to be tuned for the particular media and site. Time units are arbitrary; they can be assigned as minutes, hours, days, weeks or other units, depending on the temporal resolution and the domain of the items to rank. If no edge exists between u_i and p_j, then w_{ij} is zero. In our empirical study, we find a setting of δ = 0.85, a = 1, b = 0 leads to good performance, and thus use this setting across datasets. As we focus on short-term popularity prediction, we set the time unit as 1 day.

# Query Vector Setup

We devise the user query vector u and item query vector p0 to account for the hypotheses H2 and H3, respectively. Intuitively, if a user has more friends, his behavior is likely to influence more users. Thus we set a user’s prior score in the query vector proportional to the log value of his number of friends:

$$u_0 = \sum \log(1 + g_i)$$

where g_i is user u_i’s number of friends at the ranking time.

Note that we use add-1 smoothing to address the case where a user has no friends. H3 models the current popularity factor on items. As such, the item query vector should encode our prior belief on each item’s popularity prior to examining its recent comments. We capture this potential “rich-get-richer” effect by defining an item’s score in the query vector as:

$$p_0 = \sum \log v_j$$

where v_j denotes the view count of item p_j at ranking time.

# 6.2 Personalized Recommendation

In this subsection, we apply the generalized, n-partiteRank to the application of personalized recommendation. This is a more challenging task than popularity prediction, since it needs to generate a personalized ranking of items for each user. In what follows, we first show how to employ BiRank to encode the well-known collaborative filtering effect for recommendation. Then we use the TripartiteRank (short for TriRank) to additionally model aspects (extracted from comments’ texts) for enhanced recommendation.

# 6.2.1 Collaborative Filtering with BiRank

In recommendation systems, collaborative filtering (CF) is the most successful and widely-used technique for personalization. It exploits user–item interactions (e.g., ratings, click histories) by assuming that users with similar interest consume similar items. The core of a CF algorithm lies in how it models the similarity among users and items. For example, neighbor-based CF directly estimates the similarity by adopting statistical measure on the user–item matrix, while latent factor-based CF estimates the similarity by projecting items/users into a latent space. Under our BiRank paradigm, similarity is estimated by means of smoothing the user–item graph with the target user’s known preference, embodied as a query vector. We use an example in Figure 3 to illustrate how the smoothness works.

|Inputs:|u1|p1| |
|---|---|---|---|
|&lt;u1, p1, 5&gt;|5| | |
|&lt;u2, p1, 5&gt;|u2|4|p2|
|&lt;u2, p2, 4&gt;|3| | |
|&lt;u3, p1, 3&gt;|u3|2|p3|
|&lt;u3, p3, 2&gt;| | |users|
| | | |items|

Fig. 3: A toy example of using BiRank to model the collaborative filtering effect. The target user u1 has previously rated item p1 with a rating score 5 (in tail). Users and items represent the two types of vertices, and edge weights denote the rating scores (here, a zero score means the user did not rate the item; a missing value).

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 10

# TABLE 2: Top automatically extracted aspects.

|Yelp|bar|salad|menu|chicken|sauce|restaurant|
|---|---|---|---|---|---|---|
| |rice|cheese|fries|bread|sandwich|drinks|
|Amazon|camera|quality|sound|price|battery| |
| |pictures|screen|size|memory|lens| |

collaborative filtering effect: in cases with explicit feedback, it can be the rating score (as illustrated previously in Section 6.2.1); for implicit feedback, it can denote whether the user has interacted with or browsed the item (measured as either a binary yes/no, or an integer view count). Our datasets provide explicit user ratings, so we use these ratings as-is. The setting of aspect connected edges should reflect the aspect filtering effect: if a user is interested in an aspect, then the system should rank the items that are good at this aspect high. Thus, we set the edge weights of user–aspect relation and item–aspect relation to connote the degree of user interest (item specialty) with respect to the aspect.

Finally, since the edge weights of &lt; u2,p2 &gt; and &lt; u3,p3 &gt; are identical, BiRank will assign p2 a higher score than p3, meaning that p2 is a more suitable candidate to recommend for u1 than p3. From this qualitative analysis, we see that by properly setting the query vector’s values, smoothing the user–item relation results in a collaborative filtering effect. More specifically, by setting the query vector as the rated items of the target user, BiRank functions similar to item-based CF [32] which represents a user by his historical actions for personalization.

# 6.2.2 Modelling Aspects with TriRank

Aside from ratings, which form the basis for collaborative filtering, most Web 2.0 systems also encourage users to pen reviews. These reviews justify a user’s ratings, offering the underlying reasons for the rating by discussing the specific properties of the item. We term these specific properties as aspects, which are nouns or noun phrases that represent the features of items (see Table 2 as examples). Aspects are well-suited as a complementary data source for CF, since a mention of an aspect implies the user’s interest in the aspect, which in turn reveals the user’s preference. In this subsection, we model the aspects with TriRank to improve the CF-based recommendation.

Similar to the application of BiRank to popularity prediction, we first show how to construct the tripartite graph, and then design the query vectors to implement the personalized ranking.

# Tripartite Graph Construction

After extracting aspects from user reviews, we construct a tripartite graph with users, items and aspects as the three types of vertices. We formalize the input as a list of triples, where each triple &lt; ui, pj, ak &gt; denotes that user ui has rated item pj with a review mentioning aspect ak and is represented as a triangle with edges eij, eik and ejk in the graph. Figure 4 shows an example of the tripartite graph.

Each edge carries a weight, which is crucial to determine the meaning of smoothness and the behavior of TriRank. The setting of user–item edge weights should encode the.

# 7 EXPERIMENTS

In this section, we empirically examine BiRank’s properties and effectiveness. We first conduct experiments on synthetic data to study BiRank’s convergence and time efficiency. Then we perform experiments on real-world datasets to evaluate BiRank performance for the two applications of popularity prediction and personalized recommendation.

# 7.1 Experiments on Synthetic Data

# 7.1.1 Datasets

We concern ourselves with two forms of generated graphs:

1. Synthetic Random Graphs. These random graphs are generated by sampling edges from a uniform distribution.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# 11

We control the density of generated bipartite graphs to simulate real-world graph sparsity. Given the expected density of the graph, we visit each potential edge and generate a uniformly random number in the range (0,1); if the number is less (or equal) than the density value, we add the edge into the graph.

# 2. Synthetic Power-law Graphs

Considering that many real-world graphs follow a power-law distribution, such as document–word and user–item graphs, we also generate the power-law bipartite graphs. We adopt the power-law graph generation algorithm in [33]: starting from an empty graph, it follows two main steps: first it assigns a degree x to each vertex v from the distribution p(dv = x) ∝ x−λ where λ > 1; then, it sorts the vertices by degree in decreasing order, and assigns neighbors to each vertex according to the degree. We adjust the second step for generating bipartite graph – sampling neighbors of a vertex only from the vertices of the other side.

# 7.1.2 Convergence Study

There are two natural questions need to be answered empirically regarding BiRank’s convergence4:

1. Will BiRank iterations converge to the optimal solution of the regularization function as analyzed theoretically?
2. How does the algorithm hyper-parameters (i.e., α and β) influence BiRank’s convergence rate?

Since our findings were consistent across many settings, we only report results with a set of representative settings. The convergence threshold is set as 0.0001 for Vector Diff, a strict condition that guarantees a sufficient convergence. Figure 6 plots the number of iterations to converge on two graphs of size 10K × 20K. Both graphs show the same trend that BiRank needs more iterations to converge with a larger α (and β). This verifies our qualitative analysis in Section 4.2.2 that smaller value of α (and β) leads to a larger effect of the static query vectors, helpful in achieving quick convergence.

# 7.1.3 Time Efficiency

In Section 4.1.1, we analyzed the theoretic time complexity of BiRank: O(n), in the number of graph edges. We now empirically validate this property. We adopt sparse representation for matrices, and implement BiRank in Java. The experiments are run on a modern desktop (Intel 3.5GHz CPU with 16GB RAM running on a single thread).

# Convergence to optimum

Figure 5 plots the convergence status of each iteration on two representative synthetic graphs (the random setting has a density of 1%; the power-law graph sets λ = 2). The black line (Stationary) benchmarks the optimal solution from the direct calculation of the stationary solution Eq. (8). The blue line (Iterative) shows the regularization function’s value after the update of each iteration; the red line (Vector Diff, y-axis scale of the right) shows the difference (i.e., squared sum) of the ranking vector before and after the update of each iteration. As we can see, BiRank successfully finds the optima of the regularization function in all four cases. Our further examination (not shown) validates that the ranking vector obtained by BiRank iterations is actually same with the stationary solution. This demonstrates BiRank’s ability in converging to the unique and optimal solution of the regularization function, regardless of the graph structure. Moreover, the convergence rate is rather fast for these simulated problems – also usually.

# Figure 6: Convergence rate w.r.t. α and β.

|Iterations|Iterations|
|---|---|
|14|α|
|12|β|
|10| |
|8| |
|6| |
|4| |
|2|0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9|
| |20.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9|

# Figure 5: Convergence status of two generated graphs.

|(a) Random (10K × 50K)|(b) Power-law (10K × 50K)|
|---|---|
|1. Convergence to optimum.| |

# Figure 7: Running time per iteration w.r.t. number of edges.

|Time/Iteration (s)|Time/Iteration (s)|
|---|---|
|10|100K*100K|
|8|100K*200K|
|6|200K*400K|
|4| |
|2| |
|0| |

Note that due to the difficulty of controlling the eigenvalues of generated graphs, we do not empirically study the impact of the second dominant eigenvalue on convergence rate. While the impact has been theoretically proved in Theorem 2.

# 7.2 Evaluation of Popularity Prediction

We now evaluate how does BiRank perform for the application of comment-based popularity prediction.

TABLE 4: Popularity prediction evaluated by Spearman coefficient (%). “*” denotes the statistical significance for p < 0.01 judged by the one-sample paired t-test.

|Method|YouTube|Flickr|Last.fm|
|---|---|---|---|
|1. VC|73.39|58.42|67.31|
|2. CCP|83.35|59.43|67.21|
|3. ML|78.24|58.00|38.09|
|4. PageRank|80.72|28.15|10.24|
|5. Co-HITS|85.21|63.81|72.71*|
|6. BGER|84.10|63.17|68.94|
|7. BiRank (ours)|88.21*|64.76*|70.93|

Given a set of items, BiRank outputs a ranking list of the items, indicating their predicted popularity. To assess the quality of the predicted ranking with the GT ranking globally, we adopt the Spearman coefficient, which measures the agreement between two rankings.

# 7.2.1 Experimental Settings

Datasets and metrics. Table 3: Demographics of the three Web 2.0 datasets.

|Dataset|Item#|User#|Comment#|Avg C/I|Crawled Date|
|---|---|---|---|---|---|
|YouTube|21,653|3,620,487|7,246,287|334.7|2012/8/9|
|Flickr|26,815|37,690|169,150|6.3|2012/9/3|
|Last.fm|16,284|77,996|530,237|32.6|2012/10/24|

The evaluation ground-truth (GT) is the number of views (note, not the number of comments) received in the future three days after the original crawl date (i.e., ranking date t0). Further experimentation of 10-fold cross validation shows that the improvements of BiRank over Co-HITS and BGER on YouTube and Flickr datasets are consistent and statistically significant (p < 0.01, via one-sample paired t-test). Moreover, Co-HITS outperforms BGER consistently, although the random walk treatments of Co-HITS are suspicious to bias the high-degree vertices while BGER does not have this issue. We suspect the reason of Co-HITS’s strong performance might be that the bias effect is diluted by the setting of query vectors, which can regulate the random walks effectively.

# Baselines

We compare with the following six baselines:

1. View Count (VC): Rank based on the current view count of items, corresponding to our Hypothesis H3 alone.
2. Comment Count in the Past (CCP): Rank based on the number of comments received in the 3-day period prior to t0, corresponding to our Hypothesis H1.
3. Multivariate Linear model (ML) [31]: A state-of-the-art regression method for popularity prediction. We apply this method on the comment series with the time unit as 3 days.
4. PageRank [2]: This is the most widely used graph ranking method. Since the bipartite nature can cause the random walk to be non-stationary, we employ the standard method to set a uniform self-transition weight wii = 1 for all nodes before converting to a stochastic matrix.
5. Co-HITS [8]: This algorithm is devised for ranking on bipartite graphs by interleaving two random walks. To make a fair algorithmic comparison with BiRank, we apply the same query vectors to Co-HITS and tune the parameters in the same way.
6. BGER [20]: This is another algorithm designed for ranking on bipartite graphs. Instead of simulating a random walk, it normalizes the edge weights in a different way and is analogously explained as heat diffusion. We apply the same query vectors and parameter search for this method.

To expedite parameter tuning, we randomly held out 10% of the dataset as the development set, and employ grid search to find the optimal parameters. Then the performance is evaluated on the remaining 90% as the testing items.

# 7.2.2 Performance Comparison

Table 4 shows the performance of the methods on the three datasets. First, we can see that the three bipartite graph ranking methods (lines 5 – 7) significantly outperform other methods. This is because these methods model all the three ranking hypotheses we proposed, while other methods only partially model the hypotheses. Among the three bipartite ranking methods, BiRank achieves the best performance in general (best on two datasets YouTube and Flickr), followed by Co-HITS (best on Last.fm) and BGER.

Focusing on the result of PageRank (line 4), we see that it performs very poorly for Flickr and Last.fm. This indicates that just the centrality of an item in the user–item temporal graph is insufficient for accurate popularity prediction. In addition, the performance discrepancy between PageRank and CoHITS (also a random walk-based method) highlights the importance of separately handling the two vertex types within the bipartite graph.

It is surprising that the regression approach ML underperforms CCP, as ML leverages more information: comments in the recent 30 days compared with CCP’s access to only three days. We believe there are two reasons for this: 1) the nature of short-term prediction, and 2) the sparsity of comments. As the prediction task is a short-term one, the most recent data carries the most signal – “What happened yesterday will happen tomorrow”; the performance score of CCP verifies this point. Secondly, the sparsity in comment series (e.g., some time units have zero count) can negatively affect the regression process in an unexpected manner.

# 7.3 Evaluation of Personalized Recommendation

In this subsection, we study how do our BiRank and TriRank perform for the task of personalized item recommendation.

# 7.3.1 Experimental Settings

Datasets. We experiment with two public review datasets: Yelp5 and Amazon Electronics6. We follow the common practice in evaluating recommendation algorithms [28], [26] that filters out users and items with fewer than 10 reviews. We used the sentiment analysis tool developed by [34] for extracting aspects from review texts. Table 5: summarizes the statistics of the filtered datasets and Table 2: shows examples of the top aspects extracted.

# Baselines

We compare with the following methods that are commonly used in top-K recommendation:

5. yelp.com/dataset challenge. Downloaded on October 2014.

6. snap.stanford.edu/data/web-Amazon-links.html

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

# TABLE 5: Statistics of datasets in evaluation.

|Dataset|Review#|Item#|User#|Aspect#|
|---|---|---|---|---|
|Yelp|114,316|4,043|3,835|6,025|
|Amazon|55,677|14,370|2,933|1,617|

# TABLE 6: Recommendation performance (%) evaluated at Rank 50.

|Metric(%)| | | | |Yelp| |Amazon| |
|---|---|---|---|---|---|---|---|---|
|HR| | | | |10.61| |6.13|2.37|
| |ItemPop| | | |10.61|4.08|6.13|2.37|
| | |ItemKNN| | |15.72|6.37|12.69|10.15|
| | | | |PureSVD|14.94|6.16|14.94|10.55|
| | | | |BiRank (ours)|17.00*|6.90*|15.97*|11.16*|
| | | | |PageRank|15.90|6.52|17.49|11.78|
| | | | |TagRW|15.25|6.02|17.47|10.65|
| | | | |TriRank (ours)|18.58*|7.69*|18.44*|12.36*|

# 1. Popularity (ItemPop)

Items are ranked by their popularity judged by number of ratings. This non-personalized method benchmarks the performance of the top-K task.

# 2. ItemKNN [32]

This is standard item-based CF. We tested the method with different number of neighbors, finding that using all neighbors works best.

# 3. PureSVD [35]

A state-of-the-art model-based CF method for top-K recommendation, which performs SVD on the whole matrix. We tuned the number of latent factors.

# 4. PageRank [4]

This graph method has been widely used for top-K recommendation, such as by [36]. For a fair comparison, we set the personalized vector the same with TriRank’s query vectors and tuned the damping factor.

# 5. TagRW [37]

A state-of-the-art tag-based recommendation solution, which performs random walks on the user–user and item–item similarity graph. Since tags have a similar form with aspects, we feed aspect as tags into the method.

For each user, we sort her reviews in chronological order. The first 80% are used for training, followed by 10% as validation (for parameter tuning) and 10% as test set (for evaluation). Given a test user, we assess the ranked list of top-K items with Hit Ratio [28] and NDCG [29].

# 8 CONCLUSION

We focus on the problem of ranking vertices of bipartite graphs, and more generally, n-partite graphs. We devise a new, generic algorithm – BiRank – which ranks vertices by accounting for both the graph structure and prior knowledge. BiRank is theoretically guaranteed to converge to a stationary solution, and can be explained from both a regularization view and a Bayesian view. This appealing feature allows future extensions to BiRank to be grounded in a principled way.

To demonstrate the efficacy of our proposal, we examine two ranking scenarios: a general ranking scenario of item popularity prediction by modeling the user–item binary relationship, and a personalized ranking scenario of item recommendation by modeling the user–item–aspect ternary relationship. By properly setting the graph’s edge weights and query vectors, BiRank can be customized to encode various ranking hypotheses. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness of our method. In future, we will study how to optimally learn the hyper-parameters of BiRank. Owing to the two views of BiRank, two solutions can be explored.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

— by adapting the parameters based on the validation set [38], or by integrating over the parameters under the Bayesian network formalism. Moreover, we will explore how to integrate the graph regularization framework with matrix factorization methods, which have been shown to be very effective for many tasks such as recommendation [26] and clustering [39].

# ACKNOWLEDGMENTS

This research is supported by the National Key Research and Development Program of China (No. 2016YFB1000905) and NSFC under Grant No. U1401256. NExT research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative. The authors thank the anonymous reviewers for their valuable comments, and acknowledge the additional discussion and help from Liqiang Nie, Jun-Ping Ng, Tao Chen and Yiqun Liu. This paper is an extended version of the SIGIR ’14 conference paper [1]. Xiangnan He is the corresponding author.

# REFERENCES

1. X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama, “Predicting the popularity of web 2.0 items based on user comments,” in SIGIR, 2014, pp. 233–242.
2. L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the Web,” Stanford InfoLab, Technical Report, 1999.
3. J. M. Kleinberg, “Authoritative sources in a hyperlinked environment,” Journal of the ACM, vol. 46, no. 5, pp. 604–632, 1999.
4. T. H. Haveliwala, “Topic-sensitive PageRank,” in WWW, 2002, pp. 517–526.
5. R. Lempel and S. Moran, “The stochastic approach for link-structure analysis (salsa) and the tkc effect,” Computer Networks, vol. 33, no. 1, pp. 387–401, 2000.
6. Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, and H. Li, “Browserank: letting web users vote for page importance,” in SIGIR, 2008, pp. 451–458.
7. B. Gao, T.-Y. Liu, W. Wei, T. Wang, and H. Li, “Semi-supervised ranking on very large graphs with rich metadata,” in KDD, 2011, pp. 96–104.
8. H. Deng, M. R. Lyu, and I. King, “A generalized Co-HITS algorithm and its application to bipartite graphs,” in KDD, 2009, pp. 239–248.
9. J. Sun, H. Qu, D. Chakrabarti, and C. Faloutsos, “Relevance search and anomaly detection in bipartite graphs,” SIGKDD Explor. Newsl., vol. 7, no. 2, pp. 48–55, 2005.
10. X. Li, M. Zhang, Y. Liu, S. Ma, Y. Jin, and L. Ru, “Search engine click spam detection based on bipartite graph propagation,” in WSDM, 2014, pp. 93–102.
11. L. Getoor and C. P. Diehl, “Link mining: A survey,” SIGKDD Explor. Newsl., vol. 7, no. 2, pp. 3–12, 2005.
12. A. Y. Ng, A. X. Zheng, and M. I. Jordan, “Stable algorithms for link analysis,” in SIGIR, 2001, pp. 258–266.
13. C. Ding, X. He, P. Husbands, H. Zha, and H. D. Simon, “Pagerank, hits and a unified framework for link analysis,” in SIGIR, 2002, pp. 353–354.
14. D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning with local and global consistency,” in NIPS, 2004, pp. 321–328.
15. A. Smola and R. Kondor, “Kernels and regularization on graphs,” in Learning Theory and Kernel Machines, ser. LNCS. Springer Berlin Heidelberg, 2003, vol. 2777, pp. 144–158.
16. S. Agarwal, “Ranking on graph data,” in ICML, 2006, pp. 25–32.
17. D. Zhou, J. Weston, A. Gretton, O. Bousquet, and B. Schlkopf, “Ranking on data manifolds,” in NIPS, 2004.
18. D. Zhou and B. Schölkopf, “Regularization on discrete spaces,” in Springer, 2005, pp. 361–368.
19. D. Zhou, J. Huang, and B. Schölkopf, “Learning from labeled and unlabeled data on a directed graph,” in ICML, 2005, pp. 1036–1043.

Xiangnan He is currently a postdoctoral research fellow with the School of Computing at the National University of Singapore. His research interests include information retrieval, recommender systems, multimedia and machine learning. His works have appeared in several major international conferences such as SIGIR, WWW, MM, CIKM and AAAI. He has served as program committee member of international conferences such as SIGIR, WWW and EMNLP.

Ming Gao is an associate professor of Software Engineering Institute with the East China Normal University, China. He received his doctorate from the School of Computer Science, Fudan University. His research interests include uncertain data management, streaming data processing, social network analysis and data mining. His work appears in major international conferences including ICDE, ICDM, DASFAA and WebSci.

# IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2016

Min-Yen Kan is an associate professor at the National University of Singapore. He previously served as a member of the executive committee of the Association of Computational Linguistics (ACL) and maintains the ACL Anthology, the community’s largest archive of published research. He is an associate editor for the Springer “Information Retrieval” journal. His research interests include digital libraries and applied natural language processing and information retrieval. Specific projects include work in the areas of scientific discourse analysis, full-text literature mining, machine translation, lexical semantics and applied text summarization.

Dingxian Wang is a Research Engineer with the Search Science Department in eBay. He is specifically working on the project of product classification for improving search ranking. His research interests mainly include information retrieval, software engineering, natural language processing and applied machine learning. His works have appeared in international conferences including WISE, CSE and ICSSP.

