# Good models borrow, great models steal: intellectual property rights and generative AI

Simon Chesterman*

1Faculty of Law, National University of Singapore

2AI Governance, AI Singapore

*I’m grateful to N Chesterman, Mikhail Filippov, Gao Yuting, Brian Judge, Shaleen Khanal, Marijn Janssen, Hahn Jungpil, Mark Lim, Gong Min, Peter Schoppert, Kritika Sha, Araz Taeihagh, David Tan, Inga Ulnicane, Hongzhou Zhang, and two anonymous reviewers for comments on earlier versions of this text, which was first presented at a workshop at the Lee Kuan Yew School of Public Policy in October 2023. This research is supported by the Ministry of Education, Singapore, under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). Errors and omissions are the responsibility of the author alone. Corresponding author: Simon Chesterman. Email: chesterman@nus.edu.sg

# Abstract

Two critical policy questions will determine the impact of generative artificial intelligence (AI) on the knowledge economy and the creative sector. The first concerns how we think about the training of such models—in particular, whether the creators or owners of the data that are “scraped” (lawfully or unlawfully, with or without permission) should be compensated for that use. The second question revolves around the ownership of the output generated by AI, which is continually improving in quality and scale. These topics fall in the realm of intellectual property, a legal framework designed to incentivize and reward only human creativity and innovation. For some years, however, Britain has maintained a distinct category for “computer-generated” outputs; on the input issue, the EU and Singapore have recently introduced exceptions allowing for text and data mining or computational data analysis of existing works. This article explores the broader implications of these policy choices, weighing the advantages of reducing the cost of content creation and the value of expertise against the potential risk to various careers and sectors of the economy, which might be rendered unsustainable. Lessons may be found in the music industry, which also went through a period of unrestrained piracy in the early digital era, epitomized by the rise and fall of the file-sharing service Napster. Similar litigation and legislation may help navigate the present uncertainty, along with an emerging market for “legitimate” models that respect the copyright of humans and are clear about the provenance of their own creations.

# Keywords:

artificial intelligence; generative AI; large language models; copyright; intellectual property

When people think of the risks associated with artificial intelligence (AI), Hollywood looms large. Movies have long conjured the worst-case scenarios: from Hal refusing to open the pod bay doors in 2001, to a murderous Arnold Schwarzenegger travelling back through time. If there is a robot apocalypse, however, it is unlikely to resemble a Terminator movie. A more probable scenario is what was more recently seen off-screen in—ironically enough—the Writers Guild of America (WGA) strike of 2023.

© The Author(s) 2024. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

S. Chesterman

Hollywood’s scriptwriters were protesting, in part, about the threat of many jobs being replaced by new generative AI tools that can perform similar functions at little or no cost. The concern is not that humanity will wake up to discover that it has been replaced by AI; rather, it is that AI will progressively reduce the economic viability of certain careers by salami-slicing fulltime jobs into tasks that can be commoditized and outsourced. This can be thought of as the dark side of the gig economy (Prassl, 2018). Where Uber, Grab, and the like offered flexible arrangements that were attractive for young workers who later discovered that no foundation had been laid for a career, ChatGPT threatens to take existing careers and break them into gig work for hire.

Such precarity is not limited to scriptwriters. After an initial panic by academics worldwide that this new technology might enable students to cheat on their papers, it became clear that generative AI had larger implications for the knowledge economy, comparable perhaps to the impact of the industrial revolution on manufacturing. “Knowledge workers” was the term introduced in 1959 by management consultant Peter Drucker for non-routine problem solvers (Drucker, 1959). People who “think for a living” earn through their ability to analyze and write—something that ChatGPT can replicate in almost no time and at almost no cost. Journalists, already taking a beating as readers turn from traditional to social media, now face the prospect of technology taking over the writing task as well. Yet that same threat confronts anyone who analyses or writes for a living, such as lawyers and even—gasp—academics. Applications are not limited to prose, as large language models (LLMs) have demonstrated proficiency in coding as well as poetry (Dwivedi et al., 2023). Similar developments have shaken the art world, with generative AI images flooding social media and, increasingly, traditional media. Video and multimodal content are close behind.

This article will consider two policy questions facing governments around the world in relation to how generative AI will impact the knowledge economy and the creative sector. The first concerns how we think about the training of such models, in particular whether the creators or owners of the data that are “scraped”—lawfully or unlawfully, with or without permission—should be compensated for that use. The second question is who (if anyone) should own the output of generative AI, which is being produced at ever greater quality on ever greater scale. Both issues are linked to intellectual property, a body of laws that was adopted to incentivize and reward human creativity and innovation. Section three of the article considers the larger implications of the answers to those questions, weighing the benefits of lowering the cost of creation and the value of expertise against the possibility that diverse careers and sectors of the economy might be rendered unsustainable (D’Auria & Sundararajan, 2023; Mims, 2023).

# I think, therefore I’m paid

AI has always depended on access to data (Roberts et al., 2021). LLMs in particular are trained on huge datasets, comprising publicly available material as well as copyrighted and pirated material available online (O’Leary, 2013; Zikopoulos et al., 2012). That scale transformed public debate about the impact of AI with the release of ChatGPT by OpenAI in November 2022, quickly followed by competitors such as Google’s Gemini, Anthropic’s Claude, and Meta’s Llama. Excitement and trepidation about the uses for systems able to respond to natural language queries with human-like responses—in text as well as images—suggested that the long-heralded economic promise of AI might be at hand. Goldman Sachs breathlessly reported that generative AI could increase global GDP by seven percent (Generative AI could raise global GDP).

How (if at all) should the rights of creators, whose text and images train such models, be recognized and compensated? The use of pirated or illegally obtained material appears at first blush to be a simple case of theft of intellectual property, but has been notoriously difficult to prove. Around the world, concepts like fair use are being stretched by the wholesale consumption of books, photographs, and other materials. In some jurisdictions, new rights to data mine have sought to balance the interests of developers against those of creators.

If a work is not in the public domain, even temporary unauthorized use can be an infringement. This is the subject of ongoing litigation brought by Getty Images against Stability AI, for example, alleging that the Stable Diffusion text-to-image model was trained on millions of copyrighted images and metadata. Getty claims that this deprived it of the revenue from licensing those images. Evidence of the alleged infringement includes content generated by Stable Diffusion with distortions of the watermark Getty uses to protect its product. (See Figure 1.)

# Policy and Society

# Figure 1

From the complaint in the Getty Images litigation, Getty image (left) and Stability AI image (right). Available at https://fingfx.thomsonreuters.com/gfx/legaldocs/byvrlkmwnve/GETTY%20IMAGES%20AI%20LAWSUIT%20complaint.pdf.

Given the secretive nature of much model training, proving infringement is rarely as easy as this. Even in the Getty lawsuit, infringement may need to be demonstrated on a case-by-case basis, establishing substantial similarity for each image one by one—rather than the systemic infringement alleged by Getty (Tan, 2023c).

Even if infringement can be established, fair use is a defence that balances the rights of creators and the interests of the wider public in distributing and using their works. It generally considers the purpose of the use, the nature of the work, the amount used, and the effect on the market for the original work.1 When an individual records a televised broadcast to watch at a later time, for example, that can be considered fair use. Projecting such a recording for an audience and charging for tickets, by contrast, would not be (Beebe, 2008).

An example of the current state of debate is the 2023 US Supreme Court case involving two versions of an image of the late musical artist Prince. Lynn Goldsmith took the original photograph in 1981. Three years later, Vanity Fair licensed the photograph to be used as source material by Andy Warhol for an illustration of an article entitled “Purple Fame.” Warhol’s silk screen image was one of more than a dozen he created, despite the license—for which Goldsmith was paid $400—being limited to one image to be used one time only in the magazine. After Prince’s death in 2016, the other images and two pencil drawings, collectively referred to as the “Prince Series” were published, including on the cover of another magazine owned by Vanity Fair’s parent company, Condé Nast. (See Figure 2.)

Litigation followed, with the Supreme Court ultimately concluding that, while Warhol’s art might be fair use if hung in a museum, using the image for a magazine cover was precisely the kind of purpose for which Goldsmith licensed her own photos. She was therefore entitled to compensation (Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith, 2023).

Returning to generative AI, a key question is whether using data to train models, which are then used to produce works that directly compete with the authors of those data, constitutes fair use. This appears to be distinct from other forms of data mining. When Google began scanning vast quantities of books in 2002, there were challenges that this infringed copyright. Google was, for the most part, successful in arguing that it made the information available but was not itself providing a substantial substitute or threatening the market for the original works (Authors Guild v. Google, 2015; Maguire, 2020).

1 The 1886 Berne Convention provides that countries can permit the reproduction of works in “special cases,” provided that this did not conflict with normal exploitation of the work and “does not unreasonably prejudice the legitimate interests of the author”: Convention for the Protection of Literary and Artistic Works, 9 September 1886, art. 9(2).

# PRIncE

# THE Geius OF

# raterviews: PURPLE FAME

# unsibg Pnrk Grlatest

Figure 2. (A) A black and white portrait photograph of Prince taken in 1981 by Lynn Goldsmith; (B) a purple silkscreen portrait of Prince created in 1984 by Andy Warhol to illustrate an article in Vanity Fair; (C) an orange silkscreen portrait of Prince on the cover of a special edition magazine published in 2016 by Condé Nast. Available at link.

The ability of generative AI to produce text and images that may, in fact, compete directly with past and present works produced by the authors and artists whose works trained those models is central to several of the lawsuits currently underway, including prominent authors such as John Grisham, Jonathan Franzen, and Elin Hilderbrand who are suing OpenAI, the creator of ChatGPT (Alter & Harris, 2023; Reisner, 2023b). A separate lawsuit brought by the New York Times alleges that by training the model on millions of articles directly competes with the Times as a source of reliable information and analysis for which one would normally need a subscription (Metz, 2024).

Mark Lemley, among others, has argued that model training should be regarded as fair use on the basis that machine learning is a transformative use of the underlying data. He and coauthor Bryan Casey also argue that this will encourage the creation of new databases with greater transparency, as well as recognizing that licensing materials for such large training sets is impractical given their scale (Lemley & Casey, 2021). Lemley, who is part of Stability AI’s defence team, has gone on to argue that the infringement question may be inapplicable to generative AI “for the simple reason that generative AI is not about copying existing works but about creating new ones” (Guadamuz, 2023; Lemley, 2023).

In the absence of statutory reform, lawsuits will proliferate.

Singapore is an example of a jurisdiction that has tried to thread this needle through legislation. Amendments to its Copyright law in 2021 include a permitted use to make a copy of a work for the purpose of “computational data analysis,” which includes extracting and analyzing information and using it to “improve the functioning of a computer program in relation to that type of information or data” (Copyright Act, 2021, ss. 243–244).

The provision still requires lawful access to the underlying data, but appears more open to data mining and model training than traditional conceptions of fair use (Lim, 2023), the “non-commercial” text and data analysis exception adopted in the UK in 2014 (Copyright, Designs and Patents Act, 1988, s. 29A), or the “text and data mining” (TDM) exception adopted by the European Union in 2019 (EU Directive on Copyright in the Digital Single Market 2019). An information sheet produced by the Intellectual Property Office of Singapore (IPOS) explicitly states that the provision is intended to allow “training machine learning.” Yet, analyzing text or images for the purpose of making recommendations or optimizing workflows is quite distinct from using those texts and images to generate more text and images. The difference is not just the usage, where copying is central to the process, but also the economic impact of that usage (Tan, 2023a; Torrance & Tomlinson, forthcoming).

This is no longer a hypothetical problem. In addition to diluting the value of human authors’ works, it is possible that they will simply be swamped by the volume of generative AI produced. An early example was the science fiction magazine Clarkesworld which had to stop receiving unsolicited submissions because it was being flooded with AI content (Silberling, 2023). Amazon, today one of the world’s largest publishers of books, was becoming so overwhelmed by submissions that it imposed a limit that its self-published authors may now publish “only” three books per day (Creamer, 2023).

Returning to the question of lawful access, much of the data used by LLMs for training is pirated in the first place. More than 70,000 pirated books were found when Peter Schoppert analyzed the “Books3” dataset (Reisner, 2023a; Schoppert, 2023). No one is seriously suggesting that generative AI should not

# Policy and Society

be trained. But it is reasonable to expect that models are not trained on stolen data, and that those who profit from this technology pay something to—or at least recognize—the creators whose works serve as its fuel (Tan, 2023b).

# Author, author!

A second set of questions concerns who should own the outputs of generative AI. In an unscientific experiment, the author decided to ask ChatGPT itself and got two very different answers.² “I do not have the ability to own intellectual property or any other legal rights,” ChatGPT replied at first. “Any text or other content that I generate is the property of OpenAI, as the creator and owner of the tool that I am.”

The author pointed out that OpenAI itself now explicitly states that it will not claim copyright over any content generated by ChatGPT (Ellison, 2022; Guadamuz, 2022; Schade, 2023).³ This led to a revised answer: “The text generated is not the intellectual property of the model itself. Instead, the intellectual property rights belong to the person or entity who has commissioned the model to generate the text.”

Clear and concise, but also wrong.

In most jurisdictions, automatically generated text does not receive copyright protection at all. The US Copyright Office has stated that legislative protection of “original works of authorship” is limited to works “created by a human being” (17 USC § 102(a)). It will not register works “produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author” (Compendium of U.S. Copyright Office Practices, 3rd edition, 2019) (emphasis added). The word “any” is key and begs the question of what level of human involvement is required to assert authorship (Gervais, 2020; Phelan & Carey, 2023).

Early photographs, for example, were not protected because the mere capturing of light through the lens of a camera obscura was not regarded as true “authorship” (de Cock Buning, 2018, p. 524). It took an iconic picture of Oscar Wilde (see Figure 3) going all the way to the US Supreme Court before copyright was recognized in mechanically produced creations (Burrow-Giles Lithographic Co v. Sarony, 1884).

Arguments continued in other jurisdictions, however, with Germany withholding full copyright of photographs until 1965 (Nordemann, 1999).

The issue today is distinct: not whether a photographer can own images passively captured by a machine, but who might own new works actively created by one. Computer programs like word processors do not own the text typed on them, any more than a pen owns the words that it writes. But AI systems now generate news reports, compose songs, paint pictures. One could argue that it is less like a pen or a mechanical function, and more like the work produced by one’s child or one’s student (Chesterman, 2021, p. 133). These activities generate value—can and should they be protected by the law?

At present, the answer in most places is no. Unless there is an identifiable human author, copyright will not apply. The policy behind this is often said to be incentivizing and rewarding innovation. This has long been dismissed as unnecessary or inappropriate for computers. “All it takes,” Pamela Samuelson wrote in 1986, “is electricity (or some other motive force) to get the machines into production” (Samuelson, 1986, p. 1199).

Indeed, protecting such works might disincentivize innovation—by humans, at least. AI has already unleashed an economic tornado in the art world, massively lowering the cost of producing original images (Menéndez, 2023). If we wish to have a thriving arts sector that gainfully employs humans, it is arguable that their creations should be protected while machine creations should not be. Automatically generated content may not be eligible for copyright protection, but edited and curated content that draws on such material could still be owned by the person doing the editing and curating.

The author fed that into ChatGPT, which agreed that this was correct—sensibly adding that legal advice should be sought if there were any further questions.

An alternative approach, adopted in Britain, is to have more limited protections for “computer-generated” work, the “author” of which is deemed to be the person who undertook “the arrangements

² The “conversation” was conducted in January 2023 with ChatGPT’s publicly available model at the time; responses have been edited for brevity.

³ OpenAI had, in fact, initially claimed ownership of all output from DALL⋅E. This was amended in late 2022 to state that users now own the generated content, a position extended to the product of ChatGPT. The company similarly abjures responsibility for the content, with prominent warnings that it “can make mistakes. Consider checking important information.”

# 6 S. Chesterman

# NEY

Figure 3. Oscar Wilde, photograph by Napoleon Sarony, US Library of Congress. Available at https://www.loc.gov/pictures/item/98519699/.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# Policy and Society

# Figure 4. Théâtre D’opéra Spatial

—Midjourney, using a prompt from Jason M. Allen. Available at https://commons.wikimedia.org/wiki/File:Théâtre_D’opéra_Spatial.jpg.

“Computer-generated” is defined as meaning that the work was “generated by computer in circumstances such that there is no human author of the work” (Copyright, Designs and Patents Act, 1988). Similar legislation has been adopted in New Zealand (Copyright Act, 1994), India (Copyright Amendment Act, 1994), Hong Kong (Copyright Ordinance, 1997), and Ireland (Copyright and Related Rights Act, 2000). Though disputes about who took the “arrangements necessary” may arise, ownership by a recognized legal person or by no one at all remains the only possible outcome (Brown et al., 2019, pp. 100–01; Nova Productions v. Mazooma Games, 2007). The duration is generally for a shorter period, and the deemed “author” is unable to assert moral rights—including the right to be identified as the author of the work (Copyright, Designs and Patents Act, 1988).

A World Intellectual Property Organization (WIPO) issues paper recognized the dilemma, noting that excluding these works would favor “the dignity of human creativity over machine creativity” at the expense of making the largest number of creative works available to consumers. A middle path, it observed, was to offer “a reduced term of protection and other limitations” (Revised Issues Paper on Intellectual Property Policy and Artificial Intelligence, 2020). Several commentators have suggested similar approaches (Abbott, 2020, pp. 71–91; du Sautoy, 2019, p. 102).

As human authorship becomes more ambiguous, that middle ground may help preserve and reward flesh and blood authorship, while also encouraging experiments in collaboration with our silicon and metal partners. Europe is actively considering such a measure (Séjourné, 2020). The Singapore Academy of Law’s Law Reform Committee proposed something similar in 2020 (Rethinking Database Rights and Data Ownership in an AI World, 2020), but only traditional human authorship remains recognized under the new Copyright Act adopted the following year. AI-assisted works may still warrant protection if there is a causal connection to a human exercising input or control, though determining the threshold for that connection is left to the courts (Tan & Liang Tan, 2022).

An indication of the difficulty can be seen in the case of Jason M. Allen, who was denied protection for the work “Théâtre D’opéra Spatial” (see Figure 4), which won first prize at the Colorado State Fair in 2022—before he revealed that it was created using Midjourney. In his subsequent request for copyright protection, he claimed that he revised his prompts “at least 624 times” to achieve the final work, which he also edited with Photoshop.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

S. Chesterman

Figure 5. A recent entrance to paradise. Available at https://commons.wikimedia.org/wiki/File:A_Recent_Entrance_to_Paradise.jpg.

raises a distinction between “ideas” and “expression”—if I explain to an artist in great detail what I want her to draw, that does not give me ownership of the finished work; the Photoshop edits, by contrast, might have been a form of expression. In any case, the US Copyright Office Review Board found that, because Allen was “unwilling to disclaim the AI-generated material,” it was unable to recognize the work.

This was consistent with other high-profile examples of AI-generated works that have been denied protection, such as images generated by Midjourney for Kris Kashtanova’s graphic novel Zarya of the Dawn (while allowing protection for human-arranged portions of the work) (Edwards, 2023a) and Stephen Thaler’s AI-generated “A Recent Entrance to Paradise” (Brodkin, 2023). (See Figure 5.)

Thaler has been a frequent litigant in efforts to persuade courts and intellectual property offices that AI systems themselves can create and own patents and copyrightable works. Despite brief successes in Australia and South Africa (the former reversed on appeal), it remains the case—for the time being—that AI systems themselves can neither create nor own copyrightable or patentable works in their own right (Chesterman, 2020; Padmanabhan & Wadsworth, forthcoming).

# Brave new world?

Generative AI has the potential to transform the arts as well as the knowledge economy. The precise impact is presently unknowable, with a recent study suggesting a “jagged frontier” of innovation across different fields based on a survey of complex, realistic, and knowledge-intensive tasks (Dell’Acqua et al., 2023). Protecting IP rights too strictly could hinder the development of new tools and works enhanced by AI; failing to protect those rights could render millions of jobs unsustainable and undermine the viability of the arts sector in particular. Figure 6 offers a schematic view of how such policy choices might play out, based on the policy responses to the questions raised by AI training and ownership of its output.

Over-protection of IP rights has long been a concern in developing countries. In the past, access to patented pharmaceuticals was a battleground, resolved in part through mechanisms such as Gavi, the

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# Policy and Society 9

# Figure 6. Incentives for humans and AI developers.

Vaccine Alliance (Editorial, 2022). In the context of AI, for all the worries about misuse of technology, there are also concerns about “missed uses” if the Global South is not able to take advantage of it. That in turn requires access to the data, compute, and talent that power AI (UN AI Advisory Body, 2023).

In the near-term, the most important regulatory steps are two forms of transparency in how such models are developed and deployed. Development should at least disclose the origins of the data used to train them, with compensation paid where appropriate; deployment should make clear the relative contribution of AI to new “works,” with a new category of computer-generated work offering a reasonable middle ground between purely human- and purely AI-generated content.

With regard to development and economic sustainability, the music industry offers interesting parallels (Huber, 2023). It also went through a period of unrestrained piracy in the early digital era, which radically transformed the economics of copying and gave rise to file-sharing services such as Napster (Tan, 2017). Lawsuits and legislative changes led to most media platforms adopting copyright policies and takedown protocols (Digital Millennium Copyright Act [DMCA], 1998; Seng, 2014), while those like Napster were shuttered completely (Menn, 2003). Producers and distributors developed technical means to limit copying, but a certain amount of piracy is often priced in as the cost of doing business (Aguiar et al., 2018; Herings et al., 2018).

It is possible that a similar evolution will take place in AI, at least with regard to LLMs. For all the concerns that IP protection will constrain the development of new models, the market for “legitimate” models appears to be growing. Adobe, for example, has built its Firefly tools using training sets consisting only of public domain and licensed works. Shutterstock has also committed to building AI tools with a Contributor Fund to compensate artists (Hayes, 2023). Other approaches are possible, such as the manner in which YouTube allows certain usages of music and other copyrighted material by sharing advertising revenue with owners of the original work through its Content ID system (Edwards, 2018). Another option is provision for content creators to “opt out” of being scraped for their data, either through the site’s robots.txt file or registering its internet protocol (IP) address. At the very least, recognition of the intellectual input that goes into the data training the models should be uncontroversial.

On the deployment of AI models, this connects to the larger question of whether consumers should know whether a given work is the product of a machine or a human. That might seem like a simple question, but AI-assisted decision-making increasingly blurs that line. For many years, certain customer relations chatbots have started on automatic for basic queries, moving through suggested responses that are vetted by a human, escalating up to direct contact with a person for unusual or more complex interactions (Kucherbaev et al., 2018).

For the raw text and images produced by AI, at least, it should be possible to disclose their provenance. To guard against misrepresentation, various efforts are underway to detect AI-generated text through anti-plagiarism software, though these have had mixed success, at best (Barrett et al., 2023; Morris, 2023). A more difficult but effective approach would be to “watermark” text and images in a.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

S. Chesterman

manner that is invisible to users but detectable using a key (Mingjie Li et al., 2023a; Sun et al., 2023; Wang et al., 2023). The prospect of one AI system being policed by another is understandably attractive. Given the likely spread of the underlying software, this would be practical only if it is required by law. Even then, however, the spread of deepfake porn points to the difficulty of policing any such rules and there are practical challenges to the watermarking of text in particular (Elkhatat et al., 2023; Xin Li et al., 2023b).

Much of the energy in this context comes from governments around the world concerned about generative AI being used to produce ever more realistic content at ever greater scale. “Fake news” existed long before Donald Trump—it appeared in the New York Times at least by 1894 (The “A.P.” News, 1894) and in a headline by 1901 (Bartlett, 1901)—but AI-generated videos of Ukrainian President Volodymyr Zelenskyy “surrendering” in 2022 made clear how it might be operationalized as a weapon of war (Geng, 2023, pp. 159–60). Yuval Noah Harari has gone further to argue that such usages of generative AI threaten democracy itself (Harari, 2023). Hyperbole aside, greater understanding of what content is produced by AI and how it is generated would aid regulators in the world in making better decisions, rather than relying on the market and the good graces of technology companies.

# Conclusion

T.S. Eliot once observed that “good authors borrow, great authors steal.” Occasionally, this is taken literally, a case in point being the German writer Helene Hegemann, whose 2010 best-selling novel Axolotl Roadkill lifted entire pages from another novel. When confronted with the apparent theft, the 17-year-old responded that “There’s no such thing as originality, just authenticity” (Ellis, 2010).

Eliot was not, of course, condoning plagiarism. His larger point was to challenge naïve idealization of the creative process: in arts, as much as in science, each new thinker and writer builds on the work of those who have come before. Painters inspire and echo one another; writers offer variations on plots and structures that can be mapped and catalogued (Booker, 2006; Koestler, 1964). This is perhaps clearest in music, where the limits of the heptatonic scale and chord progressions mean that melodies will inevitably echo one another, as Ed Sheeran successfully argued in a case concerning similarities between his hit song “Thinking Out Loud” and Marvin Gaye’s “Let’s Get It On” (Seabrook, 2023).

Precisely how LLMs, multimodal models, and future iterations of “machine learning” actually learn remains a topic of ongoing debate that is beyond the scope of this paper. At base, they rely on statistical engines for pattern matching—leading to their characterization as “stochastic parrots” (Bender et al., 2021) or “autocomplete on steroids.” This is not, generally, how the human mind is understood to work (Chomsky et al., 2023) and one of the known consequences of such machine learning techniques is “hallucinations” or “confabulations” (Edwards, 2023b). Yet, the ability to inspire genuine emotions in humans as early as the computer “psychotherapist” Eliza in the 1960s (Wallace, 2009, pp. 184–85) and ongoing interest in the Turing Test suggest that what is happening within the machine may be less important than our response to it. As AI plays a greater role in our lives, these questions may become more salient, potentially linked with arguments about legal personality and rights for such entities (Chesterman, 2020).

The present article is more focused on the medium term social and economic implications of the activities of those models. With regard to the first question considered here, it may seem pointless to argue that AI models should pay for the use of data when the entire Internet has already been absorbed (Guadamuz, 2023). In addition to the market for “legitimate” models, however, there is evidence that further refinement of those models and the training of new ones depends not just on the volume of data but its quality. In particular, early suggestions that LLMs might continue improving based on synthetic data that they themselves create have foundered on projections that such AI-generated data will “poison” future models (Martínez et al., 2023; Rao, 2023). Presuming that there is an ongoing market for data and the political will to regulate it, the idea that generative AI will have its own “Napster moment” is at least plausible. At the very least, greater transparency on how models are trained is low-hanging fruit that regulators in the European Union (AI Act (EU) 2021), the US (Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, 2023), and elsewhere are grasping.

In that vein, I note that Peter Schoppert independently but prior to this article used a variation of this aphorism (which is sometimes attributed to Picasso by way of Steve Jobs) in a speech to the Asian Pacific Copyright Association in Singapore on 15 November 2022.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# Policy and Society

As regards AI-produced content, existing laws appear capable of holding the line on protection of the rights of human creators. Much of the regulatory attention is focused on the threats posed by the quality and scale of synthetic content and its ability to overwhelm the market by sheer volume or exert influence on populations through deception. Jurisdictions like Singapore that adopted laws intended to address misinformation and disinformation online were criticized as draconian (Jayakumar et al., 2021), but similar tools are increasingly being considered by western liberal democracies also (Bollinger & Stone, 2022; Giusti & Piras, 2021).

Underpinning all of this is the question of how societies choose to regulate the sector. In theory, governments regulate activities to address market failures, or in support of social or other policies. In practice, relationships with industry and political interests may cause politicians to act—or refrain from acting—in less principled ways (Baldwin et al., 2011, pp. 15–24). Though the troubled relationship between Big Tech and government is well documented (Alfonsi, 2019; Romm, 2020), this article focuses on the principles that should guide regulators rather than the interests that might shape regulatory and policy choices in practice.

From a market perspective, failing to protect human-authored works used in the training of generative AI—or offering too much protection to the computer-generated outputs—would reduce the incentive for additional human creations. It is conceivable that this would not be a net loss if AI content more than makes up for the deficit. That would certainly be the case with regard to quantity—and may yet be so with regard to quality. (In discussions of AI content, it is common to hear sage observations that AI will never produce Michalangelo’s “David” or Jane Austen’s Pride and Prejudice. That may be true. Yet, I will never produce such works either—nor, most likely, will you.) Nonetheless, if the concerns about model poisoning are correct, even the AI models themselves will continue to require human creativity to achieve further improvements.

In any case, regulation is not simply about market optimization. If societies value the arts, then investments should be made in them. Again, there is precedent for this—even if it is not particularly inspiring. Photography all but killed portraiture, though painting remains a niche activity (Graw & Lajer-Burcharth, 2016). Motion pictures and television did not lead to the end of live theatre, but far fewer see it today than a century ago. Such art forms, along with dance, opera, orchestral music and the like continue with government subsidies—and, even then, are often regarded as bourgeois conceits (Bennett, 2016). It is possible that human-generated text and images will become similarly rarefied, preserved as a callback to a different era, like Shakespeare’s Globe Theatre.

There are far larger implications, of course, connected to how we relate to knowledge. For the past two decades, “to Google” came to reflect how many questions were formulated. The answers came with a ranked list of responses, which had the salutary consequence of making clear that there were multiple possible answers—along with subtle indications that some of them might be supported by advertisers who paid for the whole enterprise. If, as appears likely, generative AI leads to our interactions with ChatGPT, Gemini, Claude, and so on becoming the first point of inquiry, it is probable that answers will be clear, succinct, and opaque. At that point, understanding the inputs that go into generative AI and who is responsible for its outputs will have political as well as economic consequences.

# Conflict of interest

None declared.

# References

Abbott, R. (2020). The reasonable robot: Artificial intelligence and the law. Cambridge University Press.

Aguiar, L., Claussen, J., & Peukert, C. (2018). Catch me if you can: Effectiveness and consequences of online copyright enforcement. Information Systems Research, 29(3), 656–678. http://dx.doi.org/10.2139/ssrn.2604197

Alfonsi, C. (2019). Taming tech giants requires fixing the revolving door. Kennedy School Review, 19, 166–170.

Alter, A., & Harris, E. A. (2023). Franzen, grisham and other prominent authors sue OpenAI. New York Times, 20 September 2023.

Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith. (2023). 598 U.S. ___ (Supreme Court of the United States).

The ‘A.P.’ News. (1894). New York Times, 10 July 1894.

12 S. Chesterman

Authors Guild v. Google. (2015). 721 F.3d 132 (2nd Cir).

Baldwin, R., Cave, M., & Lodge, M. (2011). Understanding regulation: Theory, strategy, and practice (2nd ed.). Oxford University Press.

Barrett, C., Boyd, B., Burzstein, E., Carlini, N., Chen, B., Choi, J., Roy Chowdhury, A., Christodorescu, M., Datta, A., Feizi, S., Fisher, K., Hashimoto, T., Hendrycks, D., Jha, S., Kang, D., Kerschbaum, F., Mitchell, E., Mitchell, J., Ramzan, Z., … Yang, D. (2023). Identifying and mitigating the security risks of generative AI. arXiv 2308.14840 [Cs.ai].

Bartlett, C. (1901). ‘Fake’ news for Spain. New York Times, 6 October 1901.

Beebe, B. (2008). An empirical study of U.S. copyright fair use opinions, 1978-2005. University of Pennsylvania Law Review, 156(3), 549–624.

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? FAccT ‘21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency March, pp. 610–623.

Bennett, J. T. (2016). Subsidizing culture: Taxpayer enrichment of the creative class. Routledge.

Bollinger, L. C., & Stone, G. R. (eds.) (2022). Social media, freedom of speech, and the future of our democracy. Oxford University Press.

Booker, C. (2006). The seven basic plots. Continuum.

Brodkin, J. (2023). U.S. judge: Art created solely by artificial intelligence cannot be copyrighted. Ars Technica.

Brown, A., Kheria, S., Cornwell, J., & Iljadica, M. (2019). Contemporary intellectual property: Law and policy (5th ed.). Oxford University Press.

Burrow-Giles Lithographic Co v. Sarony. (1884). 111 U.S. 53

Chesterman, S. (2020). Artificial intelligence and the limits of legal personality. International and Comparative Law Quarterly, 69(4), 819–844. https://doi.org/10.1017/S0020589320000366

Chesterman, S. (2021). We, the robots? Regulating artificial intelligence and the limits of the law. Cambridge University Press.

Chomsky, N., Roberts, I., & Watumull, J. (2023). The false promise of ChatGPT. New York Times, 8 March 2023.

Compendium of U.S. Copyright Office Practices, 3rd edition. (2019). U.S. Copyright Office 2019, www.copyright.gov/comp3/docs/3-15-19/compendium-draft.pdf.

Convention for the Protection of Literary and Artistic Works, done at Berne, 9 September 1886.

Copyright Act. (1994). (New Zealand).

Copyright Act. (2021). (Singapore).

Copyright Amendment Act. (1994). (India).

Copyright and Related Rights Act. (2000). (Ireland).

Copyright, Designs and Patents Act. (1988). (United Kingdom).

Copyright Ordinance. (1997). (Hong Kong).

Creamer, E. (2023). Amazon restricts authors from self-publishing more than three books a day after AI concerns. Guardian, 20 September 2023.

D’Auria, G., & Sundararajan, A. Rethinking intellectual property in an era of generative AI. TechREG Chronicle, November 2023.

de Cock Buning, M. (2018). Artificial intelligence and the creative industry: New challenges for the EU paradigm for art and technology by autonomous creation. In W. Barfield & U. Pagallo (Eds.), Research handbook on the law of artificial intelligence (pp. 511–535). Edward Elgar.

Dell’Acqua, F., McFowland, E., III, Mollick, E., Lifshitz-Assaf, H., Kellogg, K. C., Rajendran, S., Krayer, L., Candelon, F., & Lakhani, K. R. (2023). Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality (Harvard Business School Working Paper, September 2023).

Digital Millennium Copyright Act (DMCA). (1998). (U.S.). 105-304.

Directive (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on copyright and related rights in the Digital Single Market and amending Directives 96/9/EC and 2001/29/EC 2019 (EU).

Drucker, P. F. (1959). The landmarks of tomorrow. Harper and Row.

du Sautoy, M. (2019). The creativity code: Art and innovation in the age of AI. Harvard University Press.

Dwivedi, Y. K., Kshetri, N., Hughes, L., Louise Slade, E., Jeyaraj, A., Kumar Kar, A., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Ahmad Albashrawi, M., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., & Wright, R. (2023). “So what if ChatGPT wrote

# Policy and Society

it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71(102642). https://doi.org/10.1016/j.ijinfomgt.2023.102642

Editorial. (2022). Why a vaccine hub for low-income countries must succeed. Nature, 607(7918), 211–212. https://doi.org/10.1038/d41586-022-01895-6

Edwards, D. W. (2018). Circulation gatekeepers: Unbundling the platform politics of Youtube’s content ID. Computers and Composition, 47, 61–74. https://doi.org/10.1016/j.compcom.2017.12.001

Edwards, B. (2023a). Artist receives first known U.S. copyright registration for latent diffusion AI art.” Ars Technica.

Edwards, B. (2023b). Why ChatGPT and Bing Chat are so good at making things up. Ars Technica.

Elkhatat, A. M., Elsaid, K., & Almeer, S. (2023). Evaluating the efficacy of AI content detection tools in differentiating between human and AI-generated text. International Journal for Educational Integrity, 19(1), 17. https://doi.org/10.1007/s40979-023-00140-5

Ellis, L. (2010). T. S. Eliot was wrong. New Yorker, 18 February 2010.

Ellison, S. (2022). Who owns DALL-E images? FindLaw.

Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. (2023). White House.

Generative AI could raise global GDP by 7%. (Goldman Sachs Research, 5 April 2023), https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html.

Geng, Y. (2023). Comparing “Deepfake” regulatory regimes in the United States, the European Union, and China. Georgetown Law Technology Review, 7, 157–178.

Gervais, D. J. (2020). The machine as author. Iowa Law Review, 105, 2053–2106.

Giusti, S., & Piras, E. (eds.) (2021). Democracy and fake news: Information manipulation and post-truth politics. Routledge.

Graw, I., & Lajer-Burcharth, E. (eds.) (2016). Painting beyond itself: The medium in the post-medium condition. Sternberg.

Guadamuz, A. (2022). DALL⋅E goes commercial, but what about copyright? TechnoLlama.

Guadamuz, A. (2023). A scanner darkly: Copyright liability and exceptions in artificial intelligence inputs and outputs. https://ssrn.com/abstract=4371204

Harari, Y. N. (2023). AI has hacked the operating system of human civilisation. Economist.

Hayes, C. (2023). Generative artificial intelligence and copyright: Both sides of the Black Box. https://ssrn.com/abstract=4517799

Herings, P. J.-J., Peeters, R., & Michael, S. Y. (2018). Piracy on the internet: Accommodate it or fight it? A dynamic approach. European Journal of Operational Research, 266(1), 328–339. https://doi.org/10.1016/j.ejor.2017.09.011

Huber, N. (2023). Rapid advances in AI set to upend intellectual property. Financial Times, 14 June 2023.

Jayakumar, S., Ang, B., & Anwar, N. D. (eds.) (2021). Disinformation and fake news. Palgrave Macmillan.

Koestler, A. (1964). The act of creation. Hutchinson.

Kucherbaev, P., Bozzon, A., & Houben, G.-J. (2018). Human-aided bots. IEEE Internet Computing, 22(6), 36–43. https://doi.org/10.1109/MIC.2018.252095348

Lemley, M. A. (2023). How generative AI turns copyright upside down. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4517702

Lemley, M. A., & Casey, B. (2021). Fair learning. Texas Law Review, 99(4), 743–785. https://doi.org/10.2139/ssrn.3528447

Lim, J. (2023). A.I. & copyright – Did Singapore’s Copyright Act 2021 solve copyright problems in the training of A.I.? Legal500.

Li, X., Rongrong, N., Yang, P., Zhiqiang, F., & Zhao, Y. (2023b). Artifacts-disentangled adversarial learning for Deepfake detection. IEEE Transactions on Circuits and Systems for Video Technology, 33(4), 1658–1670. https://doi.org/10.1109/TCSVT.2022.3217950

Li, M., Wang, Z., & Zhang, X. (2023a). An effective framework for intellectual property protection of NLG models. Symmetry, 15(1287), 1287. https://doi.org/10.3390/sym15061287

Maguire, L. (2020). The rhetoric of the page. Oxford University Press.

Martínez, G., Watson, L., Reviriego, P., Alberto Hernández, J., Juarez, M., & Sarkar, R. (2023). Towards understanding the interplay of generative artificial intelligence and the Internet. arXiv 2306.06130. https://doi.org/10.48550/arXiv.2306.06130

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# References

Chesterman, S.

Menéndez, J. (2023). AI-generated artwork is blowing up the economics of art. Medium.

Menn, J. (2003). All the rave: The rise and fall of Shawn Fanning’s Napster. Crown.

Metz, C. (2024). OpenAI says New York Times lawsuit against it is ‘without merit’. New York Times, 8 January 2024.

Mims, C. (2023). AI tech enables industrial-scale intellectual-property theft, say critics. Wall Street Journal, 4 February 2023.

Morris, M. R. (2023). Scientists’ perspectives on the potential for generative AI in their fields. arXiv 2304.01420 [Cs.cy]. https://doi.org/10.48550/arXiv.2304.01420

Nordemann, A. (1999). Germany. In Y. Gendreau, A. Nordemann & R. Oesch (Eds.), Copyright and photographs: An international survey (pp. 135). Kluwer.

Nova Productions v. Mazooma Games. (2007). EWCA Civ [2007] 219.

O’Leary, D. E. (2013). Artificial intelligence and big data. IEEE Intelligent Systems, 28(2), 96–99. https://doi.org/10.1109/MIS.2013.39

Padmanabhan, A., & Wadsworth, T. (forthcoming). A common law theory of ownership for AI-created properties. Journal of the Patent and Trademark Office Society, https://ssrn.com/abstract=4411194

Phelan, R. N., & Carey, M. (2023). ChatGPT and intellectual property (IP) related topics. IP Litigator, 8.

Prassl, J. (2018). Humans as a service: The promise and perils of work in the gig economy. Oxford University Press.

Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts. European Commission 2021. eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206.

Rao, R. (2023). AI-generated data can poison future AI models. Scientific American.

Reisner, A. (2023a). Revealed: The authors whose pirated books are powering generative AI. The Atlantic.

Reisner, A. (2023b). These 183,000 books are fueling the biggest fight in publishing and tech. The Atlantic.

Rethinking Database Rights and Data Ownership in an AI World. Singapore Academy of Law: Law Reform Committee. July 2020.

Revised Issues Paper on Intellectual Property Policy and Artificial Intelligence. World Intellectual Property Organisation 21 May 2020. www.wipo.int/edocs/mdocs/mdocs/en/wipo_ip_ai_2_ge_20/wipo_ip_ai_2_ge_20_1_rev.pdf.

Roberts, H., Cowls, J., Morley, J., Taddeo, M., Wang, V., & Floridi, L. (2021). The Chinese approach to artificial intelligence: An analysis of policy, ethics, and regulation. AI & Society, 36(1), 59–77. https://doi.org/10.1007/s00146-020-00992-2

Romm, T. (2020). Tech giants led by Amazon, Facebook, and Google spent nearly half a billion on lobbying over the past decade, new data shows. Washington Post, 22 January 2020.

Samuelson, P. (1986). Allocating ownership rights in computer-generated works. University of Pittsburgh Law Review, 47, 1185–1228.

Schade, M. (2023). Will OpenAI claim copyright over what outputs I generate with the API? OpenAI, 2023, https://help.openai.com/en/articles/5008634-will-openai-claim-copyright-over-what-outputs-i-generate-with-the-api.

Schoppert, P. The books used to train LLMs. AI and Copyright (Substack), 11 March 2023.

Seabrook, J. The case for and against Ed Sheeran. New Yorker, 5 June 2023.

Second request for reconsideration for refusal to register Théâtre D’opéra Spatial (SR # 1-11743923581; Correspondence ID: 1-5T5320R) 2023. (United States Copyright Office: Copyright Review Board, 5 September 2023). https://fingfx.thomsonreuters.com/gfx/legaldocs/byprrqkqxpe/AI%20COPYRIGHT%20REGISTRATION%20decision.pdf.

Séjourné, S. (2020). Draft report on intellectual property rights for the development of artificial intelligence technologies (European Parliament, Committee on Legal Affairs 24 April 2020), www.europarl.europa.eu/doceo/document/JURI-PR-650527_EN.pdf.

Seng, D. (2014). The state of the discordant union: An empirical analysis of DMCA takedown notices. Virginia Journal of Law & Technology, 18, 369–473. https://doi.org/10.2139/ssrn.2411915

Silberling, A. Science fiction publishers are being flooded with AI-generated stories. TechCrunch, 22 February 2023.

Sun, Y., Liu, T., Panhe, H., Liao, Q., Shaojing, F., Nenghai, Y., Guo, D., Liu, Y., & Liu, L. (2023). Deep intellectual property protection: A survey. arXiv 2304.14613v2. https://doi.org/10.48550/arXiv.2304.14613

Tan, D. (2017). Fair use and transformative play in the digital age. In M. Richardson & S. Ricketson (Eds.), Research handbook on intellectual property in media and entertainment (pp. 102–131). Edward Elgar.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# Policy and Society

# 15

Tan, D. The best things in life are not for free: Copyright and generative AI learning. Singapore Law Gazette, April 2023a.

Tan, D. (2023b). Generative AI and copyright (part 2): Computational data analysis exception and fair use. SAL Practitioner [2023], 25.

Tan, D. (2023c). Generative AI and copyright (Part I): Copyright infringement. SAL Practitioner [2023], 24.

Tan, D., & Liang Tan, W. (2022). AI, author, amanuensis. Journal of Intellectual Property Studies, 5(2), 1–32.

Torrance, A. W., & Tomlinson, B. (forthcoming). Training is everything: Artificial intelligence, copyright, and fair training. Dickinson Law Review https://ssrn.com/abstract=4437680

UN AI Advisory Body. (2023). Interim report: Governing AI for humanity (United Nations, New York, 2023). https://www.un.org/en/ai-advisory-body

Wallace, R. S. (2009). The anatomy of ALICE. In R. Epstein, G. Roberts & G. Beber (Eds.), Parsing the turing test: Philosophical and methodological issues in the quest for the thinking computer (pp. 181–210). Springer.

Wang, J., Xinyang, L., Zhao, Z., Dai, Z., Foo, C.-S., See-Kiong, N., & Kian Hsiang Low, B. (2023). WASA: Watermark-based source attribution for large language model-generated data. arXiv 2310.00646 [Cs.lg]. https://doi.org/10.48550/arXiv.2310.00646

Zikopoulos, P., deRoos, D., Parasuraman, K., Deutsch, T., Giles, J., & Corrigan, D. (2012). Harness the power of big data: The IBM big data platform. McGraw Hill.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

# Policy and Society, 2024, 00(00), 1–15

DOI: https://doi.org/10.1093/polsoc/puae006

# Original Research Article

© The Author(s) 2024. Published by Oxford University Press.

This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

Downloaded from https://academic.oup.com/policyandsociety/advance-article/doi/10.1093/polsoc/puae006/7606572 by National University of Singapore user on 10 March 2025

