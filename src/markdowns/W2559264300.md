# Deep Joint Rain Detection and Removal from a Single Image

# Wenhan Yang, Robby T. Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan

# Abstract

In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in the new rain image models and a novel deep learning architecture. We first modify an existing model comprising a rain streak layer and a background layer, by adding a binary map that locates rain streak regions. Second, we create a new model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the first model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and outputs better representation for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our novel models and architecture, outperforming the state-of-the-art methods significantly. Our codes and data sets will be publicly available.

# 1. Introduction

Restoring rain images is important for many computer vision applications in outdoor scenes. Rain degrades visibility significantly and causes many computer vision systems to likely fail. Generally, rain introduces a few types of visibility degradation. Raindrops obstruct, deform and/or blur the background scenes. Distant rain streaks accumulate and generate atmospheric veiling effects similar to mist or fog, which severely reduce the visibility by scattering light.

- Due to the intrinsic overlapping between rain streaks and background texture patterns, most methods tend to remove texture details in non-rain regions, leading to over-smoothing the regions.
- The degradation of rain is complex, and the existing rain model widely used in previous methods is insufficient to cover some important factors in real rain images, such as the atmospheric veils due to rain streak accumulation, and different shapes or directions of streaks.
- The basic operation of many existing algorithms is on a local image patch or a limited receptive field (a limited).

spatial range). Thus, spatial contextual information in serving rich local details. larger regions, which has been proven to be useful for rain removal [18], is rarely used.

# 4. The first method that addresses heavy rain by introducing an recurrent rain detection and removal network,

where it removes rain progressively, enabling us to obtain good results even in significantly complex cases. Our training and testing data, as well as our codes, will be publicly available.

To achieve the goal, we explore the possible rain models and deep learning architectures that can effectively restore rain images even in the presence of heavy rain. Our ideas are as follows. First, we introduce novel region-dependent rain models. In the models, we use a rain-streak binary map, where 1 indicates the presence of individually visible rain streaks in the pixels, and 0 otherwise. We also model the appearance of rain streak accumulation (which is similar to mist or fog), and the various shapes and directions of overlapping streaks, to simulate heavy rain.

# 2. Related Work

Compared with the video based deraining problem, the single image based problem is more ill-posed, due to the lack of temporal information. Some single-image based rain removal methods regard the problem as a layer separation problem. Huang et al. [22] attempted to separate the rain streaks from the high frequency layer by sparse coding, with a learned dictionary from the HOG features. However, the capacity of morphological component analysis, the layer separation, and learned dictionary are limited. Thus, it usually causes the over-smoothness of the background. In [8], a generalized low rank model is proposed, where the rain streak layer is assumed to be low rank. In [24], Kim et al. first detected rain streaks and then removed them with the nonlocal mean filter. In [27], Luo et al. proposed a discriminative sparse coding method to separate rain streaks from background images. A recent work of [25] exploits the Gaussian mixture models to separate the rain streaks, achieving the state-of-the-art performance, however, still with slightly smooth background.

In recent years, deep learning-based image processing applications emerged with promising performance. These applications include denoising [32, 5, 6, 20, 1], completion [35], super-resolution [10, 11, 9, 29], deblurring [30], deconvolution [36] and style transfer [16, 37], etc. There are also some recent works on bad weather restoration or image enhancement, such as dehazing [7, 34], rain drop and dirt removal [12] and light enhancement [26]. Besides, with the superior modeling capacity than shallow models, DL-based methods begin to solve harder problems, such as blind image denoising [40]. In this paper, we use deep learning to jointly detect and remove rain.

# 3. Region-Dependent Rain Image Model

We briefly review the commonly used rain model, and generalize it to explicitly include a rain-streak binary map. Subsequently, we introduce a novel rain model that captures rain accumulation (atmospheric veils) and rain streaks that have various shapes and directions, which are absent in the existing rain models.

# 3.1. Region-Dependent Rain Image Formation

The widely used rain model [25, 27, 18] is expressed as:

O = B + ˜S, (1)

where B is the background scene without rain streaks, and ˜S is the rain streak layer. O is the captured image with rain streaks. Based on this model, rain removal is regarded as a two-signal separation problem. Namely, given the observation O, removing rain streaks is to estimate the background B and rain streak ˜S, based on the different characteristics of the rain-free images and rain streaks. Existing rain removal methods relying on Eq. (1) suffer from following two deficiencies. First, ˜S has heterogeneous density, and heavy rain regions are much denser than light rain regions. Thus, it is hard to model ˜S with a uniform sparsity level assumption, which is needed for most of existing sparsity-based methods. Second, solving the signal separation problem in Eq. (1) without distinguishing the rain and non-rain regions will cause over-smoothness on the rain-free regions. The main reason for these difficulties lies in the intrinsic complexity to model ˜S. In Eq. (1), S needs to model both location and intensity of rains, thus it is hard for the existing methods to jointly to localize and remove the rain streaks.

To overcome these drawbacks, we first propose a generalized rain model as follows:

O = B + SR, (2)

which includes a new region-dependent variable R to indicate the location of individually visible rain streaks. Here elements in R takes binary values, where 1 indicates rain regions and 0 indicates non-rain regions. Note that, although R can be easily estimated from S via hard-thresholding, modeling R separately from S provides two desirable benefits for learning based rain removal methods: (1) it gives additional information for the network to learn about rain streak regions, (2) it allows a new rain removal pipeline to detect rain regions first, and then to operate differently on rain-streak and non-rain-streak regions, preserving background details.

# 3.2. Rain Accumulation and Heavy Rain

The rain image model introduced in Eq. (2) captures region-dependent rain streaks. In the real world, rain appearance is not only formed by individual rain streaks, but also by accumulating multiple rain streaks. When rain accumulation is dense, the individual streaks can not be seen clearly. This rain streak accumulation, whose visual effect is similar to mist or fog, causes the atmospheric veiling effect as well as blur, especially for distance scenes, as shown in Fig. 2.a. Aside from rain accumulation, in many occasions, particularly in heavy rain, rain streaks can have various shapes and directions that overlap to each other, as shown in Fig. 2.a and 2.b.

# 4. Convolutional Joint Rain Detection and Removal

We construct a convolutional multi-task network to perform JOint Rain DEtection and Removal (JORDER) that solves the inverse problem in Eq. (2) through end-to-end learning. Rain regions are first detected by JORDER to further constrain the rain removal. To leverage more context without losing local details, we propose a novel network structure – the contextualized dilated network – for extracting the rain discriminative features and facilitating the following rain detection and removal.

Figure 3. Synthesized rain images (b) and (c) from (a) following the process of Eq. (2) and Eq. (3), respectively. Instead of synthesizing the rain image with very sparse rain streaks in (b), our proposed rain image model considers both rain accumulation and multiple rain streaks overlapping in (c). Rain streaks with various shapes in different directions are overlapped in our synthesized result and distant scenes are invisible because of the rain accumulation.

# 4.1. Convolutional Multi-Task Networks for Joint Rain Detection and Removal

Relying on Eq. (2), given the observed rain image O, our goal is to estimate B, S and R. Due to the nature of the ill-posed problem, it leads to a maximum-a-posteriori (MAP) estimation:

arg min ||O−B−SR||2 + Pb(B) + Ps(S) + Pr(R),

B,S,R

where Pb(B), Ps(S) and Pr(R) are the enforced priors on B, S and R, respectively. Previous priors on B and S include hand-crafted features, e.g. cartoon texture decomposition [22], and some data-driven models, such as sparse dictionary [27] and Gaussian mixture models [25]. For deep learning methods, the priors about B, S and R are learned from the training data and are embedded into the network implicitly.

The estimation of B, S and R is intrinsically correlated. Thus, the estimation of B benefits from the predicted S and R. To address this, the natural choice is to employ a multi-task learning architecture, which can be trained using multiple loss functions based on the ground truths of R, S and B (see the blue dash box in Fig. 4 and please here ignore the subscript t to denote the recurrence number).

As shown in the figure, we first exploit a contextualized dilated network to extract the rain feature representation F. Subsequently, R, S and B are predicted in a sequential order, implying a continuous process of rain streak detection, estimation and removal. Each of them is predicted based on F:

1. R is estimated by two convolutions on F,
2. S is predicted by a convolution on the concatenation [F, R],
3. B is computed from a convolution on the concatenation [F, R, S, O − RS].

There are several potential choices for the network structures, such as estimating the three variables in the order of S, R, B, or in parallel (instead of sequential). We compare some alternative architectures and demonstrate the superiority of ours empirically in our supplementary material.

# 4.2. Contextualized Dilated Networks

For rain removal tasks, contextual information from an input image is demonstrated to be useful for automatically identifying and removing the rain streaks [18]. Thus, we propose a contextualized dilated network to aggregate context information at multiple scales for learning the rain features. The network gains contextual information in two ways: 1) it takes a recurrent structure, similar to the recurrent ResNet in [38], which provides an increasingly larger receptive field for the following layers; 2) in each recurrence, the output features aggregate representations from three convolution paths with different dilated factors and receptive fields.

Specifically, as shown in the gray region of Fig. 4, the network first transforms the input rain image into feature space via the first convolution. Then, the network enhances the features progressively. In each recurrence, the results from the three convolution paths with different dilated factors are aggregated with the input features from the last recurrence via the identity forward. The dilated convolution [39] weights pixels with a step size of the dilated factor, and thus increases its receptive field without losing resolution. Our three dilated paths consist of two convolutions with the same kernel size 3×3. However, with different dilated factors, different paths have their own receptive field. As shown in the top part of the gray region in Fig. 4, path P2 consists of two convolutions with the dilated factor 2. The convolution kernel is shown as the case of DF= 2. Thus, cascading two convolutions, the three paths have their receptive fields of 5 × 5, 9 × 9 and 13 × 13.

# 4.3. Network Training

Let Frr(·), Frs(·) and Fbg(·) denote the inverse recovery functions modeled by the learned network to generate the estimated rain streak binary map R, rain streak map S and background image B based on the input rain image O. We use Θ to collectively represent all the parameters of the network.

We use n sets of corresponding rain images, background images, rain region maps and rain streak maps {(oi, gi, ri, si)}i=1n for training. We adopt the following joint loss function to train the network parametrized by Θ such that it is capable to jointly estimate ri, si and gi based on the input rain image O.

# Joint Rain Detection and Removal

|P1|DF-1|P2|DF-2|P3|DF-3|
|---|---|---|---|---|---|
| |P1|Rain Mask (R)|Rain Streak (S)|Background (B)| |
|Input (O)|P2|Rain Features Convs|R|y|L|
|Rain Images (O)|P3| |t|Convs|F|
|0,= B| | | |Convs|L|

Figure 4. The architecture of our proposed recurrent rain detection and removal. Each recurrence is a multi-task network to perform a joint rain detection and removal (in the blue dash box). In such a network, a contextualized dilated network (in the gray region) extracts rain features Fₜ from the input rain image Oₜ. Then, Rₜ, Sₜ and Bₜ are predicted to perform joint rain detection, estimation and removal. The contextualized dilated network has two features: 1) it takes a recurrent structure, which refines the extracted features progressively; 2) for each recurrence, the output features are aggregated from three convolution paths (P1, P2 and P3) with different dilated factors (DF) and receptive fields.

on rain image oi: between O and B as T(·). Then, the recurrent rain detection and removal works as follows,

L(Θ) = 1/n ∑ (||Frs (oi; Θ) − si||2 + λ1||Fbg (oi; Θ) − gi||2 − λ2 (log ri,1 ri,1 + log(1 − ri,2)(1 − ri,2))),

[R, S] = T(Ot),

Bt = Ot − t,

with ri,j = ∑ exp {Frs,j (oi; Θ)}, j ∈ {1, 2}. In each iteration t, the predicted residue t is accumulated and propagated to the final estimation via updating Ot and Bt. Note that, although the estimated rain mask Rt and streak St are not casted into the next recurrence directly. However, the losses to regularize them in fact provide strong side information for learning an effective T(·). The final estimation can be expressed as:

Bτ = O0 + ∑ t,

where τ is the total iteration number. Hence, the process removes the rain streak progressively, part by part, based on the intermediate results from the previous step. The complexity of rain removal in each iteration is consequently reduced, enabling better estimation, especially in the case of heavy rains.

# 5. Removing Rains from Real Rain Images

In the previous section, we construct a convolutional multi-task learning network to jointly detect and remove rain streaks from rain images. In this section, we further enhance our network to handle rain accumulation and rain streaks that possibly have various shapes and directions in one image.

# 5.1. Recurrent Rain Detection and Removal

The recurrent JORDAR model can be understood as a cascade of the convolutional joint rain detection and removal networks to perform progressive rain detection and removal and recover the image with increasingly better visibility.

# Architecture

We define the process of the network in the blue dash box of Fig. 4 that generates the residual image be-

Network Training. The recurrent JORDAR network introduces an extra time variable t to the loss function L(Θ) in Eq. (5) and gives L(Θt, t), where L(Θ0,0) = L(Θ0).

When t > 1, L(Θt, t) is equivalent to L(Θ) that replaces oi and Θ by oi,t and Θt, respectively, where oi,t is generated from the t-th iterations of the process Eq. (6) on the initial oi. Then, the total loss function LIter for training T is

τ

LIter({Θ0, ...,Θτ}) = ∑ L(Θt, t). (8)

# 5.2. Joint Derain and Dehaze

Distant rain streaks accumulate and form an atmospheric veil, causing visibility degradation. To resolve this, clearing up the atmospheric veil, which is similar to dehazing or de-fogging, is necessary.

Eq. (3) suggests that dehazing should be the first step in the process of joint deraining and dehazing. Thus, we propose to estimate (B + ∑t=1 St˜) first. However, placing dehazing as a preprocessing has complicated effects on deraining, since all rain streaks (including the ones that are already sharp and clearly visible) are boosted, making the streaks look different from those in the training images.

Hence, in our proposed pipeline, we derain first, then followed by dehazing and at last finished with deraining. This, as it turns out, is beneficial, since dehazing will make the appearance of less obvious rain streaks (which are likely unnoticed by the first round of deraining) become more obvious.

We implement a dehazing network based on the structure of contextualized dilated network, with only one recurrence, trained with the synthesized data generated with the random background reliance and transmission value [7]. We find that the sequential process of derain-dehaze-derain is generally effective (see the supplementary material for the evaluation of other possible sequences). The reason is that some obvious rain streaks, noises and artifacts are removed in the first round deraining. Then, the dehazing cleans up the rain accumulation, enhances the contrast and visibility, and at the same time boosts weak rain streaks. The subsequent deraining removes these boosted rain streaks, as well as artifacts caused by dehazing, making the results cleaner.

# Baseline Methods

We compare the four versions of our approaches, JORDER- (one version of our methods that has only one convolution path in each recurrence without using dilated convolutions), JORDER (Section 4), JORDER-R (Section 5.1), JORDER-R-DEHAZE (Section 5.2) with five state-of-the-art methods: image decomposition (ID) [22], CNN-based rain drop removal (CNN) [12], discriminative sparse coding (DSC) [27], layer priors (LP) [25] and a common CNN baseline for image processing – SRCNN [21], trained for deraining. SRCNN is implemented and trained by ourselves, while other methods are kindly provided by the authors.

# 6. Experimental Results

# Datasets

We compare our method with state-of-the-art methods on a few benchmark datasets: (1) Rain121 [25], which includes 12 synthesized rain images with only one type of rain streak; Rain100L, which is the synthesized data set with only one type of rain streak (Fig. 5.c); (2) Rain20L, which is a subset of Rain100L used for testing the potential network structures in the supplementary material; (3) Rain100H, which is our synthesized data set with five streak directions (Fig. 5.d). Note, while it is rare for a real rain image to contain rain streaks in many different directions, synthesizing this kind of images for training is observed to boost the capacity of the network.

The images for synthesizing Rain100L, Rain20L and Rain100H are selected from BSD200 [28].

1 http://yu-li.github.io/

# Quantitative Evaluation

Table 1 shows the results of different methods on Rain12. As observed, our method considerably outperforms other methods in terms of both PSNR and SSIM. Table 2 presents the results of JORDER and JORDER-R on Rain100H. Note that, our JODDER-R

# Figure 6. Results of different methods on real images.

From top to down: rain image, DSC, LP and JORDER-R.

# Table 1. PSNR and SSIM results among different rain streak removal methods on Rain12 and Rain100L.

|Baseline|Rain12| |Rain100L| | | | |
|---|---|---|---|---|---|---|---|
|Metric|PSNR|SSIM|PSNR|SSIM| | | |
|ID|27.21|0.75|23.13|0.70| | | |
|DSC|30.02|0.87|24.16|0.87| | | |
|LP|32.02|0.91|29.11|0.88| | | |
|CNN|26.65|0.78|23.70|0.81| | | |
|SRCNN|34.41|0.94|32.63|0.94| | | |
|JORDER-|35.86|0.95|35.41|0.96| | | |
|JORDER|36.02|0.96|36.11|0.97| | | |

# Table 2. PSNR and SSIM results among different rain streak removal methods on Rain100H.

|Metric|ID|LP|DSC|
|---|---|---|---|
|PSNR|14.02|14.26|15.66|
|SSIM|0.5239|0.4225|0.5444|
|Metric|JORDER-|JORDER|JORDER-R|
|PSNR|20.79|22.15|23.45|
|SSIM|0.5978|0.6736|0.7490|

# Qualitative Evaluation.

Fig. 6 shows the results of real images. For fair comparisons, we use JORDER-R to process these rain images and do not handle the atmospheric veils on these results, to be consistent with other methods. As observed, our method significantly outperforms them and is successful in removing the majority of rain streaks.

We also compare all the methods in two extreme cases: dense rain accumulation, and heavy rain as shown in Fig. 7.

Figure 7. The examples of JORDER-R-DEHAZE on heavy rain (left two images) and mist images (right two images).

**Table 3. The time complexity (in seconds) of JORDER compared with state-of-the-art methods. JR and JRD denote JORDER-R and JORDER-R-DEHAZE, respectively. (G) and (D) denote the implementation on GPU and CPU, respectively.**
|Scale|CNN (G)|ID|DSC|LP|
|---|---|---|---|---|
|80*80|0.85|449.94|14.32|35.97|
|500*500|6.39|1529.85|611.91|2708.20|
|Scale|JORDER (C)|JORDER (G)|JR (G)|JRD (G)|
|80*80|2.97|0.11|0.32|0.72|
|500*500|69.79|1.46|3.08|7.16|

Figure 8. The results of JORDER-R-DEHAZE in different orders.

# 7. Conclusion and Future Works

In this paper, we have introduced a new deep learning based method to effectively learn to joint remove rain from a single image, even in the presence of rain streak accumulation and heavy rain. A new region-dependent rain image model is proposed for additional rain detection and is further extended to simulate rain accumulation and heavy rains. Based on this model, we developed a fully convolutional network that jointly detects and removes rain. Rain regions are first detected by the network which naturally provides additional information for rain removal. To restore images captured in the environment with both rain accumulation and heavy rain, we introduced a recurrent rain detection and removal network that progressively removes rain streaks, embedded with a dehazing network to remove atmospheric veils. Evaluations on real images demonstrated that our method outperforms state-of-the-art methods significantly.

# References

1. K. Garg and S. K. Nayar. Detection and removal of rain from videos. In Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, volume 1, pages I–528, 2004.
2. F. Agostinelli, M. R. Anderson, and H. Lee. Adaptive multi-column deep neural networks with application to robust image denoising. In Proc. Annual Conf. Neural Information Processing Systems. 2013.
3. P. C. Barnum, S. Narasimhan, and T. Kanade. Analysis of rain and snow in frequency space. Int’l Journal of Computer Vision, 86(2-3):256–274, 2010.
4. J. Bossu, N. Hautière, and J.-P. Tarel. Rain or snow detection in image sequences through use of a histogram of orientation of streaks. International journal of computer vision, 93(3):348–367, 2011.
5. N. Brewer and N. Liu. Using the shape characteristics of rain to identify and remove rain from video. In Joint IAPR International Workshops on SPR and SSPR, pages 451–458, 2008.
6. H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds. arXiv:1211.1544.
7. H. C. Burger, C. J. Schuler, and S. Harmeling. Image denoising with multi-layer perceptrons, part 2: training trade-offs and analysis of their mechanisms. arXiv:1211.1552.
8. B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An end-to-end system for single image haze removal. IEEE Trans. on Image Processing, PP(99):1–1, 2016.
9. Y.-L. Chen and C.-T. Hsu. A generalized low-rank appearance model for spatio-temporally correlated rain streaks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1968–1975, 2013.
10. Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen. Deep network cascade for image super-resolution. In Proc. IEEE European Conf. Computer Vision. 2014.
11. C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. TPAMI, 2015.
12. C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. In ECCV. 2014.
13. D. Eigen, D. Krishnan, and R. Fergus. Restoring an image taken through a window covered with dirt or rain. In Proc. IEEE Int’l Conf. Computer Vision, December 2013.
14. K. Garg and S. K. Nayar. Photorealistic rendering of rain streaks. In ACM Trans. Graphics, volume 25, pages 996–1002, 2006.
15. K. Garg and S. K. Nayar. Vision and rain. Int’l Journal of Computer Vision, 75(1):3–27, 2007.
16. L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm of artistic style. arXiv:1508.06576, 2015.
17. D.-A. Huang, L.-W. Kang, Y.-C. F. Wang, and C.-W. Lin. Self-learning based image decomposition with applications to single image denoising. IEEE Transactions on multimedia, 16(1):83–93, 2014.
18. D.-A. Huang, L.-W. Kang, M.-C. Yang, C.-W. Lin, and Y.-C. F. Wang. Context-aware single image rain removal. In Proc. IEEE Int’l Conf. Multimedia and Expo, pages 164–169, 2012.
19. Q. Huynh-Thu and M. Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800–801, 2008.
20. V. Jain and S. Seung. Natural image denoising with convolutional networks. In Proc. Annual Conf. Neural Information Processing Systems. 2009.
21. Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Trans. Multimedia, pages 675–678, 2014.
22. L. W. Kang, C. W. Lin, and Y. H. Fu. Automatic single-image-based rain streaks removal via image decomposition. IEEE Trans. on Image Processing, 21(4):1742–1755, April 2012.
23. J.-H. Kim, C. Lee, J.-Y. Sim, and C.-S. Kim. Single-image deraining using an adaptive nonlocal means filter. In IEEE Trans. on Image Processing, pages 914–917, 2013.
24. J. H. Kim, C. Lee, J. Y. Sim, and C. S. Kim. Single-image deraining using an adaptive nonlocal means filter. In Proc. IEEE Int’l Conf. Image Processing, pages 914–917, Sept 2013.
25. Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown. Rain streak removal using layer priors. In Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition, pages 2736–2744, 2016.

# References

1. K. G. Lore, A. Akintayo, and S. Sarkar. Llnet: A deep autoencoder approach to natural low-light image enhancement. arXiv preprint arXiv:1511.03995, 2015.
2. Y. Luo, Y. Xu, and H. Ji. Removing rain from a single image via discriminative sparse coding. In Proc. IEEE Int’l Conf. Computer Vision, pages 3397–3405, 2015.
3. D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. IEEE Int’l Conf. Computer Vision, volume 2, pages 416–423, July 2001.
4. C. Osendorfer, H. Soyer, and P. van der Smagt. Image super-resolution with fast approximate convolutional sparse coding. In Neural Information Processing, 2014.
5. C. J. Schuler, M. Hirsch, S. Harmeling, and B. Schölkopf. Learning to deblur. arXiv:1406.7444, 2014.
6. S.-H. Sun, S.-P. Fan, and Y.-C. F. Wang. Exploiting image structural similarity for single image rain removal. In Proc. IEEE Int’l Conf. Image Processing, pages 4482–4486, 2014.
7. P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 2010.
8. Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. on Image Processing, 13(4):600–612, 2004.
9. H. Z. J. P. X. C. Wenqi Ren, Si Liu and M.-H. Yang. Single image dehazing via multi-scale convolutional neural networks. In Proc. IEEE European Conf. Computer Vision, pages 914–917, October 2016.
10. J. Xie, L. Xu, and E. Chen. Image denoising and inpainting with deep neural networks. In Proc. Annual Conf. Neural Information Processing Systems, 2012.
11. L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural network for image deconvolution. In Proc. Annual Conf. Neural Information Processing Systems, 2014.

