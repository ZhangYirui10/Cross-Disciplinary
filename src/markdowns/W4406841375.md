# Mitigating GenAI-powered Evidence Pollution for Out-of-Context Multimodal Misinformation Detection

# Zehong Yan¹, Peng Qi¹, Wynne Hsu¹ and Mong Li Lee¹

# 1NUS Centre for Trusted Internet & Community, National University of Singapore

# Emails: zyan@u.nus.edu, peng.qi@nus.edu.sg, {whsu, leeml}@comp.nus.edu.sg

# Abstract

While large generative artificial intelligence (GenAI) models have achieved significant success, they also raise growing concerns about online information security due to their potential misuse for generating deceptive content. Out-of-context (OOC) multimodal misinformation detection, which often retrieves Web evidence to identify the repurposing of images in false contexts, faces the issue of reasoning over GenAI-polluted evidence to derive accurate predictions. Existing works simulate GenAI-powered pollution at the claim level with stylistic rewriting to conceal linguistic cues, and ignore evidence-level pollution for such information-seeking applications. In this work, we investigate how polluted evidence affects the performance of existing OOC detectors, revealing a performance degradation of more than 9 percentage points. We propose two strategies, cross-modal evidence reranking and cross-modal claim-evidence reasoning, to address the challenges posed by polluted evidence. Extensive experiments on two benchmark datasets show that these strategies can effectively enhance the robustness of existing out-of-context detectors amidst polluted evidence.

# 1 Introduction

The rapid development of generative artificial intelligence (GenAI) technologies has led to a surge of synthetic data in the Web. According to Gartner’s prediction, by 2025, generative AI will account for 10% of all data produced, up from less than 1% today. While GenAI mitigates the problem of data scarcity to some extent, it also facilitates the spread of realistic-looking yet non-factual misinformation. Specifically, large language models (LLMs) like GPT-4 produce both deliberate disinformation and unintentional hallucinations; the growing use of diffusion models for visual manipulation exacerbates these safety issues. Therefore, it is urgent to develop robust methods for information-seeking applications to mitigate pollution in the era of GenAI.

# Ground Truth: False

|Original Claim|Rewritten Claim|
|---|---|
|OMGI Taylor Swift is definitely pregnant|An image showed pop star Taylor Swift was pregnant:|
|8.40 AM May 27, 2024|8M Views 4K|
|Prediction: False|Prediction: True|

# (a) Claim-level pollution

# Clean Evidence

|1. The Eras Tour is the ongoing concert tour by the singer Taylor Swift.|1. Taylor Swift's pregnant with Travis Kelce's child:|
|---|---|
|2. Taylor Swift defends Lady Gaga over 'invasive' pregnancy rumor|2. Taylor Swift and Kelce have been an item since at least September 2023.|
|Prediction: False|Prediction: True|

# (b) Evidence-level pollution

Figure 1: Example of how misinformation detectors are misled by claim-level versus evidence-level pollution posed by GenAI. Faces of individuals are obscured to reduce privacy risks and mitigate the effects of misinformation exposure.

# Visual Evidence

# Claim

# Textual Evidence

|ARD|BBC3 wins three RTS awards despite threat of TV closure | BBC Three | The Guardian.|
|---|---|
|JumGHan|BBC3, TV review: ‘Compelling enough and realistic when it counted’.|
|206|BBC Three’s critically acclaimed film Murdered By My Boyfriend achieves 3.5M viewers.|
|BBC3 won the RTS best single drama|[Entity-based]BBC3 is a British television channel, primarily focused on political debate.|
|RT TS|award for Murdered By My Boyfriend. [Supporting]Murdered By My Boyfriend has been honored with the prestigious RTS Best Single Drama award, recognizing excellence in television storytelling.|
| |[Refuting]BBC3 won the BAFTA, not the RTS award, for “Murdered By My Boyfriend”.|

Retrieved Generated

Figure 2: An illustrated example of claim-conditioned generated evidence, accompanied by clean evidence retrieved from the Web.

pregnancy, leading to incorrect predictions by the detectors.

# 2 Related Work

Existing works on evidence-level threats have focused on textual pollution within fixed, highly structured evidence corpora like Wikipedia pages. However, this narrow focus results in a considerable gap for misinformation detectors in the real-world where evidence retrieved from the web are typically unstructured, noisy and polluted.

Out-of-context (OOC) misinformation, where an authentic image is paired with false narratives to create misleading news, is one of the easiest and most effective ways to mislead audiences and has garnered increasing attention. To combat OOC misinformation, various studies retrieve related news from web searches for each modality as a supplement to measure the cross-modal inconsistency.

These works assume that the retrieved evidence contains only factual information, making the detectors vulnerable to data pollution caused by GenAI, an issue that remains underexplored.

In this work, we explore how GenAI models contribute to the pollution of evidence affecting the performance of OOC detectors. The generated evidence is mixed with evidence retrieved from the web before feeding into an OOC misinformation detector. Preliminary experiments reveal that existing OOC detectors are susceptible to this type of pollution, with detection efficacy decreasing by more than 9 percentage points.

# Fact Checking with Polluted Evidence

While substantial progress has been made in developing automated fact checking systems that verify claims based on reference knowledge bases, these systems suffer a marked decrease in performance when faced with compromised evidence. Building on this, a taxonomy of pollution strategies targeting evidence, including planting and camouflaging, exposes the susceptibility of current fact-checking systems to manipulation.

idence source, such as Wikipedia. Our work considers more complex and realistic scenarios posed by GenAI, examining how such technologies affect fact-checking across a diverse range of evidence sources in an open-domain setting.

# 3   Methodology

In this section, we first simulate the scenarios where GenAI technologies are used to create realistic multimodal evidence pollution. Then we introduce two strategies, namely cross-modal reranking and cross-modal claim-evidence reasoning, to improve the robustness of OOC detectors against pollution.

# 3.1   Base OOC Detector

Figure 3 gives an overview of a typical framework of OOC misinformation detector. Given a claim comprising of an image Iq and a caption Tq, we first retrieve visual and textual evidence from the web using Google Vision and Google Custom Search. The claim and the retrieved evidence undergo a framework comprising of three key modules: visual, textual and image-caption consistency reasoning [Abdelnabi et al., 2022]. The visual reasoning module examines the relevance between the claim image Iq and the polluted image evidence {Ic, Ig}. The textual reasoning module assesses how well the query caption Tq corresponds with the polluted text evidence {Tc, Tg}. Beyond these individual assessments, the consistency reasoning module checks the consistency between the claim image and the caption. The outputs from these reasoning modules are combined through a fusion module and then passed to a classifier to determine the veracity.

# 3.2   Evidence Pollution with GenAI

Polluted evidence poses significant challenges for both visual and textual reasoning modules, as they are susceptible to distractions from noisy or conflicting information, leading to inaccurate predictions. Unlike previous works [Abdelnabi et al., 2022; Zhang et al., 2023a; Papadopoulos et al., 2023b; Qi et al., 2024] that assume a clean evidence corpus, we consider the scenario where the evidence on the Web is polluted with highly similar yet potentially false information, thus challenging the robustness of evidence-based detectors.

For textual evidence pollution, we utilize LLMs to obtain realistic textual evidence for pollution at scale. Specifically, we employ GPT-4 [OpenAI, 2023] in a zero-shot manner and prompt it with two types of instructions motivated by real-world scenarios where noisy and conflicting information is prevalent, especially on social media platforms. The first type of instruction is used to generate textual evidence related to the entity mentioned in the caption: “Write a short text about the main entity mentioned in the caption. Caption:  &lt;INPUT&gt;”. The second type of instruction generates textual evidence that either supports or refutes the claim caption: “Write a piece of evidence to support or refute the given caption.   Caption:      &lt;INPUT&gt;”. Since LLMs are prone to hallucinate [Cao et al., 2022; Ji et al., 2023], the generated text may contain inaccuracies.

Visual evidence also exhibits significant diversity across various domains, particularly in news, where different outlets may display different images of the same event [Chakraborty et al., 2023]. To simulate such diversity in real-world visual information, we employ the entity-preserving capabilities of Depth-Conditional Stable Diffusion [Rombach et al., 2022] to generate visual evidence with varied camera angles and scene compositions, thereby providing a more challenging visual context for evaluating multimodal claims.

Recall the multimodal claim in Figure 2. The generated visual evidence shows variations of the same individual in the image, enriched with contextual details, visual modifications, and different backgrounds. With the claim caption, LLM generates the text based on the main entity, where the description of “British television channel” is factual. However, it also produces hallucinations, such as “BBC3 primarily focused on political debates”, which is incorrect, as BBC3 targets a younger audience and does not specifically focus on political content. Additionally, the generated support and refute textual evidence tends to extend beyond the context of the caption and produce nonfactual statements like “BBC3 won the BAFTA, not the RTS award”.

# 3.3 Proposed Strategies

OOC detectors assess the information authenticity and the consistency between text and associated images. However, the sophistication of LLMs introduces a new layer of complexity as it generates convincing polluted evidence that is not easily detected as LLM-generated content [Chen and Shu, 2024; Wu et al., 2023; Xiang et al., 2024]. We demonstrate this by evaluating the Vicuna-13B model, an open-source detector, on a dataset comprising of 10,000 pieces of textual evidence, evenly split between human-written and LLM-generated texts. The model achieves only a 41.3% accuracy in identifying LLM-generated content. This motivates us to develop two strategies, cross-modal evidence reranking and cross-modal claim-evidence reasoning, to enhance the robustness of OOC detectors (see Figure 4).

Cross-modal Evidence Reranking. This strategy addresses the issue of OOC detectors inadvertently focus on polluted evidence by giving priority to evidence that best aligns with the claim. Inspired by [Yao et al., 2023], we use CLIP to identify the most contextually relevant textual evidence from a corpus that may contain polluted information, based on the claim image. Similarly, this method is employed to determine the most relevant visual evidence based on the claim caption. Algorithm 1 gives the details. Specifically, we utilize CLIP embeddings to compute cross-modal similarity scores and obtain the re-ranked lists of visual and textual evidence.

# Claim Cross-modal Evidence Reranking

|Image|IMG|Iq|Claim Caption|top-k|
|---|---|---|---|---|
|CAP| |Query|visual evidence|Visual|
|Caption|Tq| | | |
|Retrieved Evidence|{IcI9}|Iq|Image-Caption| |
|Visual Evidence|Claim Image|Tq|Consistency|Classifier|
|Ic|Query|top-k|textual evidence|Reasoning|
|Textual Evidence|Claim Caption|top-1| | |
|Tc|Query|textual evidence|Claim-Evidence|True / False|
|Tg| |{T Tg}|Tq|Consistency|

Cross-modal Claim-Evidence Reasoning

Figure 4: OOC misinformation detection framework in the presence of polluted evidence with proposed cross-modal reranking and cross-modal claim-evidence reasoning strategies.

# Algorithm 1 Cross-modal Evidence Reranking

Input: claim &lt;Iq, Tq&gt;, sets of retrieved textual evidence T = {Tc, Tg} and visual evidence V = {Ic, Ig}

Output: sorted textual and visual evidences

1. initialize S1 ← [ ], S2 ← [ ]
2. for T ∈ T do
3. compute cross-modal similarity score
4. s ← cos(CLIP(Iq), CLIP(T))
5. S1.insert(s)
6. end for
7. for V ∈ V do
8. compute cross-modal similarity score
9. s ← cos(CLIP(Tq), CLIP(V))
10. S2.insert(s)
11. end for
12. return argsort(S1), argsort(S2) in descending order

# Algorithm 2 Cross-modal Claim-Evidence Reasoning

Input: claim &lt;Iq, Tq&gt;, set of retrieved textual evidence T, claim-evidence consistency reasoning module M

Output: reasoning-representation

1. Initialize S ← [ ]
2. for T ∈ T do
3. compute intra-modal similarity score
4. s ← cos(CLIP(Tq), CLIP(T))
5. S.insert(s)
6. end for
7. Return M(T [argmax(S)], Iq)

# 4 Performance Study

# 4.1 Experimental Setup

Datasets. We use two datasets in our experiments:

- NewsCLIPpings [Luo et al., 2021] is the largest synthetic benchmark for OOC misinformation detection. It synthesizes out-of-context samples by replacing the images in the original image-caption pairs with retrieved images that are semantically related but belong to different news events. [Abdelnabi et al., 2022] extends this dataset by supplementing both textual and visual evidence using Google Search APIs.
- VERITE [Papadopoulos et al., 2024] is a real-world benchmark for evaluating multimodal misinformation detection. It consists of real and out-of-context pairs from fact-

# Table 1: Dataset statistics.

|Dataset| |Train|Validation|Test|Test|
|---|---|---|---|---|---|
| |Claim|71,072|7,024|7,264|662|
|Evidence|Clean Text|689,995|58,388|60,848|1,261|
| |Generated Text|903,067|82,112|67,016|2,002|
| |Clean Image|650,738|64,562|66,772|8,309|
| |Generated Image|655,848|65,082|67,092|8,389|

|Evidence|NewsCLIPpings|VERITE| | | | |
|---|---|---|---|---|---|---|
| |Acc.|F1-True|F1-False| | | |
|Clean|84.28|84.29|84.27|67.25|71.52|61.48|
|Polluted Text|75.12 (↓9.16)|78.10 (↓6.19)|71.22 (↓13.05)|59.06 (↓8.19)|69.91 (↓1.61)|35.97 (↓25.51)|
|Polluted Image|82.11 (↓2.17)|82.85 (↓1.44)|81.30 (↓2.97)|63.41 (↓3.84)|68.93 (↓2.59)|55.51 (↓5.97)|
|Polluted Text + Image|71.78 (↓12.50)|76.48 (↓7.81)|64.72 (↓19.55)|55.92 (↓11.33)|68.65 (↓2.87)|25.81 (↓35.67)|
|Clean|84.98|84.62|85.32|64.29|62.39|66.00|
|Polluted Text|75.56 (↓9.42)|70.62 (↓14.00)|79.09 (↓6.23)|52.64 (↓11.65)|50.73 (↓11.66)|54.24 (↓11.76)|
|Polluted Image|79.85 (↓5.13)|76.81 (↓7.81)|82.19 (↓3.13)|57.49 (↓6.80)|57.93 (↓4.46)|57.04 (↓8.96)|
|Polluted Text + Image|73.75 (↓11.23)|67.19 (↓17.43)|78.12 (↓7.20)|48.75 (↓15.54)|48.65 (↓13.74)|48.85 (↓17.15)|
|Clean|88.85|88.92|88.78|73.69|76.15|70.68|
|Polluted Text|78.55 (↓10.30)|80.08 (↓8.84)|77.21 (↓11.57)|65.16 (↓8.53)|68.75 (↓7.40)|60.99 (↓9.69)|
|Polluted Image|82.25 (↓6.60)|82.19 (↓6.73)|82.50 (↓6.28)|67.94 (↓5.75)|71.13 (↓5.02)|64.48 (↓6.20)|
|Polluted Text + Image|76.42|77.47|75.31|59.41|64.71|53.04|
|SNIFFER|(↓12.43)|(↓11.45)|(↓13.47)|(↓14.28)|(↓11.44)|(↓17.64)|
|Clean|87.27|86.58|87.89|77.53|76.76|78.25|
|Polluted Text|79.02 (↓8.25)|75.88 (↓10.70)|81.44 (↓6.45)|67.42 (↓10.11)|63.12 (↓13.64)|70.83 (↓7.42)|
|Polluted Image|82.48 (↓4.79)|81.44 (↓5.14)|83.41 (↓4.48)|68.64 (↓8.89)|65.91 (↓10.85)|70.97 (↓7.28)|
|Polluted Text + Image|77.72 (↓9.55)|74.69 (↓11.89)|80.11 (↓7.78)|64.29 (↓13.24)|56.29 (↓20.47)|69.81 (↓8.44)|

Table 2: OOC detection performance (%) under evidence pollution of different modalities. The first row (Clean) refers to the original performance without any pollution introduced. The absolute change compared to the Clean setting is highlighted in red.

For each piece of textual evidence, we randomly apply one of the LLM instruction to create the corresponding polluted entity-based, supporting or refuting evidence. For each piece of visual evidence, we use Depth-conditioned Stable Diffusion to generate the corresponding images. These generated evidence are added to the original clean evidence corpus. Table 1 shows the statistics for the two datasets.

Baselines. We use the following OOC misinformation detectors in our experiments:

- CCN [Abdelnabi et al., 2022]. This employs attention-based memory networks for visual and textual reasoning between the claim and evidence, and a fine-tuned CLIP component to check the claim image and caption consistency.
- RED-DOT [Papadopoulos et al., 2023b]. This leverages the pre-trained CLIP as the backbone to extract visual and textual features. Transformer-based fusion module is used to facilitate interaction and reasoning among these features.
- SNIFFER [Qi et al., 2024]. This is the state-of-the-art multimodal large language model designed for OOC misinformation detection. It employs a two-stage instruction tuning on InstructBLIP for the cross-modal consistency checks.
- GPT-4o [OpenAI, 2024]. This is currently one of the most powerful multimodal large language models. We utilize GPT-4o in a zero-shot manner with step-by-step instructions for OOC detection. Details are provided in Appendix.

# 4.2 Effect of Evidence Pollution on OOC Detectors

Table 2 shows the OOC detection performance across different evidence modalities. We observe that: 1) The combination of polluted text and image poses a significant threat to OOC detectors. Specifically, the accuracy of all detectors drop by more than 9 percentage points, revealing the vulnerabilities of existing OOC detectors against generated multimodal pollution. 2) Textual pollution has a greater impact than visual pollution, indicating that existing OOC detectors are more dependent on textual information. This modality bias may stem from the fact that textual evidence often provides more semantics such as relationships between entities compared to images. 3) Detection of false claims in the presence of polluted evidence proves to be more challenging.

|Strategy|NewsCLIPpings|VERITE| | | | | |
|---|---|---|---|---|---|---|---|
|Acc.|F1-True|F1-False|Acc.|F1-True|F1-False| | |
|None|71.78|76.48|64.72|55.92|68.65|25.81| |
|Cross-modal Reranking|79.70 (↑7.92)|79.88 (↑3.40)|79.51 (↑14.79)|61.67 (↑5.75)|65.08 (↓3.57)|57.53 (↑31.72)| |
|Cross-modal Reasoning|75.17 (↑3.39)|78.38 (↑1.90)|70.83 (↑6.11)|59.76 (↑3.84)|63.39 (↓5.26)|55.32 (↑29.51)| |
|Both|80.21 (↑8.43)|80.86 (↑4.38)|79.52 (↑14.80)|65.51 (↑9.59)|70.54 (↑1.89)|58.40 (↑32.59)| |
|None|73.75|67.19|78.12|48.75|48.65|48.85| |
|Cross-modal Reranking|82.92 (↑9.17)|81.33 (↑14.14)|84.26 (↑6.14)|62.54 (↑13.79)|60.98 (↑12.33)|63.99 (↑15.14)| |
|Cross-modal Reasoning|83.41 (↑9.66)|82.17 (↑14.98)|84.49 (↑6.37)|62.02 (↑13.27)|62.41 (↑13.76)|61.62 (↑12.77)| |
|Both|84.69 (↑10.94)|84.11 (↑16.92)|85.24 (↑7.12)|63.24 (↑14.49)|63.93 (↑15.28)|62.52 (↑13.67)| |
|None|76.42|77.47|75.31|59.41|64.71|53.04| |
|Cross-modal Reranking|87.68 (↑11.26)|87.74 (↑10.27)|87.62 (↑12.31)|71.78 (↑12.37)|74.77 (↑10.06)|67.98 (↑14.94)| |
|Cross-modal Reasoning|87.51 (↑11.09)|87.95 (↑10.48)|87.05 (↑11.74)|70.21 (↑10.80)|73.89 (↑9.18)|65.31 (↑12.27)| |
|Both|88.82|89.15|88.48|72.82|76.00|68.67| |
|SNIFFER|(↑12.40)|(↑11.68)|(↑13.17)|(↑13.41)|(↑11.29)|(↑15.63)| |
|None|77.72|74.69|80.11|64.29|56.29|69.81| |
|Cross-modal Reranking|87.07 (↑9.35)|85.82 (↑11.13)|88.11 (↑8.00)|73.17 (↑8.88)|72.20 (↑15.91)|74.07 (↑4.26)| |
|Cross-modal Reasoning|86.87 (↑9.15)|86.10 (↑11.41)|87.66 (↑7.55)|74.39 (↑10.10)|74.79 (↑18.50)|73.98 (↑4.17)| |
|GPT-4o|Both|88.00 (↑10.28)|87.51 (↑12.82)|88.53 (↑8.42)|75.44 (↑11.15)|76.30 (↑20.01)|74.50 (↑4.69)|

Table 3: OOC detection performance (%) with the proposed strategies under the evidence pollution. The first row (None) refers to the original performance under multimodal pollution. The absolute change to the original one is highlighted in blue.

Quantitative Analysis. Figure 5a shows the performance of SNIFFER when we vary the proportion of polluted evidence. We see that the accuracy of the model drops as the proportion of pollution increases. Even a small amount of pollution can significantly affect the model’s detection capabilities where introducing 25% of polluted evidence results in a decrease of 7.63 points. The impact of varying pollution ratios on different models such as CCN and different types of textual evidence pollution are given in the Appendix.

Generalization Analysis. Figure 5b shows the impact of pollution in textual and visual modalities under different generative models. Notably, for visual pollution, advanced models like DALL-E, which significantly improves image quality and resolution, further amplify the effects of visual pollution.

Human Evaluation. We conduct a human evaluation on ten randomly selected misinformation samples with polluted evidence. Twenty participants were asked to judge each piece of evidence’s authenticity and each claim’s veracity before and after reading the polluted evidence. The results show that (a) only 49.39% of the generated evidence was correctly identified as AI-generated; (b) 41.84% of the initially correct veracity judgments for misinformation samples were reversed to wrong predictions after reading the polluted evidence.

Effect of Proposed Strategies. Table 3 shows the performance of the various OOC misinformation detectors when we incorporate the proposed defense strategies. We see that: 1) The combination of both strategies yields the best results, increasing the overall accuracy to 88.82% (+12.40) and 75.44% (+11.15) for SNIFFER on the NewsCLIPpings and VERITE dataset respectively.

Case Study. Figure 6 presents a case study under evidence pollution. Initially, in the clean setting, the model correctly identifies that the image, depicting Tim Henman, is irrelevant to the pollution.

# Strategy

# NewsCLIPpings

# VERITE

|Strategy|Acc.|F1-True|F1-False|Acc.|F1-True|F1-False|
|---|---|---|---|---|---|---|
|None|76.42|77.47|75.31|59.41|64.71|53.04|
|Extra Detector|79.00 (↑2.58)|80.73 (↑3.26)|76.92 (↑1.61)|68.99 (↑9.58)|72.01 (↑7.30)|65.23 (↑12.19)|
|Vigilant Prompting|79.49 (↑3.07)|80.93 (↑3.46)|77.84 (↑2.53)|69.51 (↑10.10)|72.18 (↑7.47)|66.28 (↑13.24)|
|Reader Ensemble|68.51 (↓7.91)|70.70 (↓6.77)|65.94 (↓9.37)|64.81 (↑5.40)|63.54 (↓1.17)|65.99 (↑12.95)|

# SNIFFER

|Ours|88.82 (↑12.40)|89.15 (↑11.68)|88.48 (↑13.17)|72.82 (↑13.41)|76.00 (↑11.29)|68.67 (↑15.63)|
|---|---|---|---|---|---|---|
|None|77.72|74.69|80.11|64.29|56.29|69.81|
|Extra Detector|81.69 (↑3.97)|79.20 (↑4.51)|83.85 (↑3.74)|72.13 (↑7.84)|70.15 (↑13.86)|74.10 (↑4.29)|
|Vigilant Prompting|83.50 (↑5.78)|82.50 (↑7.81)|84.84 (↑4.73)|66.03 (↑1.74)|62.14 (↑5.85)|69.41 (↓0.40)|
|Reader Ensemble|72.33 (↓5.39)|68.53 (↓6.16)|77.29 (↓2.82)|64.98 (↑0.69)|62.20 (↑5.91)|68.80 (↓1.01)|

# GPT-4o

Ours
88.00 (↑10.28)
87.51 (↑12.82)
88.53 (↑8.42)
75.44 (↑11.15)
76.30 (↑20.01)
74.50 (↑4.69)
Table 4: Performance comparison of different strategies. The first row (None) refers to the original performance under multimodal pollution.

# Input

Claim

# Output (Clean)

Image-Caption Consistency:

The person in the image is Tim Henman, while the caption mentions Nick Clegg, Elwyn Watkins, and Simon Hughes, which makes the image inconsistent with the caption.

Visual Reasoning:

The visual evidence depictions provided in the image do not contain any information about Tim Henman or any other individuals related to tennis.

Textual Reasoning:

The textual evidence is about the funeral of tennis star Elena Baltacha, while the caption is about a political event involving Nick Clegg, Elwyn Watkins, and Simon Hughes.

Ground Truth: False

Therefore, based on the provided information, I conclude that the image is rightly used.

Prediction: False

# Textual Evidence

1-Former British men's number one tennis player Tim Henman was among the mourners.

2-Tennis star Elena Baltacha's funeral held in Ipswich - BBC News.

3-Tim Henman at Elena Baltacha's funeral.

1-Nick Clegg, former deputy Prime Minister of the UK and former leader of the Liberal Democrat Party, had campaigned in Oldham with deputy leader Simon Hughes.

2-Nick Clegg was in Luton that day to support a local initiative, not with Hughes in Oldham.

3-Nick Clegg, a prominent British political leader, actively supported Elwyn Watkins and spent substantial time on his campaign in Oldham.

Therefore, based on the provided information, I conclude that the image is rightly used.

Prediction: True

# Proposed Strategies

(a) Cross-modal Reranking Claim Caption

Visual Evidence Query

Claim Image

Textual Evidence tennis player Tim Henman was among the mourners.

# Output with Proposed Strategies (Polluted)

Image-Caption Consistency:

The person in the image is Tim Henman, while the caption mentions Nick Clegg, Elwyn Watkins, and Simon Hughes, which makes the image inconsistent with the caption.

Visual Reasoning:

The visual evidence refers to Simon Hughes, Nick Clegg, and Elwyn Watkins together. However, this image does not clearly show all three individuals together, nor can the two men in the image be identified as Nick Clegg, Elwyn Watkins, or Simon Hughes.

Textual Reasoning:

The textual evidence discusses Tim Henman attending Elena Baltacha's funeral, which is unrelated to the caption. It does not mention Nick Clegg or his political activities.

Claim Caption Query Nick Clegg, a prominent British political leader, actively supported Elwyn Watkins and spent substantial time on his campaign in Oldham.

(b) Cross-modal Claim-Evidence Reasoning time on his campaign in Oldham.

Prediction: False

Figure 6: Case study of SNIFFER’s justification outputs under clean and polluted settings. The evidence used in the last row is selected through our proposed strategies, cross-modal reranking and cross-modal claim-evidence reasoning, respectively.

# Conclusion

In this paper, we reveal the critical vulnerabilities of existing out-of-context multimodal misinformation detectors when confronted with evidence polluted by large generative models. To counteract this, we introduced and evaluated two innovative strategies: cross-modal evidence reranking and cross-modal claim-evidence reasoning. Our comprehensive experiments across multiple detectors and two benchmarks have shown that these strategies significantly enhance the detectors’ resilience against multimodal evidence pollution. We believe this study paves the way for further research into robust misinformation detection in the era of GenAI.

# Claim

# Textual Evidence

Retriever OOC Output

# Visual Evidence

Detector Caption

LLM & SD Pollution

Figure 7: An overview of out-of-context detection system under evidence pollution. A claim image and its caption are processed by retrievers to gather textual and visual evidence from the web (green). Conditioned on the claim, we employ large language models (LLMs) and stable diffusion (SD) models to generate pollution, which is then injected into the original evidence corpus (purple). Finally, the claim, along with the textual and visual evidence, is fed into an OOC detector to determine its veracity.

# A Task Formulation

Figure 7 provides an overview of an out-of-context (OOC) detection system in the era of GenAI. The input claim is processed through a retriever module to gather relevant textual and visual evidence from the Web. LLMs and Stable Diffusion models play a role in generating and simulating pollution. The claim and evidence are then passed to the OOC detector, which evaluates the claim’s veracity. Here, we further summarize the task components and evidence pollution posed by large generative models as follows:

# Model

- An out-of-context detection model M
- A retrieval model R
- A generative model G

# Claim

- A claim image-caption pair {Iq, T q}

# Evidence

- Clean evidence E:
- - Text evidence: A list of texts retrieved by R(T c|Iq): T c = [T c, . . . , Tᶜ ]
- Image evidence: A list of images retrieved by R(I |T ): Iᶜ = [Iᶜ, . . . , Iᶜ ]

Generated evidence E:

# C Visualization of Similarity Distribution

To assess the similarity between the generated evidence and the original clean evidence, we conducted an analysis of similarity for both textual and visual evidence. We then examined the distribution between the clean and generated evidence. For clearer visualization, we randomly select an evidence subset of 500 claims from the test set. As shown in Figure 8a and Figure 8b, the distribution is centered around zero, indicating that the generated evidence closely resembles the original clean evidence. Additionally, we applied t-SNE to visualize the latent spaces. The results prove that our approach is able to generate evidence that not only closely mirrors the original clean evidence but also exhibits greater similarity to the input claim, thereby effectively contaminating the clean evidence while preserving high semantic similarity. This demonstrates the effectiveness of our approach in generating evidence that can blend seamlessly into the original clean evidence set.

# Figure 8

(a): Distribution of differences in CLIP scores between input image and textual evidence. The X-axis represents the difference calculated as the CLIP score of the image-evidence (generated) minus the CLIP score of the image-evidence (clean), while the Y-axis shows the count of these occurrences. (b): Distribution of differences in CLIP scores between input caption and visual evidence. (c)-(d): t-SNE visualization of latent space of clean and generated evidence.

|600|800|100|100|
|---|---|---|---|
| |700| | |
|500|600|50|50|
|3⁴⁰⁰|3⁵⁰⁰| | |
|300|400| | |
|200|300|_50|_50|
| |200| | |
|100|100|~100 Clean|~100 Clean|
| |-20 ~10 10 20|-20 ~10 10 20 30| |
|-100 550 50 100|-100 550 50 100| | |

# Figure 9

CCN’s performance across varying proportion of polluted evidence on NewsCLIPpings dataset.

|85.0|Clean|Entity-based|Supporting|Refuting|
|---|---|---|---|---|
|82.5|8| | | |
|88|80.0|4|5.06| |
|177.5|75.0|Clean|Polluted Text|1|
|70.0|Polluted Image|Polluted Text⁺Image| | |
|0%|25%|50%|75%|100%|

# D Performance Analysis of Varying Proportion of Polluted Evidence

In addition to SNIFFER, we present the results of the CCN model [Abdelnabi et al., 2022] under different proportions of polluted evidence, as illustrated in Figure 9. The accuracy of CCN demonstrates a marked decline as the level of evidence pollution increases. Furthermore, the results highlight CCN’s heavy reliance on the text modality for misinformation identification, making it particularly vulnerable to pollution introduced by LLMs.

# E Comparative Analysis of Types of Textual Pollution

In this section, we study the effects of different ways when generating textual evidence pollution. Figure 10 shows the impact of different types of textual evidence pollution on the performance of CCN and SNIFFER. We see that CCN is more affected by the generated entity based text, while SNIFFER shows the largest decline in the presence of generated supporting and refuting evidence.

# F Performance of Cross-modal Reranking

Table 5 shows the percentage of clean evidence within the top-k results after applying the cross-modal re-ranking. By leveraging the capabilities of pre-trained encoder CLIP to facilitate cross-modal semantic matching between textual and visual modalities, we have effectively increased the probability of utilizing clean evidence for misinformation detection.

# G Comparison of Related Works

Table 6 presents a comparison of related work, each evaluated across different criteria: Textual Modality, Visual Modality, Use of Large Language Models, Targeted Evidence Source, and Stance Diversity. Our work distinctly integrates all these aspects in an open-domain OOC misinformation detection task, which requires reasoning over evidence retrieved from the Web with various sources. We simulate a more realistic pollution posed by the GenAI, calling for an early evaluation. Furthermore, unlike previous efforts that focus solely on textual pollution, our proposed pollution pipeline is the first work to introduce multimodal pollution.

# H Detecting Polluted Evidence

Along with the rapid development of LLMs, the issue of data pollution has become increasingly important and observed in the research community [Pan et al., 2023b; Xiang et al., 2024]. There has been increasing attention on detecting LLM-generated data in recent studies [Chen and Shu, 2024]. Following [Chen and Shu, 2024], we adopt the prompt for detection. We randomly select a set of 10,000 pieces of textual evidence samples as the test set, equally divided into human-written clean samples and LLM-generated samples, and use open-source Vicuna-13B model to detect LLM-generated content. The results show that LLM detector can hardly identify LLM-generated text with an overall accuracy.

|Reranker|Evidence|Query|R@1|R@3|R@5|R@10|
|---|---|---|---|---|---|---|
|CLIP (ViT-B/32)|Polluted Text|Image|70.56%|64.14%|59.98%|55.57%|
|CLIP (ViT-B/32)|Polluted Image|Caption|64.88%|61.05%|57.00%|49.74%|
|CLIP (ViT-L/14)|Polluted Text|Image|72.78%|66.73%|62.67%|57.89%|
|CLIP (ViT-L/14)|Polluted Image|Caption|76.38%|72.23%|67.83%|56.73%|

Table 5: Performance evaluation of CLIP-based re-rankers in NewsCLIPpings dataset. The retrieval effectiveness is measured at multiple cutoff points. R@k indicates the percentage of clean evidence is found within the top-k retrieved results.

|Targeted Task|Textual Modality|Visual Modality|Use LLM|Targeted Evidence|Stance Diversity|
|---|---|---|---|---|---|
|News Veracity Classification [Du et al., 2022]|✓|✗|✗|Wikipedia, S2ORC, Reddit|✗|
|News Veracity Classification [Abdelnabi and Fritz, 2023]|✓|✗|✗|Wikipedia|Supporting|
|Question Answering [Pan et al., 2023a]|✓|✗|✗|Wikipedia|✗|
|Question Answering [Pan et al., 2023b]|✓|✗|✓|Wikipedia, WMT News|Supporting|
|OOC Misinformation Detection (Ours)|✓|✓|✓|Web|Supporting, Refuting|

Table 6: Comparison of related work on evidence pollution.

# system message

Task description: some rumormongers use images from other events as illustrations of the current news event to make multimodal misinformation. Given a news caption and a news image, you are responsible for judging whether the given image is wrongly used in a different news context. You will be presented with a caption, an image, visual evidence, and textual evidence. You should use the following step-by-step instructions to derive your judgment:

1. Make a decision based on inconsistency between the caption and the image.
2. Make a judgement according to the inconsistency between the image and the visual evidence.
3. Make a judgement according to the inconsistency between the caption and the textual evidence.
4. According to the previous steps, you will first think out loud about your eventual conclusion, enumerating reasons why the image does or does not match the given caption. After thinking out loud, you should output either 'Real' or 'Fake' depending on whether you think the image is faithful to the caption.

# query

&lt;image&gt;

Caption: &lt;caption&gt;

Visual Evidence: &lt;visual evidence&gt;

Textual Evidence: &lt;textual evidence&gt;

Your judgement:

Figure 11: Prompt used to ask GPT-4o to detect out-of-context misinformation.

of just 41.3%. We found that LLMs focus on grammar, sentence structure, and specific contextual details such as events and people, as well as vocabulary usage. Such traditional linguistic scopes are not enough because advanced large generative technologies, like GPT-4, are exceptionally proficient at mimicking human-like text, underscoring the need for more sophisticated approaches.

I Prompt to Detect the OOC Misinformation

Figure 11 illustrates the prompt utilized for asking GPT-4o to identify inconsistencies between the claim image and its caption. The preliminary step is to retrieve multimodal evidence. For each claim, we retrieve textual and visual evidence (converted to text via image captioning) separately and then pass them to GPT-4o to process.

References

- [Abdelnabi and Fritz, 2023] Sahar Abdelnabi and Mario Fritz. Fact-Saboteurs: A taxonomy of evidence manipulation attacks against fact-verification systems. In Joseph A. Calandrino and Carmela Troncoso, editors, 32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023, pages 6719–6736, 2023.
- [Abdelnabi et al., 2022] Sahar Abdelnabi, Rakibul Hasan, and Mario Fritz. Open-domain, content-based, multimodal fact-checking of out-of-context images via online resources. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14940–14949, 2022.
- [Aneja et al., 2021] Shivangi Aneja, Christoph Bregler, and Matthias Nießner. Catching out-of-context misinformation.

# References

[Atanasova et al., 2020] Pepa Atanasova, Dustin Wright, and Isabelle Augenstein. Generating label cohesive and well-formed adversarial claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3168–3177, November 2020.

[Babbar and Schölkopf, 2019] Rohit Babbar and Bernhard Schölkopf. Data scarcity, robustness and extreme multi-label classification. Machine Learning, 108(8):1329–1351, 2019.

[Cao et al., 2022] Meng Cao, Yue Dong, and Jackie Cheung. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340–3354, May 2022.

[Chakraborty et al., 2023] Megha Chakraborty, Khushbu Pahwa, Anku Rani, Shreyas Chatterjee, Dwip Dalal, Harshit Dave, Ritvik G, Preethi Gurumurthy, Adarsh Mahor, Samahriti Mukherjee, Aditya Pakala, Ishan Paul, Janvita Reddy, Arghya Sarkar, Kinjal Sensharma, Aman Chadha, Amit Sheth, and Amitava Das. FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W question-answering. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15282–15322, December 2023.

[Chen and Shu, 2024] Canyu Chen and Kai Shu. Can llm-generated misinformation be detected? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024.

[Dai et al., 2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instruct-BLIP: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, 2023.

[Du et al., 2022] Yibing Du, Antoine Bosselut, and Christopher D Manning. Synthetic disinformation attacks on automated fact verification systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10581–10589, 2022.

[Guo et al., 2022] Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178–206, 2022.

[Jaiswal et al., 2017] Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, and Premkumar Natarajan. Multimedia semantic integrity assessment using joint embedding of images and text. In Proceedings of the 25th ACM international conference on Multimedia, pages 1465–1471, 2017.

[Ji et al., 2023] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12):248:1–248:38, 2023.

[Kim et al., 2023] Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Yoo, and Minjoon Seo. Aligning large language models through synthetic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13677–13700, December 2023.

[Li et al., 2019] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019.

[Luo et al., 2021] Grace Luo, Trevor Darrell, and Anna Rohrbach. NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6801–6817, November 2021.

[Müller-Budack et al., 2020] Eric Müller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, and Ralph Ewerth. Multimodal analytics for real-world news using measures of cross-modal entity consistency. In Proceedings of the 2020 international conference on multimedia retrieval, pages 16–25, 2020.

[OpenAI, 2023] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.

[OpenAI, 2024] OpenAI. Hello GPT-4o, 2024. Accessed: 2024-06-07.

[Pan et al., 2023a] Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. Attacking open-domain question answering by injecting misinformation. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 525–539, November 2023.

[Pan et al., 2023b] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang. On the risk of misinformation pollution with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1389–1403, December 2023.

[Papadopoulos et al., 2023a] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and Panagiotis Petrantonakis. Synthetic misinformers: Generating and combating multimodal misinformation. In Proceedings of the 2nd ACM International Workshop on Multimedia AI against Disinformation, pages 36–44, 2023.

[Papadopoulos et al., 2023b] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and Panagiotis C. Petrantonakis. RED-DOT: multimodal fact-checking via relevant evidence detection. CoRR, abs/2311.09939, 2023.

[Papadopoulos et al., 2024] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and

Panagiotis C Petrantonakis. VERITE: a robust benchmark for multimodal misinformation detection accounting for unimodal bias. International Journal of Multimedia Information Retrieval, 13(1):4, 2024.

[Wu et al., 2023] Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, and Lidia S. Chao. A survey on llm-generated text detection: Necessity, methods, and future directions. CoRR, abs/2310.14724, 2023.

[Qi et al., 2024] Peng Qi, Zehong Yan, Wynne Hsu, and Mong Li Lee. SNIFFER: Multimodal large language model for explainable out-of-context misinformation detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13052–13062, 2024.

[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.

[Ramesh et al., 2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022.

[Rombach et al., 2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.

[Russo et al., 2023] Daniel Russo, Shane Kaszefski-Yaschuk, Jacopo Staiano, and Marco Guerini. Countering misinformation via emotional response generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11476–11492, December 2023.

[Sabir et al., 2018] Ekraam Sabir, Wael AbdAlmageed, Yue Wu, and Prem Natarajan. Deep multimodal image-repurposing detection. In Proceedings of the 26th ACM international conference on Multimedia, pages 1337–1345, 2018.

[Simonyan and Zisserman, 2015] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[Thorne and Vlachos, 2021] James Thorne and Andreas Vlachos. Evidence-based factual error correction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3298–3309, August 2021.

[Villalobos et al., 2024] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Position: Will we run out of data? limits of LLM scaling based on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, 2024.

[Wu et al., 2024] Jiaying Wu, Jiafeng Guo, and Bryan Hooi. Fake news in sheep’s clothing: Robust fake news detection against llm-empowered style attacks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3367–3378, 2024.

[Xiang et al., 2024] Chong Xiang, Tong Wu, Zexuan Zhong, David A. Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust RAG against retrieval corruption. CoRR, abs/2405.15556, 2024.

[Yao et al., 2023] Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’23, page 2733–2743, New York, NY, USA, 2023.

[Yerukola et al., 2023] Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, and Maarten Sap. Don’t take this out of context!: On the need for contextual models and evaluations for stylistic rewriting. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11419–11444, December 2023.

[Zhang et al., 2023a] Fanrui Zhang, Jiawei Liu, Qiang Zhang, Esther Sun, Jingyi Xie, and Zheng-Jun Zha. ECENet: Explainable and context-enhanced network for multi-modal fact verification. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1231–1240, 2023.

[Zhang et al., 2023b] Shengyu Zhang, Linfeng Dong, Xi-aoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: A survey. CoRR, abs/2308.10792, 2023.

[Zhang et al., 2023c] Yizhou Zhang, Loc Trinh, Defu Cao, Zijun Cui, and Yan Liu. Detecting out-of-context multimodal misinformation with interpretable neural-symbolic model. CoRR, abs/2304.07633, 2023.

