# Correlation Filters with Limited Boundaries

Hamed Kiani Galoogahi, Terence Sim, and Simon Lucey

{hkiani, tsim}@comp.nus.edu.sg, simon.lucey@csiro.au

# Abstract

Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(N D logD) in the frequency domain, versus O(D3 + N D2) spatially where D is signal length, and N is the number of signals. Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties. In this paper we demonstrate, however, that this computational efficiency comes at a cost. Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance. In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, and (ii) dramatically reduces boundary effects. Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency.

# Index Terms

Correlation filters, object tracking, pattern detection

# 1 INTRODUCTION

Correlation between two signals is a standard approach to feature detection/matching. Correlation touches nearly every facet of computer vision from pattern detection to object tracking. Correlation is rarely performed naively in the spatial domain. Instead, the fast Fourier transform (FFT) affords the efficient application of correlating a desired template/filter with a signal. Contrastingly, however, most techniques for estimating a template for such a purpose (i.e. detection/tracking through convolution) are performed in the spatial domain [1], [2], [16], [18].

This has not always been the case. Correlation filters, developed initially in the seminal work of Hester and Casasent [12], are a method for learning a template/filter in the frequency domain that rose to some prominence in the 80s and 90s. Although many variants have been proposed [12], [13], [15], [14], the approach’s central tenet is to learn a filter, that when correlated with a set of training signals, gives a desired response (typically a peak at the origin of the object, with all other regions of the correlation response map being suppressed).

Like correlation itself, one of the central advantages of the approach is that it attempts to learn the filter in the frequency domain due to the efficiency of correlation/convolution in that domain.

Interest in correlation filters has been reignited in the vision world through the recent work of Bolme et al. [4] on Minimum Output Sum of Squared Error (MOSSE) correlation filters for object detection and tracking. Bolme et al.’s work was able to circumvent some of the classical problems with correlation filters and performed well in tracking under changes in rotation, scale, lighting and partial occlusion. A central strength of the correlation filter is that it is extremely efficient in terms of both memory and computation.

# 1.1 The Problem

An unconventional interpretation of a correlation filter, is that of a discriminative template that has been estimated from

Fig. 1. (a) Defines the example of fixed spatial support within the image from which the peak correlation output should occur. (b) The desired output response, based on (a), of the correlation filter when applied to the entire image. (c) A subset of patch examples used in a canonical correlation filter where green denotes a non-zero correlation output, and red denotes a zero correlation output in direct accordance with (b). (d) A subset of patch examples used in our proposed correlation filter. Note that our proposed approach uses patches stemming from different parts of the image, whereas the canonical correlation filter simply employs circular shifted versions of the same single patch. The central dilemma in this paper is how to perform (d) efficiently in the Fourier domain. The two last patches of (d) show that D−1 patches near the image border are affected by circular shift in our method which can be greatly diminished by choosing D << T, where D and T indicate the length of the vectorized face patch in (a) and the image in (a), respectively.

an unbalanced set of “real-world” and “synthetic” examples. These synthetic examples are created through the application of a circular shift on the real-world examples, and are supposed to be representative of those examples at different translational shifts. We use the term synthetic, as all these shifted examples are plagued by circular boundary effects and are not truly representative of the shifted example (see Figure 1(c)). As a result the training set used for learning the template is extremely unbalanced with one real-world example for every D − 1 synthetic examples (where D is the dimensionality of the examples). These boundary effects can dramatically affect the resulting performance of the estimated template. Fortunately, these effects can be largely removed (see Section 2) if the correlation filter objective is slightly augmented, but has to be now solved in the spatial rather than frequency domains. Unfortunately, this shift to the spatial domain destroys the computational efficiency that make correlation filters so attractive. It is this dilemma that is at the heart of our paper.

# 1.2 Contribution

In this paper we make the following contributions:

- We propose a new correlation filter objective that can drastically reduce the number of examples in a correlation filter that are affected by boundary effects. We further demonstrate, however, that solving this objective in closed form drastically decreases computational efficiency: Ø(D3 + N D2) versus Ø(N D logD) for the canonical objective where D is the length of the vectorized image and N is the number of examples.
- We demonstrate how this new objective can be efficiently optimized in an iterative manner through an Augmented Lagrangian Method (ALM) so as to take advantage of inherent redundancies in the frequency domain. The efficiency of this new approach is Ø([N + K]T logT) where K is the number of iterations and T is the size of the search window.
- We present impressive results for both object detection and tracking outperforming MOSSE and other leading non-correlation filter methods for object tracking.

# 1.3 Related Work

Bolme et al. [4] recently proposed an extension to traditional correlation filters referred to as Minimum Output Sum of Squared Error (MOSSE) filters. This approach has proven invaluable for many object tracking tasks, outperforming current state of the art methods such as [2], [18]. What made the approach of immediate interest in the vision community was the dramatically faster frame rates than current state of the art (600 fps versus 30 fps). A strongly related method to MOSSE was also proposed by Bolme et al. [5] for object detection/localization referred to as Average of Synthetic Exact Filters (ASEF) which also reported superior performance to state of the art. A full discussion on other variants of correlation filters such as Optimal Tradeoff Filters (OTF) [17], Unconstrained MACE (UMACE) [19] filters, etc. is outside the scope of this paper. Readers are encouraged to inspect [14] for a full treatment on the topic.

# 1.4 Notation

Vectors are always presented in lower-case bold (e.g., **a), Matrices are in upper-case bold (e.g., A) and scalars in italicized (e.g. *a or A). a*(i) refers to the ith element of the vector a. All M-mode array signals shall be expressed in vectorized form a**. M-mode arrays are also known as M-mode matrices, multidimensional matrices, or tensors. We shall be assuming M = 2 mode matrix signals (e.g. 2D image arrays) in nearly all our discussions throughout this paper. This does not preclude, however, the application of our approach to other M = 2 signals.

A M-mode convolution operation is represented as the ∗ operator. One can express a M-dimensional discrete circular shift ∆τ to a vectorized M-mode matrix **a through the notation a[∆τ]. The matrix I denotes a D × D identity matrix and 1 denotes a D dimensional vector of ones. â applied to any vector denotes the M-mode Discrete Fourier Transform (DFT) of a vectorized M-mode matrix signal a such that â ← F(a) = DFa. Where F() is the Fourier transforms operator and F is the orthonormal D × D matrix of complex basis vectors for mapping to the Fourier domain for any D dimensional vectorized image/signal. We have chosen to employ a Fourier representation in this paper due to its particularly useful ability to represent circular convolutions as a Hadamard product in the Fourier domain. Additionally, we take advantage of the fact that diag(â) â = h ◦ a Hadamard product, and diag() â, where ◦ represents the operator that transforms a D dimensional vector into a matrix. The role of filter h or signal â can be interchanged with this property. Any transpose operator > on a complex vector or matrix in this paper additionally takes the complex conjugate in a similar fashion to the Hermitian adjoint [14]. The operator conj(â) applies the complex conjugate to the complex vector a**̂.

# 2 CORRELATION FILTERS

Due to the efficiency of correlation in the frequency domain, correlation filters have canonically been posed in the frequency domain. There is nothing, however, stopping one (other than computational expense) from expressing a correlation filter in the spatial domain. In fact, we argue that viewing a correlation filter in the spatial domain can give: (i) important links to existing spatial methods for learning templates/detectors, and (ii) crucial insights into fundamental problems in current correlation filter methods.

Bolme et. al’s [4] MOSSE correlation filter can be expressed in the spatial domain as solving the following ridge regression problem,

E(**h) = 1/2 ∑i=1N ∑j=1D ||yi(j) − h>xi[∆τj]||2 + λ||h**||2

where **yi ∈ RD is the desired response for the i-th observation xi ∈ RD and λ is a regularization term. C = [∆τ1, . . . , ∆τD] represents the set of all circular shifts for a signal of length D. Bolme et al. advocated the use of a 2D Gaussian of small variance (2-3 pixels) for y**i centered at the

location of the object (typically the centre of the image patch). It is clear in Equation 4, that boundary effects could be removed completely by summing over only a T − D + 1 subset of all the T possible circular shifts. However, as we will see in the following section such a change along with the introduction of P is not possible if we want to solve this objective efficiently in the frequency domain.

The solution to this objective becomes,

N D

h = H-1 ∑ ∑ yi(j)xi[∆τj] (2)

i=1 j=1

where,

N D

H = λI + ∑ ∑ xi[∆τj]xi[∆τj]T (3)

i=1 j=1

Solving a correlation filter in the spatial domain quickly becomes intractable as a function of the signal length D, as the cost of solving Equation 2 becomes O(D3 + N D2).

# 2.3 Efficiency in the Frequency Domain

It is well understood in signal processing that circular convolution in the spatial domain can be expressed as a Hadamard product in the frequency domain. This allows one to express the objective in Equation 1 more succinctly and equivalently as,

N

E(ˆh) = 1 ∑ ||yi - ˆxi ◦ conj(h)||2 + 2||h||2 (5)

i=1

where ˆh, ˆx, ˆy are the Fourier transforms of h, x, y. The complex conjugate of h is employed to ensure the operation is correlation not convolution. The equivalence between Equations 1 and 5 also borrows heavily upon another well known property from signal processing namely, Parseval’s theorem which states that

xTx = D-1 ˆxT ˆx ∀i, j, where x ∈ RD. (6)

The solution to Equation 5 becomes

ˆh = [diag(ˆsxx) + λI]-1 diag(ˆsxy) ˆyi (7)

= ˆsxy ◦ (ˆsxx + λ1)-1

# 2.2 Boundary Effects

A deeper problem with the objective in Equation 1, however, is that the shifted image patches x[∆τ] at all values of ∆τ ∈ C, except where ∆τ = 0, are not representative of image patches one would encounter in a normal correlation operation (Figure 1(c)). In signal-processing, one often refers to this as the boundary effect. One simple way to circumvent this problem spatially is to allow the training signal x ∈ RT to be a larger size than the filter h ∈ RD such that T > D. Through the use of a D × T masking matrix P one can reformulate Equation 1 as,

N T

E(h) = 1 ∑ ∑ ||yi(j) - hTP x[∆τj]||2 + λ||h||2 (4)

i=1 j=1

The masking matrix P of ones and zeros encapsulates what part of the signal should be active/inactive. The central benefit of this augmentation in Equation 4 is the dramatic increase in the proportion of examples unaffected by boundary effects (T−D+1 instead of 1). From this insight it becomes clear that if one chooses T >> D then boundary effects become greatly diminished (Figure 1(d)). The computational cost O(D3 + N T D) of solving this objective is only slightly larger than the cost of Equation 1, as the role of P in practice can be accomplished efficiently through a lookup table.

# 3 OUR APPROACH

A problem arises, however, when one attempts to apply the same Fourier insight to the augmented spatial objective in Equation 4,

N

E(h) = 1 ∑ ||yi - diag(ˆxi) DFP h||2 + 2 ||h||2. (9)

i=1

Unfortunately, since we are enforcing a spatial constraint the efficiency of this objective balloons to O(D3 + N D2) as h must be solved in the spatial domain.

# 3.1 Augmented Lagrangian

Our proposed approach for solving Equation 9 involves the introduction of an auxiliary variable ˆg,

where ˆg and h are the current solutions to the above subproblems at iteration i + 1 within the iterative ADMM.

E(h, ˆg) = 1/N ∑i=1N ||yi - diag(ˆxi) ˆg||2 + λ1 ||h||2

s.t. ˆg = DFP h.

We propose to handle the introduced equality constraints through an Augmented Lagrangian Method (ALM) [6]. The augmented Lagrangian of our proposed objective can be formed as,

Ł(ˆg, h, ζ) = 1/N ∑i=1N ||yi - diag(ˆxi) ˆg||2 + λ2 ||h||2 + ζT(ˆg - DFP h) + μ ||ˆg - DFP h||2

where μ is the penalty factor that controls the rate of convergence of the ALM, and ζ is the Fourier transform of the Lagrangian vector needed to enforce the newly introduced equality constraint in Equation 10. ALMs are not new to learning and computer vision, and have recently been used to great effect in a number of applications [6], [7]. Specifically, the Alternating Direction Method of Multipliers (ADMMs) has provided a simple but powerful algorithm that is well suited to distributed convex optimization for large learning and vision problems. A full description of ADMMs is outside the scope of this paper (readers are encouraged to inspect [6] for a full treatment and review), but they can be loosely interpreted as applying a Gauss-Seidel optimization strategy to the augmented Lagrangian objective. Such a strategy is advantageous as it often leads to extremely efficient subproblem decompositions. A full description of our proposed algorithm can be seen in Algorithm 1. We detail each of the subproblems as follows:

# 3.2 Subproblem g

ˆg* = arg min Ł(ˆg; h, ζ)

= (ˆs + μh - ζ) ◦ (ˆsxy + μ1)

where h = DFPTh. In practice ˆh can be estimated extremely efficiently by applying a FFT to h padded with zeros implied by the PT masking matrix.

# 3.3 Subproblem h

h* = arg min Ł(h; g, l)

= (μ + λ1 -1) (μg + l)

where g = 1/PFTˆ and l = √D/PF ζ. In practice both g and l can be estimated extremely efficiently by applying an inverse FFT and then applying the lookup table implied by the masking matrix P.

# 3.4 Lagrange Multiplier Update

ζ(i+1) ← ζ(i) + μ(ˆg(i+1) - h(i+1))

# 3.5 Choice of μ

A simple and common scheme for selecting μ is the following μ(i+1) = min(μmax, βμ(i)).

We found experimentally μ(0) = 10-2, β = 1.1 and μmax = 20 to perform well.

# 3.6 Computational Cost

Inspecting Algorithm 1 the dominant cost per iteration of the ADMM optimization process is Ø(T log T) for FFT. There is a pre-computation cost (before the iterative component, steps 4 and 5) in the algorithm for estimating the auto- and cross-spectral energy vectors ˆs and ˆsxy respectively. This cost is Ø(N T log T) where K represents the number of ADMM iterations the overall cost of the algorithm is therefore Ø([N + K]T log T).

# Algorithm 1 Our approach using ADMMs

1. Initialize h(0), l(0). Pad with zeros and apply FFT: DFPTh(0) → ˆ(0)
2. Apply FFT: DFl(0) → ˆ(0) h.
3. Estimate auto-spectral energy ˆsxx using Eqn. (8).
4. Estimate cross-spectral energy ˆsxy using Eqn. (8).
5. i = 0
6. repeat
7. Solve for ˆg(i+1) using Eqn. (12), h(i) & ζ(i).
8. Inverse FFT then crop: DFPTg(i+1) → ˆ(i+1).
9. Inverse FFT then crop: DFPTζ(i) → l(i+1).
10. Solve for h using Eqn. (13), g(i+1) & l(i).
11. Pad and apply FFT: DFPTh(i+1) → ˆ(i+1).
12. Update Lagrange multiplier vector Eqn. (14).
13. Update penalty factor Eqn. (15).
14. i = i + 1 until ˆg, h, ζ has converged.

# 4 EXPERIMENTS

# 4.1 Localization Performance

In the first experiment, we evaluated our method on the problem of eye localization, comparing with prior correlation filters, e.g. OTF [17], MACE [15], UMACE [19], ASEF [5], and MOSSE [4]. The CMU Multi-PIE face database was used for this experiment, containing 900 frontal faces with neutral expression and normal illumination. We randomly

1. http://www.multipie.org/

selected 400 of these images for training and the reminder

|MOSSE|ASEF|UMACE|MACE|OTF|Our method|
|---|---|---|---|---|---|
|1|1| | | | |
|0.5|0.5|0.5| | | |
|0|Localization rate| | | | |
|0|Localization rate at d &lt; 0.10|0|1|50|100|
|150|200|250|300|350|400|
| |0.05|0.10|0.15|0.20| |

Number of training images

Threshold (fraction of interocular distance)

(a)

(b)

Fig. 2. Eye localization performance as a function of (a) number of training images, and (b) localization thresholds.

|UMACE|ASEF|MOSSE|Our method|
|---|---|---|---|
| | | |D|

PSR = 3.1 PSR = 8.4 PSR = 9.3 PSR = 15.7

Fig. 3. An example of eye localization is shown for an image with normal lighting. The outputs (bottom) are produced using 64x64 correlation filters (top). The green box represents the approximated location of the right eye (output peak). The peak strength measured by PSR shows the sharpness of the output peak.

The average of evaluation results across 10 random runs are depicted in Figure 2, where our method outperforms the other approaches across all thresholds and training set sizes. The accuracy of OTF and MACE declines by increasing the number of training images due to over-fitting. During the experiment, we observed that the low performance of the UMACE, ASEF and MOSSE was mainly caused by wrong localizations of the left eye and the nose. This was not the case for our method, as our filter was trained in a way that return zero correlation values when centred upon non-target patches of the face image. A visual depiction of the filters and their outputs can be seen in Figure 3, illustrating examples of wrong and correct localizations. The Peak-to-Sidelobe Ratio (PSR) values show that our method returns stronger output compared to the other filters.

Moreover, we examined the influence of T (the size of training images) on the performance of eye localization. For this purpose, we employed cropped patches of the right eye with varying sizes of T = {D, 1.5D, 2D, 2.5D, 3D, 3.5D, 4D} to train filters of size D = 32 × 32. The localization results are illustrated in Figure 4(a), showing that the lowest performance obtained when T is equal to D and the localization rate improved by increasing the size of the training patches with respect to the filter size. The reason is that by choosing T > D the portion of patches unaffected by boundary effects reduces.

# 4.2 Runtime Performance

This experiment demonstrates the advantage of our approach to other iterative methods. Specifically, we compared our proposed approach against other methods in literature for learning. We evaluated our method with different number of iterations {1, 2, 4, 8, 16, 32, 64}, as shown in Figure 4(b), and eventually.

# 6

|Sequence|Frames|Main Challenges|
|---|---|---|
|0.7|0.6|0.5|
|Faceocc1|886|Moving camera, occlusion|
|Faceocc2|812|Appearance change, occlusion|
|Girl|502|Moving camera, scale change|
|Sylv|1344|Illumination and pose change|
|Tiger1|354|Fast motion, pose change|
|David|462|Moving camera, illumination change|
|Cliffbar|472|Scale change, motion blur|
|Coke Can|292|Illumination change, occlusion|
|Dollar|327|Similar object, appearance change|
|Twinings|472|Scale and pose change|

# Fig. 4

(a) The localization rate obtained by different sizes of training images (T), the size of the trained filter is D = 32 × 32. (b) The position error of tracking versus the number of ADMM iterations. We selected 4 iterations as a tradeoff between tracking performance and computation.

# Fig. 5

Runtime performance of our method against another naive iterative method (steepest descent method) [20]. Our approach enjoys superior performance in terms of: (a) convergence speed to train two filters with different sizes (32x32 and 64x64) and (b) the number of iterations required to converge.

A visual depiction of tracking results for some selected videos is shown in Figures 6 and 7, where our method achieved higher precisions over all videos except Tiger1 and Twinings. Moreover, Figure 6(b) shows that our approach suffers from less drift over the selected test videos.

# 5 CONCLUSIONS

A method for estimating a correlation filter is presented here that dramatically limits circular boundary effects while preserving many of the computational advantages of canonical frequency domain correlation filters. Our approach demonstrated superior empirical results for both object detection and real-time tracking compared to current state of the arts.

# REFERENCES

[1] A. Adam, E. Rivlin, and I. Shimshoni. Robust fragments based tracking using the integral histogram. In CVPR, 2006.

[2] B. Babenko, M. H. Yang, and S. Belongie. Visual tracking with online multiple instance learning. In CVPR, 2009.

[3] B. Babenko, M.-H. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. PAMI, 33(8):1619–1632, 2011.

[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In CVPR, 2010.

[5] D. S. Bolme, B. A. Draper, and J. R. Beveridge. Average of synthetic exact filters. In CVPR, 2009.

[6] S. Boyd. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 3:1–122, 2010.

[7] A. Del Bue, J. Xavier, L. Agapito, and M. Paladini. Bilinear Modelling via Augmented Lagrange Multipliers (BALM). PAMI, 34(8):1–14, Dec. 2011.

[8] H. Grabner, M. Grabner, and H. Bischof. Real-time tracking via on-line boosting. In BMVC, 2006.

[9] H. Grabner, C. Leistner, and H. Bischof. Semi-supervised on-line boosting for robust tracking. In ECCV. Springer, 2008.

[10] S. Hare, A. Saffari, and P. H. Torr. Struck: Structured output tracking with kernels. In ICCV, 2011.

| |MOSSE|KMOSSE|MILTrack|STRUCK|OAB(1)|SemiBoost|FragTrack|Our method|
|---|---|---|---|---|---|---|---|---|
|FaceOcc1|{1.00, 7}|{1.00, 5}|{0.75, 17}|{0.97, 8}|{0.22, 43}|{0.97, 7}|{0.94, 7}|{1.00, 8}|
|FaceOcc2|{0.74, 13}|{0.95, 8}|{0.42, 31}|{0.93, 7}|{0.61, 21}|{0.60, 23}|{0.59, 27}|{0.97, 7}|
|Girl|{0.82, 14}|{0.44, 35}|{0.37, 29}|{0.94, 10}|-|-|{0.53, 27}|{0.90, 12}|
|Sylv|{0.87, 7}|{1.00, 6}|{0.96, 8}|{0.95, 9}|{0.64, 25}|{0.69, 16}|{0.74, 25}|{1.00, 4}|
|Tiger1|{0.61, 25}|{0.62, 25}|{0.94, 9}|{0.95, 9}|{0.48, 35}|{0.44, 42}|{0.36, 39}|{0.79, 18}|
|David|{0.56, 14}|{0.50, 16}|{0.54, 18}|{0.93, 9}|{0.16, 49}|{0.46, 39}|{0.28, 72}|{1.00, 7}|
|Cliffbar|{0.88, 8}|{0.97, 6}|{0.85, 12}|-|{0.44, 46}|{0.76, -}|-|{1.00, 5}|
|Coke Can|{0.96, 7}|{1.00, 7}|{0.58, 17}|{0.97, 7}|{0.45, 25}|{0.78, 13}|{0.15, 66}|{0.97, 7}|
|Dollar|{1.00, 4}|{1.00, 4}|{1.00, 7}|{1.00, 13}|{0.67, 25}|{0.37, 67}|{0.40, 55}|{1.00, 6}|
|Twinings|{0.48, 16}|{0.89, 11}|{0.76, 15}|{0.99, 7}|{0.74, -}|-|{0.82, 14}|{0.99, 9}|
|mean|{0.80, 11}|{0.84, 12}|{0.72, 16}|{0.91, 12}|{0.53, 31}|{0.62, 29}|{0.51, 37}|{0.97, 8}|
|fps|600|100|25|11|25|25|2|50|

TABLE 2

The tracking performance is shown as a tuple of {precision within 20 pixels, average position error in pixels}, where our method achieved the best performance over 8 of 10 videos. The best fps was obtained by MOSSE. Our method obtained a real-time tracking speed of 50 fps using four iterations of ADMM. The best result for each video is highlighted in bold.

Coke

Clifbar

David

Faceocc2

| |MOSSE|KMOSSE|MILTrack|STRUCK|FragTrack|Our method| | | | |
|---|---|---|---|---|---|---|---|---|---|---|
|Precision|1|1|1|1|1|1| | | | |
| |0.8|0.8|0.8|0.8|0.8|0.8| | | | |
| | | | | |0.6|0.6|0.6|0.6|0.6|0.6|
|0.4|MILTrack|MILTrack|MILTrack|MILTrack|MILTrack| | | | | |
|0.2|FragTrack|FragTrack|FragTrack|FragTrack|FragTrack| | | | | |
| |0|0|0|0|0|0| | | | |

Threshold

Girl

Sylv

Twinings

Tiger1

| | |MOSSE|KMOSSE|MILTrack|STRUCK|FragTrack|Our method| | |
|---|---|---|---|---|---|---|---|---|---|
|Position Error (pixel)|140| | | |120|160|120|100|80|
|100|80|60|40|20|0| | | | |

Frame #

Girl

Sylv

Twinings

Tiger1

| | |MOSSE|KMOSSE|MILTrack|STRUCK|FragTrack|Our method| | |
|---|---|---|---|---|---|---|---|---|---|
|Position Error (pixel)| |200| | |120|40|30|20|10|
|0|0|0|0|0|0| | | | |

(a)

Fig. 6. Tracking results for selected videos, (a) precision versus the thresholds, and (b) position error per frame.

# Cliffbar

|#57|#1QQ|#222|#269|#322| |
|---|---|---|---|---|---|
|David|#4T|#160|#289|4380| |
|Faceocc2|#100 L|#396|#490|#700|#803|
|Girl|#30|#193|#325|#362|#461|
|Sylv|#301|#619|#10C61|#1282| |

Fig. 7. Tracking results of our method over the test videos with challenging variations of pose, scale, illumination and partial occlusion. The blue (dashed) and red boxes respectively represent the ground truth and the positions predicted by our method. For each frame, we illustrate the target, trained filter and correlation output.

1. J. F. Henriques, R. Caseiro, P. Martines, and J. Batista. Exploiting the circulant structure of tracking-by-detection with kernels. In ECCV, 2012.
2. C. F. Hester and D. Casasent. Multivariant technique for multiclass pattern recognition. Appl. Opt., 19(11):1758–1761, 1980.
3. B. V. K. V. Kumar. Minimum-variance synthetic discriminant functions. J. Opt. Soc. Am. A, 3(10):1579–1584, 1986.
4. B. V. K. V. Kumar, A. Mahalanobis, and R. D. Juday. Correlation Pattern Recognition. Cambridge University Press, 2005.
5. A. Mahalanobis, B. V. K. V. Kumar, and D. Casasent. Minimum average correlation energy filters. Appl. Opt., 26(17):3633–3640, 1987.
6. N. C. Oza. Online Ensemble Learning. PhD thesis, U. C. Berkley, 2001.
7. P. Refregier. Optimal trade-off filters for noise robustness, sharpness of the correlation peak, and horner efficiency. Optics Letters, 16:829–832, 1991.
8. D. Ross, J. Lim, R. Lin, and M. Yang. Incremental learning for robust visual tracking. IJCV, 77(1):125–141, 2008.
9. M. Savvides and B. V. K. V. Kumar. Efficient design of advanced correlation filters for robust distortion-tolerant face recognition. In AVSS, pages 45–52, 2003.
10. M. Zeiler, D. Krishnan, and G. Taylor. Deconvolutional networks. CVPR, 2010.

