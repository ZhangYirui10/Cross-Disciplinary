# Enhancing Photographs with Near Infrared Images

# Xiaopeng Zhang, Terence Sim, Xiaoping Miao

# School of Computing

# National University of Singapore

# {zhangxi7,tsim,miaoxiao}@comp.nus.edu.sg

# Abstract

Near Infra-Red (NIR) images of natural scenes usually have better contrast and contain rich texture details that may not be perceived in visible light photographs (VIS). In this paper, we propose a novel method to enhance a photograph by using the contrast and texture information of its corresponding NIR image. More precisely, we first decompose the NIR/VIS pair into average and detail wavelet subbands. We then transfer the contrast in the average subband and transfer texture in the detail subbands. We built a special camera mount that optically aligns two consumer-grade digital cameras, one of which was modified to capture NIR. Our results exhibit higher visual quality than tone-mapped HDR images, showing that NIR imaging is useful for computational photography.

# 1. Introduction

The radiance from natural scenes usually spans a very wide dynamic range, far exceeding what a digital camera can capture. For instance, in a sunny outdoor environment, the dynamic range could reach as high as 109. In contrast, a professional-grade digital camera that uses 14 bits per channel can capture a range of only 104. Consumer-grade cameras are even worse. One common technique around this problem is to first compute a high dynamic range (HDR) image, usually from multiple shots of varying exposures, and then to map this into a lower dynamic range (LDR) image suitable for display devices. However, such a tone-mapping procedure does not usually produce a perceptually pleasing result. Usually, pixels end up becoming too bright or too dark, and rich scene information such as color and texture are almost completely lost.

Fig. 1(a) shows a typical photo taken under an HDR environment, where the footpath is very bright but the region inside the building can barely be seen. Recently, many methods have been proposed to recover HDR radiance map and convert it to LDR images. Debevec and Malik presented a way to recover HDR radiance from multiply exposed photographs. Mapping HDR to LDR, also known as tone mapping, can be roughly divided into two categories: spatially uniform mapping and spatially varying mapping. For instance, Reinhard et al. used a spatially uniform operator to compress dynamic range globally and then manipulated contrast locally based on luminance values. A review of tone mapping techniques can be found in [5]. From HDR photos, it is possible to retrieve all details and color correctly. But obtaining an HDR map requires multiple images captured with different exposures. This in turn requires the scene to be static, which greatly limits its applicability.

Another class of techniques involves modifying the imaging sensor. For example, Nayar and Branzoi use an LCD panel to modulate the light falling onto the sensor, while Nayar et al. use a digital micromirror array for the same purpose. The goal is to adaptively control the exposure of small groups of pixels according to the radiance falling on them. Another possible solution, widely used by professional photographers, is to take photos in RAW format and manually adjust contrast region by region. Usually, RAW pictures use 12 or 14 bits per channel to record scene radiance, thus resulting in a higher dynamic range than normal JPEG photos. Such manual adjustment is tedious and requires experience, and the dynamic range of RAW format is still quite limited.

In contrast, our method uses Near Infrared (NIR) light. This lies between visible red light and Long Infra-Red (LIR) light in the electromagnetic spectrum. NIR light has a wavelength in the range 750 − 1400 nm, which is longer than visible light (380 − 750 nm). Human eyes cannot see NIR light but most digital cameras can sense it very well. For example, some models of SONY digital cameras or camcorders have a Night Shot mode which increases cameras' visual range by letting the sensor acquire more NIR light. However, most manufacturers insert an IR cutoff filter over the camera sensor to filter out NIR light, to avoid some unwanted artifacts. In fact, NIR images usually have better brightness contrast and provide rich texture details, as seen.

# Figure 1

We proposed a novel image enhancement method by transferring contrast and texture from near infrared image to visible image.

Fig. 1(a) and Fig. 1(b) are an improper exposed photo taken under high dynamic range environment and its corresponding near infrared photo. With only these two input images, our approach can adaptively and automatically adjust contrast and enrich visible details in over- or under-exposed areas, as shown in Fig. 1(c).

In Fig. 1(a) and 1(b), the details of trees and leaves are barely seen in the visible image, but look clear and sharp in the NIR image. We exploit this fact in our work.

- A threshold-free method to detect regions that require enhancement.
- A study of the statistical properties of NIR images of natural scenes.

As far as we can tell, we are the first to use NIR for photograph enhancement.

# 2. Near Infrared Imaging

# 2.1. Dual-camera system

NIR light lies adjacent to visible red light in the electromagnetic spectrum, and has longer wavelength than visible light. NIR is not visible to human eyes, but can be recorded by CCD or CMOS sensors. However, most manufacturers of digital cameras install an IR cutoff filter over the sensor to suppress infrared light and avoid unwanted artifacts.

To capture both visible and NIR pictures for the same scene simultaneously, we built a dual-camera system which comprises two Sony F828 digital cameras and one hot mirror. A hot mirror is a specialized dielectric mirror which can reflect NIR light when incident light arrives at a certain angle. We used a 45° hot mirror, meaning it can reflect NIR light with angle of incidence of 45° but does not block visible light. Fig. 2(a) illustrates how our system works.

Although the Sony F828 has built-in Night Shot mode which can temporarily move the IR cutoff filter away to allow NIR imaging, Sony has intentionally limited such NIR imaging to only allow long exposure times. Our modified camera does not suffer from this limitation. We also modified the remote control of the camera so that it can trigger two cameras at the same time. We have carefully setup two cameras to ensure that they are optically aligned. They also share the same camera settings, such as focal length and aperture.

# Camera V

# Visible Light

|Visible Light|0.5|Average Histogram of Gradient Magnitude|0.45|Average Histogram of H−subband|
|---|---|---|---|---|
|Hot|1ear Infra-red Light|0.4| | |
|Mirror|0.35| | | |
|1IR Light|0.25| | | |
| |0.2| | | |
| |0.15| | | |
| |0.1| | | |
|Camera 1|0.05| | | |

00  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  −0.50  −0.4  −0.3  −0.2  −0.1  0  0.1  0.2  0.3  0.4  0.5

# (a) Dual-camera System

# (b) Gradient Magnitude

# (c) Wavelet Coefficient H

Figure 2. (a): Our VIS-NIR dual-camera prototype. Camera V and N are optically aligned and connected to the same remote control, allowing a VIS/NIR image pair of the same scene to be captured with a single shot. (b-c): Statistical properties of NIR images. (b) shows distribution of gradient magnitude, similar to statistics of visible images [7]. (c) shows distribution of H wavelet subband of Haar transform, similar to statistics of IR images [9]. Subbands V and D have similar distribution.

Currently, we do not force the two cameras to use the same shutter speed, because digital cameras are designed to be less sensitive to NIR thus requiring a slightly longer exposure. The NIR picture captured in this way is actually an RGB color image and looks reddish since NIR light is just adjacent to red light. However, because of the filters we use, the NIR light we capture is almost monochromatic and should not contain any color information. So we use only intensity information by converting to HSV color space and using V channel. Fig.1(a) and Fig. 1(b) show an example image pair captured by our dual-camera system. Our prototype hardware may look bulky, but this can be miniaturized. Our goal is to show the usefulness of NIR images.

# 3. Visible Image Enhancement

The workflow of our approach is illustrated in Fig. 3. There are three main steps: computing the weighted region mask, transferring contrast and transferring texture. The details are explained in Sec. 3.1, Sec. 3.2 and Sec. 3.3 respectively. Note that all inputs are the logarithm of original image values as we mentioned in the previous section.

# 2.2. Statistics of NIR images

Huang and Mumford [7] have shown that the gradient histograms of natural images follow a generalized Laplace distribution which can be expressed as Eq. 1:

P (x) = k · e−|x/s|.

Recently, Morris et al. [9] found wavelet coefficients of LIR (wavelength lies in 4000 − 120000 nm) images of natural scenes can also be well fitted with a Laplacian curve. In this paper, we show that NIR natural images share similar statistical properties, as illustrated in Fig. 2. We collect a total of 220 NIR photos for statistical analysis. Some of them are collected from web and others are captured by ourselves, mostly covering subjects of natural scene and people. Similar to [7, 9], we use gradient magnitude and the Haar wavelet coefficients. We calculate the histograms of all images for both gradient magnitude and wavelet coefficients in horizontal (H), vertical (V) and diagonal (D) directions. All these histograms are calculated on logarithm of the actual values and normalized based on image pixels. In Fig. 2, all gray lines denote the actual histograms, the blue lines show the average histogram distribution, and the red dash lines show the fitted Laplacian curve (Eq. 1). We can see that the fit is good, meaning that NIR images have similar statistical properties as visible and LIR images (Please refer details to [7, 9]). In Sec.3.2.2 we will show how we can use these statistical properties to guide the enhancement process.

# 3.1. Computing the weighted region mask

Intuitively, regions that suffer a loss of details are typically too bright or too dark, and have low saturation. From this observation, a weighted mask can be calculated based on saturation and brightness value. Let Ws and Wv denote weighted mask of saturation and brightness, W denote the final weighted region mask indicating areas to be enhanced. Then W can be obtained using following equations:

W = 1 − e−pˢ|s−1|, p ∈ [0,1], s ∈ [0,1]

Wv = 1 − e−pᵛ|v−0.5|, pv ∈ [0,1], v ∈ [0,1]

W = Wₛ · Wv,

where s and v are the saturation and brightness intensity, and ps and pv denote the probability that s and v appear in visible image respectively. ps and pv can be easily obtained from the normalized histograms of channels S and V. The meaning of ps and pv is that the pixels to be enhanced should distribute over large areas, rather than in small regions. Enhancing large areas while ignoring small regions usually achieves better perceptual quality.

# Compute Weighted Region Mask

Vis      Vis - V                V   VH             Transfer                V’ (HSV)                             VV  VD             Contrast

Vis (HSV)   Vis - V      Haar                                                         Haar⁻¹          Vis – V    Vis

1ir    1ir                  1   1H             Transfer                 VH’                    Vis – V    Vis

1V  1D                  Texture         VV’ VD’                    Enhanced Enhanced

Visible Image         Weighted Region Mask       did, we show that histogram matching in the gradient mag-

magnitude can achieve better and reliable results.

# 3.2.1 Histogram matching

The histogram matching problem can be simply defined as: given an image I and a target histogram (pdf) h(z), the problem is to find a new image J by transforming I, so as to make histogram of J be as same as h. The problem can be solved by using the cumulative distribution function (CDF), f. Define f(x) = R x h(z)dz, where x is image intensity. Let I and J denote each pixel intensity in I and J.

Then the desired image J can be obtained using Eq. 3, and the detailed proof can be found in [6].

Jij = f−1(fI(Iij))                          (3)

# 3.2.2 Large-Scale contrast transfer

The brightness contrast of a visible image is affected by environment illumination, as well as object shape and texture in the scene. Therefore, the brightness map of an image should change smoothly while preserving major features such as strong edges. To achieve a smooth brightness map of visible image V and NIR image N (V and N are actually the average subbands in the Haar decomposition, as shown in Fig. 3), we apply bilateral filtering [14] to decompose images to large-scale layer and detail layer, and use the larger-scale layer as brightness map, as in Eq. 4:

V = bf(V), VNᴸ = bf(ND)

ND = N − NL.

VL and NL are large-scale layers, and VD, ND are corresponding detail layers (after taking the logarithm). We use a similar definition for the bilateral filter function bf and parameter selection as in Bae et al.’s work.

We implement three different methods to transfer contrast from the NIR image to the VIS image. A comparison of their results can be found in Fig. 5 and 6.

# Method 1: Histogram Matching

Inspired by Bae et al.’s method [2], we can simply match intensity histogram of VL with NL to transfer intensity distribution. This method is easy and efficient, but histogram matching blindly alters pixel values and thus very possibly destroy illumination consistency. From Fig. 5, we see that histogram matching does improve the contrast significantly. However, we also see that pixels in the tree bark are over brightened and inconsistent with the illumination in the original image. After applying the gradient constraint, the result looks more natural.

# Method 2: Histogram Matching with Gradient Constraint

To maintain illumination consistency, we can check the gradient direction of the altered brightness map pixel by pixel. Once we find the gradient direction that is reversed or changed too much from the original brightness map, we force them to be zero. After applying the gradient constraint, the enhanced result looks more natural compared with method 1 (see Fig. 5). But in some cases, where gradients change abruptly along their original directions due to the histogram matching step, this constraint will fail, as shown in Fig. 6. The gradient constraint cannot remove the banding-effect on the pillar and wall, because the gradients in those areas are not actually reversed.

From VG₀ and VG₀, we reconstruct new large-scale brightness map VL₀ by using Agrawal et al.’s improved Poisson solver [1]. The final contrast transferred V₀ is obtained by blending enhanced brightness map and its original version V together using alpha-blending:

V₀ = W ·(VL₀ + VD) + (1 − W) ·V,

# Method 3: Gradient Magnitude Matching

To strictly maintain smoothness of the transferred brightness map, we match the histogram of brightness gradient magnitude instead of brightness intensity. We define VG and NG as the gradient magnitude of VL and NL:

VG = q(VGx² + VGy²) = q((∂V/∂x)² + (∂V/∂y)²)

NG = q(NGx² + NGy²) = q((∂N/∂x)² + (∂N/∂y)²).

In Sec. 2.2 we have shown that gradient magnitude histogram of NIR image can be well fitted with a generalized Laplacian curve. Because NL is a smoothed version of the NIR image, its gradient magnitude NG also has same statistical property. Let l denote the Laplacian curve that can fit histogram of NG. Instead of matching histogram of VG with histogram of NG directly, we use l as the target histogram to produce a smoother and noise-free distribution transfer. In this case, the functions fI and fJ in Eq. 3 are the CDFs of l. Let VG₀ denote the histogram matching result, we can easily compute new gradients by scaling VGₓ and VGy along their original directions respectively:

VG₀ = VG₀ · VGx

VG₀ = VG₀ · VGy.

# 3.3. Transferring texture

As we state in our workflow (Fig.3), after applying Haar wavelet transformation, the wavelet subbands in horizontal, vertical, and diagonal directions actually contain rich texture information. To transfer those details, we use alpha blending again to combine corresponding subbands together:

VH⁰ = W · NH + (1 − W)· VH.

VH₀ and VD₀ are obtained similarly. The new subbands VH₀, VV₀, and VD₀ not only inherit texture details from the VIS image, but are also enhanced by rich high frequency details from NIR image. Fig. 7(g) shows the result with high frequency details transferred. The textures on the roof in the original image are almost lost completely. By transferring.

(a) Visible Image

(a) Original Visible Image

(b) Weighted Region Mask

(b) Histogram Matching (Method 1)

(c) Cropped Roof Visible Image

(c) HM + Gradient Constraint (Method 2)

(d) Cropped Roof NIR Image

(d) HM of Gradient Magnitude (Method 3)

(e) Texture Transfer Only

(f) Contrast Transfer Only

(g) Texture Transfer + Contrast Transfer

Figure 6. Comparison of Methods 1, 2, and 3. Note that the band effects (regions in red box) due to blind histogram matching have been successfully suppressed by our guided histogram matching of gradient magnitude. The result of Method 3 achieves the least artifacts and best perceptual quality.

High frequency details from NIR to visible image, those lost textures are successfully recovered, and those weak textures are also reinforced greatly. Finally, we apply inverse Haar wavelet transform to enhance the V channel.

# 4. Experiments and Results

A common HDR scene is the natural outdoor environment under bright sunlight. To demonstrate the strength of our techniques, we test our approach with pictures taken in such HDR situations. All input visible and NIR image pairs have been geometrically aligned. In outdoor daylight, tree leaves and some objects, such as cloth and skin, reflect NIR light strongly, so they look bright and have much details even in shaded areas (see Fig. 1, 8, and 9). Such features in NIR images are useful for enhancing visible images.

We also find that contrast transfer and texture transfer are both equally important for enhancement. As shown in Fig. 7: Fig. 7(e) is the enhanced result with only texture transfer. Fig. 7(c-g) show the zoomed-in roof details.

transferred, where most of roof details are successfully re- repair). Benefiting from the better contrast in the NIR images covered but the picture still looks over-exposed; Fig. 7(f) is the result with only contrast transferred, which has lower contrast but roof details are still lost. Obviously, after transferring the contrast and texture, Fig. 7(g) exhibits better visual quality.

To show that histogram matching of gradient magnitude (Method 3) can preserve overall illumination map and achieve higher perceptual quality, we compare of our results with a naively blended output (see Fig. 8(d) and Fig. 9(d)). This trivial result is obtained using alpha-blending of V and N based on the weighted region mask W, i.e. using each pixel value in W as alpha value.

We also compare our results with tone-mapped HDR images of same scenes, as shown in Fig. 8(h) and Fig. 9(e). To get the HDR image, we take multiple images with different exposure (usually 5-7 pictures with exposure difference of 1 stop), and assemble them using the HDR Shop software developed by Debevec [3]. We recovered the camera response curve of our camera to generate HDR image more precisely, and we applied Reinhard et al.’s algorithm for tone mapping [13]. Tone-mapped HDR images are supposed to produce a range-compressed image with rich details and high visible quality. However, tone mapping algorithms usually have a strict assumption that the scene must be static, i.e. no moving objects throughout the whole image sequence. Such an assumption is easily broken in outdoor scenes, as shown in Fig. 8(h). The walking pedestrian and leaves waving in the wind cause serious “ghosting effect” (shown in red boxes) in the tone mapped results. Because the inputs of our approach are captured in a single shot, our results are free of such artifacts. Besides, our approach preserves consistency of overall illumination distribution while recovering scene details, therefore our results gain better perceptual quality on brightness contrast than tone-mapped results, as shown in Fig. 8(h) and Fig. 9(e).

# 6. Acknowledgments

We thank all reviewers for their valuable suggestions. We also thank Shaojie Zhuo and Mai Lan Ha, for their insightful discussions. We acknowledge the generous support of NUS.

# References

[1] A. Agrawal, R. Raskar, and R. Chellappa. What is the range of surface reconstructions from a gradient field? In ECCV, 2006. 5

[2] S. Bae, S. Paris, and F. Durand. Two-scale tone management for photographic look. In SIGGRAPH, 2006. 2, 4, 5

[3] P. Debevec. HDR Shop. http://www.hdrshop.com/. 7

[4] P. E. Debevec and J. Malik. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH, 1997. 1

[5] K. Devlin. A review of tone reproduction techniques. Technical report, University of Bristol, 2002. 1

[6] R. C. Gonzalez and R. E. Woods. Digital Image Processing. Prentice Hall, 2002. 4

[7] J. Huang and D. Mumford. Statistics of natural images and models. In CVPR, 1999. 3

[8] C. Maher and L. Berman. Fine art infrared photography. http://www.infrareddreams.com/. 2

[9] N. J. W. Morris, S. Avidan, W. Matusik, and H. Pfister. Statistics of infrared images. In CVPR, 2007. 2, 3

[10] S. Nayar and V. Branzoi. Adaptive Dynamic Range Imaging: Optical Control of Pixel Exposures over Space and Time. In ICCV, 2003. 1

[11] S. Nayar, V. Branzoi, and T. Boult. Programmable Imaging using a Digital Micromirror Array. In CVPR, 2004. 1

[12] L. Neumann and A. Neumann. Color style transfer techniques using hue, lightness and saturation histogram matching. In Computational Aesthetics in Graphics, Visualization and Imaging, 2005. 2

[13] E. Reinhard, M. Stark, P. Shirley, and J. Ferwerda. Photographic tone reproduction for digital images. In SIGGRAPH, 2002. 1, 7

[14] C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In ICCV, 1998. 4

# Figure 8. Comparison of our approach with alpha-blending and HDR tone mapping.

|(a) Visible Image|(b) NIR Image|(c) Weighted Region Map|(d) Alpha-blending|
|---|---|---|---|
|(e) Method 1|(f) Method 2|(g) Method 3|(h) HDR Tone Mapping|

The naive alpha-blending result appears bad, since simple pixel-wise blending cannot transfer overall contrast. As for HDR tone mapping, the result exhibits “ghosting effect” (shown in red boxes) because objects have moved during the capture of multiple exposures.

# Figure 9. Another comparison of our approach with alpha-blending and HDR tone mapping.

|(a) Visible Image|(b) NIR Image|(c) Weighted Region Map|
|---|---|---|
|JKA FC7707E FC?|(Fc7787e FC?)|'Fc?|
|4918|4918| |
|(d) Alpha-blending|(e) HDR Tone Mapping|(f) Our Result (Method 3)|

Our method successfully enhances brightness and texture by using NIR information.

