# Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures

# Wenqiang Lei‡∗, Xisen Jin§∗, Zhaochun Ren†, Xiangnan He‡, Min-Yen Kan‡, Dawei Yin†

# ‡National University of Singapore, Singapore

# §Fudan University, Shanghai, China

# †Data Science Lab, JD.com, Beijing, China

{wenqianglei,xisenjin}@gmail.com renzhaochun@jd.com kanmy@comp.nus.edu.sg xiangnanhe@gmail.com yindawei@acm.org

# Abstract

Existing solutions to task-oriented dialogue systems follow pipeline designs which introduce architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by an order of magnitude. It significantly outperforms state-of-the-art pipeline-based methods on two datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.

# 1 Introduction

The challenge of achieving both task completion and human-like response generation for task-oriented dialogue systems is gaining research interest. Wen et al. (2017b, 2016a, 2017a) pioneered a set of models to address this challenge. Their proposed architectures follow traditional pipeline designs, where the belief tracking component is the key component (Chen et al., 2017).

In the current paradigm, such a belief tracker builds a complex multi-class classifier for each slot (See §3.2) which can suffer from high complexity, especially when the number of slots and their values grow. Since all the possible slot values have to be pre-defined as classification labels, such trackers also cannot handle the requests that have out-of-vocabulary (OOV) slot values. Moreover, the belief tracker requires delexicalization, i.e., replacing slot values with their slot names in utterances (Mrkˇsi´c et al., 2017). It does not scale well, due to the lexical diversity. The belief tracker also needs to be pre-trained, making the models unrealistic for end-to-end training (Eric and Manning, 2017a). While Eric and Manning (2017a,b) investigated building task-oriented dialogue systems by using a seq2seq model, unfortunately, their methods are rather preliminary and do not perform well in either task completion or response generation, due to their omission of a belief tracker.

Questioning the basic pipeline architecture, in this paper, we re-examine the tenets of belief tracking in light of advances in deep learning. We introduce the concept of a belief span (bspan), a text span that tracks the belief states at each turn. This leads to a new framework, named Sequicity, with a single seq2seq model. Sequicity decomposes the task-oriented dialogue problem into the generation of bspans and machine responses, converting this problem into a sequence optimization problem. In practice, Sequicity decodes in two stages: in the first stage, it decodes a bspan to facilitate knowledge base (KB) search; in the second, it decodes a machine response on the condition of knowledge base search result and the bspan.

Our method represents a shift in perspective compared to existing work. Sequicity employs a single seq2seq model, resulting in a vastly simplified architecture. Unlike previous approaches with an overly parameterized delexicalization-based belief tracker, Sequicity achieves much less training complexity.

* Work performed during an internship at Data Science Lab, JD.com.

Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1437–1447 Melbourne, Australia, July 15 - 20, 2018. © 2018 Association for Computational Linguistics

ing time, better performance on larger a dataset and an exceptional ability to handle OOV cases. Furthermore, Sequicity is a theoretically and aesthetically appealing framework, as it achieves true end-to-end trainability using only one seq2seq model. As such, Sequicity leverages the rapid development of seq2seq models (Gehring et al., 2017; Vaswani et al., 2017; Yu et al., 2017) in developing solutions to task-oriented dialogue scenarios. In our implementation, we improve on CopyNet (Gu et al., 2016) to instantiate Sequicity framework in this paper, as key words present in bspans and machine responses recur from previous utterances. Extensive experiments conducted on two benchmark datasets verify the effectiveness of our proposed method.

# 1. Our contributions are fourfold:

1. We propose the Sequicity framework, which handles both task completion and response generation in a single seq2seq model;
2. We present an implementation of the Sequicity framework, called Two Stage CopyNet (TSCP), which has fewer number of parameters and trains faster than state-of-the-art baselines (Wen et al., 2017b, 2016a, 2017a);
3. We demonstrate that TSCP significantly outperforms state-of-the-art baselines on two large-scale datasets, inclusive of scenarios involving OOV;
4. We release source code of TSCP to assist the community to explore Sequicity1.

# 2. Related Work

Historically, task-oriented dialog systems have been built as pipelines of separately trained modules. A typical pipeline design contains four components: 1) a user intent classifier, 2) a belief tracker, 3) a dialogue policy maker and a 4) response generator. User intent detectors classify user utterances to into one of the pre-defined intents. SVM, CNN and RNN models (Silva et al., 2011; Hashemi et al., 2016; Shi et al., 2016) perform well for intent classification. Belief trackers, which keep track of user goals and constraints every turn (Henderson et al., 2014a,b; Kim et al., 2017) are the most important component for task accomplishment. They model the probability distribution of values over each slot (Lee, 2013). Dialogue policy makers then generate the next available system action. Recent experiments suggest that reinforcement learning is a promising paradigm to accomplish this task (Young et al., 2013a; Cuayáhuitl et al., 2015; Liu and Lane, 2017), when state and action spaces are carefully designed (Young et al., 2010). Finally, in the response generation stage, pipeline designs usually pre-define fixed templates where placeholders are filled with slot values at runtime (Dhingra et al., 2017; Williams et al., 2017; Henderson et al., 2014b,a). However, this causes rather static responses that could lower user satisfaction. Generating a fluent, human-like response is considered a separate topic, typified by the topic of conversation systems (Li et al., 2015).

# 3. Preliminaries

# 3.1 Encoder-Decoder Seq2seq Models

Current seq2seq models adopt encoder–decoder structures. Given a source sequence of tokens X = x1x2...xn, an encoder network represent X as hidden states: H(x) = h1(x)h2(x)...hn(x). Based on H(x), a decoder network generates a target sequence of tokens Y = y1y2...ym whose likelihood should be maximized given the training corpus. As of late, the recurrent neural network with attention (Att-RNN) is now considered a baseline encoder–decoder architecture. Such networks employ two (sometimes identical) RNNs, one for encoding (i.e., generating H(x)) and another for decoding. Particularly, for decoding yj, the decoder RNN takes the embedding yj−1 to generate a hidden vector h(y). Afterwards, the decoder attends to X: calculating attention scores between all hi ∈ H(x) and h(y) (Eq. (1)), and then sums all hi(x), weighted by their corresponding attention scores (Eqs. (2)). The summed result hj(x) concatenates hj as a single vector which is mapped into an output space for a softmax operation (Eq. (3)) to decode the current token:

uij = vT tanh(W1hi(x) + W2hj(y)) (1)

hj(x) = ∑i=1n ∑j euij(x) hi (2)

yj = softmax(O hj(y)) (3)

where v ∈ R1×l; W1, W2 ∈ Rl×d and O ∈ R|V|×d. d is embedding size and V is vocabulary set and |V| is its size.

# 3.2  Belief Trackers

In the multi-turn scenarios, a belief tracker is the key component for task completion as it records key information from past turns (Wen et al., 2017b; Henderson et al., 2013, 2014a,b). Early belief trackers are designed as Bayesian networks where each node is a dialogue belief state (Paek and Horvitz, 2000; Young et al., 2013b). Recent work successfully represents belief trackers as discriminative classifiers (Henderson et al., 2013; Williams, 2012; Wen et al., 2017b).

Wen et al. (2017b) apply discrimination approaches (Henderson et al., 2013) to build one classifier for each slot in their belief tracker. Following the terminology of (Wen et al., 2017b), a slot can be either informable or requestable, which have been annotated in CamRes676 and KVRET. Individually, an informable slot, specified by user utterances in previous turns, is set to a constraint for knowledge base search; whereas a requestable slot records the user’s need in the current dialogue. As an example of belief trackers in CamRes676, food type is an informable slot, and a set of food types is also predefined (e.g., Italian) as corresponding slot values. In (Wen et al., 2017b), the informable slot food type is recognized by a classifier, which takes user utterances as input to predict if and which type of food should be activated, while the requestable slot of address is a binary variable. address will be set to true if the slot is requested by the user.

# 4   Method

We now describe the Sequicity framework, by first explaining the core concept of bspans. We then instantiate the Sequicity framework with our introduction of an improved CopyNet (Gu et al., 2016).

# 4.1  Belief Spans for Belief Tracking

The core of belief tracking is keeping track of informable and requestable slot values when a dialogue progresses. In the era of pipeline-based methods, supervised classification is a straightforward solution. However, we observe that this traditional architecture can be updated by applying seq2seq models directly to the problem. In contrast to (Wen et al., 2017b) which treats slot values as classification labels, we record them in a text span, to be decoded by the model. This leverages the state-of-the-art neural seq2seq models to learn and dynamically generate them. Specifically, our bspan has an information field (marked with <Inf></Inf>) to store values of informable slots since only values are important for knowledge base search. Bspans can also feature a requested field (marked with <Req></Req>), storing requestable slot names if the corresponding value is True.

At turn t, given the user utterance Ut, we show an example of both bspan Bt and machine response Rt generation in Figure 1, where annotated slot values at each turn are decoded into bspans. B1 contains an information slot Italian because the user stated “Italian food” in U1. During the second turn, the user adds an additional constraint cheap resulting in two slot values in B2’s information field. In the third turn, the user further asks for the restaurant’s phone and address, which are stored in requested slots of B3. Our bspan solution is concise: it simplifies multiple sophisticated classifiers with a single sequence model. Furthermore, it can be viewed as an explicit data structure that expedites knowledge base search as its format is fixed: following (Wen et al., 2017b), we use the informable slots values directly for matching fields of entries in databases.

# 4.2 The Sequicity Framework

We make a key observation that at turn t, a system only needs to refer to Bt−1, Rt−1 and Ut to generate a new belief span Bt and machine response Rt, without appealing to knowing all past utterances. Such Markov assumption allows Sequicity to concatenate Bt−1, Rt−1 and Ut (denoted as Bt−1Rt−1Ut) as a source sequence for seq2seq modeling, to generate Bt and Rt as target output sequences at each turn. More formally, we represent the dialogue utterances as {(B0R0U1; B1R1); (B1R1U2; B2R2); ...; (Bt−1Rt−1Ut; BtRt)} where B0 and R0 are initialized as empty sequences. In this way, Sequicity fulfills both task accomplishment and response generation in a unified seq2seq model. Note that we process Bt and Rt separately, as the belief state Bt depends only on Bt−1Rt−1Ut, while the response Rt is additionally conditioned on Bt and the knowledge base search results (denoted as kt); that is, Bt informs the Rt’s contents. For example, Rt must include all the request slots from Bt when communicating the entities fulfilling the requests found in the knowledge base. Here, kt helps generate Rt pragmatically.

Turn   Dialogue

<Inf> Italian ; cheap </Inf>  NAME_SLOT is a cheap restaurant  Tell me the address and the phone number please.

User₁: Can I have some Italian food please? <Req></Req> serving western food

Mach <Inf> Italian </Inf><Req> </Req>

ine₁: What price range are you looking for?

User₂: I want cheap ones.

Mach <Inf> Italian ; cheap </Inf>

ine₂: <Req></Req>

NAME_SLOT is a cheap restaurant serving western food

User₃: Tell me the address and the phone number please.

Mach <Inf> Italian ; cheap </Inf> <Inf> Italian ; cheap </Inf> <Req>address ; The address is ADDRESS_SLOT and the phone number is phone</Req>

ine₃: <Req>address ; phone</Req> The address is ADDRESS_SLOT and the phone number is PHONE_SLOT

Knowledge Base

Figure 1: Sequicity overview. The left shows a sample dialogue; the right illustrates the Sequicity. Bt is employed only by the model, and not visible to users. During training, we substitute slot values with placeholders bearing the slot names for machine response. During testing, this is inverted: the placeholders are replaced by actual slot values, according to the item selected from the knowledge base.

Generally, kt has three possibilities: 1) multiple matches, 2) exact match and 3) no match, while the machine responses differ accordingly. As an example, let’s say a user requests an Italian restaurant. In the scenario of multiple matches, the system should prompt for additional constraints for disambiguation (such as restaurant price range). In the second exact match scenario where a single target (i.e., restaurant) has been found, the system should inform the user their requested information (e.g., restaurant address). If no entity is obtained, the system should inform the user and perhaps generate a cooperative response to retry a different constraint.

# 4.3 Sequicity Instantiation: A Two Stage CopyNet

Although there are many possible instantiations, in this work we purposefully choose a simplistic architecture, leaving more sophisticated modeling for future work. We term our instantiated model a Two Stage CopyNet (TSCP). We denote the first m′ tokens of target sequence Y are Bt and the rests are Rt, i.e. Bt = y1...ym′ and Rt = ym′+1...ym.

In the first stage, the seq2seq model decodes Bt unconditionally (Eq. 4a). Once Bt obtained, the decoding pauses to perform the requisite knowledge base search based on Bt, resulting in kt. Afterwards, the seq2seq model continues to the second decoding stage, where Rt is generated on the additional conditions of Bt and kt (Eq. 4b).

Bt = seq2seq(Bt−1Rt−1Ut|0, 0) (4a)

Rt = seq2seq(Bt−1Rt−1Ut|Bt, kt) (4b)

Sequicity is a general framework suitably implemented by any of the various seq2seq models. The additional modeling effort beyond a general structure learns a language model. To decode yj, we can employ a softmax (e.g., Eq. 3) to calculate the probability distribution over V i.e., Pg(v) where v ∈ V, and then choose the token with the highest generation probability. However, in our case, tokens in the target sequence Y might

be exactly copied from the input X (e.g., “Ital- In contrast to recent work (Eric and Manning, 2017a) that also employs a copy-attention mech- anism to generate a knowledge-base search API and machine responses, our proposed method ad- vances in two aspects: on one hand, bspans reduce the search space from U1R1...UtRt to Bt−1Rt−1Ut by compressing key points for the task completion given past dialogues; on the other hand, because bspans revisit context by only han- dling the Bt with a fixed length, the time com- plexity of TSCP is only O(T), comparing O(T2) in (Eric and Manning, 2017a).

Involving kt when decoding Rt. As kt has three possible values: obtaining only one, multiple or no entities. We let kt be a vector of three dimensions, one of which signals a value. We append kt to the embeddings yj, as shown in Eq. (9) that is fed into an GRU for generating h(y)j+1. This approach is also referred to as Language Model Type condition (Wen et al., 2016b).

Pc(v) = 1 ∑ eψ(xi), j ≠ m′ (5) j Z i:xi=v

where Z is a normalization term and ψ(xi) is the score of “copying” word xi and is calculated by:

ψ(xi) = σ(h(x) Wc)h(y), j ≠ m′ (6) i j

where Wc ∈ Rd×d.

# 4.4 Training

The standard cross entropy is adopted as our ob- jective function to train a language model:

∑ yj logPj(yj) (10) j=1

In response generation, every token is treated equally. However, in our case, tokens for task completion are more important. For example, when a user asks for the address of a restaurant, it matters more to decode the placeholder &lt;address&gt; than decode words for language flu- ency. We can employ reinforcement learning to fine tune the trained response decoder with an em- phasis to decode those important tokens.

Inspired by (Wen et al., 2017a), in the context of reinforcement learning, the decoding network can be viewed as a policy network, denoted as πΘ(yj) for decoding yj (m′ + 1 ≤ j ≤ m). Accordingly, the choice of word yj is an action and its hidden vector generated by decoding GRU is the corresponding state. In reinforcement tuning stage, the trained response decoder is the initial policy network. By defining a proper reward func- tion r(j) for decoding yj, we can update the trained.

# Dataset

|Cam676|Size|Train:408 / Test: 136 / Dev: 136| |
|---|---|---|---|
|Domains|restaurant reservation| | |
|Slot types|price, food style etc.| | |
|Distinct slot values|99| | |
|KVRET|Size|Train:2425 / Test: 302 / Dev: 302| |
|Domains|calendar, weather info., POI| | |
|Slot types|date, etc., location, etc., poi, etc.| | |
|Distinct slot values|79|65|140|

Table 1: Dataset demographics. Following the respective literature, Cam676 is split 3:1:1 and KVRET is split 8:1:1, into training, developing and testing sets, respectively.

# Response model with policy gradient:

m
m   1   ′ ∑     r(j) ∂logπΘ(yj)                 (11)
− m         j=m′+1      ∂Θ

where  r(j)  =                 r(j) + λr(j+1) + λ2r(j+2) + ... + λm−j+1r(m). To encourage our generated response to answer the user requested information but avoid long-winded response, we set the reward at each step r(j) as follows: once the placeholder of requested slot has been decoded, the reward for current step is 1; otherwise, current step’s reward is -0.1. λ is a decay parameter. Sec 5.2 for λ settings.

# 5 Experiments

We assess the effectiveness of Sequicity in three aspects: the task completion, the language quality, and the efficiency. The evaluation metrics are listed as follows:

- BLEU to evaluate the language quality (Papineni et al., 2002) of generated responses (hence top-1 candidate in (Wen et al., 2017b)).
- Entity match rate evaluates task completion. According to (Wen et al., 2017b), it determines if a system can generate all correct constraints to search the indicated entities of the user. This metric is either 0 or 1 for each dialogue.
- Success F1 evaluates task completion and is modified from the success rate in (Wen et al., 2017b, 2016a, 2017a). The original success rate measures if the system answered all the requested information (e.g. address, phone number). However, this metric only evaluates recall. A system can easily achieve a perfect task success by always responding all possible request slots. Instead, we here use success F1 to balance both recall and precision. It is defined as the F1 score of requested slots answered in the current dialogue.
- Training time. The training time is important for iteration cycle of a model in industry settings.

# 5.1 Datasets

We adopt the CamRest676 (Wen et al., 2017a) and KVRET (Eric and Manning, 2017b) datasets. Both datasets are created by a Wizard-of-Oz (Kelley, 1984) method on Amazon Mechanical Turk platform, where a pair of workers are recruited to carry out a fluent conversation to complete an assigned task (e.g. restaurant reservation). During conversation, both informable and requestable slots are recorded by workers.

CamRest676’s dialogs are in the single domain of restaurant searching, while KVRET is broader, containing three domains: calendar scheduling, weather information retrieval and point of interest (POI) Navigation. Detailed slot information in each domain are shown in Table 1. We follow the data splits of the original papers as shown in 1.

# 5.2 Parameter Settings

For all models, the hidden size and the embedding size d is set to 50. |V| is 800 for CamRest676 and 1400 for KVRET. We train our model with an Adam optimizer (Kingma and Ba, 2015), with a learning rate of 0.003 for supervised training and 0.0001 for reinforcement learning. Early stopping is performed on developing set. In reinforcement learning, the decay parameter λ is set to 0.8. We also use beam search strategy for decoding, with a beam size of 10.

# 5.3 Baselines and Comparisons

We first compare our model with the state-of-the-art baselines as follow:

- NDM (Wen et al., 2017b). As described in Sec 1, it adopts pipeline designs with a belief tracker component depending on delexicalization.
- NDM+Att+SS. Based on the NDM model, an additional attention mechanism is performed on the belief trackers and a snapshot learning mechanism (Wen et al., 2016a) is adopted.
- LIDM (Wen et al., 2017a). Also based on NDM, this model adopts neural variational inference with reinforcement learning.

|Model|CamRes676|KVRET| | | | | | |
|---|---|---|---|---|---|---|---|---|
| |BLEU|Succ. F1|Timefull|TimeN.B.|BLEU|Succ. F1|Timefull|TimeN.B.|
|(1) NDM|0.904|0.212|91.9 min|8.6 min|0.724|0.186|285.5 min|29.3 min|
|(2) NDM + Att + SS|0.904|0.240|93.7 min|10.4 min|0.724|0.188|289.7 min|33.5 min|
|(3) LIDM|0.912|0.246|97.7 min|14.4 min|0.721|0.173|312.8 min|56.6 min|
|(4) KVRN|N/A|0.134|21.4 min|–|0.459|0.184|46.9 min|–|
|(5) TSCP|0.927|0.253|7.3 min|–|0.845|0.219|25.5 min|–|
|(6) Att-RNN|0.851|0.248|7.2 min|–|0.805|0.208|23.0 min|–|
|(7) TSCP\kt|0.927|0.232|7.2 min|–|0.845|0.168|25.3 min|–|
|(8) TSCP\RL|0.927|0.234|4.1 min|–|0.845|0.191|17.5 min|–|
|(9) TSCP\Bt|0.888|0.197|22.9 min|–|0.628|0.182|42.7 min|–|

Table 2: Model performance on CamRes676 and KVRET. This table is split into two parts: competitors on the upper side and our ablation study on the bottom side. Mat. and Succ. F1 are for match rate and success F1 respectively. Timefull column reports training time till converge. For NDM, NDM+Att+SS and LIDM, we also calculate the training time for the rest parts except for the belief tracker (TimeN.B.).

KVRN (Eric and Manning, 2017b) uses one of TSCP. It is because KVRET dataset has significant lexical variety, making it hard to perform delexicalization for Wen et al.’s model (Rows 1–3). However, CamRes676 is relatively small with simple patterns where all systems work well. As predicted, KVRN (Row 4) performs worse than TSCP (Row 5) due to lack of belief tracking.

Compared with Wen et al.’s models (Rows 1–3), TSCP takes a magnitude less time to train. Although TSCP is implemented in PyTorch while Wen et al.’s models in Theano, such speed comparison is still valid, as the rest of the NDM model has a comparable training speed to TSCP (7.3 mins vs. 8.6 mins on CamRes676 and 25.5 mins vs. 29.3 mins on KVRET).

TSCP\kt. We removed the conditioning on kt when decoding Rt.

TSCP\RL. We removed reinforcement learning which fine tunes the models for response generation. The bottleneck in the time expense is due to belief tracker training. In addition, Wen et al.’s models perform better at the cost of more training time (Rows 1, 2 and 3), suggesting the intrinsic complexity of pipeline designs.

Importantly, ablation studies validate the necessity of bspans. With bspans, even a standard seq2seq model (Att-RNN, Row 6) beats sophisticated models such as attention copyNets (TSCP\Bt, Row 9) in KVRET. Furthermore, TSCP (Row 5) outperforms TSCP\Bt (Row 9) in all aspects: task completion, language quality and training speed. This validates our theoretical analysis in Sec 4.3.

As shown in Table 2, TSCP outperforms all baselines (Row 5 vs. Rows 1–4) in task completion (entity match rate, success F1) and language quality (BLEU). The more significant performance of TSCP in KVRET dataset indicates the scalability.

We use the delexicalization lexicon provided by the original author of KVRET (Eric and Manning, 2017b).

# 5.6 Empirical Model Complexity

By examining error cases, we find that the system is likely to generate common sentences like “you are welcome” regardless of context, due to corpus frequency. Finally, reinforcement learning effectively helps both BLEU and success F1 although it takes acceptable additional time for training.

# 5.5 OOV Tests

Previous work predefines all slot values in a belief tracker. However, a user may request new attributes that have not been predefined as a classification label, which results in an entity mismatch. TSCP employs copy mechanisms, gaining an intrinsic potential to handle OOV cases. To conduct the OOV test, we synthesize OOV test instances by adding a suffix unk to existing slot fillers. For example, we change “I would like Chinese food” into “I would like Chinese unk food.” We then randomly make a proportion of testing data OOV and measure its entity match rate. For simplicity, we only show the three most representative models pre-trained in the in-vocabulary data: TSCP, TSCP\Bt and NDM.

| |TSCP|NDM|TSCP\Bt|
|---|---|---|---|
|Model Size (million)|79|144|284|

As shown in Figure 3, TSCP has a magnitude less number of parameters than NDM and its model size is much less sensitive to distinct slot values increasing. It is because TSCP is a seq2seq language model which has an approximate linear complexity to vocabulary size. However, NDM employs a belief tracker which dominates its model size. The belief tracker is sensitive to the increase of distinct slot values because it employs complex structures to model each slot and corresponding values. Here, we only perform empirical evaluation, leaving theoretically complexity analysis for future works.

# 5.7 Discussions

In this section we discuss if Sequicity can tackle inconsistent user requests, which happens when users change their minds during a dialogue. Inconsistent user requests happen frequently and are difficult to manage.

# 6 Conclusion

We propose Sequicity, an extendable framework, which tracks dialogue believes through the decoding of novel text spans: belief spans. Such belief spans enable a task-oriented dialogue system to be holistically optimized in a single seq2seq model. One simplistic instantiation of Sequicity, called Two Stage CopyNet (TSCP), demonstrates better effectiveness and scalability of Sequicity. Experiments show that TSCP outperforms the state-of-the-art baselines in both task accomplishment and language quality. Moreover, our TSCP implementation also betters traditional pipeline architectures by a magnitude in training time and adds the capability of handling OOV. Such properties are important for real-world customer service dialog systems where users’ inputs vary frequently and models need to be updated frequently. For our future work, we will consider advanced instantiations for Sequicity, and extend Sequicity to handle unsupervised cases where information and requested slots values are not annotated.

# Acknowledgments

We would like to thank the anonymous reviewers for their detailed comments and suggestions for this paper. This work is also supported by the National Research Foundation, Prime Ministers Office, Singapore under its IRC@SG Funding Initiative.

# References

Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Recent advances and new frontiers. arXiv preprint arXiv:1711.01731.

Heriberto Cuayahuitl, Simon Keizer, and Oliver Lemon. 2015. Strategic dialogue management via deep reinforcement learning. arXiv preprint arXiv:1511.08099.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017. Towards end-to-end reinforcement learning of dialogue agents for information access. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). volume 1, pages 484–495.

Mihail Eric and Christopher D Manning. 2017a. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue.

Mihail Eric and Christopher D Manning. 2017b. Key-value retrieval networks for task-oriented dialogue. SIGDIAL.

Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 2016. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. In NAACL.

Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. 2017. A convolutional encoder model for neural machine translation. ACL.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. ACL.

Homa B Hashemi, Amir Asiaee, and Reiner Kraft. 2016. Query intent detection using convolutional neural networks. In International Conference on Web Search and Data Mining, Workshop on Query Understanding.

Matthew Henderson, Blaise Thomson, and Jason D Williams. 2014a. The second dialog state tracking challenge. In SIGDIAL Conference. pages 263–272.

Matthew Henderson, Blaise Thomson, and Jason D Williams. 2014b. The third dialog state tracking challenge. In Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, pages 324–329.

Matthew Henderson, Blaise Thomson, and Steve Young. 2013. Deep neural network approach for the dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference. pages 467–471.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition. pages 3128–3137.

John F Kelley. 1984. An iterative design methodology for user-friendly natural language office information applications. ACM Transactions on Information Systems (TOIS) 2(1):26–41.

Seokhwan Kim, Luis Fernando DHaro, Rafael E Banchs, Jason D Williams, and Matthew Henderson. 2017. The fourth dialog state tracking challenge. In Dialogues with Social Robots, Springer, pages 435–449.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.

Sungjin Lee. 2013. Structured discriminative model for dialog state tracking. In SIGDIAL Conference. pages 442–451.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055.

Bing Liu and Ian Lane. 2017. Iterative policy learning in end-to-end trainable task-oriented neural dialog models. arXiv preprint arXiv:1709.06136.

# References

Jason Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2013. The dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference. pages 404–413.

Jason D Williams. 2012. A belief tracking challenge task for spoken dialog systems. In NAACL-HLT Workshop on future directions and needs in the spoken dialog community: tools and data. Association for Computational Linguistics, pages 23–24.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).

Steve Young, Milica Gašić, Simon Keizer, François Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for pomdp-based spoken dialogue management. Computer Speech & Language 24(2):150–174.

Steve Young, Milica Gašić, Blaise Thomson, and Jason D Williams. 2013a. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE 101(5):1160–1179.

Steve Young, Milica Gašić, Blaise Thomson, and Jason D Williams. 2013b. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE 101(5):1160–1179.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI. pages 2852–2858.

